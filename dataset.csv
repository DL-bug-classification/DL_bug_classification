Id,Text,Tags,Label,LabelNum
31556268,"How to use keras for XOR. I want to practice keras by code a xor, but the result is not right, the followed is my code, thanks for everybody to help me. from keras.models import Sequential from keras.layers.core import Dense,Activation from keras.optimizers import SGD import numpy as np model = Sequential()# two layers model.add(Dense(input_dim=2,output_dim=4,init=""glorot_uniform"")) model.add(Activation(""sigmoid"")) model.add(Dense(input_dim=4,output_dim=1,init=""glorot_uniform"")) model.add(Activation(""sigmoid"")) sgd = SGD(l2=0.0,lr=0.05, decay=1e-6, momentum=0.11, nesterov=True) model.compile(loss='mean_absolute_error', optimizer=sgd) print ""begin to train"" list1 = [1,1] label1 = [0] list2 = [1,0] label2 = [1] list3 = [0,0] label3 = [0] list4 = [0,1] label4 = [1] train_data = np.array((list1,list2,list3,list4)) #four samples for epoch = 1000 label = np.array((label1,label2,label3,label4)) model.fit(train_data,label,nb_epoch = 1000,batch_size = 4,verbose = 1,shuffle=True,show_accuracy = True) list_test = [0,1] test = np.array((list_test,list1)) classes = model.predict(test) print classes Output [[ 0.31851079] [ 0.34130159]] [[ 0.49635666] [0.51274764]]",|python|neural-network|xor|keras|,Training,2
31627380,"Trying Kaggle Titanic with keras .. getting loss and valid_loss -0.0000. Hi I am getting weird results for the following code for the problem posted here (https://www.kaggle.com/c/titanic) - from keras.models import Sequential from keras.layers.core import Dense, Activation, Dropout from keras.layers.advanced_activations import PReLU, LeakyReLU from keras.layers.recurrent import SimpleRNN, SimpleDeepRNN from keras.layers.embeddings import Embedding from keras.layers.recurrent import LSTM, GRU import pandas as pd import numpy as np from sklearn import preprocessing np.random.seed(1919) ### Constants ### data_folder = ""/home/saj1919/Public/Data_Science_Mining_Study/submissions/titanic/data/"" out_folder = ""/home/saj1919/Public/Data_Science_Mining_Study/submissions/titanic/output/"" batch_size = 4 nb_epoch = 10 ### load train and test ### train = pd.read_csv(data_folder+'train.csv', index_col=0) test = pd.read_csv(data_folder+'test.csv', index_col=0) print ""Data Read complete"" Y = train.Survived train.drop('Survived', axis=1, inplace=True) columns = train.columns test_ind = test.index train['Age'] = train['Age'].fillna(train['Age'].mean()) test['Age'] = test['Age'].fillna(test['Age'].mean()) train['Fare'] = train['Fare'].fillna(train['Fare'].mean()) test['Fare'] = test['Fare'].fillna(test['Fare'].mean()) category_index = [0,1,2,4,5,6,8,9] for i in category_index: print str(i)+"" : ""+columns[i] train[columns[i]] = train[columns[i]].fillna('missing') test[columns[i]] = test[columns[i]].fillna('missing') train = np.array(train) test = np.array(test) ### label encode the categorical variables ### for i in category_index: print str(i)+"" : ""+str(columns[i]) lbl = preprocessing.LabelEncoder() lbl.fit(list(train[:,i]) + list(test[:,i])) train[:,i] = lbl.transform(train[:,i]) test[:,i] = lbl.transform(test[:,i]) ### making data as numpy float ### train = train.astype(np.float32) test = test.astype(np.float32) #Y = np.array(Y).astype(np.int32) model = Sequential() model.add(Dense(len(columns), 512)) model.add(Activation('relu')) model.add(Dropout(0.5)) model.add(Dense(512, 1)) model.add(Activation('softmax')) model.compile(loss='categorical_crossentropy', optimizer=""adam"") model.fit(train, Y, nb_epoch=nb_epoch, batch_size=batch_size, validation_split=0.20) preds = model.predict(test,batch_size=batch_size) pred_arr = [] for pred in preds: pred_arr.append(pred[0]) ### Output Results ### preds = pd.DataFrame({""PassengerId"": test_ind, ""Survived"": pred_arr}) preds = preds.set_index('PassengerId') preds.to_csv(out_folder+'test.csv') I am getting following results : Train on 712 samples, validate on 179 samples Epoch 0 712/712 [==============================] - 0s - loss: -0.0000 - val_loss: -0.0000 Epoch 1 712/712 [==============================] - 0s - loss: -0.0000 - val_loss: -0.0000 Epoch 2 712/712 [==============================] - 0s - loss: -0.0000 - val_loss: -0.0000 Epoch 3 712/712 [==============================] - 0s - loss: -0.0000 - val_loss: -0.0000 Epoch 4 712/712 [==============================] - 0s - loss: -0.0000 - val_loss: -0.0000 Epoch 5 712/712 [==============================] - 0s - loss: -0.0000 - val_loss: -0.0000 Epoch 6 712/712 [==============================] - 0s - loss: -0.0000 - val_loss: -0.0000 Epoch 7 712/712 [==============================] - 0s - loss: -0.0000 - val_loss: -0.0000 Epoch 8 712/712 [==============================] - 0s - loss: -0.0000 - val_loss: -0.0000 Epoch 9 712/712 [==============================] - 0s - loss: -0.0000 - val_loss: -0.0000 I am trying to create a simple 3 layer network. Totally basic code. I have tried these kind of classification problems before using keras on kaggle. But this time getting this error. Is it overfitting due to less data. What I am missing ? Can someone help ?",|python|keras|kaggle|,Model,0
31880720,"How to prepare a dataset for Keras?. Motivation To run a set of labeled vectors through Keras neural network. Example Looking at Keras dataset example mnist: keras.datasets import mnist (x_tr, y_tr), (x_te, y_te) = mnist.load_data() print x_tr.shape It seem to be a 3 dimensional numpy array: (60000, 28, 28) 1st dimension is for the samples 2nd and 3rd for each sample features Attempt Building the labeled vectors: X_train = numpy.array([[1] * 128] * (10 ** 4) + [[0] * 128] * (10 ** 4)) X_test = numpy.array([[1] * 128] * (10 ** 2) + [[0] * 128] * (10 ** 2)) Y_train = numpy.array([True] * (10 ** 4) + [False] * (10 ** 4)) Y_test = numpy.array([True] * (10 ** 2) + [False] * (10 ** 2)) X_train = X_train.astype(""float32"") X_test = X_test.astype(""float32"") Y_train = Y_train.astype(""bool"") Y_test = Y_test.astype(""bool"") The training code model = Sequential() model.add(Dense(128, 50)) model.add(Activation('relu')) model.add(Dropout(0.2)) model.add(Dense(50, 50)) model.add(Activation('relu')) model.add(Dropout(0.2)) model.add(Dense(50, 1)) model.add(Activation('softmax')) rms = RMSprop() model.compile(loss='binary_crossentropy', optimizer=rms) model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=2, validation_data=(X_test, Y_test)) score = model.evaluate(X_test, Y_test, show_accuracy=True, verbose=0) print('Test score:', score[0]) print('Test accuracy:', score[1]) Result Test score: 13.9705320154 Test accuracy: 1.0 Why do I get such a bad result for such a simple dataset? Is my dataset malformed? Thanks!",|python|machine-learning|keras|,Model,0
33702251,"tensorflow loss minimization type error. I have a loss function implemented in TensorFlow that computes mean squared error. All tensors being used to compute the objective are of type float64 and therefore the loss function itself is of dtype float64. In particular, print cost ==> Tensor(""add_5:0"", shape=TensorShape([]), dtype=float64) However, when I attempt to minimize I obtain a value error with respect to type of the tensor: GradientDescentOptimizer(learning_rate=0.1).minimize(cost) ==> ValueError: Invalid type <dtype: 'float64'> for add_5:0, expected: [tf.float32]. I don't understand why the expected dtype of the tensor is a single precision float when all variables leading up to the computation are of type float64. I have confirmed that when I coerce all variables to be float32 the computation executes correctly. Does anyone have any insight as to why this could be happening? My computer is a 64bit machine. Here is an example that reproduces the behavior import tensorflow as tf import numpy as np # Make 100 phony data points in NumPy. x_data = np.random.rand(2, 100) # Random input y_data = np.dot([0.100, 0.200], x_data) + 0.300 # Construct a linear model. b = tf.Variable(tf.zeros([1], dtype=np.float64)) W = tf.Variable(tf.random_uniform([1, 2], minval=-1.0, maxval=1.0, dtype=np.float64)) y = tf.matmul(W, x_data) + b # Minimize the squared errors. loss = tf.reduce_mean(tf.square(y - y_data)) optimizer = tf.train.GradientDescentOptimizer(0.5) train = optimizer.minimize(loss) # For initializing the variables. init = tf.initialize_all_variables() # Launch the graph sess = tf.Session() sess.run(init) # Fit the plane. for step in xrange(0, 201): sess.run(train) if step % 20 == 0: print step, sess.run(W), sess.run(b)",|tensorflow|,API,4
33712178,"Tensorflow NaN bug?. I'm using TensorFlow and I modified the tutorial example to take my RGB images. The algorithm works flawlessly out of the box on the new image set, until suddenly (still converging, it's around 92% accuracy usually), it crashes with the error that ReluGrad received non-finite values. Debugging shows that nothing unusual happens with the numbers until very suddenly, for unknown reason, the error is thrown. Adding print ""max W vales: %g %g %g %g""%(tf.reduce_max(tf.abs(W_conv1)).eval(),tf.reduce_max(tf.abs(W_conv2)).eval(),tf.reduce_max(tf.abs(W_fc1)).eval(),tf.reduce_max(tf.abs(W_fc2)).eval()) print ""max b vales: %g %g %g %g""%(tf.reduce_max(tf.abs(b_conv1)).eval(),tf.reduce_max(tf.abs(b_conv2)).eval(),tf.reduce_max(tf.abs(b_fc1)).eval(),tf.reduce_max(tf.abs(b_fc2)).eval()) as debug code to each loop, yields the following output: Step 8600 max W vales: 0.759422 0.295087 0.344725 0.583884 max b vales: 0.110509 0.111748 0.115327 0.124324 Step 8601 max W vales: 0.75947 0.295084 0.344723 0.583893 max b vales: 0.110516 0.111753 0.115322 0.124332 Step 8602 max W vales: 0.759521 0.295101 0.34472 0.5839 max b vales: 0.110521 0.111747 0.115312 0.124365 Step 8603 max W vales: -3.40282e+38 -3.40282e+38 -3.40282e+38 -3.40282e+38 max b vales: -3.40282e+38 -3.40282e+38 -3.40282e+38 -3.40282e+38 Since none of my values is very high, the only way a NaN can happen is by a badly handled 0/0, but since this tutorial code doesn't do any divisions or similar operations, I see no other explanation than that this comes from the internal TF code. I'm clueless on what to do with this. Any suggestions? The algorithm is converging nicely, its accuracy on my validation set was steadily climbing and just reached 92.5% at iteration 8600.",|nan|tensorflow|,Training,2
33711427,"TensorFlow initializing Tensor of ones. So suppose I have a tensor X = tf.placeholder(""float"", [None, 5]) So that I know the number of columns but not the number of rows. I need to initialize a vector of ones of dimension nrows x 1 Now the following block of code does not work, o = tf.ones(shape=(tf.shape(X)[0], 1)) ==> TypeError: List of Tensors when single Tensor expected Nor does, o = tf.ones(shape=(X.get_shape()[0].value, 1)) ==> TypeError: Input 'dims' of 'Fill' Op has type string that does not match expected type of int32. Now, I have found that one way to get around this is to actually make my vector of ones a placeholder, o = tf.placeholder(dtype=tf.float32, shape=[None, 1]) And to pass in a numpy array of ones of appropriate size in my feed_dict. But this solution strikes me as inelegant and not the intended use of a placeholder. I could be wrong here, but surely there's a better way.",|python|tensorflow|,Tensors&Inputs,1
33859864,"How to create custom objective function in Keras?. There are many objective functions in Keras here. But how can you create your own objective function, I tried to create a very basic objective function but it gives an error and I there is no way to know the size of the parameters passed to the function at run time. def loss(y_true,y_pred): loss = T.vector('float64') for i in range(1): flag = True for j in range(y_true.ndim): if(y_true[i][j] == y_pred[i][j]): flag = False if(flag): loss = loss + 1.0 loss /= y_true.shape[0] print loss.type print y_true.shape[0] return loss I am getting 2 contradicting errors, model.compile(loss=loss, optimizer=ada) File ""/usr/local/lib/python2.7/dist-packages/Keras-0.0.1-py2.7.egg/keras/models.py"", line 75, in compile updates = self.optimizer.get_updates(self.params, self.regularizers, self.constraints, train_loss) File ""/usr/local/lib/python2.7/dist-packages/Keras-0.0.1-py2.7.egg/keras/optimizers.py"", line 113, in get_updates grads = self.get_gradients(cost, params, regularizers) File ""/usr/local/lib/python2.7/dist-packages/Keras-0.0.1-py2.7.egg/keras/optimizers.py"", line 23, in get_gradients grads = T.grad(cost, params) File ""/usr/local/lib/python2.7/dist-packages/theano/gradient.py"", line 432, in grad raise TypeError(""cost must be a scalar."") TypeError: cost must be a scalar. It says cost or loss returned in the function must be a scalar but if I change the line 2 from loss = T.vector('float64') to loss = T.scalar('float64') it shows this error model.compile(loss=loss, optimizer=ada) File ""/usr/local/lib/python2.7/dist-packages/Keras-0.0.1-py2.7.egg/keras/models.py"", line 75, in compile updates = self.optimizer.get_updates(self.params, self.regularizers, self.constraints, train_loss) File ""/usr/local/lib/python2.7/dist-packages/Keras-0.0.1-py2.7.egg/keras/optimizers.py"", line 113, in get_updates grads = self.get_gradients(cost, params, regularizers) File ""/usr/local/lib/python2.7/dist-packages/Keras-0.0.1-py2.7.egg/keras/optimizers.py"", line 23, in get_gradients grads = T.grad(cost, params) File ""/usr/local/lib/python2.7/dist-packages/theano/gradient.py"", line 529, in grad handle_disconnected(elem) File ""/usr/local/lib/python2.7/dist-packages/theano/gradient.py"", line 516, in handle_disconnected raise DisconnectedInputError(message) theano.gradient.DisconnectedInputError: grad method was asked to compute the gradient with respect to a variable that is not part of the computational graph of the cost, or is used only by a non-differentiable operator: <TensorType(float64, matrix)>",|python|keras|,Training,2
33969059,"Trying to get simple Keras neural net example to work. I've been fooling-around trying to get simple examples that I create working, because I find the examples given with large complicated datasets to be hard to grasp intuitively. The program below takes a list of weights [x_0 x_1 ... x_n] and uses them to create a random scattering of points on a plane with some random noise added in. I then train the simple neural nets on this data and check the results. When I do this with the Graph models everything works perfectly, the loss score goes down to zero predictably as the model converges on the weights given. However, when I try to use a sequential model, nothing happens. Code below If you want I can post my other script that uses the Graph instead of sequential and show that it finds the input weights perfectly. #!/usr/bin/env python from keras.models import Sequential, Graph from keras.layers.core import Dense, Dropout, Activation from keras.optimizers import SGD import numpy as np import theano, sys NUM_TRAIN = 100000 NUM_TEST = 10000 INDIM = 3 mn = 1 def myrand(a, b) : return (b)*(np.random.random_sample()-0.5)+a def get_data(count, ws, xno, bounds=100, rweight=0.0) : xt = np.random.rand(count, len(ws)) xt = np.multiply(bounds, xt) yt = np.random.rand(count, 1) ws = np.array(ws, dtype=np.float) xno = np.array([float(xno) + rweight*myrand(-mn, mn) for x in xt], dtype=np.float) yt = np.dot(xt, ws) yt = np.add(yt, xno) return (xt, yt) if __name__ == '__main__' : if len(sys.argv) > 1 : EPOCHS = int(sys.argv[1]) XNO = float(sys.argv[2]) WS = [float(x) for x in sys.argv[3:]] mx = max([abs(x) for x in (WS+[XNO])]) mn = min([abs(x) for x in (WS+[XNO])]) mn = min(1, mn) WS = [float(x)/mx for x in WS] XNO = float(XNO)/mx INDIM = len(WS) else : INDIM = 3 WS = [2.0, 1.0, 0.5] XNO = 2.2 X_test, y_test = get_data(10000, WS, XNO, 10000, rweight=0.4) X_train, y_train = get_data(100000, WS, XNO, 10000) model = Sequential() model.add(Dense(INDIM, input_dim=INDIM, init='uniform', activation='tanh')) model.add(Dropout(0.5)) model.add(Dense(2, init='uniform', activation='tanh')) model.add(Dropout(0.5)) model.add(Dense(1, init='uniform', activation='softmax')) sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True) model.compile(loss='mean_squared_error', optimizer=sgd) model.fit(X_train, y_train, shuffle=""batch"", show_accuracy=True, nb_epoch=EPOCHS) score, acc = model.evaluate(X_test, y_test, batch_size=16, show_accuracy=True) print score print acc predict_data = np.random.rand(100*100, INDIM) predictions = model.predict(predict_data) for x in range(len(predict_data)) : print ""%s --> %s"" % (str(predict_data[x]), str(predictions[x])) The output is as follows $ ./keras_hello.py 20 10 5 4 3 2 1 Using gpu device 0: GeForce GTX 970 (CNMeM is disabled) Epoch 1/20 100000/100000 [==============================] - 0s - loss: 60726734.3061 - acc: 1.0000 Epoch 2/20 100000/100000 [==============================] - 0s - loss: 60726734.3061 - acc: 1.0000 Epoch 3/20 100000/100000 [==============================] - 0s - loss: 60726734.3061 - acc: 1.0000 Epoch 4/20 100000/100000 [==============================] - 0s - loss: 60726734.3061 - acc: 1.0000 Epoch 5/20 100000/100000 [==============================] - 0s - loss: 60726734.3061 - acc: 1.0000 Epoch 6/20 100000/100000 [==============================] - 0s - loss: 60726734.3061 - acc: 1.0000 Epoch 7/20 100000/100000 [==============================] - 0s - loss: 60726734.3061 - acc: 1.0000 Epoch 8/20 100000/100000 [==============================] - 0s - loss: 60726734.3061 - acc: 1.0000 Epoch 9/20 100000/100000 [==============================] - 0s - loss: 60726734.3061 - acc: 1.0000 Epoch 10/20 100000/100000 [==============================] - 0s - loss: 60726734.3061 - acc: 1.0000 Epoch 11/20 100000/100000 [==============================] - 0s - loss: 60726734.3061 - acc: 1.0000 Epoch 12/20 100000/100000 [==============================] - 0s - loss: 60726734.3061 - acc: 1.0000 Epoch 13/20 100000/100000 [==============================] - 0s - loss: 60726734.3061 - acc: 1.0000 Epoch 14/20 100000/100000 [==============================] - 0s - loss: 60726734.3061 - acc: 1.0000 Epoch 15/20 100000/100000 [==============================] - 0s - loss: 60726734.3061 - acc: 1.0000 Epoch 16/20 100000/100000 [==============================] - 0s - loss: 60726734.3061 - acc: 1.0000 Epoch 17/20 100000/100000 [==============================] - 0s - loss: 60726734.3061 - acc: 1.0000 Epoch 18/20 100000/100000 [==============================] - 0s - loss: 60726734.3061 - acc: 1.0000 Epoch 19/20 100000/100000 [==============================] - 0s - loss: 60726734.3061 - acc: 1.0000 Epoch 20/20 100000/100000 [==============================] - 0s - loss: 60726734.3061 - acc: 1.0000 10000/10000 [==============================] - 0s 60247198.6661 1.0 [ 0.06698217 0.70033048 0.4317502 0.78504855 0.26173543] --> [ 1.] [ 0.28940025 0.21746189 0.93097653 0.94885535 0.56790348] --> [ 1.] [ 0.69430499 0.1622601 0.22802859 0.75709315 0.88948355] --> [ 1.] [ 0.90714721 0.99918648 0.31404901 0.83920051 0.84081288] --> [ 1.] [ 0.02214092 0.03132355 0.14417082 0.33901317 0.91491426] --> [ 1.] [ 0.31426055 0.80830795 0.46686523 0.58353359 0.50000842] --> [ 1.] [ 0.27649579 0.77914451 0.33572287 0.08703303 0.50865592] --> [ 1.] [ 0.99280349 0.24028343 0.05556034 0.31411902 0.41912574] --> [ 1.] [ 0.91897031 0.96840695 0.23561379 0.16005505 0.06567748] --> [ 1.] [ 0.27392867 0.44021533 0.44129147 0.40658522 0.47582736] --> [ 1.] [ 0.82063221 0.95182938 0.64210378 0.69578691 0.2946907 ] --> [ 1.] [ 0.12672415 0.35700418 0.89303047 0.80726545 0.79870725] --> [ 1.] [ 0.6662085 0.41358115 0.76637022 0.82093095 0.76973305] --> [ 1.] [ 0.96201937 0.29706843 0.22856618 0.59924945 0.05653825] --> [ 1.] [ 0.34120276 0.71866377 0.18758929 0.52424856 0.64061623] --> [ 1.] [ 0.25471237 0.35001821 0.63248632 0.45442404 0.96967989] --> [ 1.] [ 0.79390087 0.00100834 0.49645204 0.55574269 0.33487764] --> [ 1.] [ 0.41330261 0.38061826 0.33766183 0.23133121 0.80999653] --> [ 1.] [ 0.49603561 0.33414841 0.10180184 0.9227252 0.35073833] --> [ 1.] [ 0.17960345 0.05259438 0.565135 0.40465603 0.91518233] --> [ 1.] [ 0.36129943 0.903603 0.63047644 0.96553285 0.94006713] --> [ 1.] [ 0.7150973 0.93945141 0.31802763 0.15849441 0.92902078] --> [ 1.] [ 0.23730571 0.65360248 0.68776259 0.79697206 0.86814652] --> [ 1.] [ 0.47414382 0.75421265 0.32531333 0.43218305 0.4680773 ] --> [ 1.] [ 0.4887811 0.66130135 0.79913557 0.68948405 0.48376372] --> [ 1.] ....",|python|neural-network|theano|keras|,Model,0
34311586,"Cannot train a neural network solving XOR mapping. I am trying to implement a simple classifier for the XOR problem in Keras. Here is the code: from keras.models import Sequential from keras.layers.core import Dense, Dropout, Activation from keras.optimizers import SGD import numpy X = numpy.array([[1., 1.], [0., 0.], [1., 0.], [0., 1.], [1., 1.], [0., 0.]]) y = numpy.array([[0.], [0.], [1.], [1.], [0.], [0.]]) model = Sequential() model.add(Dense(2, input_dim=2, init='uniform', activation='sigmoid')) model.add(Dense(3, init='uniform', activation='sigmoid')) model.add(Dense(1, init='uniform', activation='softmax')) sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True) model.compile(loss='mean_squared_error', optimizer=sgd) model.fit(X, y, nb_epoch=20) print() score = model.evaluate(X, y) print() print(score) print(model.predict(numpy.array([[1, 0]]))) print(model.predict(numpy.array([[0, 0]]))) I tried changing the number of epochs, learning rate, and other parameters. But the error remains constant from the first to the last epoch. Epoch 13/20 6/6 [==============================] - 0s - loss: 0.6667 Epoch 14/20 6/6 [==============================] - 0s - loss: 0.6667 Epoch 15/20 6/6 [==============================] - 0s - loss: 0.6667 Epoch 16/20 6/6 [==============================] - 0s - loss: 0.6667 Epoch 17/20 6/6 [==============================] - 0s - loss: 0.6667 Epoch 18/20 6/6 [==============================] - 0s - loss: 0.6667 Epoch 19/20 6/6 [==============================] - 0s - loss: 0.6667 Epoch 20/20 6/6 [==============================] - 0s - loss: 0.6667 6/6 [==============================] - 0s 0.666666686535 [[ 1.]] [[ 1.]] How do you train this network in Keras? Also, is there a better library for implementing neural networks? I tried PyBrain, but it has been abandoned, tried scikit-neuralnetwork but the documentation is really cryptic so couldn't figure out how to train it. And I seriously doubt if Keras even works.",|python|neural-network|keras|,Model,0
34328233,"Feeding parameters into placeholders in tensorflow. I'm trying to get into tensorflow, setting up a network and then feeding data to it. For some reason I end up with the error message ValueError: setting an array element with a sequence. I made a minimal example of what I'm trying to do: import tensorflow as tf K = 10 lchild = tf.placeholder(tf.float32, shape=(K)) rchild = tf.placeholder(tf.float32, shape=(K)) parent = tf.nn.tanh(tf.add(lchild, rchild)) input = [ tf.Variable(tf.random_normal([K])), tf.Variable(tf.random_normal([K])) ] with tf.Session() as sess : print(sess.run([parent], feed_dict={ lchild: input[0], rchild: input[1] })) Basically, I'm setting up a network with place holders and a sequence of input embeddings that I want to learn, and then I try to run the network, feeding the input embeddings into it. From what I can tell by searching for the error message, there might be something wrong with my feed_dict, but I can't see any obvious mismatches in eg. dimensionality. So, what did I miss, or how did I get this completely backwards? EDIT: I've edited the above to clarify that the input represents embeddings that need to be learned. I guess the question can be asked more sharply as: Is it possible to use placeholders for parameters?",|python|tensorflow|,API,4
34673164,"How to train and tune an artificial multilayer perceptron neural network using Keras?. I am building my first artificial multilayer perceptron neural network using Keras. This is my input data: This is my code which I used to build my initial model which basically follows the Keras example code: model = Sequential() model.add(Dense(64, input_dim=14, init='uniform')) model.add(Activation('tanh')) model.add(Dropout(0.5)) model.add(Dense(64, init='uniform')) model.add(Activation('tanh')) model.add(Dropout(0.5)) model.add(Dense(2, init='uniform')) model.add(Activation('softmax')) sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True) model.compile(loss='mean_squared_error', optimizer=sgd) model.fit(X_train, y_train, nb_epoch=20, batch_size=16) Output: Epoch 1/20 1213/1213 [==============================] - 0s - loss: 0.1760 Epoch 2/20 1213/1213 [==============================] - 0s - loss: 0.1840 Epoch 3/20 1213/1213 [==============================] - 0s - loss: 0.1816 Epoch 4/20 1213/1213 [==============================] - 0s - loss: 0.1915 Epoch 5/20 1213/1213 [==============================] - 0s - loss: 0.1928 Epoch 6/20 1213/1213 [==============================] - 0s - loss: 0.1964 Epoch 7/20 1213/1213 [==============================] - 0s - loss: 0.1948 Epoch 8/20 1213/1213 [==============================] - 0s - loss: 0.1971 Epoch 9/20 1213/1213 [==============================] - 0s - loss: 0.1899 Epoch 10/20 1213/1213 [==============================] - 0s - loss: 0.1957 Epoch 11/20 1213/1213 [==============================] - 0s - loss: 0.1923 Epoch 12/20 1213/1213 [==============================] - 0s - loss: 0.1910 Epoch 13/20 1213/1213 [==============================] - 0s - loss: 0.2104 Epoch 14/20 1213/1213 [==============================] - 0s - loss: 0.1976 Epoch 15/20 1213/1213 [==============================] - 0s - loss: 0.1979 Epoch 16/20 1213/1213 [==============================] - 0s - loss: 0.2036 Epoch 17/20 1213/1213 [==============================] - 0s - loss: 0.2019 Epoch 18/20 1213/1213 [==============================] - 0s - loss: 0.1978 Epoch 19/20 1213/1213 [==============================] - 0s - loss: 0.1954 Epoch 20/20 1213/1213 [==============================] - 0s - loss: 0.1949 How do I train and tune this model and get my code to output my best predictive model? I am new to neural networks and am just wholly confused as to what is the next step after building the model. I know I want to optimize it, but I'm not sure which features to tweak or if I am supposed to do it manually or how to write code to do so.",|python|machine-learning|neural-network|keras|,Training,2
34775522,"Tensorflow multiple sessions with multiple GPUs. I have a workstation with 2 GPUs and I am trying to run multiple tensorflow jobs at the same time, so I can train more than one model at once, etc. For example, I've tried to separate the sessions into different resources via the python API using in script1.py: with tf.device(""/gpu:0""): # do stuff in script2.py: with tf.device(""/gpu:1""): # do stuff in script3.py with tf.device(""/cpu:0""): # do stuff If I run each script by itself I can see that it is using the specified device. (Also the models fit very well into a single GPU and doesn't use another one even if both are available.) However, if one script is running and I try to run another, I always get this error: I tensorflow/core/common_runtime/local_device.cc:40] Local device intra op parallelism threads: 8 I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:909] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero I tensorflow/core/common_runtime/gpu/gpu_init.cc:103] Found device 0 with properties: name: GeForce GTX 980 major: 5 minor: 2 memoryClockRate (GHz) 1.2155 pciBusID 0000:01:00.0 Total memory: 4.00GiB Free memory: 187.65MiB I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:909] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero I tensorflow/core/common_runtime/gpu/gpu_init.cc:103] Found device 1 with properties: name: GeForce GTX 980 major: 5 minor: 2 memoryClockRate (GHz) 1.2155 pciBusID 0000:04:00.0 Total memory: 4.00GiB Free memory: 221.64MiB I tensorflow/core/common_runtime/gpu/gpu_init.cc:127] DMA: 0 1 I tensorflow/core/common_runtime/gpu/gpu_init.cc:137] 0: Y Y I tensorflow/core/common_runtime/gpu/gpu_init.cc:137] 1: Y Y I tensorflow/core/common_runtime/gpu/gpu_device.cc:702] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980, pci bus id: 0000:01:00.0) I tensorflow/core/common_runtime/gpu/gpu_device.cc:702] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX 980, pci bus id: 0000:04:00.0) I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Allocating 187.40MiB bytes. E tensorflow/stream_executor/cuda/cuda_driver.cc:932] failed to allocate 187.40M (196505600 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY F tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Check failed: gpu_mem != nullptr Could not allocate GPU device memory for device 0. Tried to allocate 187.40MiB Aborted (core dumped) It seems each tensorflow process is trying to grab all of the GPUs on the machine when it loads even if not all devices are going to be used to run the model. I see there is an option to limit the amount of GPU each process uses tf.GPUOptions(per_process_gpu_memory_fraction=0.5) ...I haven't tried it, but this seems like it would make two processes try to share 50% of each GPU instead of running each process on a separate GPU... Does anyone know how to configure tensorflow to use only one GPU and leave the other available for another tensorflow process?",|gpu|tensorflow|,GPU Usage,3
34900246,"Tensorflow: Passing a session to a python multiprocess. I'm using tensorflow to preprocess some large images. I was having a problem where the memory was rapidly collapsing. I turned to use multiprocessing in python so the memory would free up entirely whenever I want. The thing is, I'm using python's multiprocess queues and for some reason unknown I can't pass my tensorflow session from my parent process to the children. Using some advanced debugging techniques (i.e. printing something every few lines) I noticed that python just goes idle inside the line where I make use of the session, it doesn't throw an error message. My code looks something like this: def subprocess(some_image, sess, q): with sess.as_default(): # ... use sess and q ... print ""All good and well"" #This is printed some_image.eval() #Nothing happens here in console print ""Still all good and well"" #This is not printed if __name__ == '__main__': # ... some initial operations ... some_image = read_some_image() sess = tf.Session() q = Queue() q.put(something) p = Process(target=subprocess, args=(some_image, sess, q)) p.start() p.join() What could be the problem? Many thanks!",|python|python-2.7|python-multiprocessing|tensorflow|,API,4
35413618,"TensorFlow: PlaceHolder error when using tf.merge_all_summaries(). I am getting a placeholder error. I do not know what it means, because I am mapping correctly on sess.run(..., {_y: y, _X: X})... I provide here a fully functional MWE reproducing the error: import tensorflow as tf import numpy as np def init_weights(shape): return tf.Variable(tf.random_normal(shape, stddev=0.01)) class NeuralNet: def __init__(self, hidden): self.hidden = hidden def __del__(self): self.sess.close() def fit(self, X, y): _X = tf.placeholder('float', [None, None]) _y = tf.placeholder('float', [None, 1]) w0 = init_weights([X.shape[1], self.hidden]) b0 = tf.Variable(tf.zeros([self.hidden])) w1 = init_weights([self.hidden, 1]) b1 = tf.Variable(tf.zeros([1])) self.sess = tf.Session() self.sess.run(tf.initialize_all_variables()) h = tf.nn.sigmoid(tf.matmul(_X, w0) + b0) self.yp = tf.nn.sigmoid(tf.matmul(h, w1) + b1) C = tf.reduce_mean(tf.square(self.yp - y)) o = tf.train.GradientDescentOptimizer(0.5).minimize(C) correct = tf.equal(tf.argmax(_y, 1), tf.argmax(self.yp, 1)) accuracy = tf.reduce_mean(tf.cast(correct, ""float"")) tf.scalar_summary(""accuracy"", accuracy) tf.scalar_summary(""loss"", C) merged = tf.merge_all_summaries() import shutil shutil.rmtree('logs') writer = tf.train.SummaryWriter('logs', self.sess.graph_def) for i in xrange(1000+1): if i % 100 == 0: res = self.sess.run([o, merged], feed_dict={_X: X, _y: y}) else: self.sess.run(o, feed_dict={_X: X, _y: y}) return self def predict(self, X): yp = self.sess.run(self.yp, feed_dict={_X: X}) return (yp >= 0.5).astype(int) X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1]]) y = np.array([[0],[1],[1],[0]]]) m = NeuralNet(10) m.fit(X, y) yp = m.predict(X)[:, 0] print accuracy_score(y, yp) The error: I tensorflow/core/common_runtime/local_device.cc:40] Local device intra op parallelism threads: 8 I tensorflow/core/common_runtime/direct_session.cc:58] Direct session inter op parallelism threads: 8 0.847222222222 W tensorflow/core/common_runtime/executor.cc:1076] 0x2340f40 Compute status: Invalid argument: You must feed a value for placeholder tensor 'Placeholder_1' with dtype float [[Node: Placeholder_1 = Placeholder[dtype=DT_FLOAT, shape=[], _device=""/job:localhost/replica:0/task:0/cpu:0""]()]] W tensorflow/core/common_runtime/executor.cc:1076] 0x2340f40 Compute status: Invalid argument: You must feed a value for placeholder tensor 'Placeholder' with dtype float [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=[], _device=""/job:localhost/replica:0/task:0/cpu:0""]()]] Traceback (most recent call last): File ""neuralnet.py"", line 64, in <module> m.fit(X[tr], y[tr, np.newaxis]) File ""neuralnet.py"", line 44, in fit res = self.sess.run([o, merged], feed_dict={self._X: X, _y: y}) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 368, in run results = self._do_run(target_list, unique_fetch_targets, feed_dict_string) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 444, in _do_run e.code) tensorflow.python.framework.errors.InvalidArgumentError: You must feed a value for placeholder tensor 'Placeholder_1' with dtype float [[Node: Placeholder_1 = Placeholder[dtype=DT_FLOAT, shape=[], _device=""/job:localhost/replica:0/task:0/cpu:0""]()]] Caused by op u'Placeholder_1', defined at: File ""neuralnet.py"", line 64, in <module> m.fit(X[tr], y[tr, np.newaxis]) File ""neuralnet.py"", line 16, in fit _y = tf.placeholder('float', [None, 1]) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py"", line 673, in placeholder name=name) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py"", line 463, in _placeholder name=name) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py"", line 664, in apply_op op_def=op_def) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1834, in create_op original_op=self._default_original_op, op_def=op_def) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1043, in __init__ self._traceback = _extract_stack() If I remove the tf.merge_all_summaries() or remove merged from self.sess.run([o, merged], ...) then it runs okay. This looks similar to this post: Error when computing summaries in TensorFlow However, I am not using iPython...",|python|neural-network|tensorflow|,API,4
35968973,"Keras IndexError: indices are out-of-bounds. I'm new to Keras and im trying to do Binary MLP on a dataset, and keep getting indices out of bounds with no idea why. from keras.models import Sequential from keras.layers.core import Dense, Dropout, Activation from keras.optimizers import SGD model = Sequential() model.add(Dense(64, input_dim=20, init='uniform', activation='relu')) model.add(Dropout(0.5)) model.add(Dense(64, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(1, activation='sigmoid')) model.compile(loss='binary_crossentropy', optimizer='rmsprop') model.fit(trainx, trainy, nb_epoch=20, batch_size=16) # THROWS INDICES ERROR Error: model.fit(trainx, trainy, nb_epoch=20, batch_size=16) Epoch 1/20 Traceback (most recent call last): File ""<ipython-input-6-c81bd7606eb0>"", line 1, in <module> model.fit(trainx, trainy, nb_epoch=20, batch_size=16) File ""C:\Users\Thiru\Anaconda3\lib\site-packages\keras\models.py"", line 646, in fit shuffle=shuffle, metrics=metrics) File ""C:\Users\Thiru\Anaconda3\lib\site-packages\keras\models.py"", line 271, in _fit ins_batch = slice_X(ins, batch_ids) File ""C:\Users\Thiru\Anaconda3\lib\site-packages\keras\models.py"", line 65, in slice_X return [x[start] for x in X] File ""C:\Users\Thiru\Anaconda3\lib\site-packages\keras\models.py"", line 65, in <listcomp> return [x[start] for x in X] File ""C:\Users\Thiru\Anaconda3\lib\site-packages\pandas\core\frame.py"", line 1963, in __getitem__ return self._getitem_array(key) File ""C:\Users\Thiru\Anaconda3\lib\site-packages\pandas\core\frame.py"", line 2008, in _getitem_array return self.take(indexer, axis=1, convert=True) File ""C:\Users\Thiru\Anaconda3\lib\site-packages\pandas\core\generic.py"", line 1371, in take convert=True, verify=True) File ""C:\Users\Thiru\Anaconda3\lib\site-packages\pandas\core\internals.py"", line 3619, in take indexer = maybe_convert_indices(indexer, n) File ""C:\Users\Thiru\Anaconda3\lib\site-packages\pandas\core\indexing.py"", line 1750, in maybe_convert_indices raise IndexError(""indices are out-of-bounds"") IndexError: indices are out-of-bounds Does anyone have any idea why this is happening? Im able to run other models just fine",|neural-network|theano|keras|,API,4
36007883,"TensorFlow: ""Attempting to use uninitialized value"" in variable initialization. I am trying to implement multivariate linear regression in Python using TensorFlow, but have run into some logical and implementation issues. My code throws the following error: Attempting to use uninitialized value Variable Caused by op u'Variable/read' Ideally the weights output should be [2, 3] def hypothesis_function(input_2d_matrix_trainingexamples, output_matrix_of_trainingexamples, initial_parameters_of_hypothesis_function, learning_rate, num_steps): # calculate num attributes and num examples number_of_attributes = len(input_2d_matrix_trainingexamples[0]) number_of_trainingexamples = len(input_2d_matrix_trainingexamples) #Graph inputs x = [] for i in range(0, number_of_attributes, 1): x.append(tf.placeholder(""float"")) y_input = tf.placeholder(""float"") # Create Model and Set Model weights parameters = [] for i in range(0, number_of_attributes, 1): parameters.append( tf.Variable(initial_parameters_of_hypothesis_function[i])) #Contruct linear model y = tf.Variable(parameters[0], ""float"") for i in range(1, number_of_attributes, 1): y = tf.add(y, tf.multiply(x[i], parameters[i])) # Minimize the mean squared errors loss = tf.reduce_mean(tf.square(y - y_input)) optimizer = tf.train.GradientDescentOptimizer(learning_rate) train = optimizer.minimize(loss) #Initialize the variables init = tf.initialize_all_variables() # launch the graph session = tf.Session() session.run(init) for step in range(1, num_steps + 1, 1): for i in range(0, number_of_trainingexamples, 1): feed = {} for j in range(0, number_of_attributes, 1): array = [input_2d_matrix_trainingexamples[i][j]] feed[j] = array array1 = [output_matrix_of_trainingexamples[i]] feed[number_of_attributes] = array1 session.run(train, feed_dict=feed) for i in range(0, number_of_attributes - 1, 1): print (session.run(parameters[i])) array = [[0.0, 1.0, 2.0], [0.0, 2.0, 3.0], [0.0, 4.0, 5.0]] hypothesis_function(array, [8.0, 13.0, 23.0], [1.0, 1.0, 1.0], 0.01, 200)",|python|machine-learning|linear-regression|tensorflow|,API,4
36136562,"Wrong number of dimensions on model.fit. I'm trying to run this SimpleRNN: model.add(SimpleRNN(init='uniform',output_dim=1,input_dim=len(pred_frame.columns))) model.compile(loss=""mse"", optimizer=""sgd"") model.fit(X=predictor_train, y=target_train, batch_size=len(pred_frame.index),show_accuracy=True) The error is on model.fit, as you can see below: File ""/Users/file.py"", line 1496, in Pred model.fit(X=predictor_train, y=target_train, batch_size=len(pred_frame.index),show_accuracy=True) File ""/Library/Python/2.7/site-packages/keras/models.py"", line 581, in fit shuffle=shuffle, metrics=metrics) File ""/Library/Python/2.7/site-packages/keras/models.py"", line 239, in _fit outs = f(ins_batch) File ""/Library/Python/2.7/site-packages/keras/backend/theano_backend.py"", line 365, in __call__ return self.function(*inputs) File ""/Library/Python/2.7/site-packages/theano/compile/function_module.py"", line 513, in __call__ allow_downcast=s.allow_downcast) File ""/Library/Python/2.7/site-packages/theano/tensor/type.py"", line 169, in filter data.shape)) TypeError: ('Bad input argument to theano function with name ""/Library/Python/2.7/site-packages/keras/backend/theano_backend.py:362"" at index 0(0-based)', 'Wrong number of dimensions: expected 3, got 2 with shape (88, 88).') The error is telling me it's got the wrong number of dimensions, it should be 3 and it has only got 2. What are the dimensions it is referring to?",|python|keras|recurrent-neural-network|,Tensors&Inputs,1
36565430,"Adding multiple layers to TensorFlow causes loss function to become Nan. I'm writing a neural-network classifier in TensorFlow/Python for the notMNIST dataset. I've implemented l2 regularization and dropout on the hidden layers. It works fine as long as there is only one hidden layer, but when I added more layers (to improve accuracy), the loss function increases rapidly at each step, becoming NaN by step 5. I tried temporarily disabling Dropout and L2 regularization, but I get the same behavior as long as there are 2+ layers. I even rewrote my code from scratch (doing some refactoring to make it more flexible), but with the same results. The number and size of layers is controlled by hidden_layer_spec. What am I missing? #works for np.array([1024]) with about 96.1% accuracy hidden_layer_spec = np.array([1024, 300]) num_hidden_layers = hidden_layer_spec.shape[0] batch_size = 256 beta = 0.0005 epochs = 100 stepsPerEpoch = float(train_dataset.shape[0]) / batch_size num_steps = int(math.ceil(float(epochs) * stepsPerEpoch)) l2Graph = tf.Graph() with l2Graph.as_default(): #with tf.device('/cpu:0'): # Input data. For the training data, we use a placeholder that will be fed # at run time with a training minibatch. tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size)) tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels)) tf_valid_dataset = tf.constant(valid_dataset) tf_test_dataset = tf.constant(test_dataset) weights = [] biases = [] for hi in range(0, num_hidden_layers + 1): width = image_size * image_size if hi == 0 else hidden_layer_spec[hi - 1] height = num_labels if hi == num_hidden_layers else hidden_layer_spec[hi] weights.append(tf.Variable(tf.truncated_normal([width, height]), name = ""w"" + `hi + 1`)) biases.append(tf.Variable(tf.zeros([height]), name = ""b"" + `hi + 1`)) print(`width` + 'x' + `height`) def logits(input, addDropoutLayer = False): previous_layer = input for hi in range(0, hidden_layer_spec.shape[0]): previous_layer = tf.nn.relu(tf.matmul(previous_layer, weights[hi]) + biases[hi]) if addDropoutLayer: previous_layer = tf.nn.dropout(previous_layer, 0.5) return tf.matmul(previous_layer, weights[num_hidden_layers]) + biases[num_hidden_layers] # Training computation. train_logits = logits(tf_train_dataset, True) l2 = tf.nn.l2_loss(weights[0]) for hi in range(1, len(weights)): l2 = l2 + tf.nn.l2_loss(weights[0]) loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(train_logits, tf_train_labels)) + beta * l2 # Optimizer. global_step = tf.Variable(0) # count the number of steps taken. learning_rate = tf.train.exponential_decay(0.5, global_step, int(stepsPerEpoch) * 2, 0.96, staircase = True) optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step) # Predictions for the training, validation, and test data. train_prediction = tf.nn.softmax(train_logits) valid_prediction = tf.nn.softmax(logits(tf_valid_dataset)) test_prediction = tf.nn.softmax(logits(tf_test_dataset)) saver = tf.train.Saver() with tf.Session(graph=l2Graph) as session: tf.initialize_all_variables().run() print(""Initialized"") for step in range(num_steps): # Pick an offset within the training data, which has been randomized. # Note: we could use better randomization across epochs. offset = (step * batch_size) % (train_labels.shape[0] - batch_size) # Generate a minibatch. batch_data = train_dataset[offset:(offset + batch_size), :] batch_labels = train_labels[offset:(offset + batch_size), :] # Prepare a dictionary telling the session where to feed the minibatch. # The key of the dictionary is the placeholder node of the graph to be fed, # and the value is the numpy array to feed to it. feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels} _, l, predictions = session.run( [optimizer, loss, train_prediction], feed_dict=feed_dict) if (step % 500 == 0): print(""Minibatch loss at step %d: %f"" % (step, l)) print(""Learning rate: "" % learning_rate) print(""Minibatch accuracy: %.1f%%"" % accuracy(predictions, batch_labels)) print(""Validation accuracy: %.1f%%"" % accuracy( valid_prediction.eval(), valid_labels)) print(""Test accuracy: %.1f%%"" % accuracy(test_prediction.eval(), test_labels)) save_path = saver.save(session, ""l2_degrade.ckpt"") print(""Model save to "" + `save_path`)",|python|neural-network|tensorflow|deep-learning|,Model,0
36599237,"Keras. ValueError: I/O operation on closed file. I use jupyter notebook with anaconda. I use kerast firstly, and i can't do tutorial. About this issues are two themes in stackoverflow, but solve not found. My code: model = Sequential() model.add(Dense(1, input_dim=1, activation='softmax')) model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy']) X_train_shape = X_train.reshape(len(X_train), 1) Y_train_shape = Y_train.reshape(len(Y_train), 1) model.fit(X_train, Y_train, nb_epoch=5, batch_size=32) And I have error, it's some random and sometimes one or two epoch competed: Epoch 1/5 4352/17500 [======>.......................] --------------------------------------------------------------------------- ValueError Traceback (most recent call last) in () 2 # of 32 samples 3 #sleep(0.1) ----> 4 model.fit(X_train, Y_train, nb_epoch=5, batch_size=32) 5 #sleep(0.1) C:\Anaconda3\envs\py27\lib\site-packages\keras\models.pyc in fit(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, **kwargs) 395 shuffle=shuffle, 396 class_weight=class_weight, --> 397 sample_weight=sample_weight) 398 399 def evaluate(self, x, y, batch_size=32, verbose=1, C:\Anaconda3\envs\py27\lib\site-packages\keras\engine\training.pyc in fit(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight) 1009 verbose=verbose, callbacks=callbacks, 1010 val_f=val_f, val_ins=val_ins, shuffle=shuffle, -> 1011 callback_metrics=callback_metrics) 1012 1013 def evaluate(self, x, y, batch_size=32, verbose=1, sample_weight=None): C:\Anaconda3\envs\py27\lib\site-packages\keras\engine\training.pyc in _fit_loop(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics) 753 batch_logs[l] = o 754 --> 755 callbacks.on_batch_end(batch_index, batch_logs) 756 757 epoch_logs = {} C:\Anaconda3\envs\py27\lib\site-packages\keras\callbacks.pyc in on_batch_end(self, batch, logs) 58 t_before_callbacks = time.time() 59 for callback in self.callbacks: ---> 60 callback.on_batch_end(batch, logs) 61 self._delta_ts_batch_end.append(time.time() - t_before_callbacks) 62 delta_t_median = np.median(self._delta_ts_batch_end) C:\Anaconda3\envs\py27\lib\site-packages\keras\callbacks.pyc in on_batch_end(self, batch, logs) 187 # will be handled by on_epoch_end 188 if self.verbose and self.seen < self.params['nb_sample']: --> 189 self.progbar.update(self.seen, self.log_values) 190 191 def on_epoch_end(self, epoch, logs={}): C:\Anaconda3\envs\py27\lib\site-packages\keras\utils\generic_utils.pyc in update(self, current, values) 110 info += ((prev_total_width - self.total_width) * "" "") 111 --> 112 sys.stdout.write(info) 113 sys.stdout.flush() 114 C:\Anaconda3\envs\py27\lib\site-packages\ipykernel\iostream.pyc in write(self, string) 315 316 is_child = (not self._is_master_process()) --> 317 self._buffer.write(string) 318 if is_child: 319 # newlines imply flush in subprocesses ValueError: I/O operation on closed file",|python|machine-learning|keras|,API,4
36854940,"TensorFlow GPU: is cudnn optional? Couldn't open CUDA library libcudnn.so. I installed the tensorflow-0.8.0 GPU version, tensorflow-0.8.0-cp27-none-linux_x86_64.whl. It says it requires CUDA toolkit 7.5 and CuDNN v4. # Ubuntu/Linux 64-bit, GPU enabled. Requires CUDA toolkit 7.5 and CuDNN v4. For # other versions, see ""Install from sources"" below. However, I accidently forget to install CuDNN v4, but it works OK besides the error message, ""Couldn't open CUDA library libcudnn.so"". But it works and says, ""Creating TensorFlow device (/gpu:0)"". msg without CuDNN I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally I tensorflow/stream_executor/dso_loader.cc:99] Couldn't open CUDA library libcudnn.so. LD_LIBRARY_PATH: /usr/local/cuda/lib64: I tensorflow/stream_executor/cuda/cuda_dnn.cc:1562] Unable to load cuDNN DSO I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally ('Extracting', 'MNIST_data/train-images-idx3-ubyte.gz') /usr/lib/python2.7/gzip.py:268: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future chunk = self.extrabuf[offset: offset + size] /home/ubuntu/TensorFlow-Tutorials/input_data.py:42: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future data = data.reshape(num_images, rows, cols, 1) ('Extracting', 'MNIST_data/train-labels-idx1-ubyte.gz') ('Extracting', 'MNIST_data/t10k-images-idx3-ubyte.gz') ('Extracting', 'MNIST_data/t10k-labels-idx1-ubyte.gz') I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:900] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: name: GRID K520 major: 3 minor: 0 memoryClockRate (GHz) 0.797 pciBusID 0000:00:03.0 Total memory: 4.00GiB Free memory: 3.95GiB I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0: Y I tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GRID K520, pci bus id: 0000:00:03.0) I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 1704 get requests, put_count=1321 evicted_count=1000 eviction_rate=0.757002 and unsatisfied allocation rate=0.870305 I tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 100 to 110 I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 1704 get requests, put_count=1812 evicted_count=1000 eviction_rate=0.551876 and unsatisfied allocation rate=0.536972 I tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 256 to 281 Later, I installed CuDNN, but I don't see the differences. msg with CuDNN I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally ('Extracting', 'MNIST_data/train-images-idx3-ubyte.gz') /usr/lib/python2.7/gzip.py:268: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future chunk = self.extrabuf[offset: offset + size] /home/ubuntu/TensorFlow-Tutorials/input_data.py:42: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future data = data.reshape(num_images, rows, cols, 1) ('Extracting', 'MNIST_data/train-labels-idx1-ubyte.gz') ('Extracting', 'MNIST_data/t10k-images-idx3-ubyte.gz') ('Extracting', 'MNIST_data/t10k-labels-idx1-ubyte.gz') I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:900] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: name: GRID K520 major: 3 minor: 0 memoryClockRate (GHz) 0.797 pciBusID 0000:00:03.0 Total memory: 4.00GiB Free memory: 3.95GiB I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0: Y I tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GRID K520, pci bus id: 0000:00:03.0) I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 1704 get requests, put_count=1321 evicted_count=1000 eviction_rate=0.757002 and unsatisfied allocation rate=0.870305 I tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 100 to 110 I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 1704 get requests, put_count=1811 evicted_count=1000 eviction_rate=0.552181 and unsatisfied allocation rate=0.537559 I tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 256 to 281 So what's differences with/without CuDNN?",|tensorflow|cuda|gpu|cudnn|,GPU Usage,3
37213388,"Keras accuracy does not change. I have a few thousand audio files and I want to classify them using Keras and Theano. So far, I generated a 28x28 spectrograms (bigger is probably better, but I am just trying to get the algorithm work at this point) of each audio file and read the image into a matrix. So in the end I get this big image matrix to feed into the network for image classification. In a tutorial I found this mnist classification code: import numpy as np from keras.datasets import mnist from keras.models import Sequential from keras.layers.core import Dense from keras.utils import np_utils batch_size = 128 nb_classes = 10 nb_epochs = 2 (X_train, y_train), (X_test, y_test) = mnist.load_data() X_train = X_train.reshape(60000, 784) X_test = X_test.reshape(10000, 784) X_train = X_train.astype(""float32"") X_test = X_test.astype(""float32"") X_train /= 255 X_test /= 255 print(X_train.shape[0], ""train samples"") print(X_test.shape[0], ""test samples"") y_train = np_utils.to_categorical(y_train, nb_classes) y_test = np_utils.to_categorical(y_test, nb_classes) model = Sequential() model.add(Dense(output_dim = 100, input_dim = 784, activation= ""relu"")) model.add(Dense(output_dim = 200, activation = ""relu"")) model.add(Dense(output_dim = 200, activation = ""relu"")) model.add(Dense(output_dim = nb_classes, activation = ""softmax"")) model.compile(optimizer = ""adam"", loss = ""categorical_crossentropy"") model.fit(X_train, y_train, batch_size = batch_size, nb_epoch = nb_epochs, show_accuracy = True, verbose = 2, validation_data = (X_test, y_test)) score = model.evaluate(X_test, y_test, show_accuracy = True, verbose = 0) print(""Test score: "", score[0]) print(""Test accuracy: "", score[1]) This code runs, and I get the result as expected: (60000L, 'train samples') (10000L, 'test samples') Train on 60000 samples, validate on 10000 samples Epoch 1/2 2s - loss: 0.2988 - acc: 0.9131 - val_loss: 0.1314 - val_acc: 0.9607 Epoch 2/2 2s - loss: 0.1144 - acc: 0.9651 - val_loss: 0.0995 - val_acc: 0.9673 ('Test score: ', 0.099454972004890438) ('Test accuracy: ', 0.96730000000000005) Up to this point everything runs perfectly, however when I apply the above algorithm to my dataset, accuracy gets stuck. My code is as follows: import os import pandas as pd from sklearn.cross_validation import train_test_split from keras.models import Sequential from keras.layers.convolutional import Convolution2D, MaxPooling2D from keras.layers.core import Dense, Activation, Dropout, Flatten from keras.utils import np_utils import AudioProcessing as ap import ImageTools as it batch_size = 128 nb_classes = 2 nb_epoch = 10 for i in range(20): print ""\n"" # Generate spectrograms if necessary if(len(os.listdir(""./AudioNormalPathalogicClassification/Image"")) > 0): print ""Audio files are already processed. Skipping..."" else: print ""Generating spectrograms for the audio files..."" ap.audio_2_image(""./AudioNormalPathalogicClassification/Audio/"",""./AudioNormalPathalogicClassification/Image/"","".wav"","".png"",(28,28)) # Read the result csv df = pd.read_csv('./AudioNormalPathalogicClassification/Result/result.csv', header = None) df.columns = [""RegionName"",""IsNormal""] bool_mapping = {True : 1, False : 0} nb_classes = 2 for col in df: if(col == ""RegionName""): a = 3 else: df[col] = df[col].map(bool_mapping) y = df.iloc[:,1:].values y = np_utils.to_categorical(y, nb_classes) # Load images into memory print ""Loading images into memory..."" X = it.load_images(""./AudioNormalPathalogicClassification/Image/"","".png"") X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0) X_train = X_train.reshape(X_train.shape[0], 784) X_test = X_test.reshape(X_test.shape[0], 784) X_train = X_train.astype(""float32"") X_test = X_test.astype(""float32"") X_train /= 255 X_test /= 255 print(""X_train shape: "" + str(X_train.shape)) print(str(X_train.shape[0]) + "" train samples"") print(str(X_test.shape[0]) + "" test samples"") model = Sequential() model.add(Dense(output_dim = 100, input_dim = 784, activation= ""relu"")) model.add(Dense(output_dim = 200, activation = ""relu"")) model.add(Dense(output_dim = 200, activation = ""relu"")) model.add(Dense(output_dim = nb_classes, activation = ""softmax"")) model.compile(loss = ""categorical_crossentropy"", optimizer = ""adam"") print model.summary() model.fit(X_train, y_train, batch_size = batch_size, nb_epoch = nb_epoch, show_accuracy = True, verbose = 1, validation_data = (X_test, y_test)) score = model.evaluate(X_test, y_test, show_accuracy = True, verbose = 1) print(""Test score: "", score[0]) print(""Test accuracy: "", score[1]) AudioProcessing.py import os import scipy as sp import scipy.io.wavfile as wav import matplotlib.pylab as pylab import Image def save_spectrogram_scipy(source_filename, destination_filename, size): dt = 0.0005 NFFT = 1024 Fs = int(1.0/dt) fs, audio = wav.read(source_filename) if(len(audio.shape) >= 2): audio = sp.mean(audio, axis = 1) fig = pylab.figure() ax = pylab.Axes(fig, [0,0,1,1]) ax.set_axis_off() fig.add_axes(ax) pylab.specgram(audio, NFFT = NFFT, Fs = Fs, noverlap = 900, cmap=""gray"") pylab.savefig(destination_filename) img = Image.open(destination_filename).convert(""L"") img = img.resize(size) img.save(destination_filename) pylab.clf() del img def audio_2_image(source_directory, destination_directory, audio_extension, image_extension, size): nb_files = len(os.listdir(source_directory)); count = 0 for file in os.listdir(source_directory): if file.endswith(audio_extension): destinationName = file[:-4] save_spectrogram_scipy(source_directory + file, destination_directory + destinationName + image_extension, size) count += 1 print (""Generating spectrogram for files "" + str(count) + "" / "" + str(nb_files) + ""."") ImageTools.py import os import numpy as np import matplotlib.image as mpimg def load_images(source_directory, image_extension): image_matrix = [] nb_files = len(os.listdir(source_directory)); count = 0 for file in os.listdir(source_directory): if file.endswith(image_extension): with open(source_directory + file,""r+b"") as f: img = mpimg.imread(f) img = img.flatten() image_matrix.append(img) del img count += 1 #print (""File "" + str(count) + "" / "" + str(nb_files) + "" loaded."") return np.asarray(image_matrix) So I run the above code and recieve: Audio files are already processed. Skipping... Loading images into memory... X_train shape: (2394L, 784L) 2394 train samples 1027 test samples -------------------------------------------------------------------------------- Initial input shape: (None, 784) -------------------------------------------------------------------------------- Layer (name) Output Shape Param # -------------------------------------------------------------------------------- Dense (dense) (None, 100) 78500 Dense (dense) (None, 200) 20200 Dense (dense) (None, 200) 40200 Dense (dense) (None, 2) 402 -------------------------------------------------------------------------------- Total params: 139302 -------------------------------------------------------------------------------- None Train on 2394 samples, validate on 1027 samples Epoch 1/10 2394/2394 [==============================] - 0s - loss: 0.6898 - acc: 0.5455 - val_loss: 0.6835 - val_acc: 0.5716 Epoch 2/10 2394/2394 [==============================] - 0s - loss: 0.6879 - acc: 0.5522 - val_loss: 0.6901 - val_acc: 0.5716 Epoch 3/10 2394/2394 [==============================] - 0s - loss: 0.6880 - acc: 0.5522 - val_loss: 0.6842 - val_acc: 0.5716 Epoch 4/10 2394/2394 [==============================] - 0s - loss: 0.6883 - acc: 0.5522 - val_loss: 0.6829 - val_acc: 0.5716 Epoch 5/10 2394/2394 [==============================] - 0s - loss: 0.6885 - acc: 0.5522 - val_loss: 0.6836 - val_acc: 0.5716 Epoch 6/10 2394/2394 [==============================] - 0s - loss: 0.6887 - acc: 0.5522 - val_loss: 0.6832 - val_acc: 0.5716 Epoch 7/10 2394/2394 [==============================] - 0s - loss: 0.6882 - acc: 0.5522 - val_loss: 0.6859 - val_acc: 0.5716 Epoch 8/10 2394/2394 [==============================] - 0s - loss: 0.6882 - acc: 0.5522 - val_loss: 0.6849 - val_acc: 0.5716 Epoch 9/10 2394/2394 [==============================] - 0s - loss: 0.6885 - acc: 0.5522 - val_loss: 0.6836 - val_acc: 0.5716 Epoch 10/10 2394/2394 [==============================] - 0s - loss: 0.6877 - acc: 0.5522 - val_loss: 0.6849 - val_acc: 0.5716 1027/1027 [==============================] - 0s ('Test score: ', 0.68490593621422047) ('Test accuracy: ', 0.57156767283349563) I tried changing the network, adding more epochs, but I always get the same result no matter what. I don't understand why I am getting the same result. Any help would be appreciated. Thank you. Edit: I found a mistake where pixel values were not read correctly. I fixed the ImageTools.py below as: import os import numpy as np from scipy.misc import imread def load_images(source_directory, image_extension): image_matrix = [] nb_files = len(os.listdir(source_directory)); count = 0 for file in os.listdir(source_directory): if file.endswith(image_extension): with open(source_directory + file,""r+b"") as f: img = imread(f) img = img.flatten() image_matrix.append(img) del img count += 1 #print (""File "" + str(count) + "" / "" + str(nb_files) + "" loaded."") return np.asarray(image_matrix) Now I actually get grayscale pixel values from 0 to 255, so now my dividing it by 255 makes sense. However, I still get the same result.",|python|audio|machine-learning|theano|keras|,Training,2
37232782,"NaN loss when training regression network. I have a data matrix in ""one-hot encoding"" (all ones and zeros) with 260,000 rows and 35 columns. I am using Keras to train a simple neural network to predict a continuous variable. The code to make the network is the following: model = Sequential() model.add(Dense(1024, input_shape=(n_train,))) model.add(Activation('relu')) model.add(Dropout(0.1)) model.add(Dense(512)) model.add(Activation('relu')) model.add(Dropout(0.1)) model.add(Dense(256)) model.add(Activation('relu')) model.add(Dropout(0.1)) model.add(Dense(1)) sgd = SGD(lr=0.01, nesterov=True); #rms = RMSprop() #model.compile(loss='categorical_crossentropy', optimizer=rms, metrics=['accuracy']) model.compile(loss='mean_absolute_error', optimizer=sgd) model.fit(X_train, Y_train, batch_size=32, nb_epoch=3, verbose=1, validation_data=(X_test,Y_test), callbacks=[EarlyStopping(monitor='val_loss', patience=4)] ) However, during the training process, I see the loss decrease nicely, but during the middle of the second epoch, it goes to nan: Train on 260000 samples, validate on 64905 samples Epoch 1/3 260000/260000 [==============================] - 254s - loss: 16.2775 - val_loss: 13.4925 Epoch 2/3 88448/260000 [=========>....................] - ETA: 161s - loss: nan I tried using RMSProp instead of SGD, I tried tanh instead of relu, I tried with and without dropout, all to no avail. I tried with a smaller model, i.e. with only one hidden layer, and same issue (it becomes nan at a different point). However, it does work with less features, i.e. if there are only 5 columns, and gives quite good predictions. It seems to be there is some kind of overflow, but I can't imagine why--the loss is not unreasonably large at all. Python version 2.7.11, running on a linux machine, CPU only. I tested it with the latest version of Theano, and I also get Nans, so I tried going to Theano 0.8.2 and have the same problem. With the latest version of Keras has the same problem, and also with the 0.3.2 version.",|python|keras|neural-network|theano|loss-function|,Training,2
37624102,"Very basic Keras CNN with 2 classes giving inexplicable answers. Tried training a very simple CNN with Keras/Theano on a binary classification problem. The loss function always converges to 8.0151 or so. Parameter/architecture modifications did not help. So I made a very simple example: new input arrays, one is all ones, the other all zeroes. No dice, same behavior. I tried all 1's and all -1's, same thing. Then, all 0's and random. Same. Lowered dimensions and depth, removed dropout, monkeyed with parameters, same. Help! What is happening? import numpy A = [] B = [] for j in range(100): npa = numpy.array([[1 for j in range(100)] for i in range(100)]) A.append(npa.reshape(1,npa.shape[0],npa.shape[1])) for j in range(100): npa = numpy.array([[0 for j in range(100)] for i in range(100)]) B.append(npa.reshape(1,npa.shape[0],npa.shape[1])) trainXA = [] trainXB = [] testXA = [] testXB = [] for j in range(len(A)): if ((j+2) % 7) != 0: trainXA.append(A[j]) trainXB.append(B[j]) else: testXA.append(A[j]) testXB.append(B[j]) X_train = numpy.array(trainXA + trainXB) X_test = numpy.array(testXA + testXB) Y_train = numpy.array([[1,0] for i in range(len(X_train)/2)] + [[0,1] for i in range(len(X_train)/2)]) import random def jumblelists(C,D): outC = [] outD = [] for j in range(len(C)): newpos = int(random.random()*(len(outC)+1)) outC = outC[:newpos]+[C[j]]+outC[newpos:] outD = outD[:newpos]+[D[j]]+outD[newpos:] return numpy.array(outC),numpy.array(outD) X_train,Y_train = jumblelists(X_train,Y_train) from keras.models import Sequential from keras.layers import Dense, Dropout, Activation, Flatten from keras.layers import Convolution2D, MaxPooling2D from keras.optimizers import SGD model = Sequential() model.add(Convolution2D(32, 3, 3, border_mode='valid', input_shape=(1,100,100))) model.add(Activation('relu')) model.add(Convolution2D(32, 3, 3)) model.add(Activation('relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Flatten()) model.add(Dense(128)) model.add(Activation('relu')) model.add(Dense(2)) model.add(Activation('softmax')) sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True) model.compile(loss='binary_crossentropy', optimizer=sgd) model.fit(X_train, Y_train, batch_size=32, nb_epoch=10)",|python|machine-learning|theano|keras|,Training,2
37891954,"Keras, how do I predict after I trained a model?. I'm playing with the reuters-example dataset and it runs fine (my model is trained). I read about how to save a model, so I could load it later to use again. But how do I use this saved model to predict a new text? Do I use models.predict()? Do I have to prepare this text in a special way? I tried it with import keras.preprocessing.text text = np.array(['this is just some random, stupid text']) print(text.shape) tk = keras.preprocessing.text.Tokenizer( nb_words=2000, filters=keras.preprocessing.text.base_filter(), lower=True, split="" "") tk.fit_on_texts(text) pred = tk.texts_to_sequences(text) print(pred) model.predict(pred) But I always get (1L,) [[2, 4, 1, 6, 5, 7, 3]] --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) <ipython-input-83-42d744d811fb> in <module>() 7 print(pred) 8 ----> 9 model.predict(pred) C:\Users\bkey\Anaconda2\lib\site-packages\keras\models.pyc in predict(self, x, batch_size, verbose) 457 if self.model is None: 458 self.build() --> 459 return self.model.predict(x, batch_size=batch_size, verbose=verbose) 460 461 def predict_on_batch(self, x): C:\Users\bkey\Anaconda2\lib\site-packages\keras\engine\training.pyc in predict(self, x, batch_size, verbose) 1132 x = standardize_input_data(x, self.input_names, 1133 self.internal_input_shapes, -> 1134 check_batch_dim=False) 1135 if self.stateful: 1136 if x[0].shape[0] > batch_size and x[0].shape[0] % batch_size != 0: C:\Users\bkey\Anaconda2\lib\site-packages\keras\engine\training.pyc in standardize_input_data(data, names, shapes, check_batch_dim, exception_prefix) 79 for i in range(len(names)): 80 array = arrays[i] ---> 81 if len(array.shape) == 1: 82 array = np.expand_dims(array, 1) 83 arrays[i] = array AttributeError: 'list' object has no attribute 'shape' Do you have any recommendations as to how to make predictions with a trained model?",|python|theano|deep-learning|keras|,API,4
38056110,"InvalidArgumentError: You must feed a value for placeholder tensor 'Placeholder' with dtype float and shape [1000,625]. I get the above unexpected error when trying to run this code: # -*- coding: utf-8 -*- """""" Created on Fri Jun 24 10:38:04 2016 @author: andrea """""" # pylint: disable=missing-docstring from __future__ import absolute_import from __future__ import division from __future__ import print_function import time from six.moves import xrange # pylint: disable=redefined-builtin import tensorflow as tf from pylab import * import argparse import mlp # Basic model parameters as external flags. tf.app.flags.FLAGS = tf.python.platform.flags._FlagValues() tf.app.flags._global_parser = argparse.ArgumentParser() flags = tf.app.flags FLAGS = flags.FLAGS flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate.') flags.DEFINE_integer('max_steps', 20, 'Number of steps to run trainer.') flags.DEFINE_integer('batch_size', 1000, 'Batch size. Must divide evenly into the dataset sizes.') flags.DEFINE_integer('num_samples', 100000, 'Total number of samples. Needed by the reader') flags.DEFINE_string('training_set_file', 'godzilla_dataset_size625', 'Training set file') flags.DEFINE_string('test_set_file', 'godzilla_testset_size625', 'Test set file') flags.DEFINE_string('test_size', 1000, 'Test set size') def placeholder_inputs(batch_size): images_placeholder = tf.placeholder(tf.float32, shape=(batch_size, mlp.NUM_INPUT)) labels_placeholder = tf.placeholder(tf.float32, shape=(batch_size, mlp.NUM_OUTPUT)) return images_placeholder, labels_placeholder def fill_feed_dict(data_set_file, images_pl, labels_pl): for l in range(int(FLAGS.num_samples/FLAGS.batch_size)): data_set = genfromtxt(""../dataset/"" + data_set_file, skip_header=l*FLAGS.batch_size, max_rows=FLAGS.batch_size) data_set = reshape(data_set, [FLAGS.batch_size, mlp.NUM_INPUT + mlp.NUM_OUTPUT]) images = data_set[:, :mlp.NUM_INPUT] labels_feed = reshape(data_set[:, mlp.NUM_INPUT:], [FLAGS.batch_size, mlp.NUM_OUTPUT]) images_feed = reshape(images, [FLAGS.batch_size, mlp.NUM_INPUT]) feed_dict = { images_pl: images_feed, labels_pl: labels_feed, } yield feed_dict def reader(data_set_file, images_pl, labels_pl): data_set = loadtxt(""../dataset/"" + data_set_file) images = data_set[:, :mlp.NUM_INPUT] labels_feed = reshape(data_set[:, mlp.NUM_INPUT:], [data_set.shape[0], mlp.NUM_OUTPUT]) images_feed = reshape(images, [data_set.shape[0], mlp.NUM_INPUT]) feed_dict = { images_pl: images_feed, labels_pl: labels_feed, } return feed_dict, labels_pl def run_training(): tot_training_loss = [] tot_test_loss = [] tf.reset_default_graph() with tf.Graph().as_default() as g: images_placeholder, labels_placeholder = placeholder_inputs(FLAGS.batch_size) test_images_pl, test_labels_pl = placeholder_inputs(FLAGS.test_size) logits = mlp.inference(images_placeholder) test_pred = mlp.inference(test_images_pl, reuse=True) loss = mlp.loss(logits, labels_placeholder) test_loss = mlp.loss(test_pred, test_labels_pl) train_op = mlp.training(loss, FLAGS.learning_rate) #summary_op = tf.merge_all_summaries() init = tf.initialize_all_variables() saver = tf.train.Saver() sess = tf.Session() #summary_writer = tf.train.SummaryWriter(""./"", sess.graph) sess.run(init) test_feed, test_labels_placeholder = reader(FLAGS.test_set_file, test_images_pl, test_labels_pl) # Start the training loop. for step in xrange(FLAGS.max_steps): start_time = time.time() feed_gen = fill_feed_dict(FLAGS.training_set_file, images_placeholder, labels_placeholder) i=1 for feed_dict in feed_gen: _, loss_value = sess.run([train_op, loss], feed_dict=feed_dict) _, test_loss_val = sess.run([test_pred, test_loss], feed_dict=test_feed) tot_training_loss.append(loss_value) tot_test_loss.append(test_loss_val) #if i % 10 == 0: #print('%d minibatches analyzed...'%i) i+=1 if step % 1 == 0: duration = time.time() - start_time print('Epoch %d (%.3f sec):\n training loss = %f \n test loss = %f ' % (step, duration, loss_value, test_loss_val)) predictions = sess.run(test_pred, feed_dict=test_feed) savetxt(""predictions"", predictions) savetxt(""training_loss"", tot_training_loss) savetxt(""test_loss"", tot_test_loss) plot(tot_training_loss) plot(tot_test_loss) figure() scatter(test_feed[test_labels_placeholder], predictions) #plot([.4, .6], [.4, .6]) run_training() #if __name__ == '__main__': # tf.app.run() this is mlp: from __future__ import absolute_import from __future__ import division from __future__ import print_function import math import tensorflow as tf NUM_OUTPUT = 1 NUM_INPUT = 625 NUM_HIDDEN = 5 def inference(images, reuse=None): with tf.variable_scope('hidden1', reuse=reuse): weights = tf.get_variable(name='weights', shape=[NUM_INPUT, NUM_HIDDEN], initializer=tf.contrib.layers.xavier_initializer()) weight_decay = tf.mul(tf.nn.l2_loss(weights), 0.00001, name='weight_loss') tf.add_to_collection('losses', weight_decay) biases = tf.Variable(tf.constant(0.0, name='biases', shape=[NUM_HIDDEN])) hidden1_output = tf.nn.relu(tf.matmul(images, weights)+biases, name='hidden1') with tf.variable_scope('output', reuse=reuse): weights = tf.get_variable(name='weights', shape=[NUM_HIDDEN, NUM_OUTPUT], initializer=tf.contrib.layers.xavier_initializer()) weight_decay = tf.mul(tf.nn.l2_loss(weights), 0.00001, name='weight_loss') tf.add_to_collection('losses', weight_decay) biases = tf.Variable(tf.constant(0.0, name='biases', shape=[NUM_OUTPUT])) output = tf.nn.relu(tf.matmul(hidden1_output, weights)+biases, name='output') return output def loss(outputs, labels): rmse = tf.sqrt(tf.reduce_mean(tf.square(tf.sub(labels, outputs))), name=""rmse"") tf.add_to_collection('losses', rmse) return tf.add_n(tf.get_collection('losses'), name='total_loss') def training(loss, learning_rate): tf.scalar_summary(loss.op.name, loss) optimizer = tf.train.GradientDescentOptimizer(learning_rate) global_step = tf.Variable(0, name='global_step', trainable=False) train_op = optimizer.minimize(loss, global_step=global_step) return train_op here the error: Traceback (most recent call last): File ""<ipython-input-1-f16dfed3b99b>"", line 1, in <module> runfile('/home/andrea/test/python/main_mlp_yield.py', wdir='/home/andrea/test/python') File ""/usr/local/lib/python2.7/dist-packages/spyderlib/widgets/externalshell/sitecustomize.py"", line 714, in runfile execfile(filename, namespace) File ""/usr/local/lib/python2.7/dist-packages/spyderlib/widgets/externalshell/sitecustomize.py"", line 81, in execfile builtins.execfile(filename, *where) File ""/home/andrea/test/python/main_mlp_yield.py"", line 127, in <module> run_training() File ""/home/andrea/test/python/main_mlp_yield.py"", line 105, in run_training _, test_loss_val = sess.run([test_pred, test_loss], feed_dict=test_feed) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 372, in run run_metadata_ptr) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 636, in _run feed_dict_string, options, run_metadata) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 708, in _do_run target_list, options, run_metadata) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 728, in _do_call raise type(e)(node_def, op, message) InvalidArgumentError: You must feed a value for placeholder tensor 'Placeholder' with dtype float and shape [1000,625] [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=[1000,625], _device=""/job:localhost/replica:0/task:0/cpu:0""]()]] Caused by op u'Placeholder', defined at: File ""/usr/local/lib/python2.7/dist-packages/spyderlib/widgets/externalshell/start_ipython_kernel.py"", line 205, in <module> __ipythonkernel__.start() File ""/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py"", line 442, in start ioloop.IOLoop.instance().start() File ""/usr/local/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py"", line 162, in start super(ZMQIOLoop, self).start() File ""/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py"", line 883, in start handler_func(fd_obj, events) File ""/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py"", line 275, in null_wrapper return fn(*args, **kwargs) File ""/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py"", line 440, in _handle_events self._handle_recv() File ""/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py"", line 472, in _handle_recv self._run_callback(callback, msg) File ""/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py"", line 414, in _run_callback callback(*args, **kwargs) File ""/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py"", line 275, in null_wrapper return fn(*args, **kwargs) File ""/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py"", line 276, in dispatcher return self.dispatch_shell(stream, msg) File ""/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py"", line 228, in dispatch_shell handler(stream, idents, msg) File ""/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py"", line 391, in execute_request user_expressions, allow_stdin) File ""/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py"", line 199, in do_execute shell.run_cell(code, store_history=store_history, silent=silent) File ""/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py"", line 2723, in run_cell interactivity=interactivity, compiler=compiler, result=result) File ""/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py"", line 2831, in run_ast_nodes if self.run_code(code, result): File ""/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py"", line 2885, in run_code exec(code_obj, self.user_global_ns, self.user_ns) File ""<ipython-input-1-f16dfed3b99b>"", line 1, in <module> runfile('/home/andrea/test/python/main_mlp_yield.py', wdir='/home/andrea/test/python') File ""/usr/local/lib/python2.7/dist-packages/spyderlib/widgets/externalshell/sitecustomize.py"", line 714, in runfile execfile(filename, namespace) File ""/usr/local/lib/python2.7/dist-packages/spyderlib/widgets/externalshell/sitecustomize.py"", line 81, in execfile builtins.execfile(filename, *where) File ""/home/andrea/test/python/main_mlp_yield.py"", line 127, in <module> run_training() File ""/home/andrea/test/python/main_mlp_yield.py"", line 79, in run_training images_placeholder, labels_placeholder = placeholder_inputs(FLAGS.batch_size) File ""/home/andrea/test/python/main_mlp_yield.py"", line 37, in placeholder_inputs images_placeholder = tf.placeholder(tf.float32, shape=(batch_size, mlp.NUM_INPUT)) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py"", line 895, in placeholder name=name) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py"", line 1238, in _placeholder name=name) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py"", line 704, in apply_op op_def=op_def) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2260, in create_op original_op=self._default_original_op, op_def=op_def) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1230, in __init__ self._traceback = _extract_stack() I really don't understand why. It looks to me that I'm feeding all the placeholders before using them. I also removed the ""merge_all_summaries"" since this problem is similar to other (this and this), but it didn't help EDIT: training data: 100000 samples x 625 features test data: 1000 samples x 625 features num. output: 1",|machine-learning|neural-network|tensorflow|,Training,2
38272699,"Importing images in tensorflow. I'm trying to import images and convert them to tensors. All the other solutions recommend making a filename_queue and using tf.reader(), but I can't get that to work... hence, I'm just going with the basics. I have a file called test.jpg in my desktop directory, and I'm running a linux environment. Here is my code: import tensorflow as tf image = tf.image.decode_jpeg(""~/Desktop/test.jpg"", channels=1) print(image) As you can see, some very simple code... however it outputs Tensor(""DecodeJpeg:0"", shape=(?, ?, 1), dtype=uint8) Which is telling me that it isn't reading the file correctly. Is there anything that I am doing wrong? Thanks!",|python|image|filesystems|tensorflow|,API,4
38294046,"Simple Recurrent Neural Network input shape. I am trying to code a very simple RNN example with keras but the results are not as expected. My X_train is a repeated list with length 6000 like: 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, ... I formatted this to shape: (6000, 1, 1) My y_train is a repeated list with length 6000 like: 1, 0.8, 0.6, 0, 0, 0, 1, 0.8, 0.6, 0, ... I formatted this to shape: (6000, 1) In my understanding, the recurrent neural network should learn to predict the 0.8 and 0.6 correctly because it can remember the 1 in X_train two timesteps ago. My model: model=Sequential() model.add(SimpleRNN(input_dim=1, output_dim=50)) model.add(Dense(output_dim=1, activation = ""sigmoid"")) model.compile(loss=""mse"", optimizer=""rmsprop"") model.fit(X_train, y_train, nb_epoch=10, batch_size=32) The model can be trained successfully with minimal loss ~0.1015 but the results are not as expected. test case --------------------------------------------- model result -------------expected result model.predict(np.array([[[1]]])) --------------------0.9825--------------------1 model.predict(np.array([[[1],[0]]])) ----------------0.2081--------------------0.8 model.predict(np.array([[[1],[0],[0]]])) ------------0.2778 -------------------0.6 model.predict(np.array([[[1],[0],[0],[0]]]))---------0.3186--------------------0 Any hints what I am misunderstanding here?",|python|neural-network|keras|recurrent-neural-network|,Tensors&Inputs,1
38546903,"semantic segmentation with tensorflow - ValueError in loss function (sparse-softmax). So, I'm working on a building a fully convolutional network (FCN), based off of Marvin Teichmann's tensorflow-fcn My input image data, for the time being is a 750x750x3 RGB image. After running through the network, I use logits of shape [batch_size, 750,750,2] for my loss calculation. It is a binary classification - I have 2 classes here, [0, 1] in my labels (of shape [batch_sizex750x750]. And these go into the loss function, below: def loss(logits, labels, num_classes): with tf.name_scope('loss mine'): logits = tf.to_float(tf.reshape(logits, [-1, num_classes])) #CHANGE labels type to int, for sparse_softmax... labels = tf.to_int64(tf.reshape(labels, [-1])) print ('shape of logits: %s' % str(logits.get_shape())) print ('shape of labels: %s' % str(labels.get_shape())) cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels, name='Cross_Entropy') tf.add_to_collection('losses', cross_entropy) loss = tf.add_n(tf.get_collection('losses'), name='total_loss') return loss These are shapes for the logits and labels after reshaping: shape of logits: (562500, 2) shape of labels: (562500,) And here, it throws me a ValueError stating: Shapes () and (562500,) are not compatible Full traceback below: File ""train.py"", line 89, in <module> loss_train = loss.loss(logits, data.train.labels, 2) File ""/tensorflow-fcn/loss.py"", line 86, in loss loss = tf.add_n(tf.get_collection('losses'), name='total_loss') File ""/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 88, in add_n result = _op_def_lib.apply_op(""AddN"", inputs=inputs, name=name) File ""/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py"", line 704, in apply_op op_def=op_def) File ""/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2262, in create_op set_shapes_for_outputs(ret) File ""/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1702, in set_shapes_for_outputs shapes = shape_func(op) File ""/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py"", line 1557, in _AddNShape merged_shape = merged_shape.merge_with(input_.get_shape()) File ""/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.py"", line 570, in merge_with (self, other)) ValueError: Shapes () and (562500,) are not compatible Suggestions? Is my implementation of the tf.add_to_collection('losses', cross_entropy) wrong? UPDATE: I tried to run this without the summing across pixels (or so I think), by returning cross_entropy in the above code directly, as the loss. It seems to have worked. (It now throws a ValueError from the training optimizer function, stating: No gradients provided for any variable. Assuming this has more to do with my weight initialization and regularization than anything else. UPDATE 2: The above (regarding ValueError due to absence of gradients) was trivial. As mentioned here, this message is usually encountered when there is no path between any of the tf.Variable objects defined and the loss tensor that that is being minimized. The initial problem with usage of tf.add_n persists though. I'm assuming it has to do with the mechanics of how Graph collections work in TensorFlow. Having initialized my variables, the error now reads: Shapes () and (?,) are not compatible",|python|tensorflow|image-segmentation|softmax|,Training,2
38584268,"Implementing a Siamese NN in Keras. So I'm trying to implement this paper about a Siamese neural network: Learning a similarity metric discriminatively, with application to face verification, by Sumit Chopra, Raia Hadsell and Yann LeCun (2005). I'm using the CIFAR10 dataset instead, though, with 10 classes. The specifications of one of the legs is reproduced for convenience. Notation: C_x is a convolution layer, S_x is a subsampling layer and F_x is a fully connected layer; with a shared index x: C1: feature maps: 15, kernel size = (7, 7) S2: feature maps: 15, field-of-view = (2, 2) C3: feature maps: 45, kernel size = (6, 6) S4: feature maps: 45, field-of-view = (4, 3) C5: feature maps: 250, kernel size = (5, 5) F6 (fully connected layer): no. of units = 50 What I've Tried model = Sequential() #C1 model.add(Convolution2D(15, 7, 7, activation='relu', border_mode='same', input_shape=input_img_shape)) print(""C1 shape: "", model.output_shape) #S2 model.add(MaxPooling2D((2,2), border_mode='same')) print(""S2 shape: "", model.output_shape) #... #C5 model.add(Convolution2D(250, 5, 5, activation='relu', border_mode='same')) print(""C5 shape: "", model.output_shape) #F6 model.add(Dense(50)) This throws a long error message, which I believe is a reshape error. A snippet of the error: Exception: Input 0 is incompatible with layer dense_13: expected ndim=2, found ndim=4 I know that the problem is isolated in that final Dense layer, because the code proceeds smoothly if I comment it out. But I'm not sure exactly how I should then shape/specify my final fully connected layer so that it's compatible with the prior convolution layer? Some Places I've Looked This is a related problem, though the implementation is slightly different (it seems that there isn't a 'Siamese' core layer in keras at the time of this writing). I'm aware that there are also implementations in Theano, which I'll bear in mind if I'm just not able to do it in keras. Thanks!",|conv-neural-network|keras|,Tensors&Inputs,1
38648195,"Keras low accuracy classification task. I was playing around with Keras and a dummy dataset. I wanted to see how much better a neural network would do compared to a standard SVM with a RBF kernel. The task was simple: predict the class for a 20-dim vector in the set {0,1,2}. I noticed that the neural network does horribly. The SVM gets around 90% correct whereas the neural network flounders at 40%. What am I doing wrong in my code? This is most likely an error on my part but after a few hours of trying various parameters on the NN, I've given up. Code from sklearn.datasets import make_multilabel_classification from sklearn.svm import SVC from keras.models import Sequential from keras.layers import Dense, Dropout, Activation from keras.optimizers import SGD, RMSprop from keras.utils import np_utils from keras.datasets import mnist # generate some data dummyX, dummyY = make_multilabel_classification(n_samples=4000, n_features=20, n_classes=3) # neural network model = Sequential() model.add(Dense(20, input_dim=20)) model.add(Activation('relu')) model.add(Dropout(0.5)) model.add(Dense(20)) model.add(Activation('relu')) model.add(Dropout(0.5)) model.add(Dense(3)) model.add(Activation('softmax')) model.compile(loss='mean_squared_error', optimizer='sgd', metrics=['accuracy']) X_train, X_test, y_train, y_test = train_test_split(dummyX, dummyY, test_size=0.20, random_state=42) model.fit(X_train, y_train,nb_epoch=20, batch_size=30, validation_data=(X_test, y_test)) # Epoch 20/20 # 3200/3200 [==============================] - 0s - loss: 0.2469 - acc: 0.4366 - val_loss: 0.2468 - val_acc: 0.4063 # Out[460]: # SVM - note that y_train and test are binary label. I haven't included the multi class converter code here for brevity svm = SVC() svm.fit(X_train, y_train) svm.score(X_test, y_test) # 0.891249 TL;DR Made dummy data; neural network sucked; SVM kicked walloped it. Please help",|python|neural-network|gpu|theano|keras|,Training,2
38760621,"Can TensorFlow schedule operations to all available GPUs automatically?. We have read the paper of TensorFlow about scheduling. It may pre-execute the Graph and find the ""right"" device to place the operations. But we have test to use tf.Session(config=tf.ConfigProto(log_device_placement=True)) and not specified any device to run. We found that all the operations are placed in the first GPU. The log looks like this. Adam/epsilon: /job:localhost/replica:0/task:0/gpu:0 I tensorflow/core/common_runtime/simple_placer.cc:818] Adam/epsilon: /job:localhost/replica:0/task:0/gpu:0 Adam/beta2: /job:localhost/replica:0/task:0/gpu:0 I tensorflow/core/common_runtime/simple_placer.cc:818] Adam/beta2: /job:localhost/replica:0/task:0/gpu:0 Adam/beta1: /job:localhost/replica:0/task:0/gpu:0 I tensorflow/core/common_runtime/simple_placer.cc:818] Adam/beta1: /job:localhost/replica:0/task:0/gpu:0 Adam/learning_rate: /job:localhost/replica:0/task:0/gpu:0 I tensorflow/core/common_runtime/simple_placer.cc:818] Adam/learning_rate: /job:localhost/replica:0/task:0/gpu:0 Variable_3/Adam_1: /job:localhost/replica:0/task:0/gpu:0 I tensorflow/core/common_runtime/simple_placer.cc:818] Variable_3/Adam_1: /job:localhost/replica:0/task:0/gpu:0 Variable_3/Adam_1/read: /job:localhost/replica:0/task:0/gpu:0 I tensorflow/core/common_runtime/simple_placer.cc:818] Variable_3/Adam_1/read: /job:localhost/replica:0/task:0/gpu:0 Variable_3/Adam_1/Assign: /job:localhost/replica:0/task:0/gpu:0 I tensorflow/core/common_runtime/simple_placer.cc:818] Variable_3/Adam_1/Assign: /job:localhost/replica:0/task:0/gpu:0 Variable_3/Adam: /job:localhost/replica:0/task:0/gpu:0 I tensorflow/core/common_runtime/simple_placer.cc:818] Variable_3/Adam: /job:localhost/replica:0/task:0/gpu:0 Variable_3/Adam/read: /job:localhost/replica:0/task:0/gpu:0 I tensorflow/core/common_runtime/simple_placer.cc:818] Variable_3/Adam/read: /job:localhost/replica:0/task:0/gpu:0 Variable_3/Adam/Assign: /job:localhost/replica:0/task:0/gpu:0 I tensorflow/core/common_runtime/simple_placer.cc:818] Variable_3/Adam/Assign: /job:localhost/replica:0/task:0/gpu:0 Variable_2/Adam_1: /job:localhost/replica:0/task:0/gpu:0 I tensorflow/core/common_runtime/simple_placer.cc:818] Variable_2/Adam_1: /job:localhost/replica:0/task:0/gpu:0 Variable_2/Adam_1/read: /job:localhost/replica:0/task:0/gpu:0 I tensorflow/core/common_runtime/simple_placer.cc:818] Variable_2/Adam_1/read: /job:localhost/replica:0/task:0/gpu:0 Variable_2/Adam_1/Assign: /job:localhost/replica:0/task:0/gpu:0 I tensorflow/core/common_runtime/simple_placer.cc:818] Variable_2/Adam_1/Assign: /job:localhost/replica:0/task:0/gpu:0 Variable_2/Adam: /job:localhost/replica:0/task:0/gpu:0 I tensorflow/core/common_runtime/simple_placer.cc:818] Variable_2/Adam: /job:localhost/replica:0/task:0/gpu:0 Variable_2/Adam/read: /job:localhost/replica:0/task:0/gpu:0 I tensorflow/core/common_runtime/simple_placer.cc:818] Variable_2/Adam/read: /job:localhost/replica:0/task:0/gpu:0 Variable_2/Adam/Assign: /job:localhost/replica:0/task:0/gpu:0 I tensorflow/core/common_runtime/simple_placer.cc:818] Variable_2/Adam/Assign: /job:localhost/replica:0/task:0/gpu:0 Variable_1/Adam_1: /job:localhost/replica:0/task:0/gpu:0 I tensorflow/core/common_runtime/simple_placer.cc:818] Variable_1/Adam_1: /job:localhost/replica:0/task:0/gpu:0 The Variables are also placed in GPU. I assure that the scheduler is not good enough right now and the best practice for users is that we should specify the operations to use CPU or GPU, especially when we have multiple GPUs. Is that right?",|gpu|tensorflow|deep-learning|,GPU Usage,3
38987680,"In Keras(Deep learning library), RepeatVector + TimeDistributed = Error?. Hi I'm a beginner keras. I'm making some model. step 1. Input batch and word list, (BATCH_SIZE, WORD_INDEX_LIST) step 2. Get word embeddings each words (BATCH_SIZE, WORD_LENGTH, EMBEDDING_SIZE) step 3. Average each each word embeddings in each batch. (BATCH_SIZE, EMBEDDING_SIZE) step 4. Repeat vector N, (BATCH_SIZE, N, EMBEDDING_SIZE) step 5. Apply Dense Layer each time step So, I write code. MAX_LEN = 20 ( = WORD_INDEX_LIST) step 1 layer_target_input = Input(shape=(MAX_LEN,), dtype=""int32"", name=""layer_target_input"") # step2 layer_embedding = Embedding(input_dim = n_symbols+1, output_dim=vector_dim,input_length=MAX_LEN, name=""embedding"", weights= [embedding_weights],trainable = False) encoded_target = layer_embedding(layer_target_input) # step 3 encoded_target_agg = KL.core.Lambda( lambda x: K.sum(x, axis=1) )(encoded_target) #step 4 encoded_target_agg_repeat = KL.RepeatVector( MAX_LEN)(encoded_target_agg) # step 5 layer_annotated_tahn = KL.Dense(output_dim=50, name=""layer_tahn"") layer_annotated_tahn_td = KL.TimeDistributed(layer_annotated_tahn) (encoded_target_agg_repeat) model = KM.Model(input=[layer_target_input], output=[ layer_annotated_tahn_td]) r = model.predict({ ""layer_target_input"":dev_targ}) # dev_targ = (2, 20, 300) But, when i run this code, result is bellow. Traceback (most recent call last): File ""Main.py"", line 127, in <module> r = model.predict({ ""layer_target_input"":dev_targ}) File ""/usr/local/anaconda/lib/python2.7/site-packages/Keras-1.0.7-py2.7.egg/keras/engine/training.py"", line 1180, in predict batch_size=batch_size, verbose=verbose) File ""/usr/local/anaconda/lib/python2.7/site-packages/Keras-1.0.7-py2.7.egg/keras/engine/training.py"", line 888, in _predict_loop outs[i][batch_start:batch_end] = batch_out ValueError: could not broadcast input array from shape (30,20,50) into shape (2,20,50) why batch size is changed? What I have wrong?",|deep-learning|keras|,Tensors&Inputs,1
39076388,"Tensorflow Deep MNIST: Resource exhausted: OOM when allocating tensor with shape[10000,32,28,28]. This is the sample MNIST code I am running: from tensorflow.examples.tutorials.mnist import input_data mnist = input_data.read_data_sets('MNIST_data', one_hot=True) import tensorflow as tf sess = tf.InteractiveSession() x = tf.placeholder(tf.float32, shape=[None, 784]) y_ = tf.placeholder(tf.float32, shape=[None, 10]) W = tf.Variable(tf.zeros([784,10])) b = tf.Variable(tf.zeros([10])) y = tf.nn.softmax(tf.matmul(x,W) + b) def weight_variable(shape): initial = tf.truncated_normal(shape, stddev=0.1) return tf.Variable(initial) def bias_variable(shape): initial = tf.constant(0.1, shape=shape) return tf.Variable(initial) def conv2d(x, W): return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME') def max_pool_2x2(x): return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') W_conv1 = weight_variable([5, 5, 1, 32]) b_conv1 = bias_variable([32]) x_image = tf.reshape(x, [-1,28,28,1]) h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1) h_pool1 = max_pool_2x2(h_conv1) W_conv2 = weight_variable([5, 5, 32, 64]) b_conv2 = bias_variable([64]) h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2) h_pool2 = max_pool_2x2(h_conv2) W_fc1 = weight_variable([7 * 7 * 64, 1024]) b_fc1 = bias_variable([1024]) h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64]) h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1) keep_prob = tf.placeholder(tf.float32) h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob) W_fc2 = weight_variable([1024, 10]) b_fc2 = bias_variable([10]) y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2) cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv), reduction_indices=[1])) train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy) correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1)) accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) init = tf.initialize_all_variables() config = tf.ConfigProto() config.gpu_options.allocator_type = 'BFC' with tf.Session(config = config) as s: sess.run(init) for i in range(20000): batch = mnist.train.next_batch(50) if i%100 == 0: train_accuracy = accuracy.eval(feed_dict={ x:batch[0], y_: batch[1], keep_prob: 1.0}) print(""step %d, training accuracy %g""%(i, train_accuracy)) train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5}) print(""test accuracy %g""%accuracy.eval(feed_dict={ x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0})) The GPU I am using is: GeForce GTX 750 Ti Error: ... ... ... step 19900, training accuracy 1 I tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (256): Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin. I tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (512): Total Chunks: 1, Chunks in use: 0 768B allocated for chunks. 1.20MiB client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin. I tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (1024): Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin. I tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (2048): Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin. I tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (4096): Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin. I tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (8192): Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin. I tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (16384): Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin. I tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (32768): Total Chunks: 1, Chunks in use: 0 36.8KiB allocated for chunks. 4.79MiB client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin. I tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (65536): Total Chunks: 1, Chunks in use: 0 78.5KiB allocated for chunks. 4.79MiB client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin. I tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (131072): Total Chunks: 1, Chunks in use: 0 200.0KiB allocated for chunks. 153.1KiB client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin. I tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (262144): Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin. I tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (524288): Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin. I tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (1048576): Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin. I tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (2097152): Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin. I tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (4194304): Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin. I tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (8388608): Total Chunks: 1, Chunks in use: 0 11.86MiB allocated for chunks. 390.6KiB client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin. I tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (16777216): Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin. I tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (33554432): Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin. I tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (67108864): Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin. I tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (134217728): Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin. I tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (268435456): Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin. I tensorflow/core/common_runtime/bfc_allocator.cc:656] Bin for 957.03MiB was 256.00MiB, Chunk State: I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x601a40000 of size 1280 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x601a40500 of size 1280 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x601a40a00 of size 31488 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x601a48500 of size 256 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x601a48600 of size 256 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x601a48700 of size 256 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x601a48800 of size 256 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x601a48900 of size 256 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x601a48a00 of size 4096 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x601a49a00 of size 256 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x601a49b00 of size 256 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x601a49c00 of size 256 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x601a49d00 of size 3328 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x601a4aa00 of size 256 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x601a4ab00 of size 204800 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x601a7cb00 of size 256 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x601a7cc00 of size 12845056 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x6026bcc00 of size 4096 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x6026bdc00 of size 40960 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x6026c7c00 of size 31488 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x6026cf700 of size 256 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x6026cf800 of size 256 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x6026cf900 of size 256 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x6026cfa00 of size 256 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x6026cfb00 of size 256 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x6026cfc00 of size 256 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x6026cfd00 of size 256 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x6026cfe00 of size 256 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x6026cff00 of size 256 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x6026d0000 of size 256 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x6026d0100 of size 256 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x6026d0500 of size 256 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x6026d0600 of size 3328 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x6026d1300 of size 40960 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x6026db300 of size 80128 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x602702600 of size 256 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x602734700 of size 204800 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x603342700 of size 4096 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x603343700 of size 3328 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x60334d700 of size 256 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x60334d800 of size 256 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x60334d900 of size 256 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x60334da00 of size 3328 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x60334e700 of size 3328 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x60334f400 of size 256 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x60334f500 of size 256 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x60334f600 of size 204800 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x603381600 of size 204800 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x6033b3600 of size 256 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x6033b3700 of size 256 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x6033b3800 of size 12845056 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x603ff3800 of size 12845056 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x604c33800 of size 4096 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x604c34800 of size 4096 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x604c35800 of size 40960 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x604c3f800 of size 40960 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x604c49800 of size 256 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x604c49900 of size 256 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x604c49a00 of size 13053184 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x6058bc700 of size 31360000 I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x6076a4b00 of size 1801385216 I tensorflow/core/common_runtime/bfc_allocator.cc:683] Free at 0x6026d0200 of size 768 I tensorflow/core/common_runtime/bfc_allocator.cc:683] Free at 0x6026eec00 of size 80384 I tensorflow/core/common_runtime/bfc_allocator.cc:683] Free at 0x602702700 of size 204800 I tensorflow/core/common_runtime/bfc_allocator.cc:683] Free at 0x602766700 of size 12435456 I tensorflow/core/common_runtime/bfc_allocator.cc:683] Free at 0x603344400 of size 37632 I tensorflow/core/common_runtime/bfc_allocator.cc:689] Summary of in-use Chunks by size: I tensorflow/core/common_runtime/bfc_allocator.cc:692] 32 Chunks of size 256 totalling 8.0KiB I tensorflow/core/common_runtime/bfc_allocator.cc:692] 2 Chunks of size 1280 totalling 2.5KiB I tensorflow/core/common_runtime/bfc_allocator.cc:692] 5 Chunks of size 3328 totalling 16.2KiB I tensorflow/core/common_runtime/bfc_allocator.cc:692] 5 Chunks of size 4096 totalling 20.0KiB I tensorflow/core/common_runtime/bfc_allocator.cc:692] 2 Chunks of size 31488 totalling 61.5KiB I tensorflow/core/common_runtime/bfc_allocator.cc:692] 4 Chunks of size 40960 totalling 160.0KiB I tensorflow/core/common_runtime/bfc_allocator.cc:692] 1 Chunks of size 80128 totalling 78.2KiB I tensorflow/core/common_runtime/bfc_allocator.cc:692] 4 Chunks of size 204800 totalling 800.0KiB I tensorflow/core/common_runtime/bfc_allocator.cc:692] 3 Chunks of size 12845056 totalling 36.75MiB I tensorflow/core/common_runtime/bfc_allocator.cc:692] 1 Chunks of size 13053184 totalling 12.45MiB I tensorflow/core/common_runtime/bfc_allocator.cc:692] 1 Chunks of size 31360000 totalling 29.91MiB I tensorflow/core/common_runtime/bfc_allocator.cc:692] 1 Chunks of size 1801385216 totalling 1.68GiB I tensorflow/core/common_runtime/bfc_allocator.cc:696] Sum Total of in-use chunks: 1.76GiB I tensorflow/core/common_runtime/bfc_allocator.cc:698] Stats: Limit: 1898266624 InUse: 1885507584 MaxInUse: 1885907712 NumAllocs: 2387902 MaxAllocSize: 1801385216 W tensorflow/core/common_runtime/bfc_allocator.cc:270] **********************************************************xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx W tensorflow/core/common_runtime/bfc_allocator.cc:271] Ran out of memory trying to allocate 957.03MiB. See logs for memory state. W tensorflow/core/framework/op_kernel.cc:968] Resource exhausted: OOM when allocating tensor with shape[10000,32,28,28] Traceback (most recent call last): File ""trainer_deepMnist.py"", line 109, in <module> x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0})) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 559, in eval return _eval_using_default_session(self, feed_dict, self.graph, session) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 3648, in _eval_using_default_session return session.run(tensors, feed_dict) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 710, in run run_metadata_ptr) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 908, in _run feed_dict_string, options, run_metadata) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 958, in _do_run target_list, options, run_metadata) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 978, in _do_call raise type(e)(node_def, op, message) tensorflow.python.framework.errors.ResourceExhaustedError: OOM when allocating tensor with shape[10000,32,28,28] [[Node: Conv2D = Conv2D[T=DT_FLOAT, data_format=""NHWC"", padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/gpu:0""](Reshape, Variable_2/read)]] Caused by op u'Conv2D', defined at: File ""trainer_deepMnist.py"", line 61, in <module> h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1) File ""trainer_deepMnist.py"", line 46, in conv2d return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME') File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py"", line 394, in conv2d data_format=data_format, name=name) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 703, in apply_op op_def=op_def) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2320, in create_op original_op=self._default_original_op, op_def=op_def) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1239, in __init__ self._traceback = _extract_stack() I read some github issues (here, here) related to the same problem but could not understand how I should change my code to solve this problem.",|python|tensorflow|gpu|mnist|,Training,2
39114832,"Tensorflow TypeError: Fetch argument None has invalid type ?. I'm building a RNN loosely based on the TensorFlow tutorial. The relevant parts of my model are as follows: input_sequence = tf.placeholder(tf.float32, [BATCH_SIZE, TIME_STEPS, PIXEL_COUNT + AUX_INPUTS]) output_actual = tf.placeholder(tf.float32, [BATCH_SIZE, OUTPUT_SIZE]) lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(CELL_SIZE, state_is_tuple=False) stacked_lstm = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * CELL_LAYERS, state_is_tuple=False) initial_state = state = stacked_lstm.zero_state(BATCH_SIZE, tf.float32) outputs = [] with tf.variable_scope(""LSTM""): for step in xrange(TIME_STEPS): if step > 0: tf.get_variable_scope().reuse_variables() cell_output, state = stacked_lstm(input_sequence[:, step, :], state) outputs.append(cell_output) final_state = state And the feeding: cross_entropy = tf.reduce_mean(-tf.reduce_sum(output_actual * tf.log(prediction), reduction_indices=[1])) train_step = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE).minimize(cross_entropy) correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(output_actual, 1)) accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) with tf.Session() as sess: sess.run(tf.initialize_all_variables()) numpy_state = initial_state.eval() for i in xrange(1, ITERATIONS): batch = DI.next_batch() print i, type(batch[0]), np.array(batch[1]).shape, numpy_state.shape if i % LOG_STEP == 0: train_accuracy = accuracy.eval(feed_dict={ initial_state: numpy_state, input_sequence: batch[0], output_actual: batch[1] }) print ""Iteration "" + str(i) + "" Training Accuracy "" + str(train_accuracy) numpy_state, train_step = sess.run([final_state, train_step], feed_dict={ initial_state: numpy_state, input_sequence: batch[0], output_actual: batch[1] }) When I run this, I get the following error: Traceback (most recent call last): File ""/home/agupta/Documents/Projects/Image-Recognition-with-LSTM/RNN/feature_tracking/model.py"", line 109, in <module> output_actual: batch[1] File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 698, in run run_metadata_ptr) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 838, in _run fetch_handler = _FetchHandler(self._graph, fetches) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 355, in __init__ self._fetch_mapper = _FetchMapper.for_fetch(fetches) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 181, in for_fetch return _ListFetchMapper(fetch) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 288, in __init__ self._mappers = [_FetchMapper.for_fetch(fetch) for fetch in fetches] File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 178, in for_fetch (fetch, type(fetch))) TypeError: Fetch argument None has invalid type <type 'NoneType'> Perhaps the weirdest part is that this error gets thrown the second iteration, and the first works completely fine. I'm ripping my hair trying to fix this, so any help would be greatly appreciated.",|python|artificial-intelligence|tensorflow|typeerror|recurrent-neural-network|,API,4
39217567,"Keras neural network outputs same result for every input. I tried to implement a feedforward neural network. This is the structure: Input layer: 8 neurons, Hidden layer: 8 neurons and Output layer: 8 neurons. The input data are vectors of 8 bits (1 bit for each neuron of the input layer). The outputs of the neural network are also vectors of 8 bits. So in total the dataset has 256 examples. Example: if given x = [0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0] the output must be y = [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0] This is the implementation: from keras.models import Sequential from keras.layers import Dense import numpy as np import random from math import ceil #Dimension of layers dim = 8 #Generate dataset X = [] for i in range(0,2**dim): n = [float(x) for x in bin(i)[2:]] X.append([0.]*(dim-len(n))+n) y = X[:] random.shuffle(y) X = np.array(X) y = np.array(y) # create model model = Sequential() model.add(Dense(dim, input_dim=dim, init='normal', activation='sigmoid')) model.add(Dense(dim, init='normal', activation='sigmoid')) model.add(Dense(dim, init='normal', activation='sigmoid')) # Compile model model.compile(loss='mse', optimizer='SGD', metrics=['accuracy']) # Fit the model model.fit(X, y, nb_epoch=1000, batch_size=50, verbose=0) # evaluate the model scores = model.evaluate(X, y) print(""%s: %.2f%%"" % (model.metrics_names[1], scores[1]*100)) output = model.predict(X) #Make the output binary for i in range(0, output[:,0].size): for j in range(0, output[0].size): if output[i][j] > 0.5 or output[i][j] == 0.5: output[i][j] = 1 else: output[i][j] = 0 print(output) This is what I get in output: acc: 50.39% [[ 1. 0. 0. ..., 0. 1. 1.] [ 1. 0. 0. ..., 0. 1. 1.] [ 1. 0. 0. ..., 0. 1. 1.] ..., [ 1. 0. 0. ..., 0. 1. 1.] [ 1. 0. 0. ..., 0. 1. 1.] [ 1. 0. 0. ..., 0. 1. 1.]] It seems that all outputs have the same value. So I dont know what's wrong about the configuration. I tried this Cannot train a neural network in keras - stackoverflow which suggests to remove the activation function at the output layer but when I run this I get all output vectors with this value: [ 0. 1. 1. ..., 1. 1. 1.] Any insights on how to make it work?",|python|machine-learning|neural-network|keras|,Training,2
39357454,"Restore checkpoint in Tensorflow - tensor name not found. Trying to run the Inceptionv3 Tensorflow model with the architecture and the checkpoint provided by Google here. My issue is that my script crashes on saver.restore(sess, ""./inception_v3.ckpt"") with the following error: tensorflow.python.framework.errors.NotFoundError: Tensor name ""InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/biases"" not found in checkpoint files ./inception_v3.ckpt Here is my code: import tensorflow as tf import inception_v3 with tf.Session() as sess: image = tf.read_file('./file.jpg') # code to decode, crop, convert jpeg eval_inputs = tf.pack([image]) logits, _ = inception_v3.inception_v3(eval_inputs, num_classes=1001, is_training=False) sess.run(tf.initialize_all_variables()) saver = tf.train.Saver() saver.restore(sess, ""./inception_v3.ckpt"") I get the same errors with the other checkpoint/model combinations so this must be an issue with my code. Not sure what I am doing wrong though. Thank you",|tensorflow|,API,4
39525358,"Neural network accuracy optimization. I have constructed an ANN in keras which has 1 input layer(3 inputs), one output layer (1 output) and two hidden layers with with 12 and 3 nodes respectively. The way i construct and train my network is: from keras.models import Sequential from keras.layers import Dense from sklearn.cross_validation import train_test_split import numpy # fix random seed for reproducibility seed = 7 numpy.random.seed(seed) dataset = numpy.loadtxt(""sorted output.csv"", delimiter="","") # split into input (X) and output (Y) variables X = dataset[:,0:3] Y = dataset[:,3] # split into 67% for train and 33% for test X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=seed) # create model model = Sequential() model.add(Dense(12, input_dim=3, init='uniform', activation='relu')) model.add(Dense(3, init='uniform', activation='relu')) model.add(Dense(1, init='uniform', activation='sigmoid')) # Compile model model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) # Fit the model model.fit(X_train, y_train, validation_data=(X_test,y_test), nb_epoch=150, batch_size=10) Sorted output csv file looks like: so after 150 epochs i get: loss: 0.6932 - acc: 0.5000 - val_loss: 0.6970 - val_acc: 0.1429 My question is: how could i modify my NN in order to achieve higher accuracy?",|python|neural-network|theano|keras|,Training,2
39714374,"NaN results in tensorflow Neural Network. I have this problem that after one iteration nearly all my parameters (cost function, weights, hypothesis function, etc.) output 'NaN'. My code is similar to the tensorflow tutorial MNIST-Expert (https://www.tensorflow.org/versions/r0.9/tutorials/mnist/pros/index.html). I looked for solutions already and so far I tried: reducing the learning rate to nearly zero and setting it to zero, using AdamOptimizer instead of gradient descent, using sigmoid function for the hypothesis function in the last layer and using only numpy functions. I have some negative and zero values in my input data, so I can't use the logarithmic cross entropy instead of the quadratic cost function. The result is the same, butMy input data consist of stresses and strains of soils. import tensorflow as tf import Datafiles3_pv_complete as soil import numpy as np m_training = int(18.0) m_cv = int(5.0) m_test = int(5.0) total_examples = 28 "" range for running "" range_training = xrange(0,m_training) range_cv = xrange(m_training,(m_training+m_cv)) range_test = xrange((m_training+m_cv),total_examples) """""" Using interactive Sessions"""""" sess = tf.InteractiveSession() """""" creating input and output vectors """""" x = tf.placeholder(tf.float32, shape=[None, 11]) y_true = tf.placeholder(tf.float32, shape=[None, 3]) """""" Standard Deviation Calculation"""""" stdev = np.divide(2.0,np.sqrt(np.prod(x.get_shape().as_list()[1:]))) """""" Weights and Biases """""" def weights(shape): initial = tf.truncated_normal(shape, stddev=stdev) return tf.Variable(initial) def bias(shape): initial = tf.truncated_normal(shape, stddev=1.0) return tf.Variable(initial) """""" Creating weights and biases for all layers """""" theta1 = weights([11,7]) bias1 = bias([1,7]) theta2 = weights([7,7]) bias2 = bias([1,7]) ""Last layer"" theta3 = weights([7,3]) bias3 = bias([1,3]) """""" Hidden layer input (Sum of weights, activation functions and bias) z = theta^T * activation + bias """""" def Z_Layer(activation,theta,bias): return tf.add(tf.matmul(activation,theta),bias) """""" Creating the sigmoid function sigmoid = 1 / (1 + exp(-z)) """""" def Sigmoid(z): return tf.div(tf.constant(1.0),tf.add(tf.constant(1.0), tf.exp(tf.neg(z)))) """""" hypothesis functions - predicted output """""" ' layer 1 - input layer ' hyp1 = x ' layer 2 ' z2 = Z_Layer(hyp1, theta1, bias1) hyp2 = Sigmoid(z2) ' layer 3 ' z3 = Z_Layer(hyp2, theta2, bias2) hyp3 = Sigmoid(z3) ' layer 4 - output layer ' zL = Z_Layer(hyp3, theta3, bias3) hypL = tf.add( tf.add(tf.pow(zL,3), tf.pow(zL,2) ), zL) """""" Cost function """""" cost_function = tf.mul( tf.div(0.5, m_training), tf.pow( tf.sub(hypL, y_true), 2)) #cross_entropy = -tf.reduce_sum(y_true*tf.log(hypL) + (1-y_true)*tf.log(1-hypL)) """""" Gradient Descent """""" train_step = tf.train.GradientDescentOptimizer(learning_rate=0.003).minimize(cost_function) """""" Training and Evaluation """""" correct_prediction = tf.equal(tf.arg_max(hypL, 1), tf.arg_max(y_true, 1)) accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) sess.run(tf.initialize_all_variables()) keep_prob = tf.placeholder(tf.float32) """""" Testing - Initialise lists """""" hyp1_test = [] z2_test = [] hyp2_test = [] z3_test = [] hyp3_test = [] zL_test = [] hypL_test = [] cost_function_test =[] complete_error_test = [] theta1_test = [] theta2_test = [] theta3_test = [] bias1_test = [] bias2_test = [] bias3_test = [] """""" ------------------------- """""" complete_error_init = tf.abs(tf.reduce_mean(tf.sub(hypL,y_true),1)) training_error=[] for j in range_training: feedj = {x: soil.input_scale[j], y_true: soil.output_scale[j] , keep_prob: 1.0} """""" ------------------------- """""" 'Testing - adding to list' z2_init = z2.eval(feed_dict=feedj) z2_test.append(z2_init) hyp2_init = hyp2.eval(feed_dict=feedj) hyp2_test.append(hyp2_init) z3_init = z3.eval(feed_dict=feedj) z3_test.append(z3_init) hyp3_init = hyp3.eval(feed_dict=feedj) hyp3_test.append(hyp3_init) zL_init = zL.eval(feed_dict=feedj) zL_test.append(zL_init) hypL_init = hypL.eval(feed_dict=feedj) hypL_test.append(hypL_init) cost_function_init = cost_function.eval(feed_dict=feedj) cost_function_test.append(cost_function_init) complete_error = complete_error_init.eval(feed_dict=feedj) complete_error_test.append(complete_error) print 'number iterations: %g, error (S1, S2, S3): %g, %g, %g' % (j, complete_error[0], complete_error[1], complete_error[2]) theta1_init = theta1.eval() theta1_test.append(theta1_init) theta2_init = theta2.eval() theta2_test.append(theta2_init) theta3_init = theta3.eval() theta3_test.append(theta3_init) bias1_init = bias1.eval() bias1_test.append(bias1_init) bias2_init = bias2.eval() bias2_test.append(bias2_init) bias3_init = bias3.eval() bias3_test.append(bias3_init) """""" ------------------------- """""" train_accuracy = accuracy.eval(feed_dict=feedj) print(""step %d, training accuracy %g"" % (j, train_accuracy)) train_step.run(feed_dict=feedj) training_error.append(1 - train_accuracy) cv_error=[] for k in range_cv: feedk = {x: soil.input_scale[k], y_true: soil.output_scale[k] , keep_prob: 1.0} cv_accuracy = accuracy.eval(feed_dict=feedk) print(""cross-validation accuracy %g"" % cv_accuracy) cv_error.append(1-cv_accuracy) for l in range_test: print(""test accuracy %g"" % accuracy.eval(feed_dict={x: soil.input_matrixs[l], y_true: soil.output_matrixs[l], keep_prob: 1.0})) The last weeks I was working on a Unit-model for this problem, but the same output occurred. I have no idea what to try next. Hope someone can help me. Edit: I checked some parameters in detail again. The hypothesis function (hyp) and activation function (z) for layer 3 and 4 (last layer) have the same entries for each data point, i.e. the same value in each line for one column.",|python|neural-network|tensorflow|nan|,Training,2
39810655,"Why is my Keras model performing so poorly on the Iris dataset?. I'm working through this Keras tutorial and I'm finding something interesting. I've trained my logistic regression model using the sklearn, and it performs fairly well: import seaborn as sns import numpy as np from sklearn.cross_validation import train_test_split from sklearn.linear_model import LogisticRegressionCV from keras.models import Sequential from keras.layers.core import Dense, Activation from keras.utils import np_utils # Load the iris dataset from seaborn. iris = sns.load_dataset(""iris"") # Use the first 4 variables to predict the species. X, y = iris.values[:, 0:4], iris.values[:, 4] # Split both independent and dependent variables in half # for cross-validation train_X, test_X, train_y, test_y = train_test_split(X, y, train_size=0.5, random_state=0) # Train a scikit-learn log-regression model lr = LogisticRegressionCV() lr.fit(train_X, train_y) # Test the model. Print the accuracy on the test data pred_y = lr.predict(test_X) print(""Accuracy is {:.2f}"".format(lr.score(test_X, test_y))) # Accuracy is 0.83 83% is quite good, but using deep learning we should be able to do better. I train a Keras model... # Define a one-hot encoding of variables in an array. def one_hot_encode_object_array(arr): '''One hot encode a numpy array of objects (e.g. strings)''' uniques, ids = np.unique(arr, return_inverse=True) return np_utils.to_categorical(ids, len(uniques)) # One-hot encode the train and test y's train_y_ohe = one_hot_encode_object_array(train_y) test_y_ohe = one_hot_encode_object_array(test_y) # Build the keras model model = Sequential() # 4 features in the input layer (the four flower measurements) # 16 hidden units model.add(Dense(16, input_shape=(4,))) model.add(Activation('sigmoid')) # 3 classes in the ouput layer (corresponding to the 3 species) model.add(Dense(3)) model.add(Activation('softmax')) model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # Train the keras model model.fit(train_X, train_y_ohe, verbose=0, batch_size=1) # Test the model. Print the accuracy on the test data loss, accuracy = model.evaluate(test_X, test_y_ohe, verbose=0) print(""Accuracy is {:.2f}"".format(accuracy)) # Accuracy is 0.60???? When I train a Keras model, my accuracy is actually worse than my logistic regression model. While this makes sense for some data, incredibly linearly separable data (like iris) should be very learnable for a Keras sequential model. I have tried increasing the number of hidden layers to 32, 64, and 128 and there was no improvement on the accuracy. Below shows the Iris data (specifically the independent variables) as a function of the species (the dependent variable): Why is my model performing so poorly?",|python-3.x|machine-learning|scikit-learn|keras|,Training,2
40179209,"Tensorflow - Testing a mnist neural net with my own images. I'm trying to write a script that will allow me to draw an image of a digit and then determine what digit it is with a model trained on MNIST. Here is my code: import random import image from tensorflow.examples.tutorials.mnist import input_data import tensorflow as tf import numpy as np import scipy.ndimage mnist = input_data.read_data_sets( ""MNIST_data/"", one_hot=True ) x = tf.placeholder(tf.float32, [None, 784]) W = tf.Variable(tf.zeros([784, 10])) b = tf.Variable(tf.zeros([10])) y = tf.nn.softmax(tf.matmul(x, W) + b) y_ = tf.placeholder(tf.float32, [None, 10]) cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1])) train_step = tf.train.GradientDescentOptimizer(0.5).minimize (cross_entropy) init = tf.initialize_all_variables() sess = tf.Session() sess.run(init) for i in range( 1000 ): batch_xs, batch_ys = mnist.train.next_batch( 1000 ) sess.run(train_step, feed_dict= {x: batch_xs, y_: batch_ys}) print (""done with training"") data = np.ndarray.flatten(scipy.ndimage.imread(""im_01.jpg"", flatten=True)) result = sess.run(tf.argmax(y,1), feed_dict={x: [data]}) print (' '.join(map(str, result))) For some reason the results are always wrong but has a 92% accuracy when I use the standard testing method. I think the problem might be how I encoded the image: data = np.ndarray.flatten(scipy.ndimage.imread(""im_01.jpg"", flatten=True)) I tried looking in the tensorflow code for the next_batch() function to see how they did it, but I have no idea how I can compare against my approach. The problem might be somewhere else too. Any help to make the accuracy 80+% would be greatly appreciated.",|python|numpy|tensorflow|mnist|,Training,2
40403297,"Specify gpu in Tensorflow code: /gpu:0 is always working?. I have 3 graphics cards in my workstation, one of them is Quadro K620, and the other two are Titan X. Now I would like to run my tensorflow code in one of the graphics card, so that I can leave the others idle for another task. However, regardless of setting tf.device('/gpu:0') or tf.device('/gpu:1'), I found the 1st Titan X graphics card is always working, I don't know why. import argparse import os import time import tensorflow as tf import numpy as np import cv2 from Dataset import Dataset from Net import Net FLAGS = None if __name__ == ""__main__"": parser = argparse.ArgumentParser() parser.add_argument('--foldername', type=str, default='./data-large/') parser.add_argument('--batch_size', type=int, default=100) parser.add_argument('--num_epoches', type=int, default=100) parser.add_argument('--learning_rate', type=float, default=0.5) FLAGS = parser.parse_args() net = Net(FLAGS.batch_size, FLAGS.learning_rate) with tf.Graph().as_default(): # Dataset is a class for encapsulate the input pipeline dataset = Dataset(foldername=FLAGS.foldername, batch_size=FLAGS.batch_size, num_epoches=FLAGS.num_epoches) images, labels = dataset.samples_train ## The following code defines the network and train with tf.device('/gpu:0'): # <==== THIS LINE logits = net.inference(images) loss = net.loss(logits, labels) train_op = net.training(loss) init_op = tf.group(tf.initialize_all_variables(), tf.initialize_local_variables()) sess = tf.Session() sess.run(init_op) coord = tf.train.Coordinator() threads = tf.train.start_queue_runners(sess=sess, coord=coord) start_time = time.time() try: step = 0 while not coord.should_stop(): _, loss_value = sess.run([train_op, loss]) step = step + 1 if step % 100 == 0: format_str = ('step %d, loss = %.2f, time: %.2f seconds') print(format_str % (step, loss_value, (time.time() - start_time))) start_time = time.time() except tf.errors.OutOfRangeError: print('done') finally: coord.request_stop() coord.join(threads) sess.close() Regarding to the line ""<=== THIS LINE:"" If I set tf.device('/gpu:0'), the monitor says: |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 Quadro K620 Off | 0000:03:00.0 On | N/A | | 34% 45C P0 2W / 30W | 404MiB / 1993MiB | 5% Default | +-------------------------------+----------------------+----------------------+ | 1 GeForce GTX TIT... Off | 0000:04:00.0 Off | N/A | | 22% 39C P2 100W / 250W | 11691MiB / 12206MiB | 8% Default | +-------------------------------+----------------------+----------------------+ | 2 GeForce GTX TIT... Off | 0000:81:00.0 Off | N/A | | 22% 43C P2 71W / 250W | 111MiB / 12206MiB | 0% Default | +-------------------------------+----------------------+----------------------+ showing the 1st Titan X card is working. If I set tf.device('/gpu:1'), the monitor says: |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 Quadro K620 Off | 0000:03:00.0 On | N/A | | 34% 45C P0 2W / 30W | 411MiB / 1993MiB | 3% Default | +-------------------------------+----------------------+----------------------+ | 1 GeForce GTX TIT... Off | 0000:04:00.0 Off | N/A | | 22% 52C P2 73W / 250W | 11628MiB / 12206MiB | 12% Default | +-------------------------------+----------------------+----------------------+ | 2 GeForce GTX TIT... Off | 0000:81:00.0 Off | N/A | | 22% 42C P2 71W / 250W | 11628MiB / 12206MiB | 0% Default | +-------------------------------+----------------------+----------------------+ showing that the two Titan X cards are working, not the 2nd Titan X alone. So any reason behind this and how to specify the gpu I want my program to run in?",|tensorflow|multi-gpu|,GPU Usage,3
40511562,TensorFlow 'module' object has no attribute 'global_variables_initializer'. I'm new to Tensorflow I'm running a Deep learning Assignment from Udacity on iPython notebook. link And it has an error. AttributeError Traceback (most recent call last) `<ipython-input-18-3446420b5935>` in `<module>`() 2 3 with tf.Session(graph=graph) as session: ----> 4 tf.global_variables_initializer().run() AttributeError: 'module' object has no attribute 'global_variables_initializer' Please help! How can I fix this? Thank you.,|python|tensorflow|deep-learning|word2vec|,API,4
40578598,"Why can't i train my model. I am currently trying to make a linear regression network capable of mapping my input data to a desired output data. My input and output is currently stored as list of matrices stored as numpy.ndarray. The input dimension for the regression network is 400 and the output dimension for the regression network is 13. Each matrix on the input side has dimensions [400,x] => output by print input[0].shape Each matrix on the output side has dimensions [13,x] => output by print output[0].shape The network i've currently defined looks like this: print ""Training!"" model = Sequential() model.add(Dense(output_dim=13, input_dim=400, init=""normal"")) model.add(Activation(""relu"")) print ""Compiling"" model.compile(loss='mean_squared_error', optimizer='sgd') model.fit(input,output,verbose=1) Problem here is at the train stage. It somehow takes very long time, and no information is provided about the progress. It seems like the system stalls, and terminated with this error message. Traceback (most recent call last): File ""tensorflow_datapreprocess_mfcc_extraction_rnn.py"", line 169, in <module> model.fit(train_set_data,train_set_output,verbose=1) File ""/usr/local/lib/python2.7/dist-packages/keras/models.py"", line 620, in fit sample_weight=sample_weight) File ""/usr/local/lib/python2.7/dist-packages/keras/engine/training.py"", line 1034, in fit batch_size=batch_size) File ""/usr/local/lib/python2.7/dist-packages/keras/engine/training.py"", line 961, in _standardize_user_data exception_prefix='model input') File ""/usr/local/lib/python2.7/dist-packages/keras/engine/training.py"", line 51, in standardize_input_data '...') Exception: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 arrays but instead got the following list of 270 arrays: [array([[ -1.52587891e-04, 3.05175781e-05, -1.52587891e-04, -5.18798828e-04, 3.05175781e-05, -3.96728516e-04, 1.52587891e-04, 3.35693359e-04, -9.15527344e-05, 3.3... I guess the error might be the way i parse my input data, which to me is black magic. The documentation states that https://keras.io/models/model/ fit(self, x, y, batch_size=32, nb_epoch=10, verbose=1, callbacks=[], validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None) x: Numpy array of training data, or list of Numpy arrays if the model has multiple inputs. If all inputs in the model are named, you can also pass a dictionary mapping input names to Numpy arrays. y: Numpy array of target data, or list of Numpy arrays if the model has multiple outputs. If all outputs in the model are named, you can also pass a dictionary mapping output names to Numpy arrays. Which is what i have a list of Numpy arrays? how it knows it is the rows it has to read in?... I don't know. I guess numpy.ndarrays are stored as list of numpy.arrays in which each arrays is a row.? It seems so according to this simple example: Input: import numpy as np lis = [] output_data = np.random.rand(5,3) output_data_1 = np.random.rand(5,2) lis.append(output_data) lis.append(output_data_1) print output_data.shape print output_data_1.shape print lis Output: (5, 3) (5, 2) [array([[ 0.15509364, 0.20140267, 0.13678847], [ 0.27932102, 0.38430659, 0.87265863], [ 0.01053336, 0.28403731, 0.19749507], [ 0.95775409, 0.96032907, 0.46996195], [ 0.29515174, 0.74466708, 0.78720968]]), array([[ 0.34216058, 0.74972468], [ 0.97262113, 0.84451951], [ 0.72230052, 0.30852572], [ 0.47586734, 0.03382701], [ 0.37998285, 0.80772875]])] So what am I doing wrong? Why can't i pass the data into model?",|python|arrays|list|numpy|keras|,Tensors&Inputs,1
40726039,"Tensorflow: CUDA_VISIBLE_DEVICES doesn't seem to work. When I run my python script with CUDA_VISIBLE_DEVICES=2, Tensorflow still shows the following: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:86:00.0) Consequently, my code fails with the following message: Could not satisfy explicit device specification '/device:GPU:2' because no devices matching that specification are registered in this process; available devices: /job:localhost/replica:0/task:0/cpu:0, /job:localhost/replica:0/task:0/gpu:0 Could someone please explain what must be going on?",|cuda|tensorflow|gpu|,GPU Usage,3
40827085,"Tensorflow - import error in CIFAR tutorial. Recently, I installed tensorflow and got python import error in CIFAR tutorial. I'm using Mac OS X, CPU only, Python 2.7. $ python cifar10_train.py Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes. Traceback (most recent call last): File ""cifar10_train.py"", line 120, in tf.app.run() File ""/Users/sunwoo/tensorflow/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 30, in run sys.exit(main(sys.argv[:1] + flags_passthrough)) File ""cifar10_train.py"", line 116, in main train() File ""cifar10_train.py"", line 76, in train class _LoggerHook(tf.train.SessionRunHook): AttributeError: 'module' object has no attribute 'SessionRunHook' How can I import tf.train.SessionRunHook?",|tensorflow|importerror|,API,4
40857445,"Keras - getting Labels as strings from directory structure. I'm trying to write a CNN in keras. My dataset consists of 20,000 images in 250 classes with the following folder structure: dataset/ class1/ 1.png 2.png ... class2/ ... ... From my understanding the easiest way to load the images/labels is to use a combination of ImageDataGenerator and flow_from_directory(). Minimal working example: from keras.layers import Activation, Convolution2D, MaxPooling2D from keras.models import Sequential from keras.preprocessing.image import ImageDataGenerator if __name__ == '__main__': # input image dimensions img_rows, img_cols = 225, 225 input_shape = (img_rows, img_cols, 1) model = Sequential() model.add(Convolution2D(64, 15, 15, input_shape=input_shape, subsample=(3, 3))) model.add(Activation('relu')) model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2))) model.compile(loss='hinge', optimizer='adadelta', metrics=['accuracy']) data = ImageDataGenerator() train_data = data.flow_from_directory(directory='dataset', color_mode='grayscale', target_size=(img_rows, img_cols)) model.fit_generator(train_data, 100, 12) This however stops with the following error: Exception: Error when checking model target: expected maxpooling2d_1 to have 4 dimensions, but got array with shape (32, 250) with 32 being the batch_size and 250 the number of classes. Is this a problem with how I retrieve my Images/Labels?",|python|machine-learning|deep-learning|keras|,Tensors&Inputs,1
41327601,"Why is binary_crossentropy more accurate than categorical_crossentropy for multiclass classification in Keras?. I'm learning how to create convolutional neural networks using Keras. I'm trying to get a high accuracy for the MNIST dataset. Apparently categorical_crossentropy is for more than 2 classes and binary_crossentropy is for 2 classes. Since there are 10 digits, I should be using categorical_crossentropy. However, after training and testing dozens of models, binary_crossentropy consistently outperforms categorical_crossentropy significantly. On Kaggle, I got 99+% accuracy using binary_crossentropy and 10 epochs. Meanwhile, I can't get above 97% using categorical_crossentropy, even using 30 epochs (which isn't much, but I don't have a GPU, so training takes forever). Here's what my model looks like now: model = Sequential() model.add(Convolution2D(100, 5, 5, border_mode='valid', input_shape=(28, 28, 1), init='glorot_uniform', activation='relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Convolution2D(100, 3, 3, init='glorot_uniform', activation='relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Dropout(0.3)) model.add(Flatten()) model.add(Dense(100, init='glorot_uniform', activation='relu')) model.add(Dropout(0.3)) model.add(Dense(100, init='glorot_uniform', activation='relu')) model.add(Dropout(0.3)) model.add(Dense(10, init='glorot_uniform', activation='softmax')) model.compile(loss='binary_crossentropy', optimizer='adamax', metrics=['accuracy'])",|machine-learning|keras|neural-network|deep-learning|conv-neural-network|,Training,2
41530218,"Keras: Loading minibatches from HDF5 and CSV. I have a large dataset, too large to fit into RAM, which is available either as HDF5 or CSV. How can I feed it into Keras in minibatches? Also, will this shuffle it for me, or do I need to pre-shuffle the dataset? (I'm also interested in this when the input is a Numpy recarray; since Keras I believe wants the input to be a ndarray.) And, if I want to do some lightweight preprocessing in Keras before learning (e.g. apply a few Python functions to the data to change the representation), hcan that be added?",|python|neural-network|keras|,Training,2
41482913,"'module' object has no attribute 'SummaryWriter'. I'm using Tensorflow version 0.12.head with Python 2.7 on a linux CentOS 7 and when I run this: import tensorflow as tf a = tf.constant(5, name=""input_a"") b = tf.constant(3, name=""input_b"") c = tf.mul(a, b, name=""mul_c"") d = tf.add(a, b, name=""add_d"") e = tf.add(c, d, name=""add_e"") sess = tf.Session() output = sess.run(e) writer = tf.train.SummaryWriter('./my_graph', sess.graph) I get this error: AttributeError Traceback (most recent call last) <ipython-input-6-29c037e85eec> in <module>() ----> 1 writer = tf.train.SummaryWriter('./my_graph', sess.graph) AttributeError: 'module' object has no attribute 'SummaryWriter' I have run these two commands because there is bug issue on Github for the same problem: >>> import six >>> print(six.__version__) 1.10.0 >>> print(dir(six.moves.queue)) ['Empty', 'Full', 'LifoQueue', 'PriorityQueue', 'Queue', '__all__', '__builtins__', '__doc__', '__file__', '__name__', '__package__', '_threading', '_time', 'deque', 'heapq'] >>> print(six.moves.queue.__file__) /usr/lib64/python2.7/Queue.pyc I'm new in Python and in Tensorflow. Do you know how can I fix this error? I have changed SummaryWriter with FileWriter: writer = tf.train.FileWriter('./my_graph', sess.graph) And I get the same error but with FileWriter function: AttributeError Traceback (most recent call last) <ipython-input-8-daa50ea2b8f9> in <module>() ----> 1 writer = tf.train.FileWriter('./my_graph', sess.graph) AttributeError: 'module' object has no attribute 'FileWriter' I have also run it in a terminal and I get the same result: [VansFannel@localhost ~]$ python Python 2.7.5 (default, Nov 6 2016, 00:28:07) [GCC 4.8.5 20150623 (Red Hat 4.8.5-11)] on linux2 Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import tensorflow as tf W tensorflow/core/platform/cpu_feature_guard.cc:95] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations. W tensorflow/core/platform/cpu_feature_guard.cc:95] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations. >>> a = tf.constant(5, name=""input_a"") >>> b = tf.constant(3, name=""input_b"") >>> c = tf.mul(a, b, name=""mul_c"") >>> d = tf.add(a, b, name=""add_d"") >>> e = tf.add(c, d, name=""add_e"") >>> sess = tf.Session() >>> output = sess.run(e) >>> writer = tf.train.FileWriter('./my_graph', sess.graph) Traceback (most recent call last): File ""<stdin>"", line 1, in <module> AttributeError: 'module' object has no attribute 'FileWriter' >>>",|python|python-2.7|tensorflow|,API,4
41600519,"Accuracy does not go up on a keras model. I'm trying to train a model on data from the Higgs Boson challenge on kaggle. The first thing I decided to do was to create a simple keras model. I've tried different amount and width of layers, different cost functions, different optimizers different functions in neurons, but the accuracy on the training set is always between 0.65-0.7 range. I don't really understand why. Here's my an example of a model that worked so weird: from keras.layers import Dense, merge, Activation, Dropout from keras.models import Model from keras.models import Sequential from keras.optimizers import SGD model = Sequential() model.add(Dense(600, input_shape=(30,),activation=""relu"")) model.add(Dropout(0.5)) model.add(Dense(400, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(100, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(1, activation='sigmoid')) sgd = SGD(lr=0.01, decay=1e-6) model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy']) model.fit(train,labels,nb_epoch=1,batch_size=1) I also tried larger models and got such an accuracy too. Please tell me what I am doing wrong. EDIT I have tried training this model with 100 epochs and the batch size 0f 100 and got loss equal to 4.9528 and accuracy to 0.6924 again. And it always outputs zero for every example.",|machine-learning|neural-network|keras|,Training,2
41651628,"Negative dimension size caused by subtracting 3 from 1 for 'Conv2D'. I'm using Keras with Tensorflow as backend , here is my code: import numpy as np np.random.seed(1373) import tensorflow as tf tf.python.control_flow_ops = tf import os from keras.datasets import mnist from keras.models import Sequential from keras.layers.core import Dense, Dropout, Activation, Flatten from keras.layers.convolutional import Convolution2D, MaxPooling2D from keras.utils import np_utils batch_size = 128 nb_classes = 10 nb_epoch = 12 img_rows, img_cols = 28, 28 nb_filters = 32 nb_pool = 2 nb_conv = 3 (X_train, y_train), (X_test, y_test) = mnist.load_data() print(X_train.shape[0]) X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols) X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols) X_train = X_train.astype('float32') X_test = X_test.astype('float32') X_train /= 255 X_test /= 255 print('X_train shape:', X_train.shape) print(X_train.shape[0], 'train samples') print(X_test.shape[0], 'test samples') Y_train = np_utils.to_categorical(y_train, nb_classes) Y_test = np_utils.to_categorical(y_test, nb_classes) model = Sequential() model.add(Convolution2D(nb_filters, nb_conv, nb_conv, border_mode='valid', input_shape=(1, img_rows, img_cols))) model.add(Activation('relu')) model.add(Convolution2D(nb_filters, nb_conv, nb_conv)) model.add(Activation('relu')) model.add(MaxPooling2D(pool_size=(nb_pool, nb_pool))) model.add(Dropout(0.25)) model.add(Flatten()) model.add(Dense(128)) model.add(Activation('relu')) model.add(Dropout(0.5)) model.add(Dense(nb_classes)) model.add(Activation('softmax')) model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=[""accuracy""]) model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1, validation_data=(X_test, Y_test)) score = model.evaluate(X_test, Y_test, verbose=0) print('Test score:', score[0]) print('Test accuracy:', score[1]) and Trackback error: Using TensorFlow backend. 60000 ('X_train shape:', (60000, 1, 28, 28)) (60000, 'train samples') (10000, 'test samples') Traceback (most recent call last): File ""mnist.py"", line 154, in <module> input_shape=(1, img_rows, img_cols))) File ""/usr/local/lib/python2.7/dist-packages/keras/models.py"", line 276, in add layer.create_input_layer(batch_input_shape, input_dtype) File ""/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py"", line 370, in create_input_layer self(x) File ""/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py"", line 514, in __call__ self.add_inbound_node(inbound_layers, node_indices, tensor_indices) File ""/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py"", line 572, in add_inbound_node Node.create_node(self, inbound_layers, node_indices, tensor_indices) File ""/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py"", line 149, in create_node output_tensors = to_list(outbound_layer.call(input_tensors[0], mask=input_masks[0])) File ""/usr/local/lib/python2.7/dist-packages/keras/layers/convolutional.py"", line 466, in call filter_shape=self.W_shape) File ""/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py"", line 1579, in conv2d x = tf.nn.conv2d(x, kernel, strides, padding=padding) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py"", line 396, in conv2d data_format=data_format, name=name) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 759, in apply_op op_def=op_def) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2242, in create_op set_shapes_for_outputs(ret) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1617, in set_shapes_for_outputs shapes = shape_func(op) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1568, in call_with_requiring return call_cpp_shape_fn(op, require_shape_fn=True) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.py"", line 610, in call_cpp_shape_fn debug_python_shape_fn, require_shape_fn) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.py"", line 675, in _call_cpp_shape_fn_impl raise ValueError(err.message) ValueError: Negative dimension size caused by subtracting 3 from 1 for 'Conv2D' (op: 'Conv2D') with input shapes: [?,1,28,28], [3,3,28,32]. First I saw some answers that problem is with Tensorflow version so I upgrade Tensorflow to 0.12.0, but still exist , is that problem with network or I missing something, what should input_shape looks like? Update Here is ./keras/keras.json: { ""image_dim_ordering"": ""tf"", ""epsilon"": 1e-07, ""floatx"": ""float32"", ""backend"": ""tensorflow"" }",|python|tensorflow|keras|,Tensors&Inputs,1
41823555,"Multi scale CNN Network Python Keras. I create a multi-scale CNN in Python Keras. The network architecture is similar to the diagram. Here, the same image is fed to 3 CNN's with different architectures. The weights are NOT shared. The code I wrote is available below. The issue is that when I run this even with 10 images in train_dir the network takes about 40GB RAM and finally is killed by the OS. This is ""Out of memory ERROR"". I am running this on CPU. Any idea why this happens in Keras? I am using Theano-0.9.0.dev5 | Keras-1.2.1 | Python 2.7.12 | OSX Sierra 10.12.3 (16D32) ## Multi-scale CNN in Keras Python ## https://i.stack.imgur.com/2H4xD.png #main CNN model - CNN1 main_model = Sequential() main_model.add(Convolution2D(32, 3, 3, input_shape=(3, 224, 224))) main_model.add(Activation('relu')) main_model.add(MaxPooling2D(pool_size=(2, 2))) main_model.add(Convolution2D(32, 3, 3)) main_model.add(Activation('relu')) main_model.add(MaxPooling2D(pool_size=(2, 2))) main_model.add(Convolution2D(64, 3, 3)) main_model.add(Activation('relu')) main_model.add(MaxPooling2D(pool_size=(2, 2))) # the main_model so far outputs 3D feature maps (height, width, features) main_model.add(Flatten()) #lower features model - CNN2 lower_model1 = Sequential() lower_model1.add(Convolution2D(32, 3, 3, input_shape=(3, 224, 224))) lower_model1.add(Activation('relu')) lower_model1.add(MaxPooling2D(pool_size=(2, 2))) lower_model1.add(Flatten()) #lower features model - CNN3 lower_model2 = Sequential() lower_model2.add(Convolution2D(32, 3, 3, input_shape=(3, 224, 224))) lower_model2.add(Activation('relu')) lower_model2.add(MaxPooling2D(pool_size=(2, 2))) lower_model2.add(Flatten()) #merged model merged_model = Merge([main_model, lower_model1, lower_model2], mode='concat') final_model = Sequential() final_model.add(merged_model) final_model.add(Dense(64)) final_model.add(Activation('relu')) final_model.add(Dropout(0.5)) final_model.add(Dense(1)) final_model.add(Activation('sigmoid')) final_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy']) print 'About to start training merged CNN' train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True) train_generator = train_datagen.flow_from_directory(train_data_dir, target_size=(224, 224), batch_size=32, class_mode='binary') test_datagen = ImageDataGenerator(rescale=1./255) test_generator = test_datagen.flow_from_directory(args.test_images, target_size=(224, 224), batch_size=32, class_mode='binary') final_train_generator = zip(train_generator, train_generator, train_generator) final_test_generator = zip(test_generator, test_generator, test_generator) final_model.fit_generator(final_train_generator, samples_per_epoch=nb_train_samples, nb_epoch=nb_epoch, validation_data=final_test_generator, nb_val_samples=nb_validation_samples)",|python|neural-network|deep-learning|keras|conv-neural-network|,Training,2
41813665,"Tensorflow Slim: TypeError: Expected int32, got list containing Tensors of type '_Message' instead. I am following this tutorial for learning TensorFlow Slim but upon running the following code for Inception: import numpy as np import os import tensorflow as tf import urllib2 from datasets import imagenet from nets import inception from preprocessing import inception_preprocessing slim = tf.contrib.slim batch_size = 3 image_size = inception.inception_v1.default_image_size checkpoints_dir = '/tmp/checkpoints/' with tf.Graph().as_default(): url = 'https://upload.wikimedia.org/wikipedia/commons/7/70/EnglishCockerSpaniel_simon.jpg' image_string = urllib2.urlopen(url).read() image = tf.image.decode_jpeg(image_string, channels=3) processed_image = inception_preprocessing.preprocess_image(image, image_size, image_size, is_training=False) processed_images = tf.expand_dims(processed_image, 0) # Create the model, use the default arg scope to configure the batch norm parameters. with slim.arg_scope(inception.inception_v1_arg_scope()): logits, _ = inception.inception_v1(processed_images, num_classes=1001, is_training=False) probabilities = tf.nn.softmax(logits) init_fn = slim.assign_from_checkpoint_fn( os.path.join(checkpoints_dir, 'inception_v1.ckpt'), slim.get_model_variables('InceptionV1')) with tf.Session() as sess: init_fn(sess) np_image, probabilities = sess.run([image, probabilities]) probabilities = probabilities[0, 0:] sorted_inds = [i[0] for i in sorted(enumerate(-probabilities), key=lambda x:x[1])] plt.figure() plt.imshow(np_image.astype(np.uint8)) plt.axis('off') plt.show() names = imagenet.create_readable_names_for_imagenet_labels() for i in range(5): index = sorted_inds[i] print('Probability %0.2f%% => [%s]' % (probabilities[index], names[index])) I seem to be getting this set of errors: Traceback (most recent call last): File ""DA_test_pred.py"", line 24, in <module> logits, _ = inception.inception_v1(processed_images, num_classes=1001, is_training=False) File ""/home/deepankar1994/Desktop/MTP/TensorFlowEx/TFSlim/models/slim/nets/inception_v1.py"", line 290, in inception_v1 net, end_points = inception_v1_base(inputs, scope=scope) File ""/home/deepankar1994/Desktop/MTP/TensorFlowEx/TFSlim/models/slim/nets/inception_v1.py"", line 96, in inception_v1_base net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3]) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py"", line 1053, in concat dtype=dtypes.int32).get_shape( File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 651, in convert_to_tensor as_ref=False) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 716, in internal_convert_to_tensor ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.py"", line 176, in _constant_tensor_conversion_function return constant(v, dtype=dtype, name=name) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.py"", line 165, in constant tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape)) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.py"", line 367, in make_tensor_proto _AssertCompatible(values, dtype) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.py"", line 302, in _AssertCompatible (dtype.name, repr(mismatch), type(mismatch).__name__)) TypeError: Expected int32, got list containing Tensors of type '_Message' instead. This is strange because all of this code is from their official guide. I am new to TF and any help would be appreciated.",|python|machine-learning|tensorflow|computer-vision|deep-learning|,API,4
41942538,"Tensorflow GPU memory error try-except not catching the error. I am trying to run a hyperparameter optimization (using spearmint) on a big network with lots of trainable variables. I am worried that when I try a network with the number of hidden units too large, the Tensorflow will throw a GPU memory error. I was wondering if there is a way of catching the GPU memory error thrown by Tensorflow and skip the batch of hyperparameters that causes the memory error. For example, I would like something like import tensorflow as tf dim = [100000,100000] X = tf.Variable( tf.truncated_normal( dim, stddev=0.1 ) ) with tf.Session() as sess: try: tf.global_variables_initializer().run() except Exception as e : print e When I try above to test the memory error exception, the code breaks and just prints the GPU memory error and does not progress to the except block.",|python|tensorflow|gpu|deep-learning|except|,GPU Usage,3
41910617,"Keras: ValueError: decode_predictions expects a batch of predictions. I'm using keras' pre-trained model VGG16, following this link: Keras VGG16 I'm trying to decode the prediction output into word of what's in the image: model = VGG16(weights='imagenet', include_top=False) img_path = 'elephant.jpg' img = image.load_img(img_path, target_size=(224, 224)) x = image.img_to_array(img) x = np.expand_dims(x, axis=0) x = preprocess_input(x) features = model.predict(x) (inID, label) = decode_predictions(features)[0] #ERROR HERE The full error is: ValueError: decode_predictions expects a batch of predictions (i.e. a 2D array of shape (samples, 1000)). Found array with shape: (1, 7, 7, 512) Any comments or suggestion is highly appreciated. Thank you.",|numpy|machine-learning|neural-network|deep-learning|keras|,Tensors&Inputs,1
41977498,"RNN is not overfitting on simple data. I am trying to predict class for each number in input vector. There are 3 classes. Class 1, if input value changed from 0 to 1. Class 2, if it changed from 1 to 0. Class 0 otherwise. After second epoch and onward accuracy is stuck at 0.8824. Higher number of training epoch does not change anything. I have tried switching LSTM to GRU or SimpleRNN, this changes nothing. I also tried to generate longer input vectors and multiple batches, same without success. Only thing that helped is increasing size of LSTM layers to 128, adding three TimeDistributedDense(128, relu) layers and BatchNormalization after each layer including LSTM. But it looks like overkill for such a simple problem and not giving perfect results anyway. I've spend more than a day trying to make it work. What could be a problem? Thanks! # complete code for training from keras.models import Sequential from keras.layers import Dense, LSTM, TimeDistributed from keras.utils.np_utils import to_categorical import numpy as np np.random.seed(1337) X = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]) Y = np.array([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0]) Y_cat = to_categorical(Y, 3).reshape((1, len(X), 3)) X_r = X.reshape((1, len(X), 1)) model = Sequential() model.add(LSTM(32, input_dim=1, return_sequences=True)) model.add(LSTM(32, return_sequences=True)) model.add(LSTM(32, return_sequences=True)) model.add(TimeDistributed(Dense(3, activation='softmax'))) model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy']) model.fit(X_r, Y_cat, nb_epoch=10) model.predict_classes(X_r) # will print array filled with zeros",|python|machine-learning|neural-network|keras|recurrent-neural-network|,Training,2
42064941,"TensorFlow float16 support is broken. Recently I tried to train a CNN in TF using float16. To my surprise it is broken in various ways even though TF claims to support it for a while. For example, float16 optimization causes NaN loss already on the second step regardless of the network. import tensorflow as tf import numpy as np slim = tf.contrib.slim dtype = tf.float16 shape = (4, 16, 16, 3) inpt = tf.placeholder(dtype, shape, name='input') net = slim.conv2d(inpt, 16, [3, 3], scope='conv', weights_initializer=tf.zeros_initializer(), # normalizer_fn=slim.batch_norm ) loss = tf.reduce_mean(net) opt = tf.train.AdamOptimizer(1e-3) train_op = slim.learning.create_train_op(loss, opt) val = np.zeros(shape) with tf.Session() as sess: sess.run(tf.global_variables_initializer()) for i in range(2): print(sess.run(train_op, feed_dict={inpt: val})) To my understanding it is clearly a bug: I apply zero convolutions on zero input, I should get zero gradients that don't change zero loss. It just can't diverge. If dtype is float32 it works. NaN loss occurs both on CPU and GPU versions. However, I was dismissed in GH issues, a random dude closed this issue saying that it is intended behaviour: https://github.com/tensorflow/tensorflow/issues/7226 If you uncomment the line with BN, it will break already on graph construction time because BN assumes moving averages (and beta, gamma) are always float32 and does not cast them properly. This issue was also closed and apparently ignored: https://github.com/tensorflow/tensorflow/issues/7164 I feel like I am talking to a first line IT support of an ISP. Can anybody explain how I should train with float16 when such a simple ""network"" fails horribly? And what is the recommended way to report bugs now?",|tensorflow|,Training,2
42217059,"tensorflow:AttributeError: 'module' object has no attribute 'mul'. I have used tensorflow for ONE day,but there comes some troubles,when I import tensorflow, there would be AttributeError: 'module' object has no attribute 'XXXXXX' Environment I use ubuntu14.04, python2.7, CUDA toolkit 8.0 and CuDNN v5. And versions of my six and protobuf are: Name: six Version: 1.10.0 Location: /usr/local/lib/python2.7/dist-packages Requires: Name: protobuf Version: 3.2.0 Location: /usr/local/lib/python2.7/dist-packages Requires: six, setuptools here is my test code: import tensorflow as tf a = tf.placeholder(tf.int16) b = tf.placeholder(tf.int16) add = tf.add(a, b) mul = tf.mul(a, b) with tf.Session() as sess: # Run every operation with variable input print ""Addition with variables: %i"" % sess.run(add, feed_dict={a: 2, b: 3}) print ""Multiplication with variables: %i"" % sess.run(mul, feed_dict={a: 2, b: 3}) I get this output: Is there any problem with the tensorflow installation? or any other problems?",|python|tensorflow|,API,4
42281589,"Keras Image Preprocessing: Tuple Index Out of Range. The goal of the script is to use Keras' existing image preprocessing module for video data augmentation. In this prototype, a sample video is split into an array of frames and processed, where the final steps involve performing random rotations, shifts, shears, and zooms: from keras import backend as K from keras.preprocessing.image import random_rotation, random_shift, random_shear, random_zoom K.set_image_dim_ordering(""th"") import cv2 import numpy as np video_file_path = ""./training-data/yes/1.mov"" samples_generated_per_sample = 10 self_rows = 100 self_columns = 150 self_frames_per_sequence = 45 # haar cascades for localizing oral region face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml') mouth_cascade = cv2.CascadeClassifier('haarcascade_mcs_mouth.xml') video = cv2.VideoCapture(video_file_path) success, frame = video.read() frames = [] success = True # convert to grayscale, localize oral region, equalize dimensions, # normalize pixels, equalize lengths, and accumulate valid frames while success: success, frame = video.read() if success: # convert to grayscale frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) # localize single facial region faces_coords = face_cascade.detectMultiScale(frame, 1.3, 5) if len(faces_coords) == 1: face_x, face_y, face_w, face_h = faces_coords[0] frame = frame[face_y:face_y + face_h, face_x:face_x + face_w] # localize oral region mouth_coords = mouth_cascade.detectMultiScale(frame, 1.3, 5) threshold = 0 for (mouth_x, mouth_y, mouth_w, mouth_h) in mouth_coords: if (mouth_y > threshold): threshold = mouth_y valid_mouth_coords = (mouth_x, mouth_y, mouth_w, mouth_h) else: pass mouth_x, mouth_y, mouth_w, mouth_h = valid_mouth_coords frame = frame[mouth_y:mouth_y + mouth_h, mouth_x:mouth_x + mouth_w] frames.append(frame) # ignore multiple facial region detections else: pass # pre-pad short sequences and equalize sequence lengths if len(frames) < self_frames_per_sequence: frames = [frames[0]]*(self_frames_per_sequence - len(frames)) + frames frames = frames[0:self_frames_per_sequence] frames = np.asarray(frames) rotated_frames = random_rotation(frames, rg=45) shifted_frames = random_shift(rotated_frames, wrg=0.25, hrg=0.25) sheared_frames = random_shear(shifted_frames, intensity=0.79) zoomed_frames = random_zoom(sheared_frames, zoom_range=(1.25, 1.25)) When the script is the run, the following error appears:",|python|numpy|machine-learning|deep-learning|keras|,Tensors&Inputs,1
42353056,"Keras Masking for RNN with Varying Time Steps. I'm trying to fit an RNN in Keras using sequences that have varying time lengths. My data is in a Numpy array with format (sample, time, feature) = (20631, max_time, 24) where max_time is determined at run-time as the number of time steps available for the sample with the most time stamps. I've padded the beginning of each time series with 0, except for the longest one, obviously. I've initially defined my model like so... model = Sequential() model.add(Masking(mask_value=0., input_shape=(max_time, 24))) model.add(LSTM(100, input_dim=24)) model.add(Dense(2)) model.add(Activation(activate)) model.compile(loss=weibull_loglik_discrete, optimizer=RMSprop(lr=.01)) model.fit(train_x, train_y, nb_epoch=100, batch_size=1000, verbose=2, validation_data=(test_x, test_y)) For completeness, here's the code for the loss function: def weibull_loglik_discrete(y_true, ab_pred, name=None): y_ = y_true[:, 0] u_ = y_true[:, 1] a_ = ab_pred[:, 0] b_ = ab_pred[:, 1] hazard0 = k.pow((y_ + 1e-35) / a_, b_) hazard1 = k.pow((y_ + 1) / a_, b_) return -1 * k.mean(u_ * k.log(k.exp(hazard1 - hazard0) - 1.0) - hazard1) And here's the code for the custom activation function: def activate(ab): a = k.exp(ab[:, 0]) b = k.softplus(ab[:, 1]) a = k.reshape(a, (k.shape(a)[0], 1)) b = k.reshape(b, (k.shape(b)[0], 1)) return k.concatenate((a, b), axis=1) When I fit the model and make some test predictions, every sample in the test set gets exactly the same prediction, which seems fishy. Things get better if I remove the masking layer, which makes me think there's something wrong with the masking layer, but as far as I can tell, I've followed the documentation exactly. Is there something mis-specified with the masking layer? Am I missing something else?",|python|numpy|neural-network|keras|recurrent-neural-network|,Model,0
42327543,"Adam optimizer goes haywire after 200k batches, training loss grows. I've been seeing a very strange behavior when training a network, where after a couple of 100k iterations (8 to 10 hours) of learning fine, everything breaks and the training loss grows: The training data itself is randomized and spread across many .tfrecord files containing 1000 examples each, then shuffled again in the input stage and batched to 200 examples. The background I am designing a network that performs four different regression tasks at the same time, e.g. determining the likelihood of an object appearing in the image and simultanously determining its orientation. The network starts with a couple of convolutional layers, some with residual connections, and then branches into the four fully-connected segments. Since the first regression results in a probability, I'm using cross entropy for the loss, whereas the others use classical L2 distance. However, due to their nature, the probability loss is around the order of 0..1, while the orientation losses can be much larger, say 0..10. I already normalized both input and output values and use clipping normalized = tf.clip_by_average_norm(inferred.sin_cos, clip_norm=2.) in cases where things can get really bad. I've been (successfully) using the Adam optimizer to optimize on the tensor containing all distinct losses (rather than reduce_suming them), like so: reg_loss = tf.reduce_sum(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)) loss = tf.pack([loss_probability, sin_cos_mse, magnitude_mse, pos_mse, reg_loss]) optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, epsilon=self.params.adam_epsilon) op_minimize = optimizer.minimize(loss, global_step=global_step) In order to display the results in TensorBoard, I then actually do loss_sum = tf.reduce_sum(loss) for a scalar summary. Adam is set to learning rate 1e-4 and epsilon 1e-4 (I see the same behavior with the default value for epislon and it breaks even faster when I keep the learning rate on 1e-3). Regularization also has no influence on this one, it does this sort-of consistently at some point. I should also add that stopping the training and restarting from the last checkpoint - implying that the training input files are shuffled again as well - results in the same behavior. The training always seems to behave similarly at that point.",|tensorflow|neural-network|deep-learning|conv-neural-network|,Training,2
42513613,"TensorFlow dynamic_rnn regressor: ValueError dimension mismatch. I would like to build a toy LSTM model for regression. This nice tutorial is already too complicated for a beginner. Given a sequence of length time_steps, predict the next value. Consider time_steps=3 and the sequences: array([ [[ 1.], [ 2.], [ 3.]], [[ 2.], [ 3.], [ 4.]], ... the target values should be: array([ 4., 5., ... I define the following model: # Network Parameters time_steps = 3 num_neurons= 64 #(arbitrary) n_features = 1 # tf Graph input x = tf.placeholder(""float"", [None, time_steps, n_features]) y = tf.placeholder(""float"", [None, 1]) # Define weights weights = { 'out': tf.Variable(tf.random_normal([n_hidden, 1])) } biases = { 'out': tf.Variable(tf.random_normal([1])) } #LSTM model def lstm_model(X, weights, biases, learning_rate=0.01, optimizer='Adagrad'): # Prepare data shape to match `rnn` function requirements # Current data input shape: (batch_size, time_steps, n_features) # Required shape: 'time_steps' tensors list of shape (batch_size, n_features) # Permuting batch_size and time_steps input dimension: Tensor(""Placeholder_:0"", shape=(?, 3, 1), dtype=float32) X = tf.transpose(X, [1, 0, 2]) transposed dimension: Tensor(""transpose_41:0"", shape=(3, ?, 1), dtype=float32) # Reshaping to (time_steps*batch_size, n_features) X = tf.reshape(X, [-1, n_features]) reshaped dimension: Tensor(""Reshape_:0"", shape=(?, 1), dtype=float32) # Split to get a list of 'time_steps' tensors of shape (batch_size, n_features) X = tf.split(0, time_steps, X) splitted dimension: [<tf.Tensor 'split_:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_:1' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_:2' shape=(?, 1) dtype=float32>] # LSTM cell cell = tf.nn.rnn_cell.LSTMCell(num_neurons) #Or GRUCell(num_neurons) output, state = tf.nn.dynamic_rnn(cell=cell, inputs=X, dtype=tf.float32) output = tf.transpose(output, [1, 0, 2]) last = tf.gather(output, int(output.get_shape()[0]) - 1) return tf.matmul(last, weights['out']) + biases['out'] We instantiating the LSTM model with pred = lstm_model(x, weights, biases) I get the following: ---> output, state = tf.nn.dynamic_rnn(cell=cell, inputs=X, dtype=tf.float32) ValueError: Dimension must be 2 but is 3 for 'transpose_42' (op: 'Transpose') with input shapes: [?,1], [3] 1) Do you know what the problem is? 2) Will multiplying the LSTM output by the weights yield the regression?",|python|tensorflow|neural-network|lstm|recurrent-neural-network|,Tensors&Inputs,1
42514960,"Keras autoencoder. I've worked a long time ago with neural networks in Java and now I'm trying to learn to use TFLearn and Keras in Python. I'm trying to build an autoencoder, but as I'm experiencing problems the code I show you hasn't got the bottleneck characteristic (this should make the problem even easier). On the following code I create the network, the dataset (two random variables), and after train it plots the correlation between each predicted variable with its input. What the network should learn, is to output the same input that receives. import matplotlib.pyplot as plt import numpy as np from keras.layers import Input, Dense from keras.models import Model from keras.models import load_model from loaders.nslKddCup99.nslKddCup99Loader import NslKddCup99 def buildMyNetwork(inputs, bottleNeck): inputLayer = Input(shape=(inputs,)) autoencoder = Dense(inputs*2, activation='relu')(inputLayer) autoencoder = Dense(inputs*2, activation='relu')(autoencoder) autoencoder = Dense(bottleNeck, activation='relu')(autoencoder) autoencoder = Dense(inputs*2, activation='relu')(autoencoder) autoencoder = Dense(inputs*2, activation='relu')(autoencoder) autoencoder = Dense(inputs, activation='sigmoid')(autoencoder) autoencoder = Model(input=inputLayer, output=autoencoder) autoencoder.compile(optimizer='adadelta', loss='mean_squared_error') return autoencoder dataSize = 1000 variables = 2 data = np.zeros((dataSize,variables)) data[:, 0] = np.random.uniform(0, 0.8, size=dataSize) data[:, 1] = np.random.uniform(0, 0.1, size=dataSize) trainData, testData = data[:900], data[900:] model = buildMyNetwork(variables,2) model.fit(trainData, trainData, nb_epoch=2000) predictions = model.predict(testData) for x in range(variables): plt.scatter(testData[:, x], predictions[:, x]) plt.show() plt.close() Even though some times the result is acceptable, many others isn't, I know neural networks have weight random initialization and therefore it may converge to different solutions, but I think this is too much and there may be some mistake in my code. Sometimes correlation is acceptable Others is quite lost ** UPDATE: ** Thanks Marcin Moejko! Indeed that was the problem, my original question was because I was trying to build an autoencoder, so to be coherent with the title here comes an example of autoencoder (just making a more complex dataset and changing the activation functions): import matplotlib.pyplot as plt import numpy as np from keras.layers import Input, Dense from keras.models import Model from keras.models import load_model from loaders.nslKddCup99.nslKddCup99Loader import NslKddCup99 def buildMyNetwork(inputs, bottleNeck): inputLayer = Input(shape=(inputs,)) autoencoder = Dense(inputs*2, activation='tanh')(inputLayer) autoencoder = Dense(inputs*2, activation='tanh')(autoencoder) autoencoder = Dense(bottleNeck, activation='tanh')(autoencoder) autoencoder = Dense(inputs*2, activation='tanh')(autoencoder) autoencoder = Dense(inputs*2, activation='tanh')(autoencoder) autoencoder = Dense(inputs, activation='tanh')(autoencoder) autoencoder = Model(input=inputLayer, output=autoencoder) autoencoder.compile(optimizer='adadelta', loss='mean_squared_error') return autoencoder dataSize = 1000 variables = 6 data = np.zeros((dataSize,variables)) data[:, 0] = np.random.uniform(0, 0.5, size=dataSize) data[:, 1] = np.random.uniform(0, 0.5, size=dataSize) data[:, 2] = data[:, 0] + data[:, 1] data[:, 3] = data[:, 0] * data[:, 1] data[:, 4] = data[:, 0] / data[:, 1] data[:, 5] = data[:, 0] ** data[:, 1] trainData, testData = data[:900], data[900:] model = buildMyNetwork(variables,2) model.fit(trainData, trainData, nb_epoch=2000) predictions = model.predict(testData) for x in range(variables): plt.scatter(testData[:, x], predictions[:, x]) plt.show() plt.close() For this example I used TanH activation function, but I tried with others and worked aswell. The dataset has now 6 variables but the autoencoder has a bottleneck of 2 neurons; as long as variables 2 to 5 are formed combining variables 0 and 1, the autoencoder only needs to pass the information of those two and learn the functions to generate the other variables on the decoding phase. The example above shows how all functions are learnt but one, the division... I don't know yet why.",|python|machine-learning|neural-network|keras|autoencoder|,Model,0
42655764,"debugging 'TypeError: Can not convert a ndarray into a Tensor or Operation' for CNN. I am trying to build a CNN, I have 8 classes in the input_samples with 45 samples in each class. so total number of input samples are 360. I have divided my first 20 samples as train samples and remaining 25 samples as test samples in each class (My input is a text file and the data is in rows is my preprocessed data, so I am reading the rows in textfile and reshaping the images which are 16x12 size). I am unable to fix the error in the code My code: import numpy as np import random import tensorflow as tf folder = 'D:\\Lab_Project_Files\\TF\\Practice Files\\' Datainfo = 'dataset_300.txt' ClassInfo = 'classTrain.txt' INPUT_WIDTH = 16 IMAGE_HEIGHT = 12 IMAGE_DEPTH = 1 IMAGE_PIXELS = INPUT_WIDTH * IMAGE_HEIGHT # 192 = 12*16 NUM_CLASSES = 8 STEPS = 500 STEP_VALIDATE = 100 BATCH_SIZE = 5 def load_data(file1,file2,folder): filename1 = folder + file1 filename2 = folder + file2 # loading the data file x_data = np.loadtxt(filename1, unpack=True) x_data = np.transpose(x_data) # loading the class information of the data loaded y_data = np.loadtxt(filename2, unpack=True) y_data = np.transpose(y_data) # divide the data in to test and train data x_data_train = x_data[np.r_[0:20, 45:65, 90:110, 135:155, 180:200, 225:245, 270:290, 315:335],:] x_data_test = x_data[np.r_[20:45, 65:90, 110:135, 155:180, 200:225, 245:270, 290:315, 335:360], :] y_data_train = y_data[np.r_[0:20, 45:65, 90:110, 135:155, 180:200, 225:245, 270:290, 315:335]] y_data_test = y_data[np.r_[20:45, 65:90, 110:135, 155:180, 200:225, 245:270, 290:315, 335:360],:] return x_data_train,x_data_test,y_data_train,y_data_test def reshapedata(data_train,data_test): data_train = np.reshape(data_train, (len(data_train),INPUT_WIDTH,IMAGE_HEIGHT)) data_test = np.reshape(data_test, (len(data_test), INPUT_WIDTH, IMAGE_HEIGHT)) return data_train,data_test def batchdata(data,label, batchsize): # generate random number required to batch data order_num = random.sample(range(1, len(data)), batchsize) data_batch = [] label_batch = [] for i in range(len(order_num)): data_batch.append(data[order_num[i-1]]) label_batch.append(label[order_num[i-1]]) return data_batch, label_batch # CNN trail def conv_net(x): weights = tf.Variable(tf.random_normal([INPUT_WIDTH * IMAGE_HEIGHT * IMAGE_DEPTH, NUM_CLASSES])) biases = tf.Variable(tf.random_normal([NUM_CLASSES])) out = tf.add(tf.matmul(x, weights), biases) return out sess = tf.Session() # get filelist and labels for training and testing data_train,data_test,label_train,label_test = load_data(Datainfo,ClassInfo,folder) data_train, data_test, = reshapedata(data_train, data_test) ############################ get files for training #################################################### image_batch, label_batch = batchdata(data_train,label_train,BATCH_SIZE) # input output placeholders x = tf.placeholder(tf.float32, [None, IMAGE_PIXELS]) y_ = tf.placeholder(tf.float32,[None, NUM_CLASSES]) # create the network y = conv_net( x ) # loss cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, y_)) # train step train_step = tf.train.AdamOptimizer( 1e-3 ).minimize( cost ) ############################## get files for validataion ################################################### image_batch_test, label_batch_test = batchdata(data_test,label_test,BATCH_SIZE) correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1)) accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) sess.run(tf.initialize_all_variables()) ################ CNN Program ############################## for i in range(STEPS): # checking the accuracy in between. if i % STEP_VALIDATE == 0: imgs, lbls = sess.run([image_batch_test, label_batch_test]) print(sess.run(accuracy, feed_dict={x: imgs, y_: lbls})) imgs, lbls = sess.run([image_batch, label_batch]) sess.run(train_step, feed_dict={x: imgs, y_: lbls}) imgs, lbls = sess.run([image_batch_test, label_batch_test]) print(sess.run(accuracy, feed_dict={ x: imgs, y_: lbls})) file can be downloaded dataset_300.txt and ClassInfo.txt",|tensorflow|neural-network|conv-neural-network|,API,4
42652161,"Unable to merge keras models after adding Lambda layer. I have a list of 3 keras models that each have an output shape of (None, 2). I also have a common keras base model that produces their input. My goal is to combine the 4 models but to only take the first output from each of the models in the list (so the final output should have shape (None, 3). My problem occurs when I try to use a Lambda layer to extract the first output from each model. If I omit the Lambda step and simply combine the models as follows, it creates a model that gives the correct output with shape (None, 6): >>> sequentials = [Sequential([base_model, m]) for m in models] >>> output = merge([s.output for s in sequentials], mode='concat') >>> combined = Model(input=base_model.layers[0].input, output=output) >>> combined.predict(X) array([[ 8.52127552e-01, 1.47872433e-01, 1.89960217e-13, 1.00000000e+00, 7.56258190e-01, 2.43741751e-01]], dtype=float32) The problem occurs when I first use a Lambda layer to extract the first value from each model: >>> print([m.output_shape for m in models]) [(None, 2), (None, 2), (None, 2)] >>> for m in models: m.add(Lambda(lambda x: x[0], output_shape=(1,))) >>> print([m.output_shape for m in models]) [(None, 1), (None, 1), (None, 1)] >>> sequentials = [Sequential([base_model, m]) for m in models] >>> print([s.output_shape for s in sequentials]) [(None, 1), (None, 1), (None, 1)] >>> output = merge([s.output for s in sequentials], output_shape=(len(sequentials),), mode='concat') >>> combined = Model(base_model.layers[0].input, output=output) >>> print(combined.output_shape) (None, 3) >>> combined.predict(X) --------------------------------------------------------------------------- ValueError Traceback (most recent call last) <ipython-input-3-4f4ed3bd605d> in <module>() ----> 1 ann.combined.predict(X) ./.virtualenvs/py3/lib/python3.4/site-packages/keras/engine/training.py in predict(self, x, batch_size, verbose) 1217 f = self.predict_function 1218 return self._predict_loop(f, ins, -> 1219 batch_size=batch_size, verbose=verbose) 1220 1221 def train_on_batch(self, x, y, ./.virtualenvs/py3/lib/python3.4/site-packages/keras/engine/training.py in _predict_loop(self, f, ins, batch_size, verbose) 904 905 for i, batch_out in enumerate(batch_outs): --> 906 outs[i][batch_start:batch_end] = batch_out 907 if verbose == 1: 908 progbar.update(batch_end) ValueError: could not broadcast input array from shape (6) into shape (1) What is the right way to merge these models while only taking the single output value from each one? Note that I can successfully use a Lambda function if I apply it after merging the models as follows: >>> sequentials = [Sequential([base_model, m]) for m in models] >>> output = merge([s.output for s in sequentials], mode='concat') >>> filtered = Lambda(lambda x: x[:,::2], lambda s: (s[-1] / 2,))(output) >>> combined = Model(input=base_model.layers[0].input, output=filtered) >>> combined.predict(X) array([[ 1.89960217e-13, 7.56258249e-01, 8.52127552e-01]], type=float32) But I would like to know how to apply it before the merge.",|python|machine-learning|neural-network|keras|keras-layer|,Tensors&Inputs,1
42616625,"ValueError: Tensor must be from the same graph as Tensor with Bidirectinal RNN in Tensorflow. I'm doing text tagger using Bidirectional dynamic RNN in tensorflow. After maching input's dimension, I tried to run a Session. this is blstm setting parts: fw_lstm_cell = BasicLSTMCell(LSTM_DIMS) bw_lstm_cell = BasicLSTMCell(LSTM_DIMS) (fw_outputs, bw_outputs), _ = bidirectional_dynamic_rnn(fw_lstm_cell, bw_lstm_cell, x_place, sequence_length=SEQLEN, dtype='float32') and this is runing part: with tf.Graph().as_default(): # Placehoder Settings x_place, y_place = set_placeholder(BATCH_SIZE, EM_DIMS, MAXLEN) # BLSTM Model Building hlogits = tf_kcpt.build_blstm(x_place) # Compute loss loss = tf_kcpt.get_loss(log_likelihood) # Training train_op = tf_kcpt.training(loss) # load Eval method eval_correct = tf_kcpt.evaluation(logits, y_place) # Session Setting & Init init = tf.global_variables_initializer() sess = tf.Session() sess.run(init) # tensor summary setting summary = tf.summary.merge_all() summary_writer = tf.summary.FileWriter(LOG_DIR, sess.graph) # Save saver = tf.train.Saver() # Run epoch for step in range(EPOCH): start_time = time.time() feed_dict = fill_feed_dict(KCPT_SET['train'], x_place, y_place) _, loss_value = sess.run([train_op, loss], feed_dict=feed_dict) But, it give me the error: ValueError: Tensor(""Shape:0"", shape=(1,), dtype=int32) must be from the same graph as Tensor(""bidirectional_rnn/fw/fw/stack_2:0"", shape=(1,), dtype=int32). Help me, please",|python|tensorflow|deep-learning|recurrent-neural-network|bidirectional|,API,4
42800203,"Keras RNN LSTM accuracy not changing. X_train [[0 1 1 1 1 0 0 0 0 0] [0 1 1 1 1 0 0 0 0 0] [0 1 1 1 0 0 0 0 0 0] [0 1 0 0 0 1 1 0 0 0] [0 1 0 0 0 1 1 0 0 0] [0 1 1 1 1 0 0 0 0 0]] y_train 1 1 1 0 0 1 The third and 4th column in X_train are a clear indicator of the output. I am trying out RNN with LSTM so I have chosen this sample data and I want to overfit this. The accuracy is not changing at all even after 50 epochs of training - Epoch 1/60 6/6 [==============================] - 1s - loss: 5.3141 - acc: 0.6667 Epoch 2/60 6/6 [==============================] - 1s - loss: 5.3141 - acc: 0.6667 Epoch 3/60 6/6 [==============================] - 1s - loss: 5.3141 - acc: 0.6667 Epoch 4/60 6/6 [==============================] - 1s - loss: 5.3141 - acc: 0.6667 Epoch 5/60 6/6 [==============================] - 1s - loss: 5.3141 - acc: 0.6667 Model model = Sequential() model.add(Embedding(MAX_NB_WORDS, embedding_vecor_length, input_length=max_length,batch_input_shape=( batch_size, input_dim))) model.add(LSTM(10, return_sequences=False)) model.add(Dense(1, activation='softmax')) Parameters MAX_NB_WORDS = 10 embedding_vecor_length = 32 max_length = 10 batch_size = 2 input_dim = max_length I am using Theano backend. Probably something missing very obvious. Please help! UPDATE Apologies for providing half baked stuff. I am compiling the model thus - opt = SGD(lr=0.001) model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy']) print(model.summary()) print np.shape(X_train) callbacks = [ # EarlyStopping(monitor='val_loss', patience=3, verbose=2), RemoteMonitor(root='http://localhost:9000'), ModelCheckpoint(filepath=""/tmp/weights.hdf5"", monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto') ] print X_train print y_train history = model.fit(X_train, y_train, nb_epoch=60, batch_size=batch_size, callbacks=callbacks) #, validation_data=(X_val, y_val) UPDATE2 Rectified this by changing the activation function from 'softmax' to 'sigmoid' A proper explanation is missing. I guess the ""why"" should be the answer not the how.",|python|keras|lstm|,Model,0
42823627,"How to use Merge layer (concat function) on Keras 2.0.0?. I am trying to reproduce the entity embedding models using Keras. Here is the github link and use the kaggle branch. There is one python file models.py and the Merge layer is used. from keras.layers.core import Dense, Dropout, Activation, Merge, Reshape ...... self.model.add(Merge(models, mode='concat')) This code should be fine for old-version of Keras, but using Keras 2.0.0 using tensorflow 1.0.0 as the backend (python 2.7), there will be wrong information: Using TensorFlow backend. Traceback (most recent call last): File ""/Users/pengjuzhao/Udacity/MLND/entity-embedding-rossmann/test_model.py"", line 2, in <module> from models import NN_with_EntityEmbedding File ""/Users/pengjuzhao/Udacity/MLND/entity-embedding-rossmann/models.py"", line 8, in <module> from keras.layers.core import Dense, Dropout, Activation, Merge, Reshape ImportError: cannot import name Merge [Finished in 1.8s with exit code 1] [shell_cmd: python -u ""/Users/pengjuzhao/Udacity/MLND/entity-embedding-rossmann/test_model.py""] [dir: /Users/pengjuzhao/Udacity/MLND/entity-embedding-rossmann] [path: /usr/bin:/bin:/usr/sbin:/sbin] Is there anyone who knows how to reach the same target(self.model.add(Merge(models, mode='concat')))or how to use the merge/Merge layer using Keras 2.0.0 ? Thank you in advance.",|python|tensorflow|deep-learning|keras|,API,4
42913869,"Keras : training with an array as an input. I am quite new to machine learning, and I recently began to learn how to implement basic neural networks on Python using the library Keras. I started with an elementary example (training a network so it can predict the value y = f(x) = x). model = Sequential() model.add(Dense(10, input_dim=1, activation='relu')) model.add(Dense(10, activation='relu')) model.add(Dense(10, activation='relu')) model.add(Dense(1)) model.compile(loss='mse', optimizer='adam') data = np.array([x for x in range(0,1000)]) for i in range(0, 1000): model.fit([np.array([data[i]]), np.array([data[i]]), nb_epoch=1, batch_size=1, verbose=0) I would like now to apply a similar algorithm to train a network so from a list of integers L, it would return a list of three values of my function f (basically : [f(L,0), f(L,1), f(L,2)]). So the input this time is an array of 5 integers, and the desired returned output is a list of 3 floats. However, I do not manage to code properly the fit method in the for loop, I get an error message: ""Expected to see 1 arrays but instead got the following list of 2 arrays: ..."". I tried to play around with some arguments of the Dense constructor (input_dim, input_shape, input_size) but I still do not seem to be able to make it work... So basically, does anyone know how to implement correctly a network that takes an array as an input and returns an array of a different length? Thank you very much for the help and support, -- smgr",|python|machine-learning|neural-network|keras|,Tensors&Inputs,1
43152053,"Appending layers with previous in keras? - Conv2D' object has no attribute 'is_placeholder'. I seem to have some problem appending layers in keras. Example: import keras from keras.layers.merge import Concatenate from keras.models import Model from keras.layers import Input, Dense from keras.layers import Dropout from keras.layers.core import Dense, Activation, Lambda, Reshape,Flatten from keras.layers import Conv2D, MaxPooling2D, Reshape, ZeroPadding2D input_img = Input(shape=(3, 6, 3)) conv2d_1_1 = Conv2D(filters = 32, kernel_size = (3,3) , padding = ""same"" , activation = 'relu' , name = ""conv2d_1_1"" )(input_img) conv2d_2_1 = Conv2D(filters = 64, kernel_size = (3,3) , padding = ""same"" , activation = 'relu' )(conv2d_1_1) conv2d_3_1 = Conv2D(filters = 64, kernel_size = (3,3) , padding = ""same"" , activation = 'relu' )(conv2d_2_1) conv2d_4_1 = Conv2D(filters = 32, kernel_size = (1,1) , padding = ""same"" , activation = 'relu' )(conv2d_3_1) conv2d_4_1_flatten = Flatten()(conv2d_4_1) conv2d_1_2 = Conv2D(filters = 32, kernel_size = (3,3) , padding = ""same"" , activation = 'relu' , name = ""conv2d_1_2"")(input_img) conv2d_2_2 = Conv2D(filters = 64, kernel_size = (3,3) , padding = ""same"" , activation = 'relu' )(conv2d_1_2) conv2d_3_2 = Conv2D(filters = 64, kernel_size = (3,3) , padding = ""same"" , activation = 'relu' )(conv2d_2_2) conv2d_4_2 = Conv2D(filters = 32, kernel_size = (1,1) , padding = ""same"" , activation = 'relu' )(conv2d_3_2) conv2d_4_2_flatten = Flatten()(conv2d_4_2) merge = keras.layers.concatenate([conv2d_4_1_flatten, conv2d_4_2_flatten]) dense1 = Dense(100, activation = 'relu')(merge) dense2 = Dense(50,activation = 'relu')(dense1) dense3 = Dense(1 ,activation = 'softmax')(dense2) model = Model(inputs = [conv2d_1_1 , conv2d_1_2] , outputs = dense3) model.compile(loss=""crossentropy"", optimizer=""adam"") print model.summary() Why am I not able to append my layers like this? The input is an image which i've manually separated in to shaped of (3,6,3)..",|python-2.7|keras|keras-layer|,Model,0
43172922,"Tensorflow Restore Model and Predict. I have trained a Tensorflow model and saved the output layer's tensor. When restoring, I restored the output layer's tensor and tried predicting with it but got an error saying I never assigned to a placeholder. My code is below, please assist. with tf.Session() as sess: model_saver = tf.train.import_meta_graph(model_save_folder + '/my-model.meta') model_saver.restore(sess, model_save_folder + '/my-model') x = tf.placeholder('float') output = tf.get_collection(""output"")[0] #output will be the tensor for model's last layer print(""Model restored."") print('Initialized') #print(sess.run(tf.get_default_graph().get_tensor_by_name('w_conv1:0'))) #collect list of preprocessed data on submission set inputData = [] with open('stage1_sample_submission.csv') as f: reader = csv.reader(f) num = 0 for row in reader: if num > 0: patient = row[0] #print(patient) inputData.append(process_data(patient, img_px_size=IMG_SIZE_PX, hm_slices=SLICE_COUNT)) num += 1 #prediction! prediction = sess.run(output, feed_dict={x: inputData}) print(prediction)",|tensorflow|,API,4
43329789,"Tensorflow rnn: name 'seq2seq' is not defined. I am trying this notebook: https://github.com/sjchoi86/Tensorflow-101/blob/master/notebooks/char_rnn_sample_tutorial.ipynb I have a problem with this line In[6]: outputs, final_state = seq2seq.rnn_decoder(inputs, istate, cell, loop_function=None, scope='rnnlm') I get this error: NameError: name 'seq2seq' is not defined I am using tensorflow 1.0.1. I tried tf.contrib.seq2seq but I am getting error: AttributeError: 'module' object has no attribute 'rnn_decoder' I think it is a probleme with the new implementation of rnn network in tensorflow 1.0.1 but I don't know how to fix it.",|python|tensorflow|attributeerror|lstm|recurrent-neural-network|,API,4
43396572,"Dimension of shape in conv1D. I have tried to build a CNN with one layer, but I have some problem with it. Indeed, the compilator says me that ValueError: Error when checking model input: expected conv1d_1_input to have 3 dimensions, but got array with shape (569, 30) This is the code import numpy from keras.models import Sequential from keras.layers.convolutional import Conv1D numpy.random.seed(7) datasetTraining = numpy.loadtxt(""CancerAdapter.csv"",delimiter="","") X = datasetTraining[:,1:31] Y = datasetTraining[:,0] datasetTesting = numpy.loadtxt(""CancereEvaluation.csv"",delimiter="","") X_test = datasetTraining[:,1:31] Y_test = datasetTraining[:,0] model = Sequential() model.add(Conv1D(2,2,activation='relu',input_shape=X.shape)) model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) model.fit(X, Y, epochs=150, batch_size=5) scores = model.evaluate(X_test, Y_test) print(""\n%s: %.2f%%"" % (model.metrics_names[1], scores[1]*100))",|python|tensorflow|keras|deep-learning|conv-neural-network|,Tensors&Inputs,1
43464835,"Convolution Neural Network input_shape dimension error (KERAS ,PYTHON). I have a train dataset of the following shape: (300, 5, 720) [[[ 6. 11. 389. ..., 0. 0. 0.] [ 2. 0. 0. ..., 62. 0. 0.] [ 0. 0. 18. ..., 0. 0. 0.] [ 38. 201. 47. ..., 0. 108. 0.] [ 0. 0. 1. ..., 0. 0. 0.]] [[ 136. 95. 0. ..., 0. 0. 0.] [ 85. 88. 85. ..., 0. 31. 0.] [ 0. 0. 0. ..., 0. 0. 0.] [ 0. 0. 0. ..., 0. 0. 0.] [ 13. 19. 0. ..., 0. 0. 0.]]] I am trying to pass each sample as input to the cnn model, each input is of size (5,720) ,I am using the following model in keras: cnn = Sequential() cnn.add(Conv2D(64, (5, 50), padding=""same"", activation=""relu"",data_format=""channels_last"", input_shape=in_shape)) cnn.add(MaxPooling2D(pool_size=(2,2),data_format=""channels_last"")) cnn.add(Flatten()) cnn.add(Dropout(0.5)) cnn.add(Dense(number_of_classes, activation=""softmax"")) cnn.compile(loss=""categorical_crossentropy"", optimizer=""adam"", metrics= ['accuracy']) cnn.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test), shuffle=True) I am using input shape as: rows,cols=x_train.shape[1:] in_shape=(rows,cols,1) but I am getting the following error: ValueError: Error when checking model input: expected conv2d_1_input to have 4 dimensions, but got array with shape (300, 5, 720) How can I fix this error?",|python-2.7|neural-network|keras|conv-neural-network|keras-layer|,Tensors&Inputs,1
43511819,ImportError: No module named 'keras.utils.visualize_util'. Hi when I am trying to run a code in keras it is showing me the following error: from keras.utils.visualize_util import plot ImportError: No module named 'keras.utils.visualize_util' How can I solve this? thanks,|python-3.x|deep-learning|keras|keras-layer|,API,4
43542226,"Keras' fit_generator extra training value. train_datagen = ImageDataGenerator( rescale=1./255, shear_range=0.1, zoom_range=0.1, rotation_range=5., width_shift_range=0.1, height_shift_range=0.1) val_datagen = ImageDataGenerator(rescale=1./255) train_generator = train_datagen.flow_from_directory( train_data_dir, target_size = (img_width, img_height), batch_size = 20, shuffle = True, classes = TYPES, class_mode = 'categorical') validation_generator = val_datagen.flow_from_directory( val_data_dir, target_size=(img_width, img_height), batch_size=20, shuffle = True, classes = TYPES, class_mode = 'categorical') model.fit_generator( train_generator, samples_per_epoch = 2000, nb_epoch = 20 ) Epoch 14/50 480/2000 [======>.......................] - ETA: 128s - loss: 0.8708 Epoch 13/50 2021/2000 [==============================] - 171s - loss: 0.7973 - acc: 0.7041 My ImageGenerators reading 2261 training and 567 testing images from folder. I am trying to train my model with 2000 samples_per_epoch and 20 batch_size. Batch_size is divisible for samples_per_epoch but somehow it is adding extra value and shows that warning: ( UserWarning: Epoch comprised more than samples_per_epoch samples, which might affect learning results. Set samples_per_epoch correctly to avoid this warning). It works with Single-Gpu but If I try to train with Multi-Gpus it gives that error: InvalidArgumentError (see above for traceback): Incompatible shapes: [21] vs. [20] [[Node: Equal = Equal[T=DT_INT64, _device=""/job:localhost/replica:0/task:0/gpu:0""](ArgMax, ArgMax_1)]] [[Node: gradients/concat_25_grad/Slice_1/_10811 = _Recvclient_terminated=false, recv_device=""/job:localhost/replica:0/task:0/gpu:1"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_101540_gradients/concat_25_grad/Slice_1"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/gpu:1""]] I am using that code for model parallelization: Thanks for your help...",|python|python-3.x|tensorflow|deep-learning|keras|,Training,2
43509881,"Keras: Exception: Received unknown keyword arguments: {'epochs': 100}. I am trying to replicate the code on http://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/ (first example). The code can be found in the part ""LSTM Networks for Regression"". However, my question mainly refers to the following line: model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2) When I execute this line, I get the following Exception: model.fit(trainX, trainY, batch_size=1, verbose=2, epochs = 100) File ""/usr/local/lib/python2.7/site-packages/keras/models.py"", line 612, in fit str(kwargs)) Exception: Received unknown keyword arguments: {'epochs': 100} If I leave the keyword 'epochs' away, everything works fine. But of course, this is highly unsatisfactory since I want to increase the number of epochs. Can anyone help?",|python|keras|,API,4
43900125,"How to implement multi-class semantic segmentation?. I'm able to train a U-net with labeled images that have a binary classification. But I'm having a hard time figuring out how to configure the final layers in Keras/Theano for multi-class classification (4 classes). I have 634 images and corresponding 634 masks that are unit8 and 64 x 64 pixels. My masks, instead of being black (0) and white (1), have color labeled objects in 3 categories plus background as follows: black (0), background red (1), object class 1 green (2), object class 2 yellow (3), object class 3 Before training runs, the array containing masks is one-hot encoded as follows: mask_train = to_categorical(mask_train, 4) This makes mask_train.shape go from (634, 1, 64, 64) to (2596864, 4). My model closely follows the Unet architecture, however the final layers seem problematic, as I'm unable to flatten the structure so as to match the one-hot encoded array. [...] up3 = concatenate([UpSampling2D(size=(2, 2))(conv7), conv2], axis=1) conv8 = Conv2D(128, (3, 3), activation='relu', padding='same')(up3) conv8 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv8) up4 = concatenate([UpSampling2D(size=(2, 2))(conv8), conv1], axis=1) conv9 = Conv2D(64, (3, 3), activation='relu', padding='same')(up4) conv10 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv9) # here I used number classes = number of filters and softmax although # not sure if a dense layer should be here instead conv11 = Conv2D(4, (1, 1), activation='softmax')(conv10) model = Model(inputs=[inputs], outputs=[conv11]) # here categorical cross entropy is being used but may not be correct model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy']) return model Do you have any suggestions on how to modify the final portions of the model so this trains successfully? I get a variety of shape mismatch errors, and the few times I managed to make it run, the loss did not change throughout epochs.",|python|machine-learning|deep-learning|keras|image-segmentation|,Training,2
43895750,"Keras input_shape for conv2d and manually loaded images. I am manually creating my dataset from a number of 384x286 b/w images. I load an image like this: x = [] for f in files: img = Image.open(f) img.load() data = np.asarray(img, dtype=""int32"") x.append(data) x = np.array(x) this results in x being an array (num_samples, 286, 384) print(x.shape) => (100, 286, 384) reading the keras documentation, and checking my backend, i should provide to the convolution step an input_shape composed by ( rows, cols, channels ) since i don't arbitrarily know the sample size, i would have expected to pass as an input size, something similar to ( None, 286, 384, 1 ) the model is built as follows: model = Sequential() model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape)) # other steps... passing as input_shape (286, 384, 1) causes: Error when checking input: expected conv2d_1_input to have 4 dimensions, but got array with shape (85, 286, 384) passing as_input_shape (None, 286, 384, 1 ) causes: Input 0 is incompatible with layer conv2d_1: expected ndim=4, found ndim=5 what am i doing wrong ? how do i have to reshape the input array?",|python|tensorflow|neural-network|keras|convolution|,Tensors&Inputs,1
44003302,"Porting loss function written in Tensorflow to Keras results in AttributeError. I have a loss function written in tensorflow which gets 3 values in y_in and 3 values in y_pred. Pseudo-Code for tensorflow-loss: def my_loss(y_in,y_pred): with tf.name_scope('loss_scope'): loss1 = tf.reduce_mean(...) loss2 = tf.reduce_mean(...) loss3 = tf.reduce_mean(...) return loss1,loss2,loss3 Now I want to use this loss in my keras model, I would simply try it this way: ... out = Dense(3,activation='linear')(con_res) model = Model(inputs=[In1,In2],output = out) model.compile(optimizer='rmsprop',loss=my_loss) Where con_res is the result from the network before. Then with the help of the Dense - Layer it will get reduced to 3 outputs. The following error occurs: File ""/usr/local/lib/python2.7/dist-packages/keras/engine/training.py"", line 910, in compile sample_weight, mask) File ""/usr/local/lib/python2.7/dist-packages/keras/engine/training.py"", line 447, in weighted ndim = K.ndim(score_array) File ""/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py"", line 479, in ndim dims = x.get_shape()._dims AttributeError: 'tuple' object has no attribute 'get_shape' The shown traceback occurs after the .compile function. I tried it with a crossentropy loss and it did not throw any error",|python|tensorflow|keras|attributeerror|,API,4
44066044,"Keras accuracy is not increasing over 50%. I am trying to build a binary classification algorithm (output is 0 or 1) on a dataset that contains normal and malicious network packets. The dataset shape (after converting IP @'s and hexa to decimal) is: IP src, IP dest, ports, TTL, etc.. Note: The final column is the output. And the Keras model is: from keras.models import Sequential from keras.layers import Dense from sklearn import preprocessing import numpy import os os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' seed = 4 numpy.random.seed(seed) dataset = numpy.loadtxt(""NetworkPackets.csv"", delimiter="","") X = dataset[:, 0:11].astype(float) Y = dataset[:, 11] model = Sequential() model.add(Dense(12, input_dim=11, kernel_initializer='normal', activation='relu')) model.add(Dense(12, kernel_initializer='normal', activation='relu')) model.add(Dense(1, kernel_initializer='normal', activation='relu')) model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy']) model.fit(X, Y, nb_epoch=100, batch_size=5) scores = model.evaluate(X, Y) print(""\n%s: %.2f%%"" % (model.metrics_names[1], scores[1]*100)) However, I tried different optimizers, activation functions, number of layers, but the accuracy is reaching 0.5 at most: Result Even I tried Grid search for searching the best parameters, but the maximum is 0.5. Does anyone knows why the output is always like that? and how can I enhance it. Thanks in advance!",|python|tensorflow|deep-learning|keras|,Training,2
44124376,"TensorFlow Dataset Shuffle Each Epoch. In the manual on the Dataset class in Tensorflow, it shows how to shuffle the data and how to batch it. However, it's not apparent how one can shuffle the data each epoch. I've tried the below, but the data is given in exactly the same order the second epoch as in the first. Does anybody know how to shuffle between epochs using a Dataset? n_epochs = 2 batch_size = 3 data = tf.contrib.data.Dataset.range(12) data = data.repeat(n_epochs) data = data.batch(batch_size) next_batch = data.make_one_shot_iterator().get_next() sess = tf.Session() for _ in range(4): print(sess.run(next_batch)) print(""new epoch"") data = data.shuffle(12) for _ in range(4): print(sess.run(next_batch))",|python|tensorflow|,API,4
44164749,"How does Keras handle multilabel classification?. I am unsure how to interpret the default behavior of Keras in the following situation: My Y (ground truth) was set up using scikit-learn's MultilabelBinarizer(). Therefore, to give a random example, one row of my y column is one-hot encoded as such: [0,0,0,1,0,1,0,0,0,0,1]. So I have 11 classes that could be predicted, and more than one can be true; hence the multilabel nature of the problem. There are three labels for this particular sample. I train the model as I would for a non multilabel problem (business as usual) and I get no errors. from keras.models import Sequential from keras.layers import Dense, Dropout, Activation from keras.optimizers import SGD model = Sequential() model.add(Dense(5000, activation='relu', input_dim=X_train.shape[1])) model.add(Dropout(0.1)) model.add(Dense(600, activation='relu')) model.add(Dropout(0.1)) model.add(Dense(y_train.shape[1], activation='softmax')) sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True) model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy',]) model.fit(X_train, y_train,epochs=5,batch_size=2000) score = model.evaluate(X_test, y_test, batch_size=2000) score What does Keras do when it encounters my y_train and sees that it is ""multi"" one-hot encoded, meaning there is more than one 'one' present in each row of y_train? Basically, does Keras automatically perform multilabel classification? Any differences in the interpretation of the scoring metrics?",|python|neural-network|keras|multilabel-classification|,Model,0
44184834,"Value error: Input arrays should have the same number of samples as target arrays. Found 1600 input samples and 6400 target samples. I'm trying to do a 8-class classification. Here is the code: import keras import numpy as np from keras.preprocessing.image import ImageDataGenerator from keras.models import Sequential from keras.layers import Dropout, Flatten, Dense from keras import applications from keras.optimizers import SGD from keras import backend as K K.set_image_dim_ordering('tf') img_width, img_height = 48,48 top_model_weights_path = 'modelom.h5' train_data_dir = 'chCdata1/train' validation_data_dir = 'chCdata1/validation' nb_train_samples = 6400 nb_validation_samples = 1600 epochs = 50 batch_size = 10 def save_bottlebeck_features(): datagen = ImageDataGenerator(rescale=1. / 255) model = applications.VGG16(include_top=False, weights='imagenet', input_shape=(48,48,3)) generator = datagen.flow_from_directory( train_data_dir, target_size=(img_width, img_height), batch_size=batch_size, class_mode='categorical', shuffle=False) bottleneck_features_train = model.predict_generator( generator, nb_train_samples // batch_size) np.save(open('bottleneck_features_train', 'wb'),bottleneck_features_train) generator = datagen.flow_from_directory( validation_data_dir, target_size=(img_width, img_height), batch_size=batch_size, class_mode='categorical', shuffle=False) bottleneck_features_validation = model.predict_generator( generator, nb_validation_samples // batch_size) np.save(open('bottleneck_features_validation', 'wb'),bottleneck_features_validation) def train_top_model(): train_data = np.load(open('bottleneck_features_train', 'rb')) train_labels = np.array([0] * (nb_train_samples // 8) + [1] * (nb_train_samples // 8) + [2] * (nb_train_samples // 8) + [3] * (nb_train_samples // 8) + [4] * (nb_train_samples // 8) + [5] * (nb_train_samples // 8) + [6] * (nb_train_samples // 8) + [7] * (nb_train_samples // 8)) validation_data = np.load(open('bottleneck_features_validation', 'rb')) validation_labels = np.array([0] * (nb_train_samples // 8) + [1] * (nb_train_samples // 8) + [2] * (nb_train_samples // 8) + [3] * (nb_train_samples // 8) + [4] * (nb_train_samples // 8) + [5] * (nb_train_samples // 8) + [6] * (nb_train_samples // 8) + [7] * (nb_train_samples // 8)) train_labels = keras.utils.to_categorical(train_labels, num_classes = 8) validation_labels = keras.utils.to_categorical(validation_labels, num_classes = 8) model = Sequential() model.add(Flatten(input_shape=train_data.shape[1:])) model.add(Dense(512, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(8, activation='softmax')) sgd = SGD(lr=1e-2, decay=0.00371, momentum=0.9, nesterov=False) model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy']) model.fit(train_data, train_labels, epochs=epochs, batch_size=batch_size, validation_data=(validation_data, validation_labels)) model.save_weights(top_model_weights_path) save_bottlebeck_features() train_top_model() I've added the full list of error here: Traceback (most recent call last): File ""<ipython-input-14-1d34826b5dd5>"", line 1, in <module> runfile('C:/Users/rajaramans2/codes/untitled15.py', wdir='C:/Users/rajaramans2/codes') File ""C:\Anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 866, in runfile execfile(filename, namespace) File ""C:\Anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 102, in execfile exec(compile(f.read(), filename, 'exec'), namespace) File ""C:/Users/rajaramans2/codes/untitled15.py"", line 71, in <module> train_top_model() File ""C:/Users/rajaramans2/codes/untitled15.py"", line 67, in train_top_model validation_data=(validation_data, validation_labels)) File ""C:\Anaconda3\lib\site-packages\keras\models.py"", line 856, in fit initial_epoch=initial_epoch) File ""C:\Anaconda3\lib\site-packages\keras\engine\training.py"", line 1449, in fit batch_size=batch_size) File ""C:\Anaconda3\lib\site-packages\keras\engine\training.py"", line 1317, in _standardize_user_data _check_array_lengths(x, y, sample_weights) File ""C:\Anaconda3\lib\site-packages\keras\engine\training.py"", line 235, in _check_array_lengths 'and ' + str(list(set_y)[0]) + ' target samples.') ValueError: Input arrays should have the same number of samples as target arrays. Found 1600 input samples and 6400 target samples. The ""ValueError: Input arrays should have the same number of samples as target arrays. Found 1600 input samples and 6400 target samples"" pops up. Kindly help with the solution and the necessary modifications to the code. Thanks in advance.",|python|arrays|numpy|keras|,Training,2
44221396,"Max-margin loss in Keras/theano. I want to train a neural network in Keras (with theano as backend) with a max-margin loss function using one negative sample per positive sample: max(0,1 -pos_score +neg_score) I have a neural network which takes two arguments i and j and return score base(i,j). For given i, I have a positive sample j and negative sample k. So, I want to compute the following: max(0, 1 - base(i, j) + base(i, k)) At abstract level, my code looks like the following: i = Input(...) # d=100 j = Input(...) # d=300 k = Input(...) # d=300 i_vec = Sequential() i_vec.add(Dense(20, input_dim=100)) j_vec = Sequential() j_vec.add(Dense(30, input_dim=300)) base = Sequential() base.add(Merge([i_vec, j_vec], mode='concat') # Here goes definition of the base network base.add(Dense(output_dim=1, bias=False)) pos = base([i, j]) neg = base([i, k]) def custom_loss(y_true, y_pred): return K.maximum(0, 1 - y_pred[0] + y_pred[1]) model = Model(input=[i,j,k], output=[pos, neg]) # Shape of I=(1000,100), J and K=(1000,300), XX=(1000,) model.fit([I, J, K], [XX,XX], nb_epoch=10) Note that the XX is useless during the training. While running the code, I got the following error: ValueError: GpuElemwise. Output dimension mismatch. Output 0 (indices start at 0), working inplace on input 0, has shape[0] == 1, but the output's size on that axis is 32. Apply node that caused the error: GpuElemwise{Composite{(i0 * (i1 * i2))}}[(0, 0)](GpuElemwise{Composite{Cast{float32}(EQ(i0, i1))}}[(0, 0)].0, GpuElemwise{Composite{(i0 / (i1 * i2))}}[(0, 0)].0, GpuFromHost.0) Toposort index: 83 Inputs types: [CudaNdarrayType(float32, vector), CudaNdarrayType(float32, (True,)), CudaNdarrayType(float32, vector)] Inputs shapes: [(1,), (1,), (32,)] Inputs strides: [(0,), (0,), (1,)] Inputs values: [CudaNdarray([ 1.]), CudaNdarray([ 1.]), 'not shown'] Outputs clients: [[GpuIncSubtensor{InplaceInc;int64}(GpuIncSubtensor{Inc;int64}.0, GpuElemwise{Composite{(i0 * (i1 * i2))}}[(0, 0)].0, Constant{1}), GpuElemwise{neg,no_inplace}(GpuElemwise{Composite{(i0 * (i1 * i2))}}[(0, 0)].0)]] I think the problem is in the computation of the loss function. Note: I have tried with the XX as raw vector and column vector. But, the error remains same. Solution for the same problem with TensorFlow as backend is available here and here. Edit 1: Changing the loss function as below works (I mean it works without any error). But, neither I know why nor I know about the correctness of the new code. def custom_loss(y_true, y_pred): return K.sum(K.maximum(0, 1 - y_pred[0] + y_pred[1]))",|python|neural-network|keras|theano|loss-function|,Training,2
44238165,"Datatypes, data shapes, and pad_sequences. I cannot understand the error message I receive in this code. The part with x_train is from a working example showing how to use LSTM in Keras. The part with mytrain is just an example I was playing with to understand the various functions. As you can see from the messages, x_train and mytrain have the same type and shape. from __future__ import print_function from keras.preprocessing import sequence from keras.models import Sequential from keras.layers import Dense, Embedding from keras.layers import LSTM from keras.datasets import imdb import numpy as np max_features = 80 maxlen = 5 # from the example (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features) print('x_train type: ', type(x_train)) print('x_train shape:', x_train.shape) sequence.pad_sequences(x_train, maxlen=maxlen) # my test code mytrain = np.ones_like(x_train) print('mytrain type:', type(mytrain)) print('mytrain shape:', mytrain.shape) mytrain2 = sequence.pad_sequences(mytrain, maxlen=maxlen) Output: D:\python\python.exe D:/workspace/YYYY/test/test_sequences.py Using TensorFlow backend. x_train type: <class 'numpy.ndarray'> x_train shape: (25000,) Traceback (most recent call last): File ""D:/workspace/YYYY/test/test_sequences.py"", line 22, in <module> mytrain2 = sequence.pad_sequences(mytrain, maxlen=10) File ""D:\python\lib\site-packages\keras\preprocessing\sequence.py"", line 42, in pad_sequences 'Found non-iterable: ' + str(x)) mytrain type: <class 'numpy.ndarray'> ValueError: `sequences` must be a list of iterables. Found non-iterable: 1 mytrain shape: (25000,) It works if I use, for example, mytrain = np.asarray([[1, 2, 3]]) (list of iterables), but I cannot understand what the different is between x_train and mytrain in the previous code.",|python|keras|,Tensors&Inputs,1
44399299,"Keras Python Multi Image Input shape error. I am trying to teach myself to build a CNN that takes more than one image as an input. Since the dataset I created to test this is large and in the long run I hope to solve a problem involving a very large dataset, I am using a generator to read images into arrays which I am passing to Keras Model's fit_generator function. When I run my generator in isolation it works fine, and produces outputs of the appropriate shape. It yields a tuple containing two entries, the first of which has shape (4, 100, 100, 1) and the second of which has shape (4, ). Reading about multiple input Keras CNNs has given me the impression that this is the right format for a generator for a 4 input CNN that is identifying which of the 4 inputs contains an image. However, when I run the code I get: ""ValueError: Error when checking input: expected input_121 to have 4 dimensions, but got array with shape (100, 100, 1)"" I've been searching for a solution for some time now and I suspect that the problem lies in getting my (100, 100, 1) shape arrays to be sent to the Inputs as (None, 100, 100, 1) shape arrays. But when I tried to modify the output of my generator I get an error about having dimension 5, which makes sense as an error because the output of the generator should have the form X, y = [X1, X2, X3, X4], [a, b, c, d], where Xn has shape (100, 100, 1), and a/b/c/d are numbers. Here is the code: https://gist.github.com/anonymous/d283494aee982fbc30f3b52f2a6f422c Thanks in advance!",|python|machine-learning|neural-network|keras|conv-neural-network|,Tensors&Inputs,1
44500733,"Tensorflow allocating GPU memory when using tf.device('/cpu:0'). System Info: 1.1.0, GPU, Windows, Python 3.5, code runs in ipython consoles. I am trying to run two different Tensorflow sessions, one on the GPU (that does some batch work) and one on the CPU that I use for quick tests while the other works. The problem is that when I spawn the second session specifying with tf.device('/cpu:0') the session tries to allocate GPU memory and crashes my other session. My code: import os os.environ[""CUDA_VISIBLE_DEVICES""] = """" import time import tensorflow as tf with tf.device('/cpu:0'): with tf.Session() as sess: # Here 6 GBs of GPU RAM are allocated. time.sleep(5) How do I force Tensorflow to ignore the GPU? UPDATE: As suggested in a comment by @Nicolas, I took a look at this answer and ran import os os.environ[""CUDA_VISIBLE_DEVICES""] = """" import tensorflow as tf from tensorflow.python.client import device_lib print(device_lib.list_local_devices()) which prints: [name: ""/cpu:0"" device_type: ""CPU"" memory_limit: 268435456 locality { } incarnation: 2215045474989189346 , name: ""/gpu:0"" device_type: ""GPU"" memory_limit: 6787871540 locality { bus_id: 1 } incarnation: 13663872143510826785 physical_device_desc: ""device: 0, name: GeForce GTX 1080, pci bus id: 0000:02:00.0"" ] It seems to me that even if I explicitly tell the script to ignore any CUDA devices, it still finds and uses them. Could this be a bug of TF 1.1?",|python|tensorflow|,GPU Usage,3
44580450,"CUDA vs. DataParallel: Why the difference?. I have a simple neural network model and I apply either cuda() or DataParallel() on the model like following. model = torch.nn.DataParallel(model).cuda() OR, model = model.cuda() When I don't use DataParallel, rather simply transform my model to cuda(), I need to explicitly convert the batch inputs to cuda() and then give it to the model, otherwise it returns the following error. torch.index_select received an invalid combination of arguments - got (torch.cuda.FloatTensor, int, torch.LongTensor) But with DataParallel, the code works fine. Rest of the other things are same. Why this happens? Why when I use DataParallel, I don't need to transform the batch inputs explicitly to cuda()?",|pytorch|,GPU Usage,3
44583254,"ValueError: Input 0 is incompatible with layer lstm_13: expected ndim=3, found ndim=4. I am trying for multi-class classification and here are the details of my training input and output: train_input.shape= (1, 95000, 360) (95000 length input array with each element being an array of 360 length) train_output.shape = (1, 95000, 22) (22 Classes are there) model = Sequential() model.add(LSTM(22, input_shape=(1, 95000,360))) model.add(Dense(22, activation='softmax')) model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) print(model.summary()) model.fit(train_input, train_output, epochs=2, batch_size=500) The error is: ValueError: Input 0 is incompatible with layer lstm_13: expected ndim=3, found ndim=4 in line: model.add(LSTM(22, input_shape=(1, 95000,360))) Please help me out, I am not able to solve it through other answers.",|python|keras|lstm|recurrent-neural-network|,Tensors&Inputs,1
44673865,"Rank error in tf.nn.dynamic_rnn. I am trying to build a CNN + RNN model and I am getting the following error. Any help will be appreciated. fc2 has shape (?,4096) cell = tf.contrib.rnn.BasicLSTMCell(self.rnn_hidden_units) stack = tf.contrib.rnn.MultiRNNCell([cell]*self.rnn_layers) initial_state = cell.zero_state(self.batch_size, tf.float32) initial_state = tf.identity(initial_state, name='initial_state') outputs, _ = tf.nn.dynamic_rnn(stack, fc2,dtype=tf.float32) File ""rcnn.py"", line 182, in model outputs, _ = tf.nn.dynamic_rnn(stack, [fc2],dtype=tf.float32) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.py"", line 574, in dynamic_rnn dtype=dtype) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.py"", line 637, in _dynamic_rnn_loop for input_ in flat_input) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.py"", line 637, in for input_ in flat_input) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.py"", line 649, in with_rank_at_least raise ValueError(""Shape %s must have rank at least %d"" % (self, rank)) ValueError: Shape (4096, ?) must have rank at least 3",|tensorflow|,Tensors&Inputs,1
44704435,"Error when checking model input: expected lstm_1_input to have 3 dimensions, but got array with shape (339732, 29). My input is simply a csv file with 339732 rows and two columns : the first being 29 feature values, i.e. X the second being a binary label value, i.e. Y I am trying to train my data on a stacked LSTM model: data_dim = 29 timesteps = 8 num_classes = 2 model = Sequential() model.add(LSTM(30, return_sequences=True, input_shape=(timesteps, data_dim))) # returns a sequence of vectors of dimension 30 model.add(LSTM(30, return_sequences=True)) # returns a sequence of vectors of dimension 30 model.add(LSTM(30)) # return a single vector of dimension 30 model.add(Dense(1, activation='softmax')) model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy']) model.summary() model.fit(X_train, y_train, batch_size = 400, epochs = 20, verbose = 1) This throws the error: Traceback (most recent call last): File ""first_approach.py"", line 80, in model.fit(X_train, y_train, batch_size = 400, epochs = 20, verbose = 1) ValueError: Error when checking model input: expected lstm_1_input to have 3 dimensions, but got array with shape (339732, 29) I tried reshaping my input using X_train.reshape((1,339732, 29)) but it did not work showing error: ValueError: Error when checking model input: expected lstm_1_input to have shape (None, 8, 29) but got array with shape (1, 339732, 29) How can I feed in my input to the LSTM ?",|python|keras|lstm|recurrent-neural-network|valueerror|,Tensors&Inputs,1
44720822,"ValueError with Concatenate Layer (Keras functional API). After some search here, I still can't find a solution for this. I'm new to Keras, apologies if there is a solution and I actually didn't understand how it was related to my problem. I am making a small RNN with Keras 2/Functional API, and I have trouble to make the Concatenate Layer work. Here is my structure : inputSentence = Input(shape=(30, 91)) sentenceMatrix = LSTM(91, return_sequences=True, input_shape=(30, 91))(inputSentence) inputDeletion = Input(shape=(30, 1)) deletionMatrix = (LSTM(30, return_sequences=True, input_shape=(30, 1)))(inputDeletion) fusion = Concatenate([sentenceMatrix, deletionMatrix]) fusion = Dense(122, activation='relu')(fusion) fusion = Dense(102, activation='relu')(fusion) fusion = Dense(91, activation='sigmoid')(fusion) F = Model(inputs=[inputSentence, inputDeletion], outputs=fusion) And here is the error: ValueError: Unexpectedly found an instance of type `<class 'keras.layers.merge.Concatenate'>`. Expected a symbolic tensor instance. Full History if it helps a bit more : Using TensorFlow backend. str(inputs) + '. All inputs to the layer ' ValueError: Layer dense_1 was called with an input that isn't a symbolic tensor. Received type: <class 'keras.layers.merge.Concatenate'>. Full input: [<keras.layers.merge.Concatenate object at 0x00000000340DC4E0>]. All inputs to the layer should be tensors. self.assert_input_compatibility(inputs) File ""C:\ProgramData\Anaconda3\lib\site-packages\keras\engine\topology.py"", line 425, in assert_input_compatibility fusion = Dense(122, activation='relu')(fusion) File ""C:\ProgramData\Anaconda3\lib\site-packages\keras\engine\topology.py"", line 552, in __call__ Traceback (most recent call last): File ""C:\ProgramData\Anaconda3\lib\site-packages\keras\engine\topology.py"", line 419, in assert_input_compatibility K.is_keras_tensor(x) File ""C:\ProgramData\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py"", line 392, in is_keras_tensor raise ValueError('Unexpectedly found an instance of type `' + str(type(x)) + '`. ' ValueError: Unexpectedly found an instance of type `<class 'keras.layers.merge.Concatenate'>`. Expected a symbolic tensor instance. I'm using Python 3.6, with Spyder 3.1.4, on Windows 7. I upgraded TensorFlow and Keras with pip this morning. Thank you for any help provided !",|python|neural-network|concatenation|keras|valueerror|,API,4
44998910,"Keras model to fit polynomial. I generated some data from a 4th degree polynomial and wanted to create a regression model in Keras to fit this polynomial. The problem is that predictions after fitting seem to be basically linear. Since this is my first time working with neural nets I assume I made a very trivial and stupid mistake. Here is my code: model = Sequential() model.add(Dense(units=200, input_dim=1)) model.add(Activation('relu')) model.add(Dense(units=45)) model.add(Activation('relu')) model.add(Dense(units=1)) model.compile(loss='mean_squared_error', optimizer='sgd') model.fit(x_train, y_train, epochs=20, batch_size=50) loss_and_metrics = model.evaluate(x_test, y_test, batch_size=100) classes = model.predict(x_test, batch_size=1) x_train and y_train are numpy arrays containing the first 9900 entries from this file. I tried different batch_sizes, number of epochs, layer sizes and amounts of training data. Nothing seems to help. Please point out everything you see that does not make sense!",|python|neural-network|keras|non-linear-regression|,Model,0
45029977,"TensorFlow Object Detection API Weird Behavior. I was playing with TensorFlow's brand new Object Detection API and decided to train it on some other publicly available datasets. I happened to stumble upon this grocery dataset which consists of images of various brands of cigarette boxes on the supermarket shelf along with a text file which lists out the bounding boxes of each cigarette box in each image. 10 major brands have been labeled in the dataset and all other brands fall into the 11th ""miscellaneous"" category. I followed their tutorial and managed to train the model on this dataset. Due to limitations on processing power, I used only a third of the dataset and performed a 70:30 split for training and testing data. I used the faster_rcnn_resnet101 model. All parameters in my config file are the same as the default parameters provided by TF. After 16491 global steps, I tested the model on some images but I am not too happy with the results - Failed to detect the Camels in top-shelf whereas it detects the product in other images Why does it fail to detect the Marlboros in the top row? Another issue I had is that the model never detected any other label except for label 1 Doesn't detected a crop instance of the product from the training data It detects cigarette boxes with 99% confidence even in negative images! Can somebody help me with what is going wrong? What can I do to improve the accuracy? And why does it detect all products to belong in category 1 even though I have mentioned that there are 11 classes in total? Edit Added my label map: item { id: 1 name: '1' } item { id: 2 name: '2' } item { id: 3 name: '3' } item { id: 4 name: '4' } item { id: 5 name: '5' } item { id: 6 name: '6' } item { id: 7 name: '7' } item { id: 8 name: '8' } item { id: 9 name: '9' } item { id: 10 name: '10' } item { id: 11 name: '11' }",|python|machine-learning|tensorflow|classification|object-detection|,Training,2
45139423,"Tensorflow error: FailedPeconditionError: attempting to use uninitialized variable. I want to put a stereo image into an optimizer. This is my code: tf.reset_default_graph() # config learning_rate = 0.5 training_epochs = 5 # init init = tf.global_variables_initializer() def conv2d(input_layer): conv1 = tf.layers.conv2d( inputs=input_layer, filters=32, kernel_size=[3, 3], padding='same', activation=tf.tanh, use_bias=False ) conv2 = tf.layers.conv2d( inputs=conv1, filters=32, kernel_size=[3, 3], padding='same', activation=tf.tanh, use_bias=False ) conv3 = tf.layers.conv2d( inputs=conv2, filters=32, kernel_size=[3, 3], padding='same', activation=tf.tanh, use_bias=False ) conv4 = tf.layers.conv2d( inputs=conv3, filters=32, kernel_size=[3, 3], padding='same', activation=tf.tanh, use_bias=False ) logits = tf.layers.conv2d( inputs=conv4, filters=32, kernel_size=[3, 3], padding='same', activation=tf.sigmoid, use_bias=False ) return logits if __name__ == '__main__': # read images # preprocessing: rgb converted to float, zero_mean, uni_variance images = reading_images() mask_tensor = images[""mask""][1] # reshape images img0 = images[""img0""][1] img1 = images[""img1""][1] img0_rs = tf.reshape(img0, [1, int(1988 / 2), int(2880 / 2), 3]) img1_rs = tf.reshape(img1, [1, int(1988 / 2), int(2880 / 2), 3]) # define symbolic placeholders t_im0 = tf.placeholder(tf.float32, [1, None, None, 3]) t_im1 = tf.placeholder(tf.float32, [1, None, None, 3]) t_img = tf.concat([t_im0, t_im1], axis=3) input_layer = tf.reshape(t_img, [1, int(1988 / 2), int(2880 / 2), 6]) logits = conv2d(input_layer) with tf.name_scope(""cost_function"") as scope: mask_tensor = tf.tile(mask_tensor, [1, 1, 3]) cost_function = -tf.reduce_mean(mask_tensor * tf.log(logits) + (1. - mask_tensor) * tf.log(1. - logits)) tf.summary.scalar(""cost_function"", cost_function) with tf.name_scope(""train"") as scope: optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost_function) merged_summary_op = tf.summary.merge_all() with tf.Session() as sess: coord = tf.train.Coordinator() threads = tf.train.start_queue_runners(sess=sess, coord=coord) sess.run(init) # summary_writer = tf.summary.FileWriter('/tmp/tensorflow_logs', graph=sess.graph) for epoch in range(training_epochs): print(""epoch "", epoch) avg_cost = 0.0 mask = sess.run(mask_tensor) np_img0_rs = sess.run(img0_rs) np_img1_rs = sess.run(img1_rs) # res = t_img.eval(feed_dict={t_im0: img0_rs_, t_im1: img1_rs_}) sess.run([optimizer], feed_dict={t_im0: np_img0_rs, t_im1: np_img1_rs}) coord.request_stop() coord.join(threads) But I always get this error. I do not know what it can be what I have to change. What can I try to debug it? I really tried a lot to fix this error. epoch 0 2017-07-17 10:26:03.719539: W tensorflow/core/kernels/queue_base.cc:294] _4_input_producer: Skipping cancelled enqueue attempt with queue not closed 2017-07-17 10:26:03.719610: W tensorflow/core/kernels/queue_base.cc:294] _5_input_producer_1: Skipping cancelled enqueue attempt with queue not closed Traceback (most recent call last): File ""/home/test/Dropbox/occlusion_thesis/occ_small/main.py"", line 111, in <module> sess.run([optimizer], feed_dict={t_im0: np_img0_rs, t_im1: np_img1_rs}) File ""/home/test/Programs/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 789, in run run_metadata_ptr) File ""/home/test/Programs/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 997, in _run feed_dict_string, options, run_metadata) File ""/home/test/Programs/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1132, in _do_run target_list, options, run_metadata) File ""/home/test/Programs/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1152, in _do_call raise type(e)(node_def, op, message) tensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value conv2d_4/kernel [[Node: conv2d_4/kernel/read = Identity[T=DT_FLOAT, _class=[""loc:@conv2d_4/kernel""], _device=""/job:localhost/replica:0/task:0/cpu:0""](conv2d_4/kernel)]] Caused by op u'conv2d_4/kernel/read', defined at: File ""/home/test/Dropbox/occlusion_thesis/occ_small/main.py"", line 84, in <module> logits = conv2d(input_layer) File ""/home/test/Dropbox/occlusion_thesis/occ_small/main.py"", line 60, in conv2d use_bias=False File ""/home/test/Programs/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/convolutional.py"", line 551, in conv2d return layer.apply(inputs) File ""/home/test/Programs/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/base.py"", line 492, in apply return self.__call__(inputs, *args, **kwargs) File ""/home/test/Programs/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/base.py"", line 434, in __call__ self.build(input_shapes[0]) File ""/home/test/Programs/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/convolutional.py"", line 137, in build dtype=self.dtype) File ""/home/test/Programs/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/base.py"", line 374, in add_variable trainable=trainable and self.trainable) File ""/home/test/Programs/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 1065, in get_variable use_resource=use_resource, custom_getter=custom_getter) File ""/home/test/Programs/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 962, in get_variable use_resource=use_resource, custom_getter=custom_getter) File ""/home/test/Programs/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 367, in get_variable validate_shape=validate_shape, use_resource=use_resource) File ""/home/test/Programs/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 352, in _true_getter use_resource=use_resource) File ""/home/test/Programs/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 725, in _get_single_variable validate_shape=validate_shape) File ""/home/test/Programs/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variables.py"", line 200, in __init__ expected_shape=expected_shape) File ""/home/test/Programs/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variables.py"", line 319, in _init_from_args self._snapshot = array_ops.identity(self._variable, name=""read"") File ""/home/test/Programs/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 1303, in identity result = _op_def_lib.apply_op(""Identity"", input=input, name=name) File ""/home/test/Programs/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 767, in apply_op op_def=op_def) File ""/home/test/Programs/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2506, in create_op original_op=self._default_original_op, op_def=op_def) File ""/home/test/Programs/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1269, in __init__ self._traceback = _extract_stack() FailedPreconditionError (see above for traceback): Attempting to use uninitialized value conv2d_4/kernel [[Node: conv2d_4/kernel/read = Identity[T=DT_FLOAT, _class=[""loc:@conv2d_4/kernel""], _device=""/job:localhost/replica:0/task:0/cpu:0""](conv2d_4/kernel)]]",|machine-learning|tensorflow|computer-vision|,API,4
45378493,"Why does a binary Keras CNN always predict 1?. I want to build a binary classifier using a Keras CNN. I have about 6000 rows of input data which looks like this: >> print(X_train[0]) [[[-1.06405307 -1.06685851 -1.05989663 -1.06273152] [-1.06295958 -1.06655996 -1.05969803 -1.06382503] [-1.06415248 -1.06735609 -1.05999593 -1.06302975] [-1.06295958 -1.06755513 -1.05949944 -1.06362621] [-1.06355603 -1.06636092 -1.05959873 -1.06173742] [-1.0619655 -1.06655996 -1.06039312 -1.06412326] [-1.06415248 -1.06725658 -1.05940014 -1.06322857] [-1.06345662 -1.06377347 -1.05890365 -1.06034568] [-1.06027557 -1.06019084 -1.05592469 -1.05537518] [-1.05550398 -1.06038988 -1.05225064 -1.05676692]]] >>> print(y_train[0]) [1] And then I've build a CNN by this way: model = Sequential() model.add(Convolution1D(input_shape = (10, 4), nb_filter=16, filter_length=4, border_mode='same')) model.add(BatchNormalization()) model.add(LeakyReLU()) model.add(Dropout(0.2)) model.add(Convolution1D(nb_filter=8, filter_length=4, border_mode='same')) model.add(BatchNormalization()) model.add(LeakyReLU()) model.add(Dropout(0.2)) model.add(Flatten()) model.add(Dense(64)) model.add(BatchNormalization()) model.add(LeakyReLU()) model.add(Dense(1)) model.add(Activation('softmax')) reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.9, patience=30, min_lr=0.000001, verbose=0) model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) history = model.fit(X_train, y_train, nb_epoch = 100, batch_size = 128, verbose=0, validation_data=(X_test, y_test), callbacks=[reduce_lr], shuffle=True) y_pred = model.predict(X_test) But it returns the following: >> print(confusion_matrix(y_test, y_pred)) [[ 0 362] [ 0 608]] Why all predictions are ones? Why does the CNN perform so bad? Here are the loss and acc charts:",|python|machine-learning|tensorflow|keras|keras-layer|,Model,0
45404056,"tf.unstack with dynamic shape. I'm trying to unstack a Tensor because I need a sequence as input for the RNN. I am using variable sequence lengths which prevents me from correctly using tf.unstack. def MapToSequences(x): # x.get_shape().as_list() = [64, 1, None, 512] x = tf.squeeze(x) # tf.shape(x) = [None, None, None], at runtime would be [64, seqlen, 512] x = tf.transpose(x, perm=[1, 0, 2]) # [seqlen, 64, 512] # Here I'd like to unstack with seqlen as num x = tf.unstack(x) # Cannot infer num from shape (?, ?, ?) return x I tried using tf.shape(x) to infer the seqlen and use it as num, but I get Expected int for argument 'num' not <tf.Tensor 'strided_slice:0' shape=() dtype=int32>",|tensorflow|,API,4
45442843,"Sigmoid layer in Keras. I have a list of values, ranging from 15000 to 25000. I have to separate them into two categories, such that (approx) 20000 will end up in category 1 and the rest in category 2. I figured out that the sigmoid activation should work for this. I am using the following layers in keras for that: model = Sequential() model.add(Dense(1 , input_dim =1 )) model.add(Activation('sigmoid')) model.add(Dense(2 , init='normal' , activation = 'softmax')) model.compile(loss='mean_absolute_error', optimizer='rmsprop') model.fit(X_train, y_train, validation_data=(X_test, y_test),epochs=10,batch_size=200,verbose=2) However, when I run the model for my sample cases, all values end up in category 2. How can I improve this?",|python|machine-learning|neural-network|keras|sigmoid|,Training,2
45467720,"ImageDataGenerator.flow: NumpyArrayIterator is set to use the data format convention ""channels_last"", when fixed causes fitting error. My data is of shape (60000, 1, 28, 28) when I try to get them on batches as following: gen = image.ImageDataGenerator() train_batches = gen.flow(x_train, y_train, batch_size=64) I get the error: ValueError: NumpyArrayIterator is set to use the data format convention ""channels_last"" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (60000, 1, 28, 28) (28 channels). in order to get rid of it, I do: train_batches = gen.flow(np.swapaxes(x_train,1,3), y_train, batch_size=64) Although this does remove the above error, it generates the following error: ValueError: Error when checking input: expected lambda_13_input to have shape (None, 1, 28, 28) but got array with shape (64, 28, 28, 1) when doing: lin_model.fit_generator(train_batches, train_batches.n, nb_epoch=1, validation_data= test_batches, nb_val_samples=test_batches.n) I made sure that I added to my code ordering specifier: import keras.backend as k k.image_dim_ordering() == 'th' The full trace for that is: ValueError Traceback (most recent call last) <ipython-input-138-f8ea3b9faad4> in <module>() ----> 1 training_routine(lin_model) <ipython-input-136-8b3171cd58ae> in training_routine(model) 2 model.optimizer.lr = 0.001 3 model.fit_generator(train_batches, train_batches.n, nb_epoch=1, ----> 4 validation_data= test_batches, nb_val_samples=test_batches.n) 5 model.optimizer.lr = 0.1 6 model.fit_generator(train_batches, train_batches.n, nb_epoch=1, /home/matar/anaconda2/lib/python2.7/site-packages/keras/legacy/interfaces.pyc in wrapper(*args, **kwargs) 85 warnings.warn('Update your `' + object_name + 86 '` call to the Keras 2 API: ' + signature, stacklevel=2) ---> 87 return func(*args, **kwargs) 88 wrapper._original_function = func 89 return wrapper /home/matar/anaconda2/lib/python2.7/site-packages/keras/models.pyc in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, initial_epoch) 1115 workers=workers, 1116 use_multiprocessing=use_multiprocessing, -> 1117 initial_epoch=initial_epoch) 1118 1119 @interfaces.legacy_generator_methods_support /home/matar/anaconda2/lib/python2.7/site-packages/keras/legacy/interfaces.pyc in wrapper(*args, **kwargs) 85 warnings.warn('Update your `' + object_name + 86 '` call to the Keras 2 API: ' + signature, stacklevel=2) ---> 87 return func(*args, **kwargs) 88 wrapper._original_function = func 89 return wrapper /home/matar/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, initial_epoch) 1838 outs = self.train_on_batch(x, y, 1839 sample_weight=sample_weight, -> 1840 class_weight=class_weight) 1841 1842 if not isinstance(outs, list): /home/matar/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc in train_on_batch(self, x, y, sample_weight, class_weight) 1557 sample_weight=sample_weight, 1558 class_weight=class_weight, -> 1559 check_batch_axis=True) 1560 if self.uses_learning_phase and not isinstance(K.learning_phase(), int): 1561 ins = x + y + sample_weights + [1.] /home/matar/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc in _standardize_user_data(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size) 1232 self._feed_input_shapes, 1233 check_batch_axis=False, -> 1234 exception_prefix='input') 1235 y = _standardize_input_data(y, self._feed_output_names, 1236 output_shapes, /home/matar/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc in _standardize_input_data(data, names, shapes, check_batch_axis, exception_prefix) 138 ' to have shape ' + str(shapes[i]) + 139 ' but got array with shape ' + --> 140 str(array.shape)) 141 return arrays 142 ValueError: Error when checking input: expected lambda_13_input to have shape (None, 1, 28, 28) but got array with shape (64, 28, 28, 1)",|python|keras|,Tensors&Inputs,1
45507821,"Image Classification. Validation loss stuck during training with inception (v1). I have built a small custom image classification training/val dataset with 4 classes. The training dataset has ~ 110.000 images. The validation dataset has ~ 6.000 images. The problem I'm experiencing is that, during training, both training accuracy (measured as an average accuracy on the last training samples) and training loss improve, while validation accuracy and loss stay the same. This only occurs when I use inception and resnet models, if I use an alexnet model on the same training and validation data, the validation loss and accuracy improve In my experiments I am employing several convolutional architectures by importing them from tensorflow.contrib.slim.nets The code is organized as follows: ... images, labels = preprocessing(..., train=True) val_images, val_labels = preprocessing(..., train=False) ... # AlexNet model with slim.arg_scope(alexnet.alexnet_v2_arg_scope()): logits, _ = alexnet.alexnet_v2(images, ..., is_training=True) tf.get_variable_scope().reuse_variables() val_logits, _ = alexnet.alexnet_v2(val_images, ..., is_training=False) # Inception v1 model with slim.arg_scope(inception_v1_arg_scope()): logits, _ = inception_v1(images, ..., is_training=True) val_logits, _ = inception_v1(val_images, ..., is_training=False, reuse=True) loss = my_stuff.loss(logits, labels) val_loss = my_stuff.loss(val_logits, val_labels) training_accuracy_op = tf.nn.in_top_k(logits, labels, 1) top_1_op = tf.nn.in_top_k(val_logits, val_labels, 1) train_op = ... ... Instead of using a separate eval script, I'm running the validation step at the end of each epoch and also, for debugging purposes, I'm running an early val step (before training) and I'm checking the training accuracies by averaging training predictions on the last x steps. When I use the Inception v1 model (commenting out the alexnet one) the logger output is as follows after 1 epoch: early Validation Step precision @ 1 = 0.2440 val loss = 1.39 Starting epoch 0 step 50, loss = 1.38, training_acc = 0.3250 ... step 1000, loss = 0.58, training_acc = 0.6725 ... step 3550, loss = 0.45, training_acc = 0.8063 Validation Step precision @ 1 = 0.2473 val loss = 1.39 As shown, training accuracy and loss improve a lot after one epoch, but the validation loss doesn't change at all. This has been tested at least 10 times, the result is always the same. I would understand if the validation loss was getting worse due to overfitting, but in this case it's not changing at all. To rule out any problems with the validation data, I'm also presenting the results while training using the AlexNet implementation in slim. Training with the alexnet model produces the following output: early Validation Step precision @ 1 = 0.2448 val loss = 1.39 Starting epoch 0 step 50, loss = 1.39, training_acc = 0.2587 ... step 350, loss = 1.38, training_acc = 0.2919 ... step 850, loss = 1.28, training_acc = 0.3898 Validation Step precision @ 1 = 0.4069 val loss = 1.25 Accuracy and validation loss, both in training and test data, correctly improve when using the alexnet model, and they keep improving in subsequent epochs. I don't understand what may be the cause of the problem, and why it presents itself when using inception/resnet models, but not when training with alexnet. Does anyone have ideas?",|validation|tensorflow|deep-learning|classification|loss|,Training,2
45525444,"Keras handling large dataset which cannot fit into memory. I'm working on facial expression recognition, and I'm using Keras. I've collected many datasets, and then I have applied data augmentation on the images, I've got about 500 000 images saved (as pixels) on a .csv file (same format as fer2013.csv). This is the code I'm using : def Zerocenter_ZCA_whitening_Global_Contrast_Normalize(list): Intonumpyarray = numpy.asarray(list) data = Intonumpyarray.reshape(img_width,img_height) data2 = ZeroCenter(data) data3 = zca_whitening(flatten_matrix(data2)).reshape(img_width,img_height) data4 = global_contrast_normalize(data3) data5 = numpy.rot90(data4,3) return data5 def load_data(): train_x = [] train_y = [] val_x = [] val_y = [] test_x = [] test_y = [] f = open('ALL.csv') csv_f = csv.reader(f) for row in csv_f: if str(row[2]) == ""Training"": temp_list_train = [] for pixel in row[1].split(): temp_list_train.append(int(pixel)) data = Zerocenter_ZCA_whitening_Global_Contrast_Normalize(temp_list_train) train_y.append(int(row[0])) train_x.append(data.reshape(data_resh).tolist()) elif str(row[2]) == ""PublicTest"": temp_list_validation = [] for pixel in row[1].split(): temp_list_validation.append(int(pixel)) data = Zerocenter_ZCA_whitening_Global_Contrast_Normalize(temp_list_validation) val_y.append(int(row[0])) val_x.append(data.reshape(data_resh).tolist()) elif str(row[2]) == ""PrivateTest"": temp_list_test = [] for pixel in row[1].split(): temp_list_test.append(int(pixel)) data = Zerocenter_ZCA_whitening_Global_Contrast_Normalize(temp_list_test) test_y.append(int(row[0])) test_x.append(data.reshape(data_resh).tolist()) return train_x, train_y, val_x, val_y, test_x, test_y And then I load data and feed them to the generator : Train_x, Train_y, Val_x, Val_y, Test_x, Test_y = load_data() Train_x = numpy.asarray(Train_x) Train_x = Train_x.reshape(Train_x.shape[0],img_rows,img_cols) Test_x = numpy.asarray(Test_x) Test_x = Test_x.reshape(Test_x.shape[0],img_rows,img_cols) Val_x = numpy.asarray(Val_x) Val_x = Val_x.reshape(Val_x.shape[0],img_rows,img_cols) Train_x = Train_x.reshape(Train_x.shape[0], img_rows, img_cols, 1) Test_x = Test_x.reshape(Test_x.shape[0], img_rows, img_cols, 1) Val_x = Val_x.reshape(Val_x.shape[0], img_rows, img_cols, 1) Train_x = Train_x.astype('float32') Test_x = Test_x.astype('float32') Val_x = Val_x.astype('float32') Train_y = np_utils.to_categorical(Train_y, nb_classes) Test_y = np_utils.to_categorical(Test_y, nb_classes) Val_y = np_utils.to_categorical(Val_y, nb_classes) datagen = ImageDataGenerator( featurewise_center=False, samplewise_center=False, featurewise_std_normalization=False, samplewise_std_normalization=False, zca_whitening=False, width_shift_range=0.2, height_shift_range=0.2, horizontal_flip=True, shear_range=0.03, zoom_range=0.03, vertical_flip=False) datagen.fit(Train_x) model.fit_generator(datagen.flow(Train_x, Train_y, batch_size=batch_size), samples_per_epoch=Train_x.shape[0], nb_epoch=nb_epoch, validation_data=(Val_x, Val_y)) When I run the code, RAM usage gets bigger and bigger until the pc freezes (I've have 16 Gb). It get stuck when loading_data() is called. Any solution for this problem that can fits my code ?",|csv|keras|training-data|large-data|,Training,2
45537372,"AttributeError: 'History' object has no attribute 'predict' - Fitting a List of train and test data. I am trying a NN model using this example. I am fitting a list of values to a NN model. However, I am getting an AttributeError. This has been asked before and has been answered. Unfortunately, it is not working for me. As shown in the example, I created the following, from keras.models import Sequential from keras.wrappers.scikit_learn import KerasRegressor from sklearn.model_selection import KFold from sklearn.model_selection import cross_val_score from keras.layers import Dense def neuralnetmodel(): #Crete model model = Sequential() model.add(Dense(13, input_dim = 13, kernel_initializer = 'normal', activation = 'relu')) model.add(Dense(1, kernel_initializer = 'normal', activation = 'relu')) model.add(Dense(1, kernel_initializer = 'normal', activation = 'relu')) ## Output layer model.add(Dense(1, kernel_initializer = 'normal')) #Compile model model.compile(loss = 'mean_squared_error', optimizer = 'adam') return model fit training data, NNmodelList = [] for i,j in zip(X_train_scaled,y_train): nn_model = KerasRegressor(build_fn= neuralnetmodel, nb_epoch = 50, batch_size = 10, verbose = 0) NNmodelList.append(nn_model.fit(i,j)) predict from test data, PredList = [] for val in X_test_scaled: for mod in NNmodelList: pred = mod.predict(val) PredList.append(pred) Now, I am getting the error: AttributeError: 'History' object has no attribute 'predict' In previous threads , it seems to be the train set was not fit to the model before predict. However, in mine, I fit them in the second code snippet. Any ideas what other possible mistakes I am making?",|python|list|machine-learning|neural-network|keras|,API,4
45632549,"Why is the accuracy for my Keras model always 0 when training?. I have built a simple Keras network: import numpy as np; from keras.models import Sequential; from keras.layers import Dense,Activation; data= np.genfromtxt(""./kerastests/mydata.csv"", delimiter=';') x_target=data[:,29] x_training=np.delete(data,6,axis=1) x_training=np.delete(x_training,28,axis=1) model=Sequential() model.add(Dense(20,activation='relu', input_dim=x_training.shape[1])) model.add(Dense(10,activation='relu')) model.add(Dense(1)); model.compile(optimizer='adam',loss='mean_squared_error',metrics=['accuracy']) model.fit(x_training, x_target) From my source data, I have removed 2 columns, as you can see. One is a column that came with dates in a string format (in the dataset, besides it, I have a column for the day, another for the month, and another for the year, so I don't need that column) and the other column is the column I use as target for the model). When I train this model I get this output: 32/816 [>.............................] - ETA: 23s - loss: 13541942.0000 - acc: 0.0000e+00 800/816 [============================>.] - ETA: 0s - loss: 11575466.0400 - acc: 0.0000e+00 816/816 [==============================] - 1s - loss: 11536905.2353 - acc: 0.0000e+00 Epoch 2/10 32/816 [>.............................] - ETA: 0s - loss: 6794785.0000 - acc: 0.0000e+00 816/816 [==============================] - 0s - loss: 5381360.4314 - acc: 0.0000e+00 Epoch 3/10 32/816 [>.............................] - ETA: 0s - loss: 6235184.0000 - acc: 0.0000e+00 800/816 [============================>.] - ETA: 0s - loss: 5199512.8700 - acc: 0.0000e+00 816/816 [==============================] - 0s - loss: 5192977.4216 - acc: 0.0000e+00 Epoch 4/10 32/816 [>.............................] - ETA: 0s - loss: 4680165.5000 - acc: 0.0000e+00 736/816 [==========================>...] - ETA: 0s - loss: 5050110.3043 - acc: 0.0000e+00 816/816 [==============================] - 0s - loss: 5168771.5490 - acc: 0.0000e+00 Epoch 5/10 32/816 [>.............................] - ETA: 0s - loss: 5932391.0000 - acc: 0.0000e+00 768/816 [===========================>..] - ETA: 0s - loss: 5198882.9167 - acc: 0.0000e+00 816/816 [==============================] - 0s - loss: 5159585.9020 - acc: 0.0000e+00 Epoch 6/10 32/816 [>.............................] - ETA: 0s - loss: 4488318.0000 - acc: 0.0000e+00 768/816 [===========================>..] - ETA: 0s - loss: 5144843.8333 - acc: 0.0000e+00 816/816 [==============================] - 0s - loss: 5151492.1765 - acc: 0.0000e+00 Epoch 7/10 32/816 [>.............................] - ETA: 0s - loss: 6920405.0000 - acc: 0.0000e+00 800/816 [============================>.] - ETA: 0s - loss: 5139358.5000 - acc: 0.0000e+00 816/816 [==============================] - 0s - loss: 5169839.2941 - acc: 0.0000e+00 Epoch 8/10 32/816 [>.............................] - ETA: 0s - loss: 3973038.7500 - acc: 0.0000e+00 672/816 [=======================>......] - ETA: 0s - loss: 5183285.3690 - acc: 0.0000e+00 816/816 [==============================] - 0s - loss: 5141417.0000 - acc: 0.0000e+00 Epoch 9/10 32/816 [>.............................] - ETA: 0s - loss: 4969548.5000 - acc: 0.0000e+00 768/816 [===========================>..] - ETA: 0s - loss: 5126550.1667 - acc: 0.0000e+00 816/816 [==============================] - 0s - loss: 5136524.5098 - acc: 0.0000e+00 Epoch 10/10 32/816 [>.............................] - ETA: 0s - loss: 6334703.5000 - acc: 0.0000e+00 768/816 [===========================>..] - ETA: 0s - loss: 5197778.8229 - acc: 0.0000e+00 816/816 [==============================] - 0s - loss: 5141391.2059 - acc: 0.0000e+00 Why is this happening? My data is a time series. I know that for time series people do not usually use Dense neurons, but it is just a test. What really tricks me is that accuracy is always 0. And, with other tests, I did even lose: gets to a ""NAN"" value. Could anybody help here?",|tensorflow|machine-learning|keras|neural-network|,Training,2
45645276,"Negative dimension size caused by subtracting 3 from 1 for 'conv2d_2/convolution'. I got this error message when declaring the input layer in Keras. ValueError: Negative dimension size caused by subtracting 3 from 1 for 'conv2d_2/convolution' (op: 'Conv2D') with input shapes: [?,1,28,28], [3,3,28,32]. My code is like this model.add(Convolution2D(32, 3, 3, activation='relu', input_shape=(1,28,28))) Sample application: https://github.com/IntellijSys/tensorflow/blob/master/Keras.ipynb",|python|tensorflow|neural-network|keras|keras-layer|,Tensors&Inputs,1
45711636,"Difficulty with cnn in keras. I'm trying to build a dcnn, but I got this error: ValueError: ('The specified size contains a dimension with value <= 0', (-192, 1024)) And really, I don't have idea the reason of this error, here's my code: The data: c_X = open(""C:/Users/PC/Desktop/Notebooks/Isabelle/mfcc_train_I_C_I_C_2.dat"", ""r"") c_y = open(""C:/Users/PC/Desktop/Notebooks/Isabelle/phoneme_train_I_C_I_C_2.dat"", ""r"") c_X = np.fromfile(c_X, np.dtype('float32')) c_y = np.fromfile(c_y, np.dtype('int8')) c_X = c_X.reshape(886887,1120) c_X = c_X.reshape(c_X.shape[0], 1, 20, 56) c_y = one_hot(c_y) #c_y = np.append(c_y, np.zeros((374975,1)), axis=1) X_3 = apendice(Colere_X, c_X) y_3 = apendice(Colere_y, c_y) #print(c_X.shape, c_y.shape) print(X_3.shape, y_3.shape) (1123867, 1, 20, 56) (1123867, 38) This is my neural network implementation (the problem is here I think): model = Sequential() model.add(Conv2D(32, (3, 3), border_mode='valid', activation='relu',input_shape=(1, 20, 56))) model.add(Dropout(0.25)) model.add(Conv2D(32, (3, 3), border_mode='valid', activation='relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Dropout(0.25)) model.add(Conv2D(32, (3, 3), border_mode='valid', activation='relu')) model.add(Conv2D(32, (3, 3), activation='relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Dropout(0.25)) model.add(Flatten()) model.add(Dense(1024, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(num_classes, activation='softmax')) # Compile the model model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) # Train the model start = time.time() model_info = model.fit(X_3, y_3, batch_size=100, \ epochs=20, verbose=2, validation_data=(X_test, y_test)) end = time.time() Here the summary of model: _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_21 (Conv2D) (None, -1, 18, 32) 16160 _________________________________________________________________ dropout_16 (Dropout) (None, -1, 18, 32) 0 _________________________________________________________________ conv2d_22 (Conv2D) (None, -3, 16, 32) 9248 _________________________________________________________________ max_pooling2d_11 (MaxPooling (None, -2, 8, 32) 0 _________________________________________________________________ dropout_17 (Dropout) (None, -2, 8, 32) 0 _________________________________________________________________ conv2d_23 (Conv2D) (None, -4, 6, 32) 9248 _________________________________________________________________ conv2d_24 (Conv2D) (None, -6, 4, 32) 9248 _________________________________________________________________ max_pooling2d_12 (MaxPooling (None, -3, 2, 32) 0 _________________________________________________________________ dropout_18 (Dropout) (None, -3, 2, 32) 0 _________________________________________________________________ flatten_6 (Flatten) (None, -192) 0 ================================================================= Total params: 43,904 Trainable params: 43,904 Non-trainable params: 0 _________________________________________________________________ --------------------------------------------------------------------------- ValueError Traceback (most recent call last) <ipython-input-17-589407073ff5> in <module>() 13 model.add(Flatten()) 14 model.summary() ---> 15 model.add(Dense(256, activation='relu')) 16 model.add(Dropout(0.5)) 17 model.add(Dense(num_classes, activation='softmax')) ~\Anaconda3\envs\tensorflow-gpu\lib\site-packages\keras\models.py in add(self, layer) 467 output_shapes=[self.outputs[0]._keras_shape]) 468 else: --> 469 output_tensor = layer(self.outputs[0]) 470 if isinstance(output_tensor, list): 471 raise TypeError('All layers in a Sequential model ' ~\Anaconda3\envs\tensorflow-gpu\lib\site-packages\keras\engine\topology.py in __call__(self, inputs, **kwargs) 567 '`layer.build(batch_input_shape)`') 568 if len(input_shapes) == 1: --> 569 self.build(input_shapes[0]) 570 else: 571 self.build(input_shapes) ~\Anaconda3\envs\tensorflow-gpu\lib\site-packages\keras\layers\core.py in build(self, input_shape) 823 name='kernel', 824 regularizer=self.kernel_regularizer, --> 825 constraint=self.kernel_constraint) 826 if self.use_bias: 827 self.bias = self.add_weight(shape=(self.units,), ~\Anaconda3\envs\tensorflow-gpu\lib\site-packages\keras\legacy\interfaces.py in wrapper(*args, **kwargs) 85 warnings.warn('Update your `' + object_name + 86 '` call to the Keras 2 API: ' + signature, stacklevel=2) ---> 87 return func(*args, **kwargs) 88 wrapper._original_function = func 89 return wrapper ~\Anaconda3\envs\tensorflow-gpu\lib\site-packages\keras\engine\topology.py in add_weight(self, name, shape, dtype, initializer, regularizer, trainable, constraint) 389 if dtype is None: 390 dtype = K.floatx() --> 391 weight = K.variable(initializer(shape), dtype=dtype, name=name) 392 if regularizer is not None: 393 self.add_loss(regularizer(weight)) ~\Anaconda3\envs\tensorflow-gpu\lib\site-packages\keras\initializers.py in __call__(self, shape, dtype) 206 limit = np.sqrt(3. * scale) 207 return K.random_uniform(shape, -limit, limit, --> 208 dtype=dtype, seed=self.seed) 209 210 def get_config(self): ~\Anaconda3\envs\tensorflow-gpu\lib\site-packages\keras\backend\theano_backend.py in random_uniform(shape, minval, maxval, dtype, seed) 2189 seed = np.random.randint(1, 10e6) 2190 rng = RandomStreams(seed=seed) -> 2191 return rng.uniform(shape, low=minval, high=maxval, dtype=dtype) 2192 2193 ~\Anaconda3\envs\tensorflow-gpu\lib\site-packages\theano\sandbox\rng_mrg.py in uniform(self, size, low, high, ndim, dtype, nstreams) 854 raise ValueError( 855 ""The specified size contains a dimension with value <= 0"", --> 856 size) 857 858 else: ValueError: ('The specified size contains a dimension with value <= 0', (-192, 256)) I will appreciate your help. Thanks in advance.",|neural-network|keras|conv-neural-network|theano-cuda|,Tensors&Inputs,1
45799474,"Keras: model.evaluate vs model.predict accuracy difference in multi-class NLP task. I am training a simple model in keras for NLP task with following code. Variable names are self explanatory for train, test and validation set. This dataset has 19 classes so final layer of the network has 19 outputs. Labels are also one-hot encoded. nb_classes = 19 model1 = Sequential() model1.add(Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False)) model1.add(LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm)) model1.add(Dropout(rate_drop_dense)) model1.add(BatchNormalization()) model1.add(Dense(num_dense, activation=act)) model1.add(Dropout(rate_drop_dense)) model1.add(BatchNormalization()) model1.add(Dense(nb_classes, activation = 'sigmoid')) model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) #One hot encode all labels ytrain_enc = np_utils.to_categorical(train_labels) yval_enc = np_utils.to_categorical(val_labels) ytestenc = np_utils.to_categorical(test_labels) model1.fit(train_data, ytrain_enc, validation_data=(val_data, yval_enc), epochs=200, batch_size=384, shuffle=True, verbose=1) After first epoch, this gives me these outputs. Epoch 1/200 216632/216632 [==============================] - 2442s - loss: 0.1427 - acc: 0.9443 - val_loss: 0.0526 - val_acc: 0.9826 Then I evaluate my model on testing dataset and this also shows me accuracy around 0.98. model1.evaluate(test_data, y = ytestenc, batch_size=384, verbose=1) However, the labels are one-hot encoded, so I need prediction vector of classes so that I can generate confusion matrix etc. So I use, PREDICTED_CLASSES = model1.predict_classes(test_data, batch_size=384, verbose=1) temp = sum(test_labels == PREDICTED_CLASSES) temp/len(test_labels) 0.83 This shows that total predicted classes were 83% accurate however model1.evaluate shows 98% accuracy!! What am I doing wrong here? Is my loss function okay with categorical class labels? Is my choice of sigmoid activation function for prediction layer okay? or there is difference in the way keras evaluates a model? Please suggest on what can be wrong. This is my first try to make a deep model so I don't have much understanding of what's wrong here.",|machine-learning|deep-learning|keras|,Training,2
45817355,"AttributeError: module 'tensorflow.python.estimator.estimator_lib' has no attribute 'LinearRegressor'. import tensorflow as tf import numpy as np feature_columns = [tf.feature_column.numeric_column(""x"", shape=[1])] estimator = tf.estimator.LinearRegressor(feature_columns=feature_columns) My code is above. Then it shows the error ""AttributeError: module 'tensorflow.python.estimator.estimator_lib' has no attribute 'LinearRegressor'"" Python 3.5.2",|python-3.x|tensorflow|attributeerror|,API,4
45966301,"TensorFlow Cannot feed value of shape (100, 784) for Tensor 'Placeholder:0'. I am learning TensorFLow. So to understand how to make something, I tried to copy some code from a source and execute it. But I'm hitting an error message. So I tried some solution from this website but it does not work (I kept my test in comments). """"""programme 1 """""" import tensorflow as tf import numpy as np from tensorflow.examples.tutorials.mnist import input_data mnist = input_data.read_data_sets('MNIST_data', one_hot=True) X = tf.placeholder(tf.float32,[None, 28, 28, 1]) #28 * 28 taille image 1 = 1pixel car noir et blanc ""X"" valeur W = tf.Variable(tf.zeros([784, 10])) # 28*28 = 784 , 10 -> 0  9 ""W"" = weight = poid b = tf.Variable(tf.zeros([10])) #chiffre de 0  9 a reconnaitre ""b"" = constante init = tf.initialize_all_variables() #model Y = tf.nn.softmax(tf.matmul(tf.reshape(X,[-1, 784]), W) + b) #fonction ""matmul"": produit matriciel ""-1"": reussite obligatoire #Place holder Y_ = tf.placeholder(tf.float32, [None, 10]) #loss function cross_entropy = -1 * tf.reduce_sum(Y_ * tf.log(Y)) #formule # % of correct annwer found in batch is_correct = tf.equal(tf.argmax(Y,1),tf.argmax(Y_,1)) accuracy = tf.reduce_mean(tf.cast(is_correct,tf.float32)) #training step optimizer = tf.train.GradientDescentOptimizer(0.003) #petit pas train_step = optimizer.minimize(cross_entropy) sess = tf.Session() sess.run(init) for i in range(10000): #load batch of image and ocrrects answer batch_X, batch_Y = mnist.train.next_batch(100) batch_X = np.reshape(batch_X, (-1, 784)) #batch_Y = np.reshape(batch_Y, (-1, 784)) train_data = {X: batch_X, Y_: batch_Y} #train sess.run(train_step, feed_dict=train_data) a,c = sess.run([accuracy,cross_entropy],feed = train_data) test_data = {X:mnist.test.images, Y_:mnist.test.labels} a,c = sess.run([accuracy,cross_entropy],feed = test_data) the log : Traceback (most recent call last): File ""d:\tensorflow\test1.py"", line 46, in <module> sess.run(train_step, feed_dict=train_data) File ""C:\Users\Proprietaire\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\client\session.py"", line 895, in run run_metadata_ptr) File ""C:\Users\Proprietaire\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\client\session.py"", line 1100, in _run % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape()))) ValueError: Cannot feed value of shape (100, 784) for Tensor 'Placeholder:0', which has shape '(?, 28, 28, 1)' 2017-08-30 19:07:37.406994: W C:\tf_jenkins\home\workspace\rel-win\M\windows\PY\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations. line 46 is sess.run(train_step, feed_dict=train_data) What can I do to resolve this error?",|python|tensorflow|deep-learning|,Tensors&Inputs,1
46177505,"Keras kernel initializer with gabor filter. I am trying create a custom initializer with Gabor kernels according to Keras documentation. I am writing this example for an easy run and debug the code. import random from cv2.cv2 import CV_64F import keras.backend as K import numpy as np from keras.layers import Conv2D from sklearn.model_selection import train_test_split from sklearn.utils import shuffle from keras.utils import np_utils from keras.layers.convolutional import MaxPooling2D from keras.layers.core import Dense, Dropout, Activation, Flatten from keras.models import Sequential import cv2 from theano import shared images_size = 100 img_rows = 200 img_cols = 200 nb_channel = 3 def custom_gabor(shape, dtype=None): total_ker = [] for i in xrange(shape[3]): kernels = [] for j in xrange(shape[2]): kernels.append( cv2.getGaborKernel(ksize=(shape[0], shape[1]), sigma=1, theta=1, lambd=0.5, gamma=0.3, psi=(3.14) * 0.5, ktype=CV_64F)) total_ker.append(kernels) np_tot = shared(np.array(total_ker)) return K.variable(np_tot, dtype=dtype) def build_model(): model = Sequential() # Layer 1 model.add(Conv2D(32, (3, 3), kernel_initializer=custom_gabor, input_shape=(nb_channel, img_rows, img_cols))) model.add(Activation('relu')) # Layer 2 model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Conv2D(32, (3, 3), kernel_initializer=custom_gabor)) model.add(Activation('relu')) model.add(MaxPooling2D(pool_size=(2, 2))) # Layer 3 model.add(Conv2D(32, (3, 3), kernel_initializer=custom_gabor)) model.add(Activation('relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Dropout(0.25)) model.add(Flatten()) model.add(Dense(64)) model.add(Activation('relu')) model.add(Dropout(0.5)) model.add(Dense(2)) model.add(Activation('softmax')) model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) return model def make_dummy_data_set(): img_matrix = [] for i in xrange(images_size): img_matrix.append([random.random() for _ in xrange(img_rows*img_cols*nb_channel)]) img_matrix = np.array(img_matrix) label = np.array([random.randint(0, 1) for _ in xrange(images_size)]) data, label = shuffle(img_matrix, label, random_state=7) # random_state=2 X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.2, random_state=7) # reshape the data X_train = X_train.reshape(X_train.shape[0], nb_channel, img_rows, img_cols) X_test = X_test.reshape(X_test.shape[0], nb_channel, img_rows, img_cols) X_train = X_train.astype('float32') X_test = X_test.astype('float32') # convert class vectore to binary class matrices y_train = np_utils.to_categorical(y_train, 2) y_test = np_utils.to_categorical(y_test, 2) return X_train, X_test, y_train, y_test def train_model(model, X_train, X_test, y_train, y_test): model.fit(X_train, y_train, batch_size=32, epochs=5, verbose=1, validation_data=(X_test, y_test)) if __name__ == ""__main__"": model = build_model() X_train, X_test, y_train, y_test = make_dummy_data_set() train_model(model, X_train, X_test, y_train, y_test) when I run it I get the error ""Input dimension mis-match"": Using Theano backend. Train on 80 samples, validate on 20 samples Epoch 1/5 Traceback (most recent call last): File ""/home/naor/Desktop/workspace/reflux_analyze/core/tests/test.py"", line 104, in <module> train_model(model, X_train, X_test, y_train, y_test) File ""/home/naor/Desktop/workspace/reflux_analyze/core/tests/test.py"", line 98, in train_model validation_data=(X_test, y_test)) File ""/home/naor/Desktop/workspace/reflux_analyze/local/lib/python2.7/site-packages/keras/models.py"", line 867, in fit initial_epoch=initial_epoch) File ""/home/naor/Desktop/workspace/reflux_analyze/local/lib/python2.7/site-packages/keras/engine/training.py"", line 1598, in fit validation_steps=validation_steps) File ""/home/naor/Desktop/workspace/reflux_analyze/local/lib/python2.7/site-packages/keras/engine/training.py"", line 1183, in _fit_loop outs = f(ins_batch) File ""/home/naor/Desktop/workspace/reflux_analyze/local/lib/python2.7/site-packages/keras/backend/theano_backend.py"", line 1222, in __call__ return self.function(*inputs) File ""/home/naor/Desktop/workspace/reflux_analyze/local/lib/python2.7/site-packages/theano/compile/function_module.py"", line 898, in __call__ storage_map=getattr(self.fn, 'storage_map', None)) File ""/home/naor/Desktop/workspace/reflux_analyze/local/lib/python2.7/site-packages/theano/gof/link.py"", line 325, in raise_with_op reraise(exc_type, exc_value, exc_trace) File ""/home/naor/Desktop/workspace/reflux_analyze/local/lib/python2.7/site-packages/theano/compile/function_module.py"", line 884, in __call__ self.fn() if output_subset is None else\ ValueError: Input dimension mis-match. (input[0].shape[1] = 3, input[1].shape[1] = 32) Apply node that caused the error: Elemwise{Add}[(0, 0)](CorrMM{valid, (1, 1), (1, 1)}.0, InplaceDimShuffle{x,0,x,x}.0) Toposort index: 73 Inputs types: [TensorType(float32, 4D), TensorType(float32, (True, False, True, True))] Inputs shapes: [(32, 3, 169, 198), (1, 32, 1, 1)] Inputs strides: [(401544, 133848, 792, 4), (128, 4, 4, 4)] Inputs values: ['not shown', 'not shown'] Outputs clients: [[Elemwise{Composite{(i0 * (i1 + Abs(i1)))}} (TensorConstant{(1, 1, 1, 1) of 0.5}, Elemwise{Add}[(0, 0)].0), Elemwise{Composite{((i0 * i1) + (i0 * i1 * sgn(i2)))}}[(0, 1)] (TensorConstant{(1, 1, 1, 1) of 0.5}, MaxPoolGrad{ignore_border=True, mode='max', ndim=2}.0, Elemwise{Add}[(0, 0)].0)]] HINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'. HINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node. Process finished with exit code 1 I know this kernel will change by CNN learning phase but i want to start from thows gabor kernel (only the real part) And my data is a pictures and not a random... :) thanks.",|python|keras|theano|conv-neural-network|gabor-filter|,Tensors&Inputs,1
46280770,"Keras LSTM layers input shape. I am trying to feed a sequence with 20 featuresto an LSTM network as shown in the code. But I get an error that my Input0 is incompatible with LSTM input. Not sure how to change my layer structure to fit the data. def build_model(features, aux1=None, aux2=None): # create model features[0] = np.asarray(features[0]) main_input = Input(shape=features[0].shape, dtype='float32', name='main_input') main_out = LSTM(40, activation='relu') aux1_input = Input(shape=(len(aux1[0]),), dtype='float32', name='aux1_input') aux1_out = Dense(len(aux1[0]))(aux1_input) aux2_input = Input(shape=(len(aux2[0]),), dtype='float32', name='aux2_input') aux2_out = Dense(len(aux2[0]))(aux2_input) x = concatenate([aux1_out, main_out, aux2_out]) x = Dense(64, activation='relu')(x) x = Dropout(0.5)(x) output = Dense(1, activation='sigmoid', name='main_output')(x) model = Model(inputs=[aux1_input, aux2_input, main_input], outputs= [output]) return model Features variable is an array of shape (1456, 20) I have 1456 days and for each day I have 20 variables.",|python-3.x|deep-learning|keras|lstm|,Tensors&Inputs,1
46443566,"Keras LSTM multiclass classification. I have this code that works for binary classification. I have tested it for keras imdb dataset. model = Sequential() model.add(Embedding(5000, 32, input_length=500)) model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2)) model.add(Dense(1, activation='sigmoid')) model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) print(model.summary()) model.fit(X_train, y_train, epochs=3, batch_size=64) # Final evaluation of the model scores = model.evaluate(X_test, y_test, verbose=0) I need the above code to be converted for multi-class classification where there are 7 categories in total. What I understand after reading few articles to convert above code I have to change model.add(Dense(7, activation='softmax')) model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) Obviously changing just above two lines doesn't work. What else do I have to change to make the code work for multiclass classification. Also I think I have to change the classes to one hot encoding but don't know how in keras.",|python|deep-learning|keras|,Training,2
46516168,"Word2Vec Tutorial: Tensorflow TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type int32 of argument 'x'. Version of Tensorflow: 1.2.1 Version of Python: 3.5 Operating System: Windows 10 Another poster has asked about this same problem on StackOverflow here, and he appears to be using code from the same Udacity Word2Vec tutorial. So, maybe I'm dense, but the code of this example is so busy and complex that I can't tell what fixed his problem. The error occurs when I call tf.reduce_means: loss = tf.reduce_mean( tf.nn.sampled_softmax_loss(softmax_weights, softmax_biases, embed, train_labels, num_sampled, vocabulary_size)) Right before the call to tf.reduce_mean the key variables have the following data types. train_dataset.dtype >> tf.int32 train_labels.dtype >> tf.int32 valid_dataset.dtype >> tf.int32 embeddings.dtype >> tf.float32_ref softmax_weights.dtype >> tf.float32_ref softmax_biases.dtype >> tf.float32_ref embed.dtype >> tf.float32 I tried every permutation of data type in the definitions of the variables train_dataset.dtype, train_labels.dtype and valid_dataset.dtype: making them all int64, all float32, all float64, and combinations of integer and floating point. Nothing worked. I didn't try altering the data types of softmax_weight and softmax_biases, because I'm afraid that might foul up the optimization algorithm. Don't these need to be floats to support the calculus that is done during backpropagation? (Tensorflow is often a very opaque black box with documentation that verges on completely useless, so I can suspect things but never know for sure.) Program Flow at Time of Error: After the call to reduce_mean program control transfers to sampled_softmax_loss() in file nn_impl.py which in turn calls _compute_sampled_logits(): logits, labels = _compute_sampled_logits( weights=weights, biases=biases, labels=labels, inputs=inputs, num_sampled=num_sampled, num_classes=num_classes, num_true=num_true, sampled_values=sampled_values, subtract_log_q=True, remove_accidental_hits=remove_accidental_hits, partition_strategy=partition_strategy, name=name) At this point I check the data types of the passed-in parameters and get the following: weights.dtype >> tf.float32_ref biases.dtype >> tf.float32_ref labels.dtype >> tf.float32 inputs.dtype >> tf.int32 On the very next step an exception occurs, and I am thrown into the StreamWrapper class in file ansitowin32.py. Running to the end, I get the following Traceback: --------------------------------------------------------------------------- ValueError Traceback (most recent call last) C:\Anaconda3\envs\aind-dog\lib\site-packages\tensorflow\python\framework\op_def_library.py in apply_op(self, op_type_name, name, **keywords) 489 as_ref=input_arg.is_ref, --> 490 preferred_dtype=default_dtype) 491 except TypeError as err: C:\Anaconda3\envs\aind-dog\lib\site-packages\tensorflow\python\framework\ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype) 740 if ret is None: --> 741 ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref) 742 C:\Anaconda3\envs\aind-dog\lib\site-packages\tensorflow\python\framework\ops.py in _TensorTensorConversionFunction(t, dtype, name, as_ref) 613 ""Tensor conversion requested dtype %s for Tensor with dtype %s: %r"" --> 614 % (dtype.name, t.dtype.name, str(t))) 615 return t ValueError: Tensor conversion requested dtype int32 for Tensor with dtype float32: 'Tensor(""sampled_softmax_loss/Reshape_1:0"", shape=(?, 1, ?), dtype=float32, device=/device:CPU:0)' During handling of the above exception, another exception occurred: TypeError Traceback (most recent call last) <ipython-input-7-66d378b94a16> in <module>() 34 loss = tf.reduce_mean( 35 tf.nn.sampled_softmax_loss(softmax_weights, softmax_biases, embed, ---> 36 train_labels, num_sampled, vocabulary_size)) 37 38 # Optimizer. C:\Anaconda3\envs\aind-dog\lib\site-packages\tensorflow\python\ops\nn_impl.py in sampled_softmax_loss(weights, biases, labels, inputs, num_sampled, num_classes, num_true, sampled_values, remove_accidental_hits, partition_strategy, name) 1266 remove_accidental_hits=remove_accidental_hits, 1267 partition_strategy=partition_strategy, -> 1268 name=name) 1269 sampled_losses = nn_ops.softmax_cross_entropy_with_logits(labels=labels, 1270 logits=logits) C:\Anaconda3\envs\aind-dog\lib\site-packages\tensorflow\python\ops\nn_impl.py in _compute_sampled_logits(weights, biases, labels, inputs, num_sampled, num_classes, num_true, sampled_values, subtract_log_q, remove_accidental_hits, partition_strategy, name) 1005 row_wise_dots = math_ops.multiply( 1006 array_ops.expand_dims(inputs, 1), -> 1007 array_ops.reshape(true_w, new_true_w_shape)) 1008 # We want the row-wise dot plus biases which yields a 1009 # [batch_size, num_true] tensor of true_logits. C:\Anaconda3\envs\aind-dog\lib\site-packages\tensorflow\python\ops\math_ops.py in multiply(x, y, name) 284 285 def multiply(x, y, name=None): --> 286 return gen_math_ops._mul(x, y, name) 287 288 C:\Anaconda3\envs\aind-dog\lib\site-packages\tensorflow\python\ops\gen_math_ops.py in _mul(x, y, name) 1375 A `Tensor`. Has the same type as `x`. 1376 """""" -> 1377 result = _op_def_lib.apply_op(""Mul"", x=x, y=y, name=name) 1378 return result 1379 C:\Anaconda3\envs\aind-dog\lib\site-packages\tensorflow\python\framework\op_def_library.py in apply_op(self, op_type_name, name, **keywords) 524 ""%s type %s of argument '%s'."" % 525 (prefix, dtypes.as_dtype(attrs[input_arg.type_attr]).name, --> 526 inferred_from[input_arg.type_attr])) 527 528 types = [values.dtype] TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type int32 of argument 'x'. Here's the complete program: # These are all the modules we'll be using later. # Make sure you can import them before proceeding further. # %matplotlib inline from __future__ import print_function import collections import math import numpy as np import os import random import tensorflow as tf import zipfile from matplotlib import pylab from six.moves import range from six.moves.urllib.request import urlretrieve from sklearn.manifold import TSNE print(""Working directory = %s\n"" % os.getcwd()) def read_data(filename): """"""Extract the first file enclosed in a zip file as a list of words"""""" with zipfile.ZipFile(filename) as f: data = tf.compat.as_str(f.read(f.namelist()[0])).split() return data filename = 'text8.zip' words = read_data(filename) print('Data size %d' % len(words)) vocabulary_size = 50000 def build_dataset(words): count = [['UNK', -1]] count.extend(collections.Counter(words).most_common(vocabulary_size - 1)) dictionary = dict() # Loop through the keys of the count collection dictionary # (apparently, zeroing out counts) for word, _ in count: dictionary[word] = len(dictionary) data = list() unk_count = 0 # count of unknown words for word in words: if word in dictionary: index = dictionary[word] else: index = 0 # dictionary['UNK'] unk_count = unk_count + 1 data.append(index) count[0][1] = unk_count reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) return data, count, dictionary, reverse_dictionary data, count, dictionary, reverse_dictionary = build_dataset(words) print('Most common words (+UNK)', count[:5]) print('Sample data', data[:10]) del words # Hint to reduce memory. data_index = 0 def generate_batch(batch_size, num_skips, skip_window): global data_index assert batch_size % num_skips == 0 assert num_skips <= 2 * skip_window batch = np.ndarray(shape=(batch_size), dtype=np.int32) labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32) span = 2 * skip_window + 1 # [ skip_window target skip_window ] buffer = collections.deque(maxlen=span) for _ in range(span): buffer.append(data[data_index]) data_index = (data_index + 1) % len(data) for i in range(batch_size // num_skips): target = skip_window # target label at the center of the buffer targets_to_avoid = [ skip_window ] for j in range(num_skips): while target in targets_to_avoid: target = random.randint(0, span - 1) targets_to_avoid.append(target) batch[i * num_skips + j] = buffer[skip_window] labels[i * num_skips + j, 0] = buffer[target] buffer.append(data[data_index]) data_index = (data_index + 1) % len(data) return batch, labels print('data:', [reverse_dictionary[di] for di in data[:8]]) for num_skips, skip_window in [(2, 1), (4, 2)]: data_index = 0 batch, labels = generate_batch(batch_size=8, num_skips=num_skips, skip_window=skip_window) print('\nwith num_skips = %d and skip_window = %d:' % (num_skips, skip_window)) print(' batch:', [reverse_dictionary[bi] for bi in batch]) print(' labels:', [reverse_dictionary[li] for li in labels.reshape(8)]) batch_size = 128 embedding_size = 128 # Dimension of the embedding vector. skip_window = 1 # How many words to consider left and right. num_skips = 2 # How many times to reuse an input to generate a label. # We pick a random validation set to sample nearest neighbors. here we limit the # validation samples to the words that have a low numeric ID, which by # construction are also the most frequent. valid_size = 16 # Random set of words to evaluate similarity on. valid_window = 100 # Only pick dev samples in the head of the distribution. valid_examples = np.array(random.sample(range(valid_window), valid_size)) num_sampled = 64 # Number of negative examples to sample. graph = tf.Graph() with graph.as_default(), tf.device('/cpu:0'): # Input data. train_dataset = tf.placeholder(tf.int32, shape=[batch_size]) train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1]) valid_dataset = tf.constant(valid_examples, dtype=tf.int32) # Variables. embeddings = tf.Variable( tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0)) softmax_weights = tf.Variable( tf.truncated_normal([vocabulary_size, embedding_size], stddev=1.0 / math.sqrt(embedding_size))) softmax_biases = tf.Variable(tf.zeros([vocabulary_size])) # Model. # Look up embeddings for inputs. embed = tf.nn.embedding_lookup(embeddings, train_dataset) # Compute the softmax loss, using a sample of the negative labels each time. loss = tf.reduce_mean( tf.nn.sampled_softmax_loss(softmax_weights, softmax_biases, embed, train_labels, num_sampled, vocabulary_size)) # Optimizer. # Note: The optimizer will optimize the softmax_weights AND the embeddings. # This is because the embeddings are defined as a variable quantity and the # optimizer's `minimize` method will by default modify all variable quantities # that contribute to the tensor it is passed. # See docs on `tf.train.Optimizer.minimize()` for more details. optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss) # Compute the similarity between minibatch examples and all embeddings. # We use the cosine distance: norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True)) normalized_embeddings = embeddings / norm valid_embeddings = tf.nn.embedding_lookup( normalized_embeddings, valid_dataset) similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))",|tensorflow|typeerror|word2vec|,API,4
46642627,"Keras: Accuracy stays zero. I am trying to get into machine learning with Keras. I am not a Mathematician and I have only a basic understanding of how neural net-works (haha get it?), so go easy on me. This is my current code: from keras.utils import plot_model from keras.models import Sequential from keras.layers import Dense from keras import optimizers import numpy # fix random seed for reproducibility numpy.random.seed(7) # split into input (X) and output (Y) variables X = [] Y = [] count = 0 while count < 10000: count += 1 X += [count / 10000] numpy.random.seed(count) #Y += [numpy.random.randint(1, 101) / 100] Y += [(count + 1) / 100] print(str(X) + ' ' + str(Y)) # create model model = Sequential() model.add(Dense(50, input_dim=1, kernel_initializer = 'uniform', activation='relu')) model.add(Dense(50, kernel_initializer = 'uniform', activation='relu')) model.add(Dense(1, kernel_initializer = 'uniform', activation='sigmoid')) # Compile model opt = optimizers.SGD(lr=0.01) model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy']) # Fit the model model.fit(X, Y, epochs=150, batch_size=100) # evaluate the model scores = model.evaluate(X, Y) predictions = model.predict(X) print(""\n%s: %.2f%%"" % (model.metrics_names[1], scores[1]*100)) print (str(predictions)) ##plot_model(model, to_file='C:/Users/Markus/Desktop/model.png') The accuracy stays zero and the predictions are an array of 1's. What am I doing wrong?",|python|python-3.x|keras|,Model,0
46595157,"How to apply the torch.inverse() function of PyTorch to every sample in the batch?. This may seem like a basic question, but I am unable to work it through. In the forward pass of my neural network, I have an output tensor of shape 8x3x3, where 8 is my batch size. We can assume each 3x3 tensor to be a non-singular matrix. I need to find the inverse of these matrices. The PyTorch inverse() function only works on square matrices. Since I now have 8x3x3, how do I apply this function to every matrix in the batch in a differentiable manner? If I iterate through the samples and append the inverses to a python list, which I then convert to a PyTorch tensor, should it be a problem during backprop? (I am asking since converting PyTorch tensors to numpy to perform some operations and then back to a tensor won't compute gradients during backprop for such operations) I also get the following error when I try to do something like that. a = torch.arange(0,8).view(-1,2,2) b = [m.inverse() for m in a] c = torch.FloatTensor(b) TypeError: 'torch.FloatTensor' object does not support indexing",|python|pytorch|,API,4
46705634,"RNN is not training (PyTorch). I can't get what I am doing wrong when training RNN. I am trying to train RNN for AND operation on sequences (to learn how it works on simple task). But my network is not learning, loss stays the same and it can't event overfit the model. Can you please help me to find the problem? Data I am using: data = [ [1, 1, 1, 1, 0, 0, 1, 1, 1], [1, 1, 1, 1], [0, 0, 1, 1], [0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1], [1, 1], [0], [1], [1, 0]] labels = [ 0, 1, 0, 0, 1, 1, 0, 1, 0 ] Code for NN: class AndRNN(nn.Module): def __init__(self): super(AndRNN, self).__init__() self.rnn = nn.RNN(1, 10, 5) self.fc = nn.Sequential( nn.Linear(10, 30), nn.Linear(30, 2) ) def forward(self, input, hidden): x, hidden = self.rnn(input, hidden) x = self.fc(x[-1]) return x, hidden def initHidden(self): return Variable(torch.zeros((5, 1, 10))) Training loop: criterion = torch.nn.CrossEntropyLoss() optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9) correct = 0 for e in range(20): for i in range(len(data)): tensor = torch.FloatTensor(data[i]).view(-1, 1, 1) label = torch.LongTensor([labels[i]]) hidden = net.initHidden() optimizer.zero_grad() out, hidden = net(Variable(tensor), Variable(hidden.data)) _, l = torch.topk(out, 1) if label[0] == l[0].data[0]: correct += 1 loss = criterion(out, Variable(label)) loss.backward() optimizer.step() print(""Loss "", loss.data[0], ""Accuracy "", (correct / (i + 1))) Shape for tensor will be (sequence_len, 1 (which is batch size), 1), that is correct according to the PyTorch docs for RNN",|machine-learning|deep-learning|pytorch|recurrent-neural-network|,Training,2
46989600,"Image adjustments with Conv2d. I am working on a project related to CNN using TensorFlow. I imported image using (20 such images) for filename in glob.glob('input_data/*.jpg'): input_images.append(cv2.imread(filename,0)) image_size_input = len(input_images[0]) The images were of size (250,250) because of grayscale. But for conv2D, it requires a 4D input tensor to feed. My input tensor looks like x = tf.placeholder(tf.float32,shape=[None,image_size_output,image_size_output,1], name='x') So i was not able to convert the above 2d image into the given shape(4D). How to deal with the ""None"" field. I tried this: input_images_padded = [] for image in input_images: temp = np.zeros((1,image_size_output,image_size_output,1)) for i in range(image_size_input): for j in range(image_size_input): temp[0,i,j,0] = image[i,j] input_images_padded.append(temp) I got the following error: File ""/opt/intel/intelpython3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 975, in _run % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape()))) ValueError: Cannot feed value of shape (20, 1, 250, 250, 1) for Tensor 'x_11:0', which has shape '(?, 250, 250, 1)' Here's the entire code(for reference): import tensorflow as tf from PIL import Image import glob import cv2 import os import numpy as np os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' input_images = [] output_images = [] for filename in glob.glob('input_data/*.jpg'): input_images.append(cv2.imread(filename,0)) for filename in glob.glob('output_data/*.jpg'): output_images.append(cv2.imread(filename,0)) image_size_input = len(input_images[0]) image_size_output = len(output_images[0]) ''' now adding padding to the input images to convert from 125x125 to 250x2050 sized images ''' input_images_padded = [] for image in input_images: temp = np.zeros((1,image_size_output,image_size_output,1)) for i in range(image_size_input): for j in range(image_size_input): temp[0,i,j,0] = image[i,j] input_images_padded.append(temp) output_images_padded = [] for image in output_images: temp = np.zeros((1,image_size_output,image_size_output,1)) for i in range(image_size_input): for j in range(image_size_input): temp[0,i,j,0] = image[i,j] output_images_padded.append(temp) sess = tf.Session() ''' Creating tensor for the input ''' x = tf.placeholder(tf.float32,shape= [None,image_size_output,image_size_output,1], name='x') ''' Creating tensor for the output ''' y = tf.placeholder(tf.float32,shape= [None,image_size_output,image_size_output,1], name='y') def create_weights(shape): return tf.Variable(tf.truncated_normal(shape, stddev=0.05)) def create_biases(size): return tf.Variable(tf.constant(0.05, shape=[size])) def create_convolutional_layer(input, bias_count, filter_height, filter_width, num_input_channels, num_out_channels, activation_function): weights = create_weights(shape=[filter_height, filter_width, num_input_channels, num_out_channels]) biases = create_biases(bias_count) layer = tf.nn.conv2d(input=input, filter=weights, strides=[1, 1, 1, 1], padding='SAME') layer += biases layer = tf.nn.max_pool(value=layer, ksize=[1, 2, 2, 1], strides=[1, 1, 1, 1], padding='SAME') if activation_function==""relu"": layer = tf.nn.relu(layer) return layer ''' Conv. Layer 1: Patch extraction 64 filters of size 1 x 9 x 9 Activation function: ReLU Output: 64 feature maps Parameters to optimize: 1 x 9 x 9 x 64 = 5184 weights and 64 biases ''' layer1 = create_convolutional_layer(input=x, bias_count=64, filter_height=9, filter_width=9, num_input_channels=1, num_out_channels=64, activation_function=""relu"") ''' Conv. Layer 2: Non-linear mapping 32 filters of size 64 x 1 x 1 Activation function: ReLU Output: 32 feature maps Parameters to optimize: 64 x 1 x 1 x 32 = 2048 weights and 32 biases ''' layer2 = create_convolutional_layer(input=layer1, bias_count=32, filter_height=1, filter_width=1, num_input_channels=64, num_out_channels=32, activation_function=""relu"") '''Conv. Layer 3: Reconstruction 1 filter of size 32 x 5 x 5 Activation function: Identity Output: HR image Parameters to optimize: 32 x 5 x 5 x 1 = 800 weights and 1 bias''' layer3 = create_convolutional_layer(input=layer2, bias_count=1, filter_height=5, filter_width=5, num_input_channels=32, num_out_channels=1, activation_function=""identity"") '''print(layer1.get_shape().as_list()) print(layer2.get_shape().as_list()) print(layer3.get_shape().as_list())''' ''' applying gradient descent algorithm ''' #loss_function loss = tf.reduce_sum(tf.square(layer3-y)) #optimiser optimizer = tf.train.GradientDescentOptimizer(0.01) train = optimizer.minimize(loss) init = tf.global_variables_initializer() sess.run(init) for i in range(len(input_images)): sess.run(train,{x: input_images_padded, y:output_images_padded}) curr_loss = sess.run([loss], {x: x_train, y: y_train}) print(""loss: %s""%(curr_loss))",|numpy|opencv|tensorflow|machine-learning|tensor|,Tensors&Inputs,1
46995209,"Neural network toy model to fit sine function fails, what's wrong?. Graduate student, new to Keras and neural networks was trying to fit a very simple feedforward neural network to a one-dimensional sine. Below are three examples of the best fit that I can get. On the plots, you can see the output of the network vs ground truth The complete code, just a few lines, is posted here example Keras I was playing with the number of layers, different activation functions, different initializations, and different loss functions, batch size, number of training samples. It seems that none of those were able to improve the results beyond the above examples. I would appreciate any comments and suggestions. Is sine a hard function for a neural network to fit? I suspect that the answer is not, so I must be doing something wrong... There is a similar question here from 5 years ago, but the OP there didn't provide the code and it is still not clear what went wrong or how he was able to resolve this problem.",|python|machine-learning|neural-network|deep-learning|keras|,Training,2
47066635,"Checkpointing keras model: TypeError: can't pickle _thread.lock objects. It seems like the error has occurred in the past in different contexts here, but I'm not dumping the model directly -- I'm using the ModelCheckpoint callback. Any idea what could be going wrong? Information: Keras version 2.0.8 Tensorflow version 1.3.0 Python 3.6 Minimal example to reproduce the error: from keras.layers import Input, Lambda, Dense from keras.models import Model from keras.callbacks import ModelCheckpoint from keras.optimizers import Adam import tensorflow as tf import numpy as np x = Input(shape=(30,3)) low = tf.constant(np.random.rand(30, 3).astype('float32')) high = tf.constant(1 + np.random.rand(30, 3).astype('float32')) clipped_out_position = Lambda(lambda x, low, high: tf.clip_by_value(x, low, high), arguments={'low': low, 'high': high})(x) model = Model(inputs=x, outputs=[clipped_out_position]) optimizer = Adam(lr=.1) model.compile(optimizer=optimizer, loss=""mean_squared_error"") checkpoint = ModelCheckpoint(""debug.hdf"", monitor=""val_loss"", verbose=1, save_best_only=True, mode=""min"") training_callbacks = [checkpoint] model.fit(np.random.rand(100, 30, 3), [np.random.rand(100, 30, 3)], callbacks=training_callbacks, epochs=50, batch_size=10, validation_split=0.33) Error output: Train on 67 samples, validate on 33 samples Epoch 1/50 10/67 [===>..........................] - ETA: 0s - loss: 0.1627Epoch 00001: val_loss improved from inf to 0.17002, saving model to debug.hdf Traceback (most recent call last): File ""debug_multitask_inverter.py"", line 19, in <module> model.fit(np.random.rand(100, 30, 3), [np.random.rand(100, 30, 3)], callbacks=training_callbacks, epochs=50, batch_size=10, validation_split=0.33) File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/site-packages/keras/engine/training.py"", line 1631, in fit ?validation_steps=validation_steps) File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/site-packages/keras/engine/training.py"", line 1233, in _fit_loop callbacks.on_epoch_end(epoch, epoch_logs) File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/site-packages/keras/callbacks.py"", line 73, in on_epoch_end callback.on_epoch_end(epoch, logs) File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/site-packages/keras/callbacks.py"", line 414, in on_epoch_end self.model.save(filepath, overwrite=True) File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/site-packages/keras/engine/topology.py"", line 2556, in save save_model(self, filepath, overwrite, include_optimizer) File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/site-packages/keras/models.py"", line 107, in save_model 'config': model.get_config() File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/site-packages/keras/engine/topology.py"", line 2397, in get_config return copy.deepcopy(config) File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/copy.py"", line 150, in deepcopy y = copier(x, memo) File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/copy.py"", line 240, in _deepcopy_dict y[deepcopy(key, memo)] = deepcopy(value, memo) File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/copy.py"", line 150, in deepcopy y = copier(x, memo) File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/copy.py"", line 215, in _deepcopy_list append(deepcopy(a, memo)) File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/copy.py"", line 150, in deepcopy y = copier(x, memo) File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/copy.py"", line 240, in _deepcopy_dict y[deepcopy(key, memo)] = deepcopy(value, memo) File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/copy.py"", line 150, in deepcopy y = copier(x, memo) File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/copy.py"", line 240, in _deepcopy_dict y[deepcopy(key, memo)] = deepcopy(value, memo) File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/copy.py"", line 150, in deepcopy y = copier(x, memo) File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/copy.py"", line 240, in _deepcopy_dict y[deepcopy(key, memo)] = deepcopy(value, memo) File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/copy.py"", line 180, in deepcopy y = _reconstruct(x, memo, *rv) File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/copy.py"", line 280, in _reconstruct state = deepcopy(state, memo) File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/copy.py"", line 150, in deepcopy y = copier(x, memo) File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/copy.py"", line 240, in _deepcopy_dict y[deepcopy(key, memo)] = deepcopy(value, memo) File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/copy.py"", line 180, in deepcopy y = _reconstruct(x, memo, *rv) File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/copy.py"", line 280, in _reconstruct state = deepcopy(state, memo) File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/copy.py"", line 150, in deepcopy y = copier(x, memo) File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/copy.py"", line 240, in _deepcopy_dict y[deepcopy(key, memo)] = deepcopy(value, memo) File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/copy.py"", line 180, in deepcopy y = _reconstruct(x, memo, *rv) File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/copy.py"", line 280, in _reconstruct state = deepcopy(state, memo) File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/copy.py"", line 150, in deepcopy y = copier(x, memo) File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/copy.py"", line 240, in _deepcopy_dict y[deepcopy(key, memo)] = deepcopy(value, memo) File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/copy.py"", line 169, in deepcopy rv = reductor(4) TypeError: can't pickle _thread.lock objects",|tensorflow|keras|pickle|,API,4
47122052,"Keras Model gives test accuracy 1.0. Below is the code to predict if it close up or down the next day (Up =1, down =0) What I did was to create a dataframe and predict just using PriceChange (today close - yesterday close) to predict Next Day price change up or down (Next day Close - Today Close) df['PriceChange'] = (df['Close'] > df['Close'].shift(1)).astype(int) df['Closeupnextday'] = (df['Close'].shift(-1) > df['Close']).astype(int) So the dataframe looks like this: PriceChange Closeupnextday 0 0 1 1 1 1 2 1 1 3 1 1 4 1 0 5 0 0 6 0 0 7 0 1 It constantly gives me an accuracy of 1.000 To be fair it should be 50+% accuracy only. I believe something is wrong in the code below but I can't find it. I should add that after epoch 20/500 it constantly gives me 1.000 accuracy Any advice, please? def load_data(stock, seq_len): amount_of_features = len(stock.columns) data = stock.as_matrix() #pd.DataFrame(stock) sequence_length = seq_len + 1 result = [] for index in range(len(data) - sequence_length): result.append(data[index: index + sequence_length]) result = np.array(result) row = round(0.9 * result.shape[0]) train = result[:int(row), :] x_train = train[:, :-1] y_train = train[:, -1][:,-1] x_test = result[int(row):, :-1] y_test = result[int(row):, -1][:,-1] x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], amount_of_features)) x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], amount_of_features)) return [x_train, y_train, x_test, y_test] def build_model(layers): model = Sequential() model.add(LSTM( input_dim=layers[0], output_dim=layers[1], return_sequences=True)) model.add(Dropout(0.0)) model.add(LSTM( layers[2], return_sequences=False)) model.add(Dropout(0.0)) model.add(Dense( output_dim=layers[2])) model.add(Activation(""linear"")) start = time.time() model.compile(loss=""mse"", optimizer=""rmsprop"",metrics=['accuracy']) print(""Compilation Time : "", time.time() - start) return model def build_model2(layers): d = 0.2 model = Sequential() model.add(LSTM(128, input_shape=(layers[1], layers[0]), return_sequences=True)) model.add(Dropout(d)) model.add(LSTM(64, input_shape=(layers[1], layers[0]), return_sequences=False)) model.add(Dropout(d)) model.add(Dense(16, activation=""relu"", kernel_initializer=""uniform"")) model.add(Dense(1, activation=""relu"", kernel_initializer=""uniform"")) model.compile(loss='mse',optimizer='adam',metrics=['accuracy']) return model window = 5 X_train, y_train, X_test, y_test = load_data(df[::-1], window) print(""X_train"", X_train.shape) print(""y_train"", y_train.shape) print(""X_test"", X_test.shape) print(""y_test"", y_test.shape) # model = build_model([3,lag,1]) model = build_model2([len(df.columns),window,1]) #11 = Dataframe axis 1 model.fit( X_train, y_train, batch_size=512, epochs=500, validation_split=0.1, verbose=1) trainScore = model.evaluate(X_train, y_train, verbose=0) print('Train Score: %.2f MSE (%.2f RMSE)' % (trainScore[0], math.sqrt(trainScore[0]))) testScore = model.evaluate(X_test, y_test, verbose=0) print('Test Score: %.2f MSE (%.2f RMSE)' % (testScore[0], math.sqrt(testScore[0]))) # print(X_test[-1]) diff=[] ratio=[] p = model.predict(X_test) for u in range(len(y_test)): pr = p[u][0] ratio.append((y_test[u]/pr)-1) diff.append(abs(y_test[u]- pr)) #print(u, y_test[u], pr, (y_test[u]/pr)-1, abs(y_test[u]- pr)) print(p) print(y_test)",|python|machine-learning|keras|neural-network|,Training,2
47128044,"Calculating input and output size for Conv2d in PyTorch for image classification. I'm trying to run the PyTorch tutorial on CIFAR10 image classification here - http://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py I've made a small change and I'm using a different dataset. I have images from the Wikiart dataset that I want to classify by artist (label = artist name). Here is the code for the Net - class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.pool = nn.MaxPool2d(2, 2) self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16*5*5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = x.view(-1, 16*5*5) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x Then there is this section of the code where I start training the Net. for epoch in range(2): running_loss = 0.0 for i, data in enumerate(wiki_train_dataloader, 0): inputs, labels = data['image'], data['class'] print(inputs.shape) inputs, labels = Variable(inputs), Variable(labels) optimizer.zero_grad() # forward + backward + optimize outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() # print statistics running_loss += loss.data[0] if i % 2000 == 1999: # print every 2000 mini-batches print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000)) running_loss = 0.0 This line print(inputs.shape) gives me torch.Size([4, 32, 32, 3]) with my Wikiart dataset whereas in the original example with CIFAR10, it prints torch.Size([4, 3, 32, 32]). Now, I'm not sure how to change the Conv2d in my Net to be compatible with torch.Size([4, 32, 32, 3]). I get this error: RuntimeError: Given input size: (3 x 32 x 3). Calculated output size: (6 x 28 x -1). Output size is too small at /opt/conda/conda-bld/pytorch_1503965122592/work/torch/lib/THNN/generic/SpatialConvolutionMM.c:45 While reading the images for the Wikiart dataset, I resize them to (32, 32) and these are 3-channel images. Things I tried: 1) The CIFAR10 tutorial uses a transform which I am not using. I could not incorporate the same into my code. 2) Changing self.conv2 = nn.Conv2d(6, 16, 5) to self.conv2 = nn.Conv2d(3, 6, 5). This gave me the same error as above. I was only changing this to see if the error message changes. Any resources on how to calculate input & output sizes in PyTorch or automatically reshape Tensors would be really appreciated. I just started learning Torch & I find the size calculations complicated.",|python|image|convolution|pytorch|tensor|,Tensors&Inputs,1
47267612,"Assign torch.cuda.FloatTensor. I'd like to know how can I do the following code, but now using pytorch, where dtype = torch.cuda.FloatTensor. There's the code straight python (using numpy): import numpy as np import random as rand xmax, xmin = 5, -5 pop = 30 x = (xmax-xmin)*rand.random(pop,1) y = x**2 [minz, indexmin] = np.amin(y), np.argmin(y) best = x[indexmin] This is my attempt to do it: import torch dtype = torch.cuda.FloatTensor def fit (position): return position**2 def main(): pop = 30 xmax, xmin = 5, -5 x= (xmax-xmin)*torch.rand(pop, 1).type(dtype)+xmin y = fit(x) [miny, indexmin] = torch.min(y,0) best = x[indexmin] print(best) The last part where I define the variable best as the value of x with index equal to indexmin it doesn't work. What am I doing wrong here. The following messenge appears: RuntimeError: expecting vector of indices at /opt/conda/conda-bld/pytorch_1501971235237/work/pytorch-0.1.12/torch/lib/THC/generic/THCTensorIndex.cu:405",|numpy|gpu|pytorch|,Tensors&Inputs,1
47318871,"ValueError: Floating point image RGB values must be in the 0..1 range. while using matplotlib. I want to visualize weights of the layer of a neural network. I'm using pytorch. import torch import torchvision.models as models from matplotlib import pyplot as plt def plot_kernels(tensor, num_cols=6): if not tensor.ndim==4: raise Exception(""assumes a 4D tensor"") if not tensor.shape[-1]==3: raise Exception(""last dim needs to be 3 to plot"") num_kernels = tensor.shape[0] num_rows = 1+ num_kernels // num_cols fig = plt.figure(figsize=(num_cols,num_rows)) for i in range(tensor.shape[0]): ax1 = fig.add_subplot(num_rows,num_cols,i+1) ax1.imshow(tensor[i]) ax1.axis('off') ax1.set_xticklabels([]) ax1.set_yticklabels([]) plt.subplots_adjust(wspace=0.1, hspace=0.1) plt.show() vgg = models.vgg16(pretrained=True) mm = vgg.double() filters = mm.modules body_model = [i for i in mm.children()][0] layer1 = body_model[0] tensor = layer1.weight.data.numpy() plot_kernels(tensor) The above gives this error ValueError: Floating point image RGB values must be in the 0..1 range. My question is should I normalize and take absolute value of the weights to overcome this error or is there anyother way ? If I normalize and use absolute value I think the meaning of the graphs change. [[[[ 0.02240197 -1.22057354 -0.55051649] [-0.50310904 0.00891289 0.15427093] [ 0.42360783 -0.23392732 -0.56789106]] [[ 1.12248898 0.99013627 1.6526649 ] [ 1.09936976 2.39608836 1.83921957] [ 1.64557672 1.4093554 0.76332706]] [[ 0.26969245 -1.2997849 -0.64577204] [-1.88377869 -2.0100112 -1.43068039] [-0.44531786 -1.67845118 -1.33723605]]] [[[ 0.71286005 1.45265901 0.64986968] [ 0.75984162 1.8061738 1.06934202] [-0.08650422 0.83452386 -0.04468433]] [[-1.36591709 -2.01630116 -1.54488969] [-1.46221244 -2.5365622 -1.91758668] [-0.88827479 -1.59151018 -1.47308767]] [[ 0.93600738 0.98174071 1.12213969] [ 1.03908169 0.83749604 1.09565806] [ 0.71188802 0.85773659 0.86840987]]] [[[-0.48592842 0.2971966 1.3365227 ] [ 0.47920835 -0.18186836 0.59673625] [-0.81358945 1.23862112 0.13635623]] [[-0.75361633 -1.074965 0.70477796] [ 1.24439156 -1.53563368 -1.03012812] [ 0.97597247 0.83084011 -1.81764793]] [[-0.80762428 -0.62829626 1.37428832] [ 1.01448071 -0.81775147 -0.41943246] [ 1.02848887 1.39178836 -1.36779451]]] ..., [[[ 1.28134537 -0.00482408 0.71610934] [ 0.95264435 -0.09291686 -0.28001019] [ 1.34494913 0.64477581 0.96984017]] [[-0.34442815 -1.40002513 1.66856039] [-2.21281362 -3.24513769 -1.17751861] [-0.93520379 -1.99811196 0.72937071]] [[ 0.63388056 -0.17022935 2.06905985] [-0.7285465 -1.24722099 0.30488953] [ 0.24900314 -0.19559766 1.45432627]]] [[[-0.80684513 2.1764245 -0.73765725] [-1.35886598 1.71875226 -1.73327696] [-0.75233924 2.14700699 -0.71064663]] [[-0.79627383 2.21598244 -0.57396138] [-1.81044972 1.88310981 -1.63758397] [-0.6589964 2.013237 -0.48532376]] [[-0.3710472 1.4949851 -0.30245575] [-1.25448656 1.20453358 -1.29454732] [-0.56755757 1.30994892 -0.39370224]]] [[[-0.67361742 -3.69201088 -1.23768616] [ 3.12674141 1.70414758 -1.76272404] [-0.22565465 1.66484773 1.38172317]] [[ 0.28095332 -2.03035069 0.69989491] [ 1.97936332 1.76992691 -1.09842575] [-2.22433758 0.52577412 0.18292744]] [[ 0.48471382 -1.1984663 1.57565165] [ 1.09911084 1.31910467 -0.51982772] [-2.76202297 -0.47073677 0.03936549]]]]",|matplotlib|deep-learning|pytorch|,Training,2
47352366,"Keras unreasonnably slower than TensorFlow. I'm trying to implement the neural network of this TensorFlow example, but using Keras. You'll find the code for both implementations at the bottom of the post. My problem is that the code takes around 1m30 with TensorFlow, and 18 minutes with Keras ! My question is : Did I make a rookie mistake in translating TensorFlow code to Keras code ? Or is Keras incredibly slow ? If so, can it be fixed ? Tensorflow code : from tensorflow.examples.tutorials.mnist import input_data mnist = input_data.read_data_sets('MNIST_data', one_hot=True) import tensorflow as tf def weight_variable(shape): initial = tf.truncated_normal(shape, stddev=0.1) return tf.Variable(initial) def bias_variable(shape): initial = tf.constant(0.1, shape=shape) return tf.Variable(initial) def conv2d(x, W): return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME') def max_pool_2x2(x): return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') x = tf.placeholder(tf.float32, [None, 784]) x_image = tf.reshape(x, [-1, 28, 28, 1]) y_ = tf.placeholder(tf.float32, [None, 10]) neurons_nb_layer_1 = 32 neurons_nb_layer_2 = 64 neurons_nb_layer_3 = 1024 W_conv1 = weight_variable([5, 5, 1, neurons_nb_layer_1]) b_conv1 = bias_variable([neurons_nb_layer_1]) h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1) h_pool1 = max_pool_2x2(h_conv1) W_conv2 = weight_variable([5, 5, neurons_nb_layer_1, neurons_nb_layer_2]) b_conv2 = bias_variable([neurons_nb_layer_2]) h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2) h_pool2 = max_pool_2x2(h_conv2) W_fc1 = weight_variable([7 * 7 * neurons_nb_layer_2, neurons_nb_layer_3]) b_fc1 = bias_variable([neurons_nb_layer_3]) h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * neurons_nb_layer_2]) h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1) keep_prob = tf.placeholder(tf.float32) h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob) W_fc2 = weight_variable([neurons_nb_layer_3, 10]) b_fc2 = bias_variable([10]) y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2 cross_entropy = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv)) train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy) correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1)) accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) import datetime start = datetime.datetime.now() with tf.Session() as sess: sess.run(tf.global_variables_initializer()) for i in range(600): batch = mnist.train.next_batch(50) if i % 100 == 0: train_accuracy = accuracy.eval(feed_dict={ x: batch[0], y_: batch[1], keep_prob: 1.0}) print('step %d, training accuracy %g' % (i, train_accuracy)) train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5}) print('test accuracy %g' % accuracy.eval(feed_dict={ x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0})) end = datetime.datetime.now() time = (end - start).seconds print(time//60, ""min"", time%60,""s"") Keras code : from tensorflow.examples.tutorials.mnist import input_data mnist = input_data.read_data_sets('MNIST_data', one_hot=True) import keras from keras.models import Sequential model = Sequential() bias_initializer = keras.initializers.Constant(value = 0.1) neurons_nb_layer_1 = 32 neurons_nb_layer_2 = 64 neurons_nb_layer_3 = 1024 from keras.layers import Reshape, Conv2D, MaxPooling2D, Dropout, Flatten, Dense model.add(Reshape((28, 28, 1), input_shape=(784,))) model.add(Conv2D(filters = neurons_nb_layer_1, kernel_size = 5*5, padding = 'same', activation = ""relu"", bias_initializer = bias_initializer)) model.add(MaxPooling2D(padding='same')) model.add(Conv2D(filters = neurons_nb_layer_2, kernel_size = 5*5, padding = 'same', activation = ""relu"", bias_initializer = bias_initializer)) model.add(MaxPooling2D(padding='same')) model.add(Reshape((1,7*7*neurons_nb_layer_2))) model.add(Dense(units = neurons_nb_layer_3, activation = ""relu"", bias_initializer = bias_initializer)) model.add(Dropout(rate = 0.5)) model.add(Flatten()) model.add(Dense(units = 10, activation = ""relu"")) model.summary() model.compile(loss = keras.losses.categorical_crossentropy, optimizer = 'adam', metrics=['accuracy'] ) import datetime start2 = datetime.datetime.now() for i in range(600): batch = mnist.train.next_batch(50) if i % 100 == 0: train_accuracy = model.evaluate(batch[0], batch[1]) print(""step"", i, "":"", train_accuracy) model.train_on_batch(batch[0], batch[1]) end2 = datetime.datetime.now() time2 = (end2 - start2).seconds print(time2//60, ""min"", time2%60,""s"")",|python|tensorflow|machine-learning|keras|,Model,0
47432905,"AttributeError on variable input of custom loss function in PyTorch. I've made a custom loss function to compute cross-entropy (CE) for a multi-output multi-label problem. Within the class, I want to set the target variable I'm feeding to not require a gradient. I do this within the forward function using a pre-defined function (taken from pytorch source code) outside the class. def _assert_no_grad(variable): assert not variable.requires_grad def forward(self, predicted, target): """""" Computes cross entropy between targets and predictions. """""" # No gradient over target _assert_no_grad(target) # Define variables p = predicted.clamp(0.01, 0.99) t = target.float() #Compute cross entropy h1 = p.log()*t h2 = (1-t)*((1-p).log()) ce = torch.add(h1, h2) ce_out = torch.mean(ce, 1) ce_out = torch.mean(ce_out, 0) # Save for backward step self.save_for_backward(ce_out) At this point when I run the code in a batched for-loop (see below), I get the following error: AttributeError: 'torch.FloatTensor' object has no attribute 'requires_grad' It seems simple enough as we should be passing a torch.autograd.Variable, however I am already doing this as can be seen in the snippet below. for t in range(50): print('Epoch {}'.format(t)) if t > 0: print('Loss ->', loss) for batch_idx, (x_batch, y_batch) in enumerate(train_loader): # Wrap in Variable x_in, target = Variable(x_batch), Variable(y_batch) predicted = model(x_in) # Compute and print loss loss = criterion(predicted, target) # Zero gradients, perform a backward pass, and update the weights. optimizer.zero_grad() loss.backward() optimizer.step() To add a note, my final goal is to generate a class which behaves like BCELoss except for multiple labels and not just binary. I feel like I've already scoured the entire PyTorch docs and primarily been using this and some forum entries. http://pytorch.org/docs/master/notes/extending.html So",|python|pytorch|,Training,2
47503514,"training vgg on flowers dataset with keras, validation loss not changing. I am doing a little experiment on VGG network with keras. The dataset I use is the flowers dataset with 5 classes including rose, sunflower, dandelion, tulip and daisy. There is something I could not figure out: When I used a small CNN network(not VGG, in the code below), it converged quickly and reached a validation accuracy about 75% after only about 8 epochs. Then I switched to VGG network(the commented out area in the code). The loss and accuracy of the network just did not change at all, it output something like: Epoch 1/50 402/401 [==============================] - 199s 495ms/step - loss: 13.3214 - acc: 0.1713 - val_loss: 13.0144 - val_acc: 0.1926 Epoch 2/50 402/401 [==============================] - 190s 473ms/step - loss: 13.3473 - acc: 0.1719 - val_loss: 13.0144 - val_acc: 0.1926 Epoch 3/50 402/401 [==============================] - 204s 508ms/step - loss: 13.3423 - acc: 0.1722 - val_loss: 13.0144 - val_acc: 0.1926 Epoch 4/50 402/401 [==============================] - 190s 472ms/step - loss: 13.3522 - acc: 0.1716 - val_loss: 13.0144 - val_acc: 0.1926 Epoch 5/50 402/401 [==============================] - 189s 471ms/step - loss: 13.3364 - acc: 0.1726 - val_loss: 13.0144 - val_acc: 0.1926 Epoch 6/50 402/401 [==============================] - 189s 471ms/step - loss: 13.3453 - acc: 0.1720 - val_loss: 13.0144 - val_acc: 0.1926 Epoch 7/50 Epoch 7/50 402/401 [==============================] - 189s 471ms/step - loss: 13.3503 - acc: 0.1717 - val_loss: 13.0144 - val_acc: 0.1926 PS: I did this experiment with other datasets and frameworks as well (place365 dataset with tensorflow and slim). The result is just the same. I have looked into the VGG paper(Simonyan&Zisserman), it says there are multiple stages to train a deep network like VGG, like from stage A to stage E with different network structures. I am not sure I have to train my VGG network the same way as it is described in the VGG paper. And other online courses did not mention this complex training process as well. Anyone has any ideas? My code: from keras.preprocessing.image import ImageDataGenerator from keras.models import Sequential from keras.layers import Conv2D, MaxPooling2D from keras.layers import Activation, Dropout, Flatten, Dense from keras import backend as K # dimensions of our images. img_width, img_height = 224, 224 train_data_dir = './data/train' validation_data_dir = './data/val' nb_train_samples = 3213 nb_validation_samples = 457 epochs = 50 batch_size = 8 if K.image_data_format() == 'channels_first': input_shape = (3, img_width, img_height) else: input_shape = (img_width, img_height, 3) # random cnn model: model = Sequential() model.add(Conv2D(32, (3, 3), input_shape=input_shape)) model.add(Activation('relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Conv2D(32, (3, 3))) model.add(Activation('relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Conv2D(64, (3, 3))) model.add(Activation('relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Flatten()) model.add(Dense(64)) model.add(Activation('relu')) model.add(Dropout(0.5)) model.add(Dense(5)) model.add(Activation('softmax')) # vgg model: '''model = Sequential([ Conv2D(64, (3, 3), input_shape=input_shape, padding='same', activation='relu'), Conv2D(64, (3, 3), activation='relu', padding='same'), MaxPooling2D(pool_size=(2, 2), strides=(2, 2)), Conv2D(128, (3, 3), activation='relu', padding='same'), Conv2D(128, (3, 3), activation='relu', padding='same',), MaxPooling2D(pool_size=(2, 2), strides=(2, 2)), Conv2D(256, (3, 3), activation='relu', padding='same',), Conv2D(256, (3, 3), activation='relu', padding='same',), Conv2D(256, (3, 3), activation='relu', padding='same',), MaxPooling2D(pool_size=(2, 2), strides=(2, 2)), Conv2D(512, (3, 3), activation='relu', padding='same',), Conv2D(512, (3, 3), activation='relu', padding='same',), Conv2D(512, (3, 3), activation='relu', padding='same',), MaxPooling2D(pool_size=(2, 2), strides=(2, 2)), Conv2D(512, (3, 3), activation='relu', padding='same',), Conv2D(512, (3, 3), activation='relu', padding='same',), Conv2D(512, (3, 3), activation='relu', padding='same',), MaxPooling2D(pool_size=(2, 2), strides=(2, 2)), Flatten(), Dense(256, activation='relu'), Dense(256, activation='relu'), Dense(5, activation='softmax') ])''' model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy']) train_datagen = ImageDataGenerator( rescale=1. / 255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True) test_datagen = ImageDataGenerator(rescale=1. / 255) train_generator = train_datagen.flow_from_directory( train_data_dir, target_size=(img_width, img_height), batch_size=batch_size, class_mode='categorical') validation_generator = test_datagen.flow_from_directory( validation_data_dir, target_size=(img_width, img_height), batch_size=batch_size, class_mode='categorical') model.fit_generator( train_generator, steps_per_epoch=nb_train_samples // batch_size, epochs=epochs, validation_data=validation_generator, validation_steps=nb_validation_samples // batch_size) model.save_weights('flowers.h5')",|deep-learning|keras|training-data|loss|convergence|,Training,2
47724077,"Keras: could not broadcast input array from shape (14,1) into shape (14). I'm using Keras with Tensorflow on Windows 10. I'm trying to create a maschine learning model for the Adult dataset (https://archive.ics.uci.edu/ml/datasets/Adult). First I am doing One Hot Encoding and then I'm trying to train my model but i get: ""ValueError: could not broadcast input array from shape (14,1) into shape (14)"" Even if I change the input_dim to something else than 14, I still get the same error with the same shapes. What am I doing wrong? df = pd.read_csv(""adult_data.csv"",header=None) X = df.iloc[:,0:14] Y = df.iloc[:,14] encoder = LabelEncoder() #X for i in [1,3,5,6,7,8,9,13]: column = X[i] encoder.fit(column) encoded_C = encoder.transform(column) X[i] = np_utils.to_categorical(encoded_C) print(X.shape) #Y encoder.fit(Y) en_Y = encoder.transform(Y) Y = np_utils.to_categorical(en_Y) #model model = Sequential() model.add(Dense(21, input_dim=14, activation=""relu"")) model.add(Dense(2, activation=""softmax"")) #compile model.compile(loss=""categorical_crossentropy"",optimizer=""adam"",metrics= [""accuracy""]) #train model.fit(X,Y, epochs=50, batch_size=100) score = model.evaluate(X,Y) print(""Accuracy: {}%"".format(score[0])) FULL ERROR: Traceback (most recent call last): File ""main.py"", line 36, in <module> model.fit(X,Y, epochs=50, batch_size=100) File ""C:\Users\K\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\models.py"", line 960, in fit validation_steps=validation_steps) File ""C:\Users\K\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\engine\training.py"", line 1574, in fit batch_size=batch_size) File ""C:\Users\K\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\engine\training.py"", line 1407, in _standardize_user_data exception_prefix='input') File ""C:\Users\K\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\engine\training.py"", line 128, in _standardize_input_data arrays[i] = array ValueError: could not broadcast input array from shape (14,1) into shape (14)",|python|machine-learning|tensorflow|keras|,API,4
47799758,"Keras - Classifier not learning from Transfer-Values of a Pre-Trained Model. I'm currently trying to use a pre-trained network and test in on this dataset. Originally, I used VGG19 and just fine-tuned only the classifier at the end to fit with my 120 classes. I let all layers trainable to maybe improve performance by having a deeper training. The problem is that the model is very slow (even if I let it run for a night, I only got couple of epochs and reach an accuracy of around 45% - I have a GPU GTX 1070). Then, my thinking was to freeze all layers from this model as I have only 10k images and only train the few last Denses layers but it's still not realy fast. After watching this video (at around 2 min 30s), I decided to replicate the principle of Transfer-Values with InceptionResnetv2. I processed every pictures and saved the output in a numpy matrix with the following code. # Loading pre-trained Model + freeze layers model = applications.inception_resnet_v2.InceptionResNetV2( include_top=False, weights='imagenet', pooling='avg') for layer in model.layers: layer.trainable = False # Extraction of features and saving a = True for filename in glob.glob('train/resized/*.jpg'): name_img = os.path.basename(filename)[:-4] class_ = label[label[""id""] == name_img][""breed""].values[0] input_img = np.expand_dims(np.array(Image.open(filename)), 0) pred = model.predict(input_img) if a: X = np.array(pred) y = np.array(class_) a = False else: X = np.vstack((X, np.array(pred))) y = np.vstack((y, class_)) np.savez_compressed('preprocessed.npz', X=X, y=y) X is a matrix of shape (10222, 1536) and y is (10222, 1). After, I designed my classifier (several topologies) and I have no idea why it is not able to perform any learning. # Just to One-Hot-Encode labels properly to (10222, 120) label_binarizer = sklearn.preprocessing.LabelBinarizer() y = label_binarizer.fit_transform(y) model = Sequential() model.add(Dense(512, input_dim=X.shape[1])) # model.add(Dense(2048, activation=""relu"")) # model.add(Dropout(0.5)) # model.add(Dense(256)) model.add(Dense(120, activation='softmax')) model.compile( loss = ""categorical_crossentropy"", optimizer = ""Nadam"", # I tried several ones metrics=[""accuracy""] ) model.fit(X, y, epochs=100, batch_size=64, callbacks=[early_stop], verbose=1, shuffle=True, validation_split=0.10) Below you can find the output from the model : Train on 9199 samples, validate on 1023 samples Epoch 1/100 9199/9199 [==============================] - 2s 185us/step - loss: 15.9639 - acc: 0.0096 - val_loss: 15.8975 - val_acc: 0.0137 Epoch 2/100 9199/9199 [==============================] - 1s 100us/step - loss: 15.9639 - acc: 0.0096 - val_loss: 15.8975 - val_acc: 0.0137 Epoch 3/100 9199/9199 [==============================] - 1s 98us/step - loss: 15.9639 - acc: 0.0096 - val_loss: 15.8975 - val_acc: 0.0137 Epoch 4/100 9199/9199 [==============================] - 1s 96us/step - loss: 15.9639 - acc: 0.0096 - val_loss: 15.8975 - val_acc: 0.0137 Epoch 5/100 9199/9199 [==============================] - 1s 99us/step - loss: 15.9639 - acc: 0.0096 - val_loss: 15.8975 - val_acc: 0.0137 Epoch 6/100 9199/9199 [==============================] - 1s 96us/step - loss: 15.9639 - acc: 0.0096 - val_loss: 15.8975 - val_acc: 0.0137 I tried to change topologies, activation functions, add dropouts but nothing creates any improvements. I have no idea what is wrong in my way of doing this. Is the X matrix incorrect ? Isn't it allowed to use the pre-trained model only as feature extractor then perform the classification with a second model ? Many thanks for your feedbacks, Regards, Nicolas",|keras|classification|conv-neural-network|pre-trained-model|,Training,2
47857437,"Keras - ImportError: cannot import name 'CuDNNLSTM'. I am trying to use the CuDNNLSTM Keras cell to improve training speed for a recurrent neural network (doc here). When I run: from keras.layers import Bidirectional, CuDNNLSTM I get this error: ImportError: cannot import name 'CuDNNLSTM' My configuration is Keras 2.0.8, python 3.5, tensorflow-gpu 1.4.0 (all managed by Anaconda) and I have both CUDA 8.0 and cudnn 6.0 installed that should be OK with the nvidia dependencies of tensorflow (here). My code setup makes Keras effectively use tensorflow backend, and every layer except the ones starting with CuDNN* work fine. Anyone has an idea about the source of this import error ?",|keras|keras-layer|cudnn|,API,4
47898147,"Tensorflow Module Import error: AttributeError: module 'tensorflow.python.ops.nn' has no attribute 'rnn_cell'. When attempting to pass my RNN call, I call tf.nn.rnn_cell and I receive the following error: AttributeError: module 'tensorflow.python.ops.nn' has no attribute 'rnn_cell' Which is odd, because I'm sure I imported everything correctly: from __future__ import print_function, division from tensorflow.contrib import rnn import numpy as np import tensorflow as tf import matplotlib.pyplot as plt But looking at the docs, things have moved around between tensorflow versions. what would you all recommend to fix this?? Line, I'm getting the error against: state_per_layer_list = tf.unstack(init_state, axis=0) rnn_tuple_state = tuple( [tf.nn.rnn_cell.LSTMStateTuple(state_per_layer_list[idx][0], state_per_layer_list[idx][1]) for idx in range(num_layers)] ) Specifically: tf.nn.rnn_cell I'm using anaconda 3 to manage all of this so, the dependancies should all be taken care of. I have already tried working around a damn rank/shape error with Tensor shapes which took ages to resolve. Cheers in advance.",|python|tensorflow|python-import|attributeerror|recurrent-neural-network|,API,4
47932589,"Neural network: estimating sine wave frequency. With an objective of learning Keras LSTM and RNNs, I thought to create a simple problem to work on: given a sine wave, can we predict its frequency? I wouldn't expect a simple neural network to be able to predict the frequency, given that the notion of time is important here. However, even with LSTMs, I am unable to learn the frequency; I'm able to learn a trivial zero as the estimated frequency (even for train samples). Here's the code to create the train set. import numpy as np import matplotlib.pyplot as plt def create_sine(frequency): return np.sin(frequency*np.linspace(0, 2*np.pi, 2000)) train_x = np.array([create_sine(x) for x in range(1, 300)]) train_y = list(range(1, 300)) Now, here's a simple neural network for this example. from keras.models import Model from keras.layers import Dense, Input, LSTM input_series = Input(shape=(2000,),name='Input') dense_1 = Dense(100)(input_series) pred = Dense(1, activation='relu')(dense_1) model = Model(input_series, pred) model.compile('adam','mean_absolute_error') model.fit(train_x[:100], train_y[:100], epochs=100) As expected, this NN doesn't learn anything useful. Next, I tried a simple LSTM example. input_series = Input(shape=(2000,1),name='Input') lstm = LSTM(100)(input_series) pred = Dense(1, activation='relu')(lstm) model = Model(input_series, pred) model.compile('adam','mean_absolute_error') model.fit(train_x[:100].reshape(100, 2000, 1), train_y[:100], epochs=100) However, this LSTM based model also doesn't learn anything useful.",|python|tensorflow|neural-network|keras|lstm|,Training,2
48082655,"Custom weighted loss function in Keras for weighing each element. I'm trying to create a simple weighted loss function. Say, I have input dimensions 100 * 5, and output dimensions also 100 * 5. I also have a weight matrix of the same dimension. Something like the following: import numpy as np train_X = np.random.randn(100, 5) train_Y = np.random.randn(100, 5)*0.01 + train_X weights = np.random.randn(*train_X.shape) Defining the custom loss function def custom_loss_1(y_true, y_pred): return K.mean(K.abs(y_true-y_pred)*weights) Defining the model from keras.layers import Dense, Input from keras import Model import keras.backend as K input_layer = Input(shape=(5,)) out = Dense(5)(input_layer) model = Model(input_layer, out) Testing with existing metrics works fine model.compile('adam','mean_absolute_error') model.fit(train_X, train_Y, epochs=1) Testing with our custom loss function doesn't work model.compile('adam',custom_loss_1) model.fit(train_X, train_Y, epochs=10) It gives the following stack trace: InvalidArgumentError (see above for traceback): Incompatible shapes: [32,5] vs. [100,5] [[Node: loss_9/dense_8_loss/mul = Mul[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](loss_9/dense_8_loss/Abs, loss_9/dense_8_loss/mul/y)]] Where is the number 32 coming from? Testing a loss function with weights as Keras tensors def custom_loss_2(y_true, y_pred): return K.mean(K.abs(y_true-y_pred)*K.ones_like(y_true)) This function seems to do the work. So, probably suggests that a Keras tensor as a weight matrix would work. So, I created another version of the loss function. Loss function try 3 from functools import partial def custom_loss_3(y_true, y_pred, weights): return K.mean(K.abs(y_true-y_pred)*K.variable(weights, dtype=y_true.dtype)) cl3 = partial(custom_loss_3, weights=weights) Fitting data using cl3 gives the same error as above. InvalidArgumentError (see above for traceback): Incompatible shapes: [32,5] vs. [100,5] [[Node: loss_11/dense_8_loss/mul = Mul[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](loss_11/dense_8_loss/Abs, loss_11/dense_8_loss/Variable/read)]] I wonder what I'm missing! I could have used the notion of sample_weight in Keras; but then I'd have to reshape my inputs to a 3d vector. I thought that this custom loss function should really have been trivial.",|python|tensorflow|keras|loss-function|,Training,2
48221692,"Create a square function estimator with Keras. I'm still very new to neural networks. I try to achieve the following with Keras: I have a set of data where f(x) = x^2 + 3. Like this: x f(x) -10 103 -9.9 101.01 -9.8 99.04 -9.7 97.09 ... 9.7 97.09 9.8 99.04 9.9 101.01 10 103 So I try to build a model that can predict values f(x) based on x. I think that must be a simple thing but I couldn't find any hint. I get only outputs ranging from 0 to 1 (I guess due to normalization?) and they also seem to be crap. # Import the dataset dataset = pd.read_csv(""simple_network/Linear Data.csv"", header=None).values X_train, X_test, Y_train, Y_test = train_test_split(dataset[:,0:1], dataset[:,1], test_size=0.25,) # Now we build the model neural_network = Sequential() # create model neural_network.add(Dense(5, input_dim=1, activation='sigmoid')) # hidden layer neural_network.add(Dense(1, activation='sigmoid')) # output layer neural_network.compile(loss='mean_squared_error', optimizer='sgd', metrics=['accuracy']) neural_network_fitted = neural_network.fit(X_train, Y_train, epochs=1000, verbose=0, batch_size=X_train.shape[0], initial_epoch=0) I suspect I need to somehow cater for the fact that expect an interval value as an output, not a nominal or ordinal value. Any idea?",|python|tensorflow|neural-network|keras|,Model,0
48251943,"my Keras model does not predict negative values. I would like to test NN model with keras using a dataset containing positive and negative continuous values. The keras model is as follows: from keras.models import Sequential from keras.layers import Dense import numpy #fix random seed for reproducibility numpy.random.seed(7) #load and read dataset dataset = numpy.loadtxt(""Phenols-toxicity.csv"", delimiter="";"") # split into input (X) and output (Y) variables X = dataset[:,2:4] Y = dataset[:,1] print (""Variables: \n"", X) print (""Target_outputs: \n"", Y) # create model model = Sequential() model.add(Dense(4, input_dim=2, activation='relu')) #model.add(Dense(4, activation='relu')) model.add(Dense(1, activation='relu')) model.summary() # Compile model model.compile(loss='mean_squared_error', optimizer='sgd', metrics=['MSE']) # Fit the model model.fit(X, Y, epochs=500, batch_size=10) #make predictions (test) F = model.predict(X) print (""Predicted values: \n"", F) Everything seem going fine, However, all the negative values are predicted to zeros. Do the program soemwhere constrains values to be positive? My target values are as follow: [ 0.085 2.468 0.07 0.68 -0.184 0.545 -0.063 0.871 0.113 -0.208 0.688 1.638 2.03 0.078 0.573 1.036 0.015 -0.03 -0.381 0.701 0.205 0.266 1.796 2.033 0.168 2.097 1.081 -0.384 0.377 -0.326 -0.143 1.292 0.701 0.334 1.157 1.638 -0.046 0.343 1.167 1.301 0.277 1.131 0.471 0.617 0.707 0.185 0.604 0.017 0.381 0.804 0.618 2.712 -0.092 -0.826 0.122 0.932 0.281 0.854 1.276 2.574 1.125 0.73 0.796 1.145 1.569 2.664 0.034 1.398 0.393 0.612 -0.78 0.228 -1.043 -0.141 0.013 1.119 0.643 -0.242 0.757 -0.299 0.599 0.36 1.778 0.053 1.268 1.276 0.516 1.167 1.638 0.478 1.229 0.735 2.049 -0.064 1.201 1.41 1.295 0.798 1.854 0.16 -0.954 0.424 -0.51 1.638 -0.598 2.373 2.222 -0.358 -0.295 0.33 0.183 0.122 1.745 0.081 2.097 0.914 0.979 0.084 0.473 -0.302 0.879 0.366 0.172 0.45 1.307 0.886 -0.524 1.174 -0.512 0.939 0.775 -1.053 -0.814 0.475 -1.021 1.42 -0.82 0.654 0.571 -0.076 0.74 1.729 0.75 1.712 0.95 0.33 1.125 1.077 1.721 0.506 0.539 0.266 1.745 1.229 0.632 1.585 -0.155 0.463 1.638 0.67 -0.155 2.053 0.379 0.181 0.253 1.356] The predicted values are as follow: [[ 0. ] [ 2.03844833] [ 0.27423561] [ 0.59996957] [ 0. ] [ 0.44271404] [ 0. ] [ 0.47064281] [ 0.29890585] [ 0. ] [ 0.95044041] [ 1.84322166] [ 1.93953323] [ 0.18019629] [ 0.68691438] [ 0.96168059] [ 0.13934678] [ 0. ] [ 0. ] [ 0.87886989] [ 0.30047321] [ 0. ] [ 1.90942693] [ 1.83728123] [ 0. ] [ 1.84627008] [ 1.25797462] [ 0. ] [ 0.01434445] [ 0. ] [ 0. ] [ 1.1421392 ] [ 0.83652729] [ 0.37334418] [ 1.72099805] [ 1.73340106] [ 0.30456764] [ 0. ] [ 1.37316585] [ 1.34221601] [ 0.6739701 ] [ 0.79646528] [ 0.03717542] [ 0.35218674] [ 0.09512168] [ 0. ] [ 0.20107687] [ 0. ] [ 0.01262379] [ 1.00669646] [ 0.96650052] [ 2.10064697] [ 0. ] [ 0. ] [ 0.25874525] [ 0.61007023] [ 0.68899512] [ 0.81215698] [ 0.88977867] [ 2.43740511] [ 1.00497019] [ 0.94933379] [ 0.83326894] [ 0.63394952] [ 1.27170706] [ 2.56578207] [ 0. ] [ 1.29493976] [ 0.599581 ] [ 0.63211834] [ 0. ] [ 0.31536853] [ 0. ] [ 0. ] [ 0.02201092] [ 0.84008563] [ 0.73076451] [ 0. ] [ 0.4879511 ] [ 0. ] [ 0.77698141] [ 0.66419512] [ 1.56657863] [ 0.25022489] [ 1.36990726] [ 1.50250816] [ 0. ] [ 0.61219454] [ 0.87011993] [ 0.72275633] [ 1.36519527] [ 0.72287238] [ 2.3798852 ] [ 0. ] [ 1.23592615] [ 1.43725252] [ 0.95585048] [ 0.63723856] [ 1.8765614 ] [ 0.31583393] [ 0. ] [ 0.14386666] [ 0. ] [ 1.68151355] [ 0. ] [ 1.63394952] [ 1.97563386] [ 0. ] [ 0. ] [ 0.38875413] [ 0.18854523] [ 0.23547113] [ 1.13463831] [ 0.30076784] [ 1.61114097] [ 0.93304199] [ 1.04891086] [ 0.26546735] [ 0.62234318] [ 0. ] [ 0. ] [ 0.21855426] [ 0. ] [ 0. ] [ 0. ] [ 0. ] [ 0. ] [ 0.39396375] [ 0.45845711] [ 0. ] [ 0. ] [ 0. ] [ 0. ] [ 0. ] [ 0. ] [ 0.4718284 ] [ 0. ] [ 0. ] [ 0.91218936] [ 0. ] [ 0.82205164] [ 0.78155482] [ 0.98432505] [ 2.15232277] [ 0.97631133] [ 0.59527659] [ 0.83814716] [ 0.80036032] [ 1.17462301] [ 0.51232517] [ 0.82968521] [ 0.9463613 ] [ 1.69353771] [ 1.21046495] [ 1.36349583] [ 0.94378138] [ 0. ] [ 0.98034143] [ 1.66670561] [ 0.52768588] [ 0.93855476] [ 1.26870298] [ 0. ] [ 0. ] [ 0. ] [ 1.69362605]] [[ 0. ] [ 2.03844833] [ 0.27423561] [ 0.59996957] [ 0. ] [ 0.44271404] [ 0. ] [ 0.47064281] [ 0.29890585] [ 0. ] [ 0.95044041] [ 1.84322166] [ 1.93953323] [ 0.18019629] [ 0.68691438] [ 0.96168059] [ 0.13934678] [ 0. ] [ 0. ] [ 0.87886989] [ 0.30047321] [ 0. ] [ 1.90942693] [ 1.83728123] [ 0. ] [ 1.84627008] [ 1.25797462] [ 0. ] [ 0.01434445] [ 0. ] [ 0. ] [ 1.1421392 ] [ 0.83652729] [ 0.37334418] [ 1.72099805] [ 1.73340106] [ 0.30456764] [ 0. ] [ 1.37316585] [ 1.34221601] [ 0.6739701 ] [ 0.79646528] [ 0.03717542] [ 0.35218674] [ 0.09512168] [ 0. ] [ 0.20107687] [ 0. ] [ 0.01262379] [ 1.00669646] [ 0.96650052] [ 2.10064697] [ 0. ] [ 0. ] [ 0.25874525] [ 0.61007023] [ 0.68899512] [ 0.81215698] [ 0.88977867] [ 2.43740511] [ 1.00497019] [ 0.94933379] [ 0.83326894] [ 0.63394952] [ 1.27170706] [ 2.56578207] [ 0. ] [ 1.29493976] [ 0.599581 ] [ 0.63211834] [ 0. ] [ 0.31536853] [ 0. ] [ 0. ] [ 0.02201092] [ 0.84008563] [ 0.73076451] [ 0. ] [ 0.4879511 ] [ 0. ] [ 0.77698141] [ 0.66419512] [ 1.56657863] [ 0.25022489] [ 1.36990726] [ 1.50250816] [ 0. ] [ 0.61219454] [ 0.87011993] [ 0.72275633] [ 1.36519527] [ 0.72287238] [ 2.3798852 ] [ 0. ] [ 1.23592615] [ 1.43725252] [ 0.95585048] [ 0.63723856] [ 1.8765614 ] [ 0.31583393] [ 0. ] [ 0.14386666] [ 0. ] [ 1.68151355] [ 0. ] [ 1.63394952] [ 1.97563386] [ 0. ] [ 0. ] [ 0.38875413] [ 0.18854523] [ 0.23547113] [ 1.13463831] [ 0.30076784] [ 1.61114097] [ 0.93304199] [ 1.04891086] [ 0.26546735] [ 0.62234318] [ 0. ] [ 0. ] [ 0.21855426] [ 0. ] [ 0. ] [ 0. ] [ 0. ] [ 0. ] [ 0.39396375] [ 0.45845711] [ 0. ] [ 0. ] [ 0. ] [ 0. ] [ 0. ] [ 0. ] [ 0.4718284 ] [ 0. ] [ 0. ] [ 0.91218936] [ 0. ] [ 0.82205164] [ 0.78155482] [ 0.98432505] [ 2.15232277] [ 0.97631133] [ 0.59527659] [ 0.83814716] [ 0.80036032] [ 1.17462301] [ 0.51232517] [ 0.82968521] [ 0.9463613 ] [ 1.69353771] [ 1.21046495] [ 1.36349583] [ 0.94378138] [ 0. ] [ 0.98034143] [ 1.66670561] [ 0.52768588] [ 0.93855476] [ 1.26870298] [ 0. ] [ 0. ] [ 0. ] [ 1.69362605]]",|python|neural-network|deep-learning|keras|regression|,Model,0
48385830,"Simple Keras neural network isn't learning. I'm trying to replicate some of the examples from Neural Networks and Deep Learning with Keras, but I'm having problems training a network based on the architecture from chapter 1. The aim is to classify written digits from the MNIST dataset. The architecture: 784 inputs (one for each of the 28 * 28 pixels in MNIST images) a hidden layer of 30 neurons an output layer of 10 neurons Weights and biases are initialized from a Gaussian distribution with mean 0 and standard deviation 1. The loss/cost function is mean squared error. The optimizer is stochastic gradient descent. Hyper-parameters: learning rate = 3.0 batch size = 10 epochs = 30 My code: from keras.datasets import mnist from keras.models import Sequential from keras.layers import Dense from keras.optimizers import SGD from keras.initializers import RandomNormal # import data (x_train, y_train), (x_test, y_test) = mnist.load_data() # input image dimensions img_rows, img_cols = 28, 28 x_train = x_train.reshape(x_train.shape[0], img_rows * img_cols) x_test = x_test.reshape(x_test.shape[0], img_rows * img_cols) input_shape = (img_rows * img_cols,) x_train = x_train.astype('float32') x_test = x_test.astype('float32') x_train /= 255 x_test /= 255 print('x_train shape:', x_train.shape) print(x_train.shape[0], 'train samples') print(x_test.shape[0], 'test samples') # convert class vectors to binary class matrices num_classes = 10 y_train = keras.utils.to_categorical(y_train, num_classes) y_test = keras.utils.to_categorical(y_test, num_classes) print('y_train shape:', y_train.shape) # Construct model # 784 * 30 * 10 # Normal distribution for weights/biases # Stochastic Gradient Descent optimizer # Mean squared error loss (cost function) model = Sequential() layer1 = Dense(30, input_shape=input_shape, kernel_initializer=RandomNormal(stddev=1), bias_initializer=RandomNormal(stddev=1)) model.add(layer1) layer2 = Dense(10, kernel_initializer=RandomNormal(stddev=1), bias_initializer=RandomNormal(stddev=1)) model.add(layer2) print('Layer 1 input shape: ', layer1.input_shape) print('Layer 1 output shape: ', layer1.output_shape) print('Layer 2 input shape: ', layer2.input_shape) print('Layer 2 output shape: ', layer2.output_shape) model.summary() model.compile(optimizer=SGD(lr=3.0), loss='mean_squared_error', metrics=['accuracy']) # Train model.fit(x_train, y_train, batch_size=10, epochs=30, verbose=2) # Run on test data and output results result = model.evaluate(x_test, y_test, verbose=1) print('Test loss: ', result[0]) print('Test accuracy: ', result[1]) Output (Using Python 3.6 and the TensorFlow backend): Using TensorFlow backend. x_train shape: (60000, 784) 60000 train samples 10000 test samples y_train shape: (60000, 10) Layer 1 input shape: (None, 784) Layer 1 output shape: (None, 30) Layer 2 input shape: (None, 30) Layer 2 output shape: (None, 10) _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_1 (Dense) (None, 30) 23550 _________________________________________________________________ dense_2 (Dense) (None, 10) 310 ================================================================= Total params: 23,860 Trainable params: 23,860 Non-trainable params: 0 _________________________________________________________________ Epoch 1/30 - 7s - loss: nan - acc: 0.0987 Epoch 2/30 - 7s - loss: nan - acc: 0.0987 (repeated for all 30 epochs) Epoch 30/30 - 6s - loss: nan - acc: 0.0987 10000/10000 [==============================] - 0s 22us/step Test loss: nan Test accuracy: 0.098 As you can see, the network isn't learning at all, and I'm not sure why. The shapes look all right as far as I can tell. What am I doing that's preventing the network from learning? (Incidentally, I know that cross-entropy loss and a softmax output layer would be better; however, from the linked book, they don't appear to be necessary. The book's manually implemented network in chapter 1 learns successfully; I'm trying to replicate that before moving on.)",|python|machine-learning|keras|neural-network|mnist|,Model,0
48400225,"pytorch model.cuda() runtime error. I'm building a text classifier using pytorch, and got into some trouble with .cuda() method. I know that .cuda() moves all parameters into gpu so that the training procedure can be faster. However, error occurred in .cuda() method like this: start_time = time.time() for model_type in ('lstm',): hyperparam_combinations = score_util.all_combination(hyperparam_dict[model_type].values()) # for selecting best scoring model for test_idx, setting in enumerate(hyperparam_combinations): args = custom_dataset.list_to_args(setting,model_type=model_type) print(args) tsv = ""test %d\ttrain_loss\ttrain_acc\ttrain_auc\tval_loss\tval_acc\tval_auc\n""%(test_idx) # tsv record avg_score = [] # cv_mean score ### 4 fold cross validation for cv_num,(train_iter,val_iter) in enumerate(cv_splits): ### model initiation model = model_dict[model_type](args) if args.emb_type is not None: # word embedding init emb = emb_dict[args.emb_type] emb = score_util.embedding_init(emb,tr_text_field,args.emb_type) model.embed.weight.data.copy_(emb) model.cuda() --------------------------------------------------------------------------- RuntimeError Traceback (most recent call last) <ipython-input-20-ff6cfce73c10> in <module>() 23 model.embed.weight.data.copy_(emb) 24 ---> 25 model.cuda() 26 27 optimizer= torch.optim.Adam(model.parameters(),lr=args.lr) ~\Anaconda3\lib\site-packages\torch\nn\modules\module.py in cuda(self, device_id) 145 copied to that device 146 """""" --> 147 return self._apply(lambda t: t.cuda(device_id)) 148 149 def cpu(self, device_id=None): ~\Anaconda3\lib\site-packages\torch\nn\modules\module.py in _apply(self, fn) 116 def _apply(self, fn): 117 for module in self.children(): --> 118 module._apply(fn) 119 120 for param in self._parameters.values(): ~\Anaconda3\lib\site-packages\torch\nn\modules\module.py in _apply(self, fn) 122 # Variables stored in modules are graph leaves, and we don't 123 # want to create copy nodes, so we have to unpack the data. --> 124 param.data = fn(param.data) 125 if param._grad is not None: 126 param._grad.data = fn(param._grad.data) RuntimeError: Variable data has to be a tensor, but got torch.cuda.FloatTensor These are error traceback and I can't see why this happens. This code worked very well before I set epoch parameter to 1 to run some tests. I set epoch to 1000 again, but the problem lingers on. Aren't torch.cuda.FloatTensor object also Tensors? Any help would be much appreciated. my model looks like this : class TR_LSTM(nn.Module): def __init__(self,args, use_hidden_average=False, pretrained_emb = None): super(TR_LSTM,self).__init__() # arguments self.emb_dim = args.embed_dim self.emb_num = args.embed_num self.num_hidden_unit = args.hidden_state_dim self.num_lstm_layer = args.num_lstm_layer self.use_hidden_average = use_hidden_average self.batch_size = args.batch_size # layers self.embed = nn.Embedding(self.emb_num, self.emb_dim) if pretrained_emb is not None: self.embed.weight.data.copy_(pretrained_emb) self.lstm_layer = nn.LSTM(self.emb_dim, self.num_hidden_unit, self.num_lstm_layer, batch_first = True) self.fc_layer = nn.Sequential(nn.Linear(self.num_hidden_unit,self.num_hidden_unit), nn.Linear(self.num_hidden_unit,2)) def forward(self,x): x = self.embed(x) # batch * max_seq_len * emb_dim h_0,c_0 = self.init_hidden(x.size(0)) x, (_, _) = self.lstm_layer(x, (h_0,c_0)) # batch * seq_len * hidden_unit_num if not self.use_hidden_average: x = x[:,x.size(1)-1,:] x = x.squeeze(1) else: x = x.mean(1).squeeze(1) x = self.fc_layer(x) return x def init_hidden(self,batch_size): h_0, c_0 = torch.zeros(self.num_lstm_layer,batch_size , self.num_hidden_unit),\ torch.zeros(self.num_lstm_layer,batch_size , self.num_hidden_unit) h_0, c_0 = h_0.cuda(), c_0.cuda() h_0_param, c_0_param = torch.nn.Parameter(h_0), torch.nn.Parameter(c_0) return h_0_param, c_0_param",|pytorch|,GPU Usage,3
48509419,"why I must reshape one image to [n,height,width,channel] in CNN. I try to apply a convolutional layer to a picture of shape [256,256,3] a have an error when I user the tensor of the image directly conv1 = conv2d(input,W_conv1) +b_conv1 #<=== error error message: ValueError: Shape must be rank 4 but is rank 3 for 'Conv2D' (op: 'Conv2D') with input shapes: [256,256,3], [3,3,3,1]. but when I reshape the function conv2d work normally x_image = tf.reshape(input,[-1,256,256,3]) conv1 = conv2d(x_image,W_conv1) +b_conv1 if I must reshape the tensor what the best value to reshape in my case and why? import tensorflow as tf import numpy as np from PIL import Image def img_to_tensor(img) : return tf.convert_to_tensor(img, np.float32) def weight_generater(shape): return tf.Variable(tf.truncated_normal(shape,stddev=0.1)) def bias_generater(shape): return tf.Variable(tf.constant(.1,shape=shape)) def conv2d(x,W): return tf.nn.conv2d(x,W,[1,1,1,1],'SAME') def pool_max_2x2(x): return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,1,1,1],padding='SAME') #read image img = Image.open(""img.tif"") sess = tf.InteractiveSession() #convetir image to tensor input = img_to_tensor(img).eval() #print(input) # get img dimension img_dimension = tf.shape(input).eval() print(img_dimension) height,width,channel=img_dimension filter_size = 3 feature_map = 32 x = tf.placeholder(tf.float32,shape=[height*width*channel]) y = tf.placeholder(tf.float32,shape=21) # generate weigh [kernal size, kernal size,channel,number of filters] W_conv1 = weight_generater([filter_size,filter_size,channel,1]) #for each filter W has his specific bais b_conv1 = bias_generater([feature_map]) """""" I must reshape the picture x_image = tf.reshape(input,[-1,256,256,3]) """""" conv1 = conv2d(input,W_conv1) +b_conv1 #<=== error h_conv1 = tf.nn.relu(conv1) h_pool1 = pool_max_2x2(h_conv1) layer1_dimension = tf.shape(h_pool1).eval() print(layer1_dimension)",|python|tensorflow|conv-neural-network|reshape|tensor|,Tensors&Inputs,1
48493755,"Keras AttributeError: 'list' object has no attribute 'ndim'. I'm running a Keras neural network model in Jupyter Notebook (Python 3.6) I get the following error AttributeError: 'list' object has no attribute 'ndim' after calling the .fit() method from Keras.model model = Sequential() model.add(Dense(5, input_dim=len(X_data[0]), activation='sigmoid' )) model.add(Dense(1, activation = 'sigmoid')) model.compile(loss='mean_squared_error', optimizer='adam', metrics=['acc']) model.fit(X_data, y_data, epochs=20, batch_size=10) I checked the requirements.txt file for Keras (in Anaconda3) and the numpy, scipy, and six module versions are all up to date. What can explain this AttributeError? The full error message is the following (seems to be somewhat related to Numpy): --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) in () 3 model.add(Dense(1, activation = 'sigmoid')) 4 model.compile(loss='mean_squared_error', optimizer='adam', metrics=['acc']) ----> 5 model.fit(X_data, y_data, epochs=20, batch_size=10) ~\Anaconda3\lib\site-packages\keras\models.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs) 963 initial_epoch=initial_epoch, 964 steps_per_epoch=steps_per_epoch, --> 965 validation_steps=validation_steps) 966 967 def evaluate(self, x=None, y=None, ~\Anaconda3\lib\site-packages\keras\engine\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs) 1591 class_weight=class_weight, 1592 check_batch_axis=False, -> 1593 batch_size=batch_size) 1594 # Prepare validation data. 1595 do_validation = False ~\Anaconda3\lib\site-packages\keras\engine\training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size) 1424 self._feed_input_shapes, 1425 check_batch_axis=False, -> 1426 exception_prefix='input') 1427 y = _standardize_input_data(y, self._feed_output_names, 1428 output_shapes, ~\Anaconda3\lib\site-packages\keras\engine\training.py in _standardize_input_data(data, names, shapes, check_batch_axis, exception_prefix) 68 elif isinstance(data, list): 69 data = [x.values if x.class.name == 'DataFrame' else x for x in data] ---> 70 data = [np.expand_dims(x, 1) if x is not None and x.ndim == 1 else x for x in data] 71 else: 72 data = data.values if data.class.name == 'DataFrame' else data ~\Anaconda3\lib\site-packages\keras\engine\training.py in (.0) 68 elif isinstance(data, list): 69 data = [x.values if x.class.name == 'DataFrame' else x for x in data] ---> 70 data = [np.expand_dims(x, 1) if x is not None and x.ndim == 1 else x for x in data] 71 else: 72 data = data.values if data.class.name == 'DataFrame' else data AttributeError: 'list' object has no attribute 'ndim'",|python|tensorflow|machine-learning|keras|jupyter-notebook|,API,4
48527808,Loss is increasing from first epoch itself. I am training my siamese network for nlp. I have used lstm in it. and BCELoss. My loss is increasing from the first epoch. The first 36 epoch loss is error after 0 is 272.4357 [torch.FloatTensor of size 1] error after 1 is 271.8972 [torch.FloatTensor of size 1] error after 2 is 271.5598 [torch.FloatTensor of size 1] error after 3 is 271.6979 [torch.FloatTensor of size 1] error after 4 is 271.7315 [torch.FloatTensor of size 1] error after 5 is 272.3965 [torch.FloatTensor of size 1] error after 6 is 273.3982 [torch.FloatTensor of size 1] error after 7 is 275.1197 [torch.FloatTensor of size 1] error after 8 is 275.8228 [torch.FloatTensor of size 1] error after 9 is 278.3311 [torch.FloatTensor of size 1] error after 10 is 277.1054 [torch.FloatTensor of size 1] error after 11 is 277.8418 [torch.FloatTensor of size 1] error after 12 is 279.0189 [torch.FloatTensor of size 1] error after 13 is 278.4090 [torch.FloatTensor of size 1] error after 14 is 281.8813 [torch.FloatTensor of size 1] error after 15 is 283.4077 [torch.FloatTensor of size 1] error after 16 is 286.3093 [torch.FloatTensor of size 1] error after 17 is 287.6292 [torch.FloatTensor of size 1] error after 18 is 297.2318 [torch.FloatTensor of size 1] error after 19 is 307.4176 [torch.FloatTensor of size 1] error after 20 is 304.6649 [torch.FloatTensor of size 1] error after 21 is 328.9772 [torch.FloatTensor of size 1] error after 22 is 300.0669 [torch.FloatTensor of size 1] error after 23 is 292.3902 [torch.FloatTensor of size 1] error after 24 is 300.8633 [torch.FloatTensor of size 1] error after 25 is 305.1822 [torch.FloatTensor of size 1] error after 26 is 333.9984 [torch.FloatTensor of size 1] error after 27 is 346.2062 [torch.FloatTensor of size 1] error after 28 is 354.6148 [torch.FloatTensor of size 1] error after 29 is 341.3568 [torch.FloatTensor of size 1] error after 30 is 369.7580 [torch.FloatTensor of size 1] error after 31 is 366.1615 [torch.FloatTensor of size 1] error after 32 is 368.2455 [torch.FloatTensor of size 1] error after 33 is 391.4102 [torch.FloatTensor of size 1] error after 34 is 394.3190 [torch.FloatTensor of size 1] error after 35 is 401.0990 [torch.FloatTensor of size 1] error after 36 is 422.3723 [torch.FloatTensor of size 1],|python|optimization|neural-network|deep-learning|pytorch|,Training,2
48518434,"Keras - Negative dimension size caused by subtracting 5 from 4 for 'conv2d_5/convolution' (op: 'Conv2D') with input shapes: [?,4,80,64], [5,5,64,64]. I have a similar model to the one below, but after modifying the architecture, I keep getting the following error: Negative dimension size caused by subtracting 5 from 4 for 'conv2d_5/convolution' (op: 'Conv2D') with input shapes: [?,4,80,64], [5,5,64,64]. I am still new to machine learning so I couldn't make much sense of the parameters. Any help? model_img = Sequential(name=""img"") # Cropping model_img.add(Cropping2D(cropping=((124,126),(0,0)), input_shape=(376,1344,3))) # Normalization model_img.add(Lambda(lambda x: (2*x / 255.0) - 1.0)) model_img.add(Conv2D(16, (7, 7), activation=""relu"", strides=(2, 2))) model_img.add(Conv2D(32, (7, 7), activation=""relu"", strides=(2, 2))) model_img.add(Conv2D(32, (5, 5), activation=""relu"", strides=(2, 2))) model_img.add(Conv2D(64, (5, 5), activation=""relu"", strides=(2, 2))) model_img.add(Conv2D(64, (5, 5), activation=""relu"", strides=(2, 2))) model_img.add(Conv2D(128, (3, 3), activation=""relu"")) model_img.add(Conv2D(128, (3, 3), activation=""relu"")) model_img.add(Flatten()) model_img.add(Dense(100)) model_img.add(Dense(50)) model_img.add(Dense(10)) model_lidar = Sequential(name=""lidar"") model_lidar.add(Dense(32, input_shape=(360,))) model_lidar.add(Dropout(0.1)) model_lidar.add(Dense(10)) model_imu = Sequential(name='imu') model_imu.add(Dense(32, input_shape=(10, ))) model_imu.add(Dropout(0.1)) model_imu.add(Dense(10)) merged = Merge([model_img, model_lidar, model_imu], mode=""concat"") model = Sequential() model.add(merged) model.add(Dense(16)) model.add(Dropout(0.2)) model.add(Dense(1)) Answer: I couldn't complete the training because of issues with sensor but the model works fine now thanks to the 2 answers below",|python|neural-network|keras|,Model,0
48594888,"CNN train accuracy gets better during training, but test accuracy stays around 40%. So in the past few months I've been learning a lot about neural networks with Tensorflow and Keras, so I wanted to try to make a model for the CIFAR10 dataset (code below). However, during the training process, the accuracy gets better (from about 35% after 1 epoch to about 60-65% after 5 epochs), but the val_acc stays the same or increases only a little. Here are the printed results: Epoch 1/5 50000/50000 [==============================] - 454s 9ms/step - loss: 1.7761 - acc: 0.3584 - val_loss: 8.6776 - val_acc: 0.4489 Epoch 2/5 50000/50000 [==============================] - 452s 9ms/step - loss: 1.3670 - acc: 0.5131 - val_loss: 8.9749 - val_acc: 0.4365 Epoch 3/5 50000/50000 [==============================] - 451s 9ms/step - loss: 1.2089 - acc: 0.5721 - val_loss: 7.7254 - val_acc: 0.5118 Epoch 4/5 50000/50000 [==============================] - 452s 9ms/step - loss: 1.1140 - acc: 0.6080 - val_loss: 7.9587 - val_acc: 0.4997 Epoch 5/5 50000/50000 [==============================] - 452s 9ms/step - loss: 1.0306 - acc: 0.6385 - val_loss: 7.4351 - val_acc: 0.5321 10000/10000 [==============================] - 27s 3ms/step loss: 7.435152648162842 accuracy: 0.5321 I've looked around on the internet and my best guess is that my model is overfitted, so I've tried removing some layers, adding more dropout layers and reducing the amount of filters, but none showed any enhancement. The weirdest thing is that a while ago I made a very similar model, based on some tutorials, which had a final accuracy of 80% after 8 epochs. (I lost that file though) Here is the code of my model: model = Sequential() model.add(Conv2D(filters=256, kernel_size=(3, 3), activation='relu', data_format='channels_last', input_shape=(32, 32, 3))) model.add(MaxPooling2D((2, 2))) model.add(Conv2D(filters=128, kernel_size=(2, 2), activation='relu')) model.add(MaxPooling2D((2, 2))) model.add(Flatten()) model.add(Dense(1024, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(10, activation='softmax')) model.compile(optimizer=adam(), loss=categorical_crossentropy, metrics=['accuracy']) model.fit(train_images, train_labels, batch_size=1000, epochs=5, verbose=1, validation_data=(test_images, test_labels)) loss, accuracy = model.evaluate(test_images, test_labels) print('loss: ', loss, '\naccuracy: ', accuracy) train_images and test_images are numpy arrays of size (50000,32,32,3) and (10000,32,32,3) and train_labels and test_labels are numpy arrays of size (50000,10) and (10000,10). My question: what causes this and what can I do about it? Edit after Maxim's answer: I changed the model to this: model = Sequential() model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', kernel_initializer='he_normal', # better for relu based networks input_shape=(32, 32, 3))) model.add(MaxPooling2D((2, 2))) model.add(Conv2D(filters=256, kernel_size=(3, 3), activation='relu', kernel_initializer='he_normal')) model.add(MaxPooling2D((2, 2))) model.add(Flatten()) model.add(Dense(512, activation='relu')) model.add(Dropout(0.3)) model.add(Dense(10, activation='softmax')) and the output is now this: Epoch 1/10 50000/50000 [==============================] - 326s 7ms/step - loss: 1.4916 - acc: 0.4809 - val_loss: 7.7175 - val_acc: 0.5134 Epoch 2/10 50000/50000 [==============================] - 338s 7ms/step - loss: 1.0622 - acc: 0.6265 - val_loss: 6.9945 - val_acc: 0.5588 Epoch 3/10 50000/50000 [==============================] - 326s 7ms/step - loss: 0.8957 - acc: 0.6892 - val_loss: 6.6270 - val_acc: 0.5833 Epoch 4/10 50000/50000 [==============================] - 324s 6ms/step - loss: 0.7813 - acc: 0.7271 - val_loss: 5.5790 - val_acc: 0.6474 Epoch 5/10 50000/50000 [==============================] - 327s 7ms/step - loss: 0.6690 - acc: 0.7668 - val_loss: 5.7479 - val_acc: 0.6358 Epoch 6/10 50000/50000 [==============================] - 320s 6ms/step - loss: 0.5671 - acc: 0.8031 - val_loss: 5.8720 - val_acc: 0.6302 Epoch 7/10 50000/50000 [==============================] - 328s 7ms/step - loss: 0.4865 - acc: 0.8319 - val_loss: 5.6320 - val_acc: 0.6451 Epoch 8/10 50000/50000 [==============================] - 320s 6ms/step - loss: 0.3995 - acc: 0.8611 - val_loss: 5.3879 - val_acc: 0.6615 Epoch 9/10 50000/50000 [==============================] - 320s 6ms/step - loss: 0.3337 - acc: 0.8837 - val_loss: 5.6874 - val_acc: 0.6432 Epoch 10/10 50000/50000 [==============================] - 320s 6ms/step - loss: 0.2806 - acc: 0.9033 - val_loss: 5.7424 - val_acc: 0.6399 10000/10000 [==============================] - 19s 2ms/step loss: 5.74234927444458 accuracy: 0.6399 It seems that I'm overfitting again, even though I changed the model with the help I've gotten so far... Any explanations or tips? The input images are (32,32,3) numpy arrays normalized to (0,1)",|python|numpy|tensorflow|keras|conv-neural-network|,Training,2
48709839,"StopIteration: generator_output = next(output_generator). I have the following code which I rewrite to work on a large scale dataset. I am using Python generator to Fit the model on data yielded batch-by-batch. def subtract_mean_gen(x_source,y_source,avg_image,batch): batch_list_x=[] batch_list_y=[] for line,y in zip(x_source,y_source): x=line.astype('float32') x=x-avg_image batch_list_x.append(x) batch_list_y.append(y) if len(batch_list_x) == batch: yield (np.array(batch_list_x),np.array(batch_list_y)) batch_list_x=[] batch_list_y=[] model = resnet.ResnetBuilder.build_resnet_18((img_channels, img_rows, img_cols), nb_classes) model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) val = subtract_mean_gen(X_test,Y_test,avg_image_test,batch_size) model.fit_generator(subtract_mean_gen(X_train,Y_train,avg_image_train,batch_size), steps_per_epoch=X_train.shape[0]//batch_size,epochs=nb_epoch,validation_data = val, validation_steps = X_test.shape[0]//batch_size) I obtain the following error: 239/249 [===========================>..] - ETA: 60s - loss: 1.3318 - acc: 0.8330Exception in thread Thread-1: Traceback (most recent call last): File ""/usr/lib/python2.7/threading.py"", line 801, in __bootstrap_inner self.run() File ""/usr/lib/python2.7/threading.py"", line 754, in run self.__target(*self.__args, **self.__kwargs) File ""/usr/local/lib/python2.7/dist-packages/keras/utils/data_utils.py"", line 560, in data_generator_task generator_output = next(self._generator) StopIteration 240/249 [===========================>..] - ETA: 54s - loss: 1.3283 - acc: 0.8337Traceback (most recent call last): File ""cifa10-copy.py"", line 125, in <module> validation_steps = X_test.shape[0]//batch_size) File ""/usr/local/lib/python2.7/dist-packages/keras/legacy/interfaces.py"", line 87, in wrapper return func(*args, **kwargs) File ""/usr/local/lib/python2.7/dist-packages/keras/engine/training.py"", line 1809, in fit_generator generator_output = next(output_generator) StopIteration I looked into a similar question posted here however, I am not able to resolve the error why StopIteration is raised.",|python|numpy|keras|,Training,2
48934338,"Non linear Regression: Why isn't the model learning?. I just started learning keras. I am trying to train a non-linear regression model in keras but model doesn't seem to learn much. #datapoints X = np.arange(0.0, 5.0, 0.1, dtype='float32').reshape(-1,1) y = 5 * np.power(X,2) + np.power(np.random.randn(50).reshape(-1,1),3) #model model = Sequential() model.add(Dense(50, activation='relu', input_dim=1)) model.add(Dense(30, activation='relu', init='uniform')) model.add(Dense(output_dim=1, activation='linear')) #training sgd = SGD(lr=0.1); model.compile(loss='mse', optimizer=sgd, metrics=['accuracy']) model.fit(X, y, nb_epoch=1000) #predictions predictions = model.predict(X) #plot plt.scatter(X, y,edgecolors='g') plt.plot(X, predictions,'r') plt.legend([ 'Predictated Y' ,'Actual Y']) plt.show() what am I doing wrong?",|python|machine-learning|keras|neural-network|regression|,Training,2
48991644,"Keras fit_generator using input and output image generators 'ndim' error. I decided to try my hand at training an auto-encoder for re-coloring grey scale images. This approach might be a tad naive, but I want to play with it and see how good (or bad) it works and examine how I can improve it. However, it unexpectedly throws the following error: File ""colorise0.py"", line 63, in <module> validation_data=(val_g_generator, val_c_generator) File ""/usr/local/lib/python2.7/dist-packages/keras/legacy/interfaces.py"", line 91, in wrapper return func(*args, **kwargs) File ""/usr/local/lib/python2.7/dist-packages/keras/engine/training.py"", line 2183, in fit_generator val_x, val_y, val_sample_weight) File ""/usr/local/lib/python2.7/dist-packages/keras/engine/training.py"", line 1483, in _standardize_user_data exception_prefix='input') File ""/usr/local/lib/python2.7/dist-packages/keras/engine/training.py"", line 76, in _standardize_input_data data = [np.expand_dims(x, 1) if x is not None and x.ndim == 1 else x for x in data] AttributeError: 'DirectoryIterator' object has no attribute 'ndim' My code is: from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D from keras.layers import Conv2DTranspose as DeConv2D from keras.models import Model from keras import backend as K from keras.preprocessing.image import ImageDataGenerator img_width, img_height = 150, 150 batch_size=32 train_data_dir = './train/' validation_data_dir = './validation/' input_shape = (img_width, img_height,3) train_datagen = ImageDataGenerator(rescale = 1./255) test_datagen = ImageDataGenerator(rescale = 1./255) train_c_generator= train_datagen.flow_from_directory( train_data_dir+'colored', target_size=(img_width, img_height), batch_size=batch_size ) train_g_generator= train_datagen.flow_from_directory( train_data_dir+'grey', target_size=(img_width, img_height), batch_size=batch_size ) val_c_generator= test_datagen.flow_from_directory( validation_data_dir+'colored', target_size=(img_width, img_height), batch_size=batch_size ) val_g_generator= test_datagen.flow_from_directory( validation_data_dir+'grey', target_size=(img_width, img_height), batch_size=batch_size ) input_img=Input(shape=(img_width,img_height,3)) x=Conv2D(32,(3,3), activation='relu', padding='same')(input_img) x=Conv2D(32,(3,3), activation='relu', padding='same')(x) x=Conv2D(32,(3,3), activation='relu', padding='same')(x) x=Conv2D(32,(3,3), activation='relu', padding='same')(x) y=DeConv2D(32,(3,3), activation='relu',padding='same')(x) y=DeConv2D(32,(3,3), activation='relu',padding='same')(y) y=DeConv2D(32,(3,3), activation='relu',padding='same')(y) decoded=DeConv2D(3,(3,3), padding='same')(y) autoencoder = Model(input_img, decoded) autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy') autoencoder.fit_generator( train_g_generator, train_c_generator, epochs=50, validation_data=(val_g_generator, val_c_generator) ) Given the error message, I think the error might stem from calling two generators (one supplying the grey input images, the second supplying the original colorful images, serving as the targets). How can I solve this? Many thanks!",|python|keras|generator|autoencoder|,API,4
48987959,"classification metrics can't handle a mix of continuous-multioutput and multi-label-indicator targets. I have created an ANN with numerical inputs and a single categorical output which is one hot encoded to be 1 of 19 categories. I set my output layer to have 19 units. I don't know how to perform the confusion matrix now nor how to classifier.predict() in light of this rather than a single binary output. I keep getting an error saying classification metrics can't handle a mix of continuous-multioutput and multi-label-indicator targets. Not sure how to proceed. #Importing Datasets dataset=pd.read_csv('Data.csv') x = dataset.iloc[:,1:36].values # lower bound independent variable to upper bound in a matrix (in this case only 1 column 'NC') y = dataset.iloc[:,36:].values # dependent variable vector print(x.shape) print(y.shape) #One Hot Encoding fuel rail column from sklearn.preprocessing import LabelEncoder, OneHotEncoder labelencoder_y= LabelEncoder() y[:,0]=labelencoder_y.fit_transform(y[:,0]) onehotencoder= OneHotEncoder(categorical_features=[0]) y = onehotencoder.fit_transform(y).toarray() print(y[:,0:]) print(x.shape) print (y.shape) #splitting data into Training and Test Data from sklearn.model_selection import train_test_split x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.1,random_state=0) #Feature Scaling from sklearn.preprocessing import StandardScaler sc = StandardScaler() #x_train = sc.fit_transform(x_train) #x_test=sc.transform(x_test) y_train = sc.fit_transform(y_train) y_test=sc.transform(y_test) # PART2 - Making ANN, deep neural network #Importing the Keras libraries and packages import keras from keras.models import Sequential from keras.layers import Dense #Initialising ANN classifier = Sequential() #Adding the input layer and first hidden layer classifier.add(Dense(activation= 'relu', input_dim =35, units=2, kernel_initializer=""uniform""))#rectifier activation function, include all input with one hot encoding #Adding second hidden layer classifier.add(Dense(activation= 'relu', units=2, kernel_initializer=""uniform"")) #rectifier activation function #Adding the Output Layer classifier.add(Dense(activation='softmax', units=19, kernel_initializer=""uniform"")) #Compiling ANN - stochastic gradient descent classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])#stochastic gradient descent #Fit ANN to training set #PART 3 - Making predictions and evaluating the model #Fitting classifier to the training set classifier.fit(x_train, y_train, batch_size=10, epochs=100)#original batch is 10 and epoch is 100 #Predicting the Test set rules y_pred = classifier.predict(x_test) y_pred = (y_pred > 0.5) #greater than 0.50 on scale 0 to 1 print(y_pred) #Making confusion matrix that checks accuracy of the model from sklearn.metrics import confusion_matrix cm = confusion_matrix(y_test, y_pred)",|python|keras|,Tensors&Inputs,1
49113140,"ImportError: cannot import name '_obtain_input_shape' from keras. In Keras, I'm trying to import _obtain_input_shape as follows: from keras.applications.imagenet_utils import _obtain_input_shape However, I get the following error: ImportError: cannot import name '_obtain_input_shape' The reason I'm trying to import _obtain_input_shape is so that I can determine the input shape(so as to load VGG-Face as follows : I'm using it to determine the correct input shape of the input tensor as follow: input_shape = _obtain_input_shape(input_shape, default_size=224, min_size=48, data_format=K.image_data_format(), require_flatten=include_top)` Please assist? Thanks in advance.",|keras|keras-layer|keras-2|,API,4
49206550,"pytorch error: multi-target not supported in CrossEntropyLoss(). I am on a project using acceleration data to predict some activities. But I have problems on the loss calculation. I am using CrossEntropyLoss for it. Data is used for it like below I use the first 4 data of each rows to predict the index like the last one of each rows. 1 84 84 81 4 81 85 85 80 1 81 82 84 80 1 1 85 84 2 0 81 85 82 80 1 81 82 84 80 1 81 25 84 80 5 The error messages are like below. minoh@minoh-VirtualBox:~/cow$ python lec5.py Traceback (most recent call last): File ""lec5.py"", line 97, in <module> train(epoch) File ""lec5.py"", line 74, in train loss = criterion(y_pred, labels) File ""/home/minoh/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 357, in __call__ result = self.forward(*input, **kwargs) File ""/home/minoh/anaconda3/lib/python3.6/site-packages/torch/nn/modules/loss.py"", line 679, in forward self.ignore_index, self.reduce) File ""/home/minoh/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py"", line 1161, in cross_entropy return nll_loss(log_softmax(input, 1), target, weight, size_average, ignore_index, reduce) File ""/home/minoh/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py"", line 1052, in nll_loss return torch._C._nn.nll_loss(input, target, weight, size_average, ignore_index, reduce) RuntimeError: multi-target not supported at /opt/conda/conda-bld/pytorch_1518243271935/work/torch/lib/THNN/generic/ClassNLLCriterion.c:22 My code is based on Sung Kim's pytorch import numpy as np import torch from torch.autograd import Variable import torch.nn.functional as F from torch.utils.data import Dataset, DataLoader import torch.nn as nn import torch.optim as optim from torchvision import datasets, transforms class CowDataset(Dataset): def __init__(self): xy_str = np.loadtxt('cow_test', delimiter = ' ', dtype = np.str) xy = xy_str.astype(np.float32) xy_int = xy_str.astype(np.int) self.len = xy.shape[0] self.x_data = torch.from_numpy(xy[:, 0:4]) self.y_data = torch.from_numpy(xy_int[:, [4]]) def __getitem__(self, index): return self.x_data[index], self.y_data[index] def __len__(self): return self.len dataset = CowDataset() train_loader = DataLoader(dataset = dataset, batch_size = 32, shuffle = True) class CowTestset(Dataset): def __init__(self): xy_str = np.loadtxt('cow_test2', delimiter = ' ', dtype =np.str) xy = xy_str.astype(np.float32) xy_int = xy_str.astype(np.int) self.len = xy.shape[0] self.x_data = torch.from_numpy(xy[:, 0:4]) self.y_data = torch.from_numpy(xy_int[:, [4]]) def __getitem__(self, index): return self.x_data[index], self.y_data[index] def __len__(self): return self.len testset = CowTestset() test_loader = DataLoader(dataset = testset, batch_size = 32, shuffle = True) class Model(torch.nn.Module): def __init__(self): super(Model, self).__init__() self.l1 = torch.nn.Linear(4,5) self.l2 = torch.nn.Linear(5,7) self.l3 = torch.nn.Linear(7,6) self.sigmoid = torch.nn.Sigmoid() def forward(self, x): out1 = self.sigmoid(self.l1(x)) out2 = self.sigmoid(self.l2(out1)) y_pred = self.sigmoid(self.l3(out2)) return y_pred model = Model() criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr = 0.1, momentum = 0.5) def train(epoch): model.train() for batch_idx, (inputs, labels) in enumerate(train_loader): inputs, labels = Variable(inputs), Variable(labels) optimizer.zero_grad() y_pred = model(inputs) loss = criterion(y_pred, labels) loss.backward() optimizer.step() if batch_idx % 10 == 0: print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format( epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.data[0])) def test(): model.eval() test_loss = 0 correct = 0 for data, target in test_loader: data, target = Variable(data, volatile = True), Variable(target) print(target) output = model(data) test_loss += criterion(output, target).data[0] pred = output.data.max(1, keepdim = True)[1] correct += pred.eq(target.data.view_as(pred)).cpu().sum() test_loss /= len(test_loader.dataset) print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(test_loss, correct, len(test_loader.dataset), 100.* correct / len(test_loader.dataset))) for epoch in range(1,7): train(epoch) test()",|python|python-3.x|machine-learning|neural-network|pytorch|,Tensors&Inputs,1
49304817,"PyTorch: Comparing predicted label and target label to compute accuracy. I'm trying to implement this loop to get the accuracy of my PyTorch CNN (The complete code of it is here) My version of the loop is so far: correct = 0 test_total = 0 for itera, testdata2 in enumerate(test_loader, 0): test_images2, test_labels2 = testdata2 if use_gpu: test_images2 = Variable(test_images2.cuda()) else: test_images2 = Variable(test_images2) outputs = model(test_images2) _, predicted = torch.max(outputs.data, 1) test_total += test_labels2.size(0) test_labels2 = test_labels2.type_as(predicted) correct += (predicted == test_labels2[0]).sum() print('Accuracy of the network on all the test images: %d %%' % ( 100 * correct / test_total)) If I run it like this, I get: > Traceback (most recent call last): File > ""c:/python_code/Customized-DataLoader-master_two/multi_label_classifier_for2classes.py"", > line 186, in <module> > main() File ""c:/python_code/Customized-DataLoader-master_two/multi_label_classifier_for2classes.py"", > line 177, in main > correct += (predicted == test_labels2[0]).sum() File ""C:\anaconda\envs\pytorch_cuda\lib\site-packages\torch\tensor.py"", > line 360, in __eq__ > return self.eq(other) RuntimeError: invalid argument 3: sizes do not match at > c:\anaconda2\conda-bld\pytorch_1519501749874\work\torch\lib\thc\generated\../THCTensorMathCompareT.cuh:65 I used test_labels2 = test_labels2.type_as(predicted) to have both tensors as LongTensors, which seems to work fine to avert the ""Expected this...but got..."" errors. They look like this now: test_labels2 after conversion: 0 1 1 0 1 0 [torch.cuda.LongTensor of size 3x2 (GPU 0)] predicted: 1 1 1 [torch.cuda.LongTensor of size 3 (GPU 0)] I supppose the problem now is, that test_labels2[0] is returning a row but not the column. How do I get this to work?",|python|python-3.x|torch|pytorch|tensor|,Tensors&Inputs,1
49686230,"Poor loss convergence and accuracy with CNN model. I've built a binary classifier using TF which classify's a 16x16 gray scale image into one of two classes with distribution 87-13. The issue that I'm having is that the model's log loss converges to ~0.4, which is better than random however I cannot get it to improve beyond this. The vision problem is in the realm of video encoding, This image should provide some understanding to the problem, where images are are either to be or not to be split (0/1) based on their homogeneity. Note squares near edges are more likely sub-split to smaller ones. When validating the model (1.1e7 examples, 87-13 distribution), I cannot achieve an F1-score better than ~50%. My training data consists of 2.2e8 examples which are oversampled/undersampled to achieve 50-50 distribution. I'm using a batch size of 1024 a substantial shuffle buffer (the data isn't ordered to begin with). Optimised using Adam, with default hyperparameters. Things I've tried to improve the performance (test (outcome)): Larger networks, changing number of layers, activations, convolutional kernel sizes and strides etc (same convergence) Dropout between dense layers(Same performance as with large nets, worse performance with small nets) Other Adam hyperparameters (all lead to same convergence, eventually) Other optimisers (same as above) Training with very small dataset to test convergence (loss saturates to 0) Regularising input (no effect) Varying batch size (just influences the noise in the loss and convergence time) I've been stuck trying to get the performance to improve, I think I've read every SO question that I could find. Any advice would be a great help. def cnn_model(features, labels, mode): # downsample to 8x8 using 2x2 local averaging features_8x8 = tf.nn.avg_pool( value=tf.cast(features[""x""], tf.float32), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=""SAME"", data_format='NHWC' ) conv2d_0 = tf.layers.conv2d(inputs=features_8x8, filters=6, kernel_size=[3, 3], strides=(1, 1), activation=tf.nn.relu, name=""conv2d_0"") pool0 = tf.layers.max_pooling2d( inputs=conv2d_0, pool_size=(2, 2), strides=(2, 2), padding=""SAME"", data_format='channels_last' ) conv2d_1 = tf.layers.conv2d(inputs=pool0, filters=16, kernel_size=[3, 3], strides=(3, 3), activation=tf.nn.relu, name=""conv2d_1"") reshape1 = tf.reshape(conv2d_1, [-1, 16]) dense0 = tf.layers.dense(inputs=reshape1, units=10, activation=tf.nn.relu, name=""dense0"") logits = tf.layers.dense(inputs=dense0, units=1, name=""logits"") # ######################################################## predictions = { ""classes"": tf.round(tf.nn.sigmoid(logits)), ""probabilities"": tf.nn.sigmoid(logits) } # ######################################################## if mode == tf.estimator.ModeKeys.PREDICT: return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions) # ######################################################## cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits( labels=tf.cast(labels['y'], tf.float32), logits=logits ) loss = tf.reduce_mean(cross_entropy) # ######################################################## # Configure the Training Op (for TRAIN mdoe) if mode == tf.estimator.ModeKeys.TRAIN: optimiser = tf.train.AdamOptimizer(learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08) train_op = optimiser.minimize( loss=loss, global_step=tf.train.get_global_step()) return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op) # Add evalutation metrics (for EVAL mode) eval_metric_ops = { ""accuracy"": tf.metrics.accuracy( labels=labels[""y""], predictions=predictions[""classes""]), } return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)",|tensorflow|loss-function|,Training,2
49763672,"TensorFlow Debugger ValueError: Node name 'Add/x' is not found in partition graphs of device. I am working on TensorFlow 1.6 and I was trying to set up the TensorFlow debugger tfdbg in my program. When I enter the command run within the tfdbg terminal, I get the following error: Traceback (most recent call last): File ""/Users/Documents/imputation/main.py"", line 346, in <module> args_ = _Parser(description='Train/evaluate the network for incidents ' File ""/Users/Documents/imputation/main.py"", line 312, in parse_args command(args, parser) File ""/Users/Documents/imputation/main.py"", line 222, in _call args_dict = _Train._call(namespace, parser) File ""/Users/Documents/imputation/main.py"", line 151, in _call train(**args_dict) File ""/Users/Documents/imputation/tf_impute.py"", line 185, in train mon_sess.run([train_op, File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 546, in run run_metadata=run_metadata) File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1022, in run run_metadata=run_metadata) File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1113, in run raise six.reraise(*original_exc_info) File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/six.py"", line 693, in reraise raise value File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1098, in run return self._sess.run(*args, **kwargs) File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1178, in run run_metadata=run_metadata)) File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/debug/wrappers/hooks.py"", line 150, in after_run self._session_wrapper.on_run_end(on_run_end_request) File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/debug/wrappers/local_cli_wrapper.py"", line 323, in on_run_end self._dump_root, partition_graphs=partition_graphs) File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/debug/lib/debug_data.py"", line 495, in __init__ self._load_all_device_dumps(partition_graphs, validate) File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/debug/lib/debug_data.py"", line 517, in _load_all_device_dumps self._load_partition_graphs(partition_graphs, validate) File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/debug/lib/debug_data.py"", line 797, in _load_partition_graphs self._validate_dump_with_graphs(debug_graph.device_name) File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/debug/lib/debug_data.py"", line 842, in _validate_dump_with_graphs ""device %s."" % (datum.node_name, device_name)) ValueError: Node name 'Add/x' is not found in partition graphs of device /job:localhost/replica:0/task:0/device:CPU:0. I was also looking at the issue in https://github.com/tensorflow/tensorflow/issues/8753 where a similar issue was discussed but this provided solution didn't worked for me. I already tried to implement the tfdbg as a wrapper for the session and also within the hooks. The part of the code where I implemented tfdbg looks as follows: class _LoggerHook(tf.train.SessionRunHook): cumulative_loss = 0 def begin(self): self._step = -1 self._start_time = time.time() def before_run(self, run_context): self._step += 1 return tf.train.SessionRunArgs(loss) def after_run(self, run_context, run_values): loss_value = run_values.results self.cumulative_loss += loss_value if self._step == 0: print('Starting training at %s' % datetime.now()) elif self._step % print_step == 0: current_time = time.time() duration = current_time - self._start_time self._start_time = current_time rms_error = math.sqrt(2 * self.cumulative_loss / print_step) self.cumulative_loss = 0 examples_per_sec = print_step * batch_size / duration sec_per_batch = float(duration / print_step) format_str = ( '%s: %d examples, rms_error = %.6f (%.1f examples/sec; ' '%.3f sec/batch)') print(format_str % ( datetime.now(), self._step * batch_size, rms_error, examples_per_sec, sec_per_batch)) max_steps = epochs * (examples // batch_size) model_saver = tf.train.Saver(var_list=tf.model_variables()) class _CheckpointSaverHook(CheckpointSaverHook): def __init__(self, *args, **kwargs): super(_CheckpointSaverHook, self).__init__(*args, **kwargs) assert self._listeners == [], 'CheckpointSaverListener not ' \ 'allowed' def end(self, session): class _FinalStepHook(FinalOpsHook): def end(self, session): super(_FinalStepHook, self).end(session) print('Saving last checkpoint at step %d' % session.run( global_step)) model_saver.save(session, os.path.join(train_dir, ""model.ckpt""), global_step) final_hook = _FinalStepHook([train_op, preds_update_op]) scaffold = tf.train.Scaffold(saver=model_saver) logger_hook = _LoggerHook() hooks = [_CheckpointSaverHook(checkpoint_dir=train_dir, save_secs=1000, scaffold=scaffold), tf.train.StopAtStepHook(last_step=max_steps - 1), tf.train.NanTensorHook(loss), logger_hook, final_hook, tf_debug.LocalCLIDebugHook()] config = tf.ConfigProto(log_device_placement=log_device_placement) config.gpu_options.allow_growth = True start_train = time.time() with tf.train.MonitoredTrainingSession(checkpoint_dir=train_dir, hooks=hooks, config=config, save_checkpoint_secs=0, scaffold=scaffold) as mon_sess: try: while not mon_sess.should_stop(): mon_sess.run([train_op, # globals_preds ]) except OutOfRangeError as e: print(e) print('global step %s' % logger_hook._step) except KeyboardInterrupt: print('Train interrupted at global step %s' % logger_hook._step) print('Training %d examples in %d epochs took %s' % ( examples, epochs, secs_to_time(time.time() - start_train))) upload_timestamped_tar(s3_url, train_dir, keep_dir, keep_tar, wait) return final_hook.final_ops_values[1] Do you know how to fix this issue?",|python|debugging|tensorflow|valueerror|,API,4
50063613,"What is the purpose of the add_loss function in Keras?. Currently I stumbled across variational autoencoders and tried to make them work on MNIST using keras. I found a tutorial on github. My question concerns the following lines of code: # Build model vae = Model(x, x_decoded_mean) # Calculate custom loss xent_loss = original_dim * metrics.binary_crossentropy(x, x_decoded_mean) kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1) vae_loss = K.mean(xent_loss + kl_loss) # Compile vae.add_loss(vae_loss) vae.compile(optimizer='rmsprop') Why is add_loss used instead of specifying it as compile option? Something like vae.compile(optimizer='rmsprop', loss=vae_loss) does not seem to work and throws the following error: ValueError: The model cannot be compiled because it has no loss to optimize. What is the difference between this function and a custom loss function, that I can add as an argument for Model.fit()? Thanks in advance! P.S.: I know there are several issues concerning this on github, but most of them were open and uncommented. If this has been resolved already, please share the link! Edit 1 I removed the line which adds the loss to the model and used the loss argument of the compile function. It looks like this now: # Build model vae = Model(x, x_decoded_mean) # Calculate custom loss xent_loss = original_dim * metrics.binary_crossentropy(x, x_decoded_mean) kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1) vae_loss = K.mean(xent_loss + kl_loss) # Compile vae.compile(optimizer='rmsprop', loss=vae_loss) This throws an TypeError: TypeError: Using a 'tf.Tensor' as a Python 'bool' is not allowed. Use 'if t is not None:' instead of 'if t:' to test if a tensor is defined, and use TensorFlow ops such as tf.cond to execute subgraphs conditioned on the value of a tensor. Edit 2 Thanks to @MarioZ's efforts, I was able to figure out a workaround for this. # Build model vae = Model(x, x_decoded_mean) # Calculate custom loss in separate function def vae_loss(x, x_decoded_mean): xent_loss = original_dim * metrics.binary_crossentropy(x, x_decoded_mean) kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1) vae_loss = K.mean(xent_loss + kl_loss) return vae_loss # Compile vae.compile(optimizer='rmsprop', loss=vae_loss) ... vae.fit(x_train, x_train, # <-- did not need this previously shuffle=True, epochs=epochs, batch_size=batch_size, validation_data=(x_test, x_test)) # <-- worked with (x_test, None) before For some strange reason, I had to explicitly specify y and y_test while fitting the model. Originally, I didn't need to do this. The produced samples seem reasonable to me. Although I could resolve this, I still don't know what the differences and disadvantages of these two methods are (other than needing a different syntax). Can someone give me more insight?",|neural-network|keras|autoencoder|,Training,2
50079585,"CNN with keras, accuracy not improving. I have started with Machine Learning recently, I am learning CNN, I planned to write an application for Car Damage severity detection, with the help of this Keras blog and this github repo. This is how car data-set looks like: F:\WORKSPACE\ML\CAR_DAMAGE_DETECTOR\DATASET\DATA3A training (979 Images for all 3 categories of training set) ?01-minor ?02-moderate ?03-severe validation (171 Images for all 3 categories of validation set) 01-minor 02-moderate 03-severe Following code gives me only 32% of accuracy. from keras.preprocessing.image import ImageDataGenerator from keras.models import Sequential from keras.layers import Conv2D, MaxPooling2D from keras.layers import Activation, Dropout, Flatten, Dense from keras import backend as K # dimensions of our images. img_width, img_height = 150, 150 train_data_dir = 'dataset/data3a/training' validation_data_dir = 'dataset/data3a/validation' nb_train_samples = 979 nb_validation_samples = 171 epochs = 10 batch_size = 16 if K.image_data_format() == 'channels_first': input_shape = (3, img_width, img_height) else: input_shape = (img_width, img_height, 3) model = Sequential() model.add(Conv2D(32, (3, 3), input_shape=input_shape)) model.add(Activation('relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Conv2D(32, (3, 3))) model.add(Activation('relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Conv2D(64, (3, 3))) model.add(Activation('relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Flatten()) model.add(Dense(64)) model.add(Activation('relu')) model.add(Dropout(0.5)) model.add(Dense(1)) model.add(Activation('sigmoid')) model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy']) # this is the augmentation configuration we will use for training train_datagen = ImageDataGenerator( rescale=1. / 255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True) # this is the augmentation configuration we will use for testing: # only rescaling test_datagen = ImageDataGenerator(rescale=1. / 255) train_generator = train_datagen.flow_from_directory( train_data_dir, target_size=(img_width, img_height), batch_size=batch_size, class_mode='binary') validation_generator = test_datagen.flow_from_directory( validation_data_dir, target_size=(img_width, img_height), batch_size=batch_size, class_mode='binary') model.fit_generator( train_generator, steps_per_epoch=nb_train_samples // batch_size, epochs=epochs, validation_data=validation_generator, validation_steps=nb_validation_samples // batch_size) model.save_weights('first_try.h5') I tried: By increasing the epochs to 10, 20,50. By increasing images in the dataset (all validation images added to training set). By updating the filter size in the Conv2D layer Tried to add couple of Conv2D layer, MaxPooling layers Also tried with different optimizers such as adam, Sgd, etc Also Tried by updating the filter strides to (1,1) and (5,5) instead of (3,3) Also tried by updating the changing image dimensions to (256, 256), (64, 64) from (150, 150) But no luck, every-time I'm getting accuracy up to 32% or less than that but not more. Any idea what I'm missing. As in the github repo we can see, it gives 72% accuracy for the same dataset (Training -979, Validation -171). Why its not working for me. I tried his code from the github link on my machine but it hanged up while training the dataset(I waited for more than 8 hours), so changed the approach, but still no luck so far. Here's the Pastebin containing output of my training epochs.",|python|keras|conv-neural-network|theano|,Model,0
50306988,"Neural net fails on toy dataset. I have created the following toy dataset: I am trying to predict the class with a neural net in keras: model = Sequential() model.add(Dense(units=2, activation='sigmoid', input_shape= (nr_feats,))) model.add(Dense(units=nr_classes, activation='softmax')) model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) With nr_feats and nr_classes set to 2. The neural net can only predict with 50 percent accuracy returning either all 1's or all 2's. Using Logistic Regression results in 100 percent accuracy. I can not find what is going wrong here. I have uploaded a notebook to github if you quickly want to try something. EDIT 1 I drastically increased the number of epochs and accuracy finally starts to improve from 0.5 at epoch 72 and converges to 1.0 at epoch 98. This still seems extremely slow for such a simple dataset. I am aware it is better to use a single output neuron with sigmoid activation but it's more that I want to understand why it does not work with two output neurons and softmax activation. I pre-process my dataframe as follows: from sklearn.preprocessing import LabelEncoder x_train = df_train.iloc[:,0:-1].values y_train = df_train.iloc[:, -1] nr_feats = x_train.shape[1] nr_classes = y_train.nunique() label_enc = LabelEncoder() label_enc.fit(y_train) y_train = keras.utils.to_categorical(label_enc.transform(y_train), nr_classes) Training and evaluation: model.fit(x_train, y_train, epochs=500, batch_size=32, verbose=True) accuracy_score(model.predict_classes(x_train), df_train.iloc[:, -1].values) EDIT 2 After changing the output layer to a single neuron with sigmoid activation and using binary_crossentropy loss as modesitt suggested, accuracy still remains at 0.5 for 200 epochs and converges to 1.0 100 epochs later.",|python|machine-learning|neural-network|keras|classification|,Training,2
50426349,"Input 0 is incompatible with layer conv2d_121: expected ndim=4, found ndim=5. I am building a deep colorization model using the CIFAR dataset. I have converted the rgb images into lab for the same. Now for that, the input X_train needs only the grayscale part of lab while the output labels are the coloured part. X_train dimensions = [50000,32,32] and Y_train = [50000,32,32,2]. I am getting this dimensional error for some reason while training the dataset. model = Sequential() model.add(Conv2D(64, (3, 3), activation='relu', padding='same',input_shape = (50000,32,32,1))) model.add(Conv2D(64, (3, 3), activation='relu', padding='same', strides=2)) model.add(Conv2D(128, (3, 3), activation='relu', padding='same')) model.add(Conv2D(512, (3, 3), activation='relu', padding='same')) model.add(Conv2D(256, (3, 3), activation='relu', padding='same')) model.add(Conv2D(128, (3, 3), activation='relu', padding='same')) #model.add(UpSampling2D((2, 2))) model.add(Conv2D(64, (3, 3), activation='relu', padding='same')) #model.add(UpSampling2D((2, 2))) model.add(Conv2D(32, (3, 3), activation='relu', padding='same')) model.add(Conv2D(2, (3, 3), activation='tanh', padding='same')) #model.add(UpSampling2D((2, 2))) model.compile(optimizer='rmsprop', loss='mse') model_info = model.fit(X_train, Y_train, batch_size=128, epochs=200)",|python|machine-learning|keras|deep-learning|conv-neural-network|,Tensors&Inputs,1
50481178,"keras MLP accuracy zero. The following is my MLP model, layers = [10,20,30,40,50] model = keras.models.Sequential() #Stacking Layers model.add(keras.layers.Dense(layers[0], input_dim = input_dim, activation='relu')) #Defining the shape of input for layer in layers[1:]: model.add(keras.layers.Dense(layer, activation='relu')) #Layer activation function # Output layer model.add(keras.layers.Dense(1, activation='sigmoid')) #Pre-training model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy']) #Training model.fit(train_set, test_set, validation_split = 0.10, epochs = 50, batch_size = 10, shuffle = True, verbose = 2) # evaluate the network loss, accuracy = model.evaluate(train_set, test_set) print(""\nLoss: %.2f, Accuracy: %.2f%%"" % (loss, accuracy*100)) #predictions predt = model.predict(final_test) print(predt) The problem is that, accuracy is always 0, error log as shown, Epoch 48/50 - 0s - loss: 1.0578 - acc: 0.0000e+00 - val_loss: 0.4885 - val_acc: 0.0000e+00 Epoch 49/50 - 0s - loss: 1.0578 - acc: 0.0000e+00 - val_loss: 0.4885 - val_acc: 0.0000e+00 Epoch 50/50 - 0s - loss: 1.0578 - acc: 0.0000e+00 - val_loss: 0.4885 - val_acc: 0.0000e+00 2422/2422 [==============================] - 0s 17us/step Loss: 1.00, Accuracy: 0.00% As suggested i've changed my learning signal from -1,1 to 0,1 and yet, the following is the error log Epoch 48/50 - 0s - loss: 8.5879 - acc: 0.4672 - val_loss: 8.2912 - val_acc: 0.4856 Epoch 49/50 - 0s - loss: 8.5879 - acc: 0.4672 - val_loss: 8.2912 - val_acc: 0.4856 Epoch 50/50 - 0s - loss: 8.5879 - acc: 0.4672 - val_loss: 8.2912 - val_acc: 0.4856 2422/2422 [==============================] - 0s 19us/step",|python|machine-learning|neural-network|keras|,Training,2
50555434,"Keras model to predict probability distribution. We are trying to build a keras model to predict a vector with probablity rates from a vector of features. The output vector should be of probabilty rates which are between 0 and one and to sum to 1, but some how the output vector consists mostly of zeros and ones, moreover during the time which the model should be training and learn loss and val_loss rates remains the same. Does anyone knows what is the problem with our model? example of input vector: (0,4,1444997,0,622,154536,0,2,11,0,5,11,10,32,4.26E-04,0,5,498,11,1,11,0,172,0,4,1,8,150) example of expected output vector: (0.25,0,0,0.083333333,0.583333333,0.083333333) example of real output vector: (1.000000000000000000e+00,5.556597260531319618e-28,1.000000000000000000e+00,0.000000000000000000e+00,0.000000000000000000e+00,0.000000000000000000e+00) the code: # Create first network with Keras from keras.models import Sequential from keras.layers import Dense from keras.layers.advanced_activations import LeakyReLU from keras import optimizers import numpy X = numpy.loadtxt(""compiledFeatures.csv"", delimiter="","") Y = numpy.loadtxt(""naive_compiledDate.csv"", delimiter="","") # create model model = Sequential() model.add(Dense(20, input_dim=28, init='normal', activation='relu')) model.add(Dense(15, init='normal', activation='relu')) model.add(Dense(6, init='normal', activation='relu')) model.add(Dense(6, init='normal', activation='sigmoid')) # Compile model model.compile(optimizer = ""adam"", loss = 'mae') # Fit the model model.fit(X, Y, epochs=2000, verbose=2, validation_split = 0.15) # calculate predictions predictions = model.predict(X)",|python|keras|,Model,0
50724488,"numpy array has different shape when I pass it as input to a keras layer. I have a keras encoder (part of an autoencoder) built this way: input_vec = Input(shape=(200,)) encoded = Dense(20, activation='relu')(input_vec) encoder = Model(input_vec, encoded) I want to generate a dummy input using numpy. >>> np.random.rand(200).shape (200,) But if i try to pass it as input to the encoder I get a ValueError: >>> encoder.predict(np.random.rand(200)) >>> Traceback (most recent call last): File ""<console>"", line 1, in <module> File ""/home/francesco/PycharmProjects/W2VAutoencoded/venv/lib/python3.6/site-packages/keras/engine/training.py"", line 1817, in predict check_batch_axis=False) File ""/home/francesco/PycharmProjects/W2VAutoencoded/venv/lib/python3.6/site-packages/keras/engine/training.py"", line 123, in _standardize_input_data str(data_shape)) ValueError: Error when checking : expected input_1 to have shape (200,) but got array with shape (1,) What am I missing?",|arrays|numpy|keras|valueerror|,Tensors&Inputs,1
50718045,"Resize PyTorch Tensor. I am currently using the tensor.resize() function to resize a tensor to a new shape t = t.resize(1, 2, 3). This gives me a deprecation warning: non-inplace resize is deprecated Hence, I wanted to switch over to the tensor.resize_() function, which seems to be the appropriate in-place replacement. However, this leaves me with an cannot resize variables that require grad error. I can fall back to from torch.autograd._functions import Resize Resize.apply(t, (1, 2, 3)) which is what tensor.resize() does in order to avoid the deprecation warning. This doesn't seem like an appropriate solution but rather a hack to me. How do I correctly make use of tensor.resize_() in this case?",|python|runtime-error|pytorch|tensor|deprecation-warning|,API,4
50776598,"TypeError: softmax() got an unexpected keyword argument 'axis'. When I use this it does not give any error out_layer = tf.add(tf.matmul(layer_4 , weights['out']) , biases['out']) out_layer = tf.nn.softmax(out_layer) But when I use this model=Sequential() model.add(Dense(100, input_dim= n_dim, activation='tanh',kernel_initializer='uniform')) keras.layers.core.Dropout(0.3, noise_shape=None, seed=None) model.add(Dense(50,input_dim=1000,activation='sigmoid')) keras.layers.core.Dropout(0.4, noise_shape=None, seed=None) model.add(Dense(15,input_dim=500,activation='sigmoid')) keras.layers.core.Dropout(0.2, noise_shape=None, seed=None) model.add(Dense(units=n_class)) model.add(Activation('softmax')) I get error as TypeError: softmax() got an unexpected keyword argument 'axis' What should I do? I am using python2 Thanks",|python-2.7|keras|softmax|,API,4
50762466,"Debugging GAN covergence error. Building a GAN to generate images. The images have 3 color channels, 96 x 96. The images that are generated by the generator at the beginning are all black, which is an issue given that is statistically highly unlikely. Also, the loss for both networks is not improving. I have posted the entire code below, and commented to allow it to be easily read. This is my first time building a GAN and I am new to Pytorch so any help is very appreciated! Thanks. import torch from torch.optim import Adam from torch.utils.data import DataLoader from torch.autograd import Variable import numpy as np import os import cv2 from collections import deque # training params batch_size = 100 epochs = 1000 # loss function loss_fx = torch.nn.BCELoss() # processing images X = deque() for img in os.listdir('pokemon_images'): if img.endswith('.png'): pokemon_image = cv2.imread(r'./pokemon_images/{}'.format(img)) if pokemon_image.shape != (96, 96, 3): pass else: X.append(pokemon_image) # data loader for processing in batches data_loader = DataLoader(X, batch_size=batch_size) # covert output vectors to images if flag is true, else input images to vectors def images_to_vectors(data, reverse=False): if reverse: return data.view(data.size(0), 3, 96, 96) else: return data.view(data.size(0), 27648) # Generator model class Generator(torch.nn.Module): def __init__(self): super(Generator, self).__init__() n_features = 1000 n_out = 27648 self.model = torch.nn.Sequential( torch.nn.Linear(n_features, 128), torch.nn.ReLU(), torch.nn.Linear(128, 256), torch.nn.ReLU(), torch.nn.Linear(256, 512), torch.nn.ReLU(), torch.nn.Linear(512, 1024), torch.nn.ReLU(), torch.nn.Linear(1024, n_out), torch.nn.Tanh() ) def forward(self, x): img = self.model(x) return img def noise(self, s): x = Variable(torch.randn(s, 1000)) return x # Discriminator model class Discriminator(torch.nn.Module): def __init__(self): super(Discriminator, self).__init__() n_features = 27648 n_out = 1 self.model = torch.nn.Sequential( torch.nn.Linear(n_features, 512), torch.nn.ReLU(), torch.nn.Linear(512, 256), torch.nn.ReLU(), torch.nn.Linear(256, n_out), torch.nn.Sigmoid() ) def forward(self, img): output = self.model(img) return output # discriminator training def train_discriminator(discriminator, optimizer, real_data, fake_data): N = real_data.size(0) optimizer.zero_grad() # train on real # get prediction pred_real = discriminator(real_data) # calculate loss error_real = loss_fx(pred_real, Variable(torch.ones(N, 1))) # calculate gradients error_real.backward() # train on fake # get prediction pred_fake = discriminator(fake_data) # calculate loss error_fake = loss_fx(pred_fake, Variable(torch.ones(N, 0))) # calculate gradients error_fake.backward() # update weights optimizer.step() return error_real + error_fake, pred_real, pred_fake # generator training def train_generator(generator, optimizer, fake_data): N = fake_data.size(0) # zero gradients optimizer.zero_grad() # get prediction pred = discriminator(generator(fake_data)) # get loss error = loss_fx(pred, Variable(torch.ones(N, 0))) # compute gradients error.backward() # update weights optimizer.step() return error # Instance of generator and discriminator generator = Generator() discriminator = Discriminator() # optimizers g_optimizer = torch.optim.Adam(generator.parameters(), lr=0.001) d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.001) # training loop for epoch in range(epochs): for n_batch, batch in enumerate(data_loader, 0): N = batch.size(0) # Train Discriminator # REAL real_images = Variable(images_to_vectors(batch)).float() # FAKE fake_images = generator(generator.noise(N)).detach() # TRAIN d_error, d_pred_real, d_pred_fake = train_discriminator( discriminator, d_optimizer, real_images, fake_images ) # Train Generator # generate noise fake_data = generator.noise(N) # get error based on discriminator g_error = train_generator(generator, g_optimizer, fake_data) # convert generator output to image and preprocess to show test_img = np.array(images_to_vectors(generator(fake_data), reverse=True).detach()) test_img = test_img[0, :, :, :] test_img = test_img[..., ::-1] # show example of generated image cv2.imshow('GENERATED', test_img[0]) if cv2.waitKey(1) & 0xFF == ord('q'): break print('EPOCH: {0}, D error: {1}, G error: {2}'.format(epoch, d_error, g_error)) cv2.destroyAllWindows() # save weights # torch.save('weights.pth')",|neural-network|statistics|deep-learning|pytorch|generative-adversarial-network|,Training,2
50801149,"Error: from tensorflow.examples.tutorials.mnist import input_data. My environment is as follows: * Windows 7, 64 bit * Anaconda Navigator 1.8.7 * python 3.6.5 * tensorflow 1.8.0 In python, I type: import tensorflow as tf from tensorflow.examples.tutorials.mnist import input_data i got error as follows: >>> from tensorflow.examples.tutorials.mnist import input_data Traceback (most recent call last): File ""<stdin>"", line 1, in <module> File ""E:\Anaconda3\envs\opencv\lib\site-packages\tensorflow\examples\tutorials\mnist\__init__.py"", line 21, in <module> from tensorflow.examples.tutorials.mnist import input_data File ""E:\Anaconda3\envs\opencv\lib\site-packages\tensorflow\examples\tutorials\mnist\input_data.py"", line 30, in <module> from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets File ""E:\Anaconda3\envs\opencv\lib\site-packages\tensorflow\contrib\__init__.py"", line 34, in <module> from tensorflow.contrib import data File ""E:\Anaconda3\envs\opencv\lib\site-packages\tensorflow\contrib\data\__init__.py"", line 67, in <module> from tensorflow.contrib.data.python.ops.error_ops import ignore_errors File ""E:\Anaconda3\envs\opencv\lib\site-packages\tensorflow\contrib\data\python\ops\error_ops.py"", line 20, in <module> from tensorflow.contrib.data.python.ops import contrib_op_loader # pylint: disable=unused-import File ""E:\Anaconda3\envs\opencv\lib\site-packages\tensorflow\contrib\data\python\ops\contrib_op_loader.py"", line 24, in <module> resource_loader.get_path_to_datafile(""../../_dataset_ops.so"")) File ""E:\Anaconda3\envs\opencv\lib\site-packages\tensorflow\contrib\util\loader.py"", line 56, in load_op_library ret = load_library.load_op_library(path) File ""E:\Anaconda3\envs\opencv\lib\site-packages\tensorflow\python\framework\load_library.py"", line 56, in load_op_library lib_handle = py_tf.TF_LoadLibrary(library_filename) tensorflow.python.framework.errors_impl.NotFoundError: E:\Anaconda3\envs\opencv\lib\site-packages\tensorflow\contrib\data\python\ops\..\..\_dataset_ops.so not found >>> It also pops up a window saying: The procedure entry point ?addcleanup@arenaimpl@internal@protobuf@google@@QEAAXPEAXP6AX0@Z@Z could not be located in the dynamic link library _pywarp_tensorflow_internal.pyd Please help. thank you very much in advance. Warmest Regards, Suryadi",|python|tensorflow|mnist|,API,4
50920908,"Get Confusion Matrix From a Keras Multiclass Model. I am building a multiclass model with Keras. model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, callbacks=[checkpoint], validation_data=(X_test, y_test)) # starts training Here is how my test data looks like (it's text data). X_test Out[25]: array([[621, 139, 549, ..., 0, 0, 0], [621, 139, 543, ..., 0, 0, 0]]) y_test Out[26]: array([[0, 0, 1], [0, 1, 0]]) After generating predictions... predictions = model.predict(X_test) predictions Out[27]: array([[ 0.29071924, 0.2483743 , 0.46090645], [ 0.29566404, 0.45295066, 0.25138539]], dtype=float32) I did the following to get the confusion matrix. y_pred = (predictions > 0.5) confusion_matrix(y_test, y_pred) Traceback (most recent call last): File ""<ipython-input-38-430e012b2078>"", line 1, in <module> confusion_matrix(y_test, y_pred) File ""/Users/abrahammathew/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py"", line 252, in confusion_matrix raise ValueError(""%s is not supported"" % y_type) ValueError: multilabel-indicator is not supported However, I am getting the above error. How can I get a confusion matrix when doing a multiclass neural network in Keras?",|python|keras|scikit-learn|multiclass-classification|,API,4
50896412,"Channel wise CrossEntropyLoss for image segmentation in pytorch. I am doing an image segmentation task. There are 7 classes in total so the final outout is a tensor like [batch, 7, height, width] which is a softmax output. Now intuitively I wanted to use CrossEntropy loss but the pytorch implementation doesn't work on channel wise one-hot encoded vector So I was planning to make a function on my own. With a help from some stackoverflow, My code so far looks like this from torch.autograd import Variable import torch import torch.nn.functional as F def cross_entropy2d(input, target, weight=None, size_average=True): # input: (n, c, w, z), target: (n, w, z) n, c, w, z = input.size() # log_p: (n, c, w, z) log_p = F.log_softmax(input, dim=1) # log_p: (n*w*z, c) log_p = log_p.permute(0, 3, 2, 1).contiguous().view(-1, c) # make class dimension last dimension log_p = log_p[ target.view(n, w, z, 1).repeat(0, 0, 0, c) >= 0] # this looks wrong -> Should rather be a one-hot vector log_p = log_p.view(-1, c) # target: (n*w*z,) mask = target >= 0 target = target[mask] loss = F.nll_loss(log_p, target.view(-1), weight=weight, size_average=False) if size_average: loss /= mask.data.sum() return loss images = Variable(torch.randn(5, 3, 4, 4)) labels = Variable(torch.LongTensor(5, 3, 4, 4).random_(3)) cross_entropy2d(images, labels) I get two errors. One is mentioned on the code itself, where it expects one-hot vector. The 2nd one says the following RuntimeError: invalid argument 2: size '[5 x 4 x 4 x 1]' is invalid for input with 3840 elements at ..\src\TH\THStorage.c:41 For example purpose I was trying to make it work on a 3 class problem. So the targets and labels are (excluding the batch parameter for simplification ! ) Target: Channel 1 Channel 2 Channel 3 [[0 1 1 0 ] [0 0 0 1 ] [1 0 0 0 ] [0 0 1 1 ] [0 0 0 0 ] [1 1 0 0 ] [0 0 0 1 ] [0 0 0 0 ] [1 1 1 0 ] [0 0 0 0 ] [0 0 0 1 ] [1 1 1 0 ] Labels: Channel 1 Channel 2 Channel 3 [[0 1 1 0 ] [0 0 0 1 ] [1 0 0 0 ] [0 0 1 1 ] [.2 0 0 0] [.8 1 0 0 ] [0 0 0 1 ] [0 0 0 0 ] [1 1 1 0 ] [0 0 0 0 ] [0 0 0 1 ] [1 1 1 0 ] So how can I fix my code to calculate channel wise CrossEntropy loss ?",|image-segmentation|pytorch|loss-function|cross-entropy|semantic-segmentation|,Tensors&Inputs,1
51032181,"Getting different output in the Pytorch NLP example Part-of-Speech Tagging. I am following the NLP tutorials on Pytorchs tutorials website. I am getting different output than what it should show, so I just copy pasted the whole code as it is and still the output is different. My code is shared in this gist: Example: An LSTM for Part-of-Speech Tagging For the 1st sentence [The? dog? ate? the? apple] [DET? NN? V? DET? NN] the output is coming as below: tensor([[-0.7662, -0.6405, -4.8002], [-2.7163, -0.0698, -6.6515], [-3.1324, -5.7668, -0.0479], [-0.0528, -3.3832, -4.0481], [-2.4527, -0.0931, -5.8702]]) I am getting the sequence: 1 1 2 0 1 rather than 0 1 2 0 1 Can anyone please check this and point out why I am getting different output?",|lstm|pytorch|,Training,2
51031519,"can't import keras.layers.Merge. I want to merge two LSTM models in Keras. I have seen many examples of importing Merge as: from keras.layers import Merge When I do this, I get an import error. ImportError: cannot import name 'Merge'. Has there been some refactor and now Merge is elsewhere?",|python|keras|,API,4
51198474,"PyTorch Linear Regression Issue. I am trying to implement a simple linear model in PyTorch that can be given x data and y data, and then trained to recognize the equation y = mx + b. However, whenever I try to test my model after training, it thinks that the equation is y= mx + 2b. I'll show my code, and hopefully someone will be able to spot an issue. Thank you in advance for any help. import torch D_in = 500 D_out = 500 batch=200 model=torch.nn.Sequential( torch.nn.Linear(D_in,D_out), ) Next I create some data and set a rule. Let's do 3x+4. x_data=torch.rand(batch,D_in) y_data=torch.randn(batch,D_out) for i in range(batch): for j in range(D_in): y_data[i][j]=3*x_data[i][j]+5 # model thinks y=mx+c -> y=mx+2c? loss_fn=torch.nn.MSELoss(size_average=False) optimizer=torch.optim.Adam(model.parameters(),lr=0.001) Now to training... for epoch in range(500): y_pred=model(x_data) loss=loss_fn(y_pred,y_data) optimizer.zero_grad() loss.backward() optimizer.step() Then I test my model with a Tensor/matrix of just 1's. test_data=torch.ones(batch,D_in) y_pred=model(test_data) Now, I'd expect to get 3*1 + 4 = 7, but instead, my model thinks it is 11. [[ 10.7286, 11.0499, 10.9448, ..., 11.0812, 10.9387, 10.7516], [ 10.7286, 11.0499, 10.9448, ..., 11.0812, 10.9387, 10.7516], [ 10.7286, 11.0499, 10.9448, ..., 11.0812, 10.9387, 10.7516], ..., [ 10.7286, 11.0499, 10.9448, ..., 11.0812, 10.9387, 10.7516], [ 10.7286, 11.0499, 10.9448, ..., 11.0812, 10.9387, 10.7516], [ 10.7286, 11.0499, 10.9448, ..., 11.0812, 10.9387, 10.7516]]) Similarly, if I change the rule to y=3x+8, my model guesses 19. So, I am not sure what is going on. Why is the constant being added twice? By the way, if I just set the rule to y=3x, my model correctly infers 3, and for y=mx in general my model correctly infers m. For some reason, the constant term is throwing it off. Any help to solve this problem is much appreciated. Thanks!",|python|machine-learning|regression|pytorch|gradient-descent|,Training,2
51181393,"Simple Linear Regression using Keras. I have been trying to implement a simple linear regression model using neural networks in Keras in hopes to understand how do we work in Keras library. Unfortunately, I am ending up with a very bad model. Here is the implementation: from pylab import * from keras.models import Sequential from keras.layers import Dense #Generate dummy data data = data = linspace(1,2,100).reshape(-1,1) y = data*5 #Define the model def baseline_model(): model = Sequential() model.add(Dense(1, activation = 'linear', input_dim = 1)) model.compile(optimizer = 'rmsprop', loss = 'mean_squared_error', metrics = ['accuracy']) return model #Use the model regr = baseline_model() regr.fit(data,y,epochs =200,batch_size = 32) plot(data, regr.predict(data), 'b', data,y, 'k.') The generated plot is as follows: Can somebody point out the flaw in the above definition of the model (which could ensure a better fit)?",|python|machine-learning|neural-network|keras|linear-regression|,Training,2
51341479,"Keras: functional API what should the Input layer be for the embedding layer?. I am using the Keras functional API to create a neural net that takes a word embedding layer as input for a sentence classification task. But my code breaks right at the beginning of connecting the input and the embedding layers. Following a tutorial at https://medium.com/tensorflow/predicting-the-price-of-wine-with-the-keras-functional-api-and-tensorflow-a95d1c2c1b03, I have code like below: max_seq_length=100 #i.e., sentence has a max of 100 words word_weight_matrix = ... #this has a shape of 9825, 300, i.e., the vocabulary has 9825 words and each is a 300 dimension vector deep_inputs = Input(shape=(max_seq_length,)) embedding = Embedding(9825, 300, input_length=max_seq_length, weights=word_weight_matrix, trainable=False)(deep_inputs) # line A hidden = Dense(targets, activation=""softmax"")(embedding) model = Model(inputs=deep_inputs, outputs=hidden) Then line A causes an error that states below: ValueError: You called `set_weights(weights)` on layer ""embedding_1"" with a weight list of length 9825, but the layer was expecting 1 weights. Provided weights: [[-0.04057981 0.05743935 0.0109863 ..., 0.0072... And I don't really understand what the error means... It seems that the Input layer isn't defined properly... Previously when I use the Sequential model with the embedding layer defined exactly the same, everything works OK. But when I switch to functional API, I have this error. Any help much appreciated, thanks in advance",|python|keras|,Tensors&Inputs,1
51421885,"expected dense to have shape but got array with shape. I am getting the following error while calling the model.predict function when running a text classification model in keras. I searched the everywhere but it isn't working for me. ValueError: Error when checking input: expected dense_1_input to have shape (100,) but got array with shape (1,) My data has 5 classes and has a total of 15 examples only. Below is the dataset query tags 0 hi intro 1 how are you wellb 2 hello intro 3 what's up wellb 4 how's life wellb 5 bye gb 6 see you later gb 7 good bye gb 8 thanks gratitude 9 thank you gratitude 10 that's helpful gratitude 11 I am great revertfine 12 fine revertfine 13 I am fine revertfine 14 good revertfine This is the code of my model from keras.preprocessing.text import Tokenizer from sklearn.preprocessing import LabelBinarizer from keras.models import Sequential import pandas as pd from keras.layers import Dense, Activation data = pd.read_csv('text_class.csv') train_text = data['query'] train_labels = data['tags'] tokenize = Tokenizer(num_words=100) tokenize.fit_on_texts(train_text) x_data = tokenize.texts_to_matrix(train_text) encoder = LabelBinarizer() encoder.fit(train_labels) y_data = encoder.transform(train_labels) model = Sequential() model.add(Dense(512, input_shape=(100,))) model.add(Activation('relu')) model.add(Dense(5)) model.add(Activation('softmax')) model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc']) model.fit(x_data, y_data, batch_size=8, epochs=10) predictions = model.predict(x_data[0]) tag_labels = encoder.classes_ predicted_tags = tag_labels[np.argmax(predictions)] print (predicted_tags) I am not able to figure out where the problem lies and how to fix it.",|python|keras|shapes|text-classification|,Tensors&Inputs,1
51478595,"What are ways I can debug this keras layer?. I am new to Keras and am attempting to implement the decorrelated batch Norm paper (https://arxiv.org/abs/1804.08450) in Keras as a learning experience. The layer is very similar to standard batch norm with a few additional components. Instead of centering the input data to each layer and normalizing by the variance, we now center the data and apply a whitening transform which is computed by doing an eigenvalue decomposition on the covariance matrix. The entire procedure is clearly laid out in the paper (Algorithm 1, page 5) and composed of only like 5 equations, whose implementation I marked in the code below. I successfully re-implemented standard batch norm layer, but am getting NaN loss and low accuracy when I incorporate the whitening procedure. I am wondering if there any advice I should follow to debug this code. I am not sure if I made a dimensionality mistake or incorrectly implemented the equations, but any help would be appreciated. Here is the code if you are interested (Edited to include Daniel Mller's corrections). The input to the layer is a tensor of dimension (batch_size height width channels). input_shape = K.int_shape(inputs) # (batch_size height width channels) # unroll all dimensions except feature maps dim (c X hwb) pool_shape = (-1, input_shape[-1]) x = K.reshape(x,pool_shape) x = K.permute_dimensions(x, (1,0)) #if you do want to invert the dimensions mean = K.mean(x,1,keepdims=True) # standard batch norm #stddev = K.std(x,1,keepdims=True) + self.epsilon #normed = (x - mean) / stddev #normed = K.reshape(normed,((-1,)+ input_shape[1:])) # center inputs centered_inputs = x - mean #vvvvvERROR SOMEWHERE IN HEREvvvvv# # compute covariance matrix for reshaped inputs xxt covar = K.batch_dot(K.expand_dims(x, axis=-1), K.expand_dims(x, axis=-1),axes=(2,2)) # fuzz covariance matrix to prevent singularity covar = covar + self.epsilon # execute eigenvalue decomposition #Lambda, D,_ = tf.svd(covar,compute_uv=True) Lambda, D = tf.self_adjoint_eig(covar) Lambda = tf.linalg.diag(Lambda) # calculate PCA-whitening matrix 1/sqrt(L) * D^T U = K.batch_dot(1. / K.sqrt(Lambda), D, axes=(2,2)) # calculate PCA-whitened activation x_a = U(x - \mu) x_a = K.batch_dot(U, centered_inputs,axes=(2,1)) # calculate ZCA-whitened output Dx_a x_whitened = K.batch_dot(D, x_a) #^^^^^ERROR SOMEWHERE IN HERE^^^^^# # reshape whitened activations back to input dimension x_normed = K.permute_dimensions(x_whitened,(1,0)) # permute back to (bhw X c) x_normed = K.reshape(x_normed,((-1,), input_shape[1:])) # reroll dimensions",|python|keras|keras-layer|batch-normalization|,Training,2
51511052,"Keras loss constant. def myloss(y_true, y_pred): b = k.constant([1, 1, 1, 50, 50, 50], shape=[6, 1]) return (k.mean(k.sqrt(k.dot(k.square(y_pred - y_true), b))) This is our loss function and we got this result. 2800/2799 [==============================] - 245s - loss: 204.2003 - soft_acc: 0.5136 - val_loss: 64.3844 - val_soft_acc: 0.4648 We tried changing the learning rates and optimiser but the loss didn't improve we refered to this link Keras Extremely High Loss epoch 1/200 ===========================] - 254s - loss: 4.0631 - rmse: 5.1670 - val_loss: 4.6882 - val_rmse: 4.7807 and added logarithmic error and got the above loss value. How to reduce the loss further?",|keras|loss-function|,Training,2
51545026,"Implementing a custom dataset with PyTorch. I'm attempting to modify this feedforward network taken from https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/feedforward_neural_network/main.py to utilize my own dataset. I define a custom dataset of two 1 dim arrays as input and two scalars the corresponding output : x = torch.tensor([[5.5, 3,3,4] , [1 , 2,3,4], [9 , 2,3,4]]) print(x) y = torch.tensor([1,2,3]) print(y) import torch.utils.data as data_utils my_train = data_utils.TensorDataset(x, y) my_train_loader = data_utils.DataLoader(my_train, batch_size=50, shuffle=True) I've updated the hyperparameters to match new input_size (2) & num_classes (3). I've also changed images = images.reshape(-1, 28*28).to(device) to images = images.reshape(-1, 4).to(device) As the training set is minimal I've changed the batch_size to 1. Upon making these modifications I receive error when attempting to train : RuntimeError Traceback (most recent call last) in () 51 52 # Forward pass ---> 53 outputs = model(images) 54 loss = criterion(outputs, labels) 55 /home/.local/lib/python3.6/site-packages/torch/nn/modules/module.py in call(self, *input, **kwargs) 489 result = self._slow_forward(*input, **kwargs) 490 else: --> 491 result = self.forward(*input, **kwargs) 492 for hook in self._forward_hooks.values(): 493 hook_result = hook(self, input, result) in forward(self, x) 31 32 def forward(self, x): ---> 33 out = self.fc1(x) 34 out = self.relu(out) 35 out = self.fc2(out) /home/.local/lib/python3.6/site-packages/torch/nn/modules/module.py in call(self, *input, **kwargs) 489 result = self._slow_forward(*input, **kwargs) 490 else: --> 491 result = self.forward(*input, **kwargs) 492 for hook in self._forward_hooks.values(): 493 hook_result = hook(self, input, result) /home/.local/lib/python3.6/site-packages/torch/nn/modules/linear.py in forward(self, input) 53 54 def forward(self, input): ---> 55 return F.linear(input, self.weight, self.bias) 56 57 def extra_repr(self): /home/.local/lib/python3.6/site-packages/torch/nn/functional.py in linear(input, weight, bias) 990 if input.dim() == 2 and bias is not None: 991 # fused op is marginally faster --> 992 return torch.addmm(bias, input, weight.t()) 993 994 output = input.matmul(weight.t()) RuntimeError: size mismatch, m1: [3 x 4], m2: [2 x 3] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:249 How to amend code to match expected dimensionality ? I'm unsure what code to change as I've changed all parameters that require updating ? Source prior to changes : import torch import torch.nn as nn import torchvision import torchvision.transforms as transforms # Device configuration device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # Hyper-parameters input_size = 784 hidden_size = 500 num_classes = 10 num_epochs = 5 batch_size = 100 learning_rate = 0.001 # MNIST dataset train_dataset = torchvision.datasets.MNIST(root='../../data', train=True, transform=transforms.ToTensor(), download=True) test_dataset = torchvision.datasets.MNIST(root='../../data', train=False, transform=transforms.ToTensor()) # Data loader train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True) test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False) # Fully connected neural network with one hidden layer class NeuralNet(nn.Module): def __init__(self, input_size, hidden_size, num_classes): super(NeuralNet, self).__init__() self.fc1 = nn.Linear(input_size, hidden_size) self.relu = nn.ReLU() self.fc2 = nn.Linear(hidden_size, num_classes) def forward(self, x): out = self.fc1(x) out = self.relu(out) out = self.fc2(out) return out model = NeuralNet(input_size, hidden_size, num_classes).to(device) # Loss and optimizer criterion = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) # Train the model total_step = len(train_loader) for epoch in range(num_epochs): for i, (images, labels) in enumerate(train_loader): # Move tensors to the configured device images = images.reshape(-1, 28*28).to(device) labels = labels.to(device) # Forward pass outputs = model(images) loss = criterion(outputs, labels) # Backward and optimize optimizer.zero_grad() loss.backward() optimizer.step() if (i+1) % 100 == 0: print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' .format(epoch+1, num_epochs, i+1, total_step, loss.item())) # Test the model # In test phase, we don't need to compute gradients (for memory efficiency) with torch.no_grad(): correct = 0 total = 0 for images, labels in test_loader: images = images.reshape(-1, 28*28).to(device) labels = labels.to(device) outputs = model(images) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum().item() print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total)) # Save the model checkpoint torch.save(model.state_dict(), 'model.ckpt') Source post changes : x = torch.tensor([[5.5, 3,3,4] , [1 , 2,3,4], [9 , 2,3,4]]) print(x) y = torch.tensor([1,2,3]) print(y) import torch.utils.data as data_utils my_train = data_utils.TensorDataset(x, y) my_train_loader = data_utils.DataLoader(my_train, batch_size=50, shuffle=True) print(my_train) print(my_train_loader) import torch import torch.nn as nn import torchvision import torchvision.transforms as transforms # Device configuration device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # Hyper-parameters input_size = 2 hidden_size = 3 num_classes = 3 num_epochs = 5 batch_size = 1 learning_rate = 0.001 # MNIST dataset train_dataset = my_train # Data loader train_loader = my_train_loader # Fully connected neural network with one hidden layer class NeuralNet(nn.Module): def __init__(self, input_size, hidden_size, num_classes): super(NeuralNet, self).__init__() self.fc1 = nn.Linear(input_size, hidden_size) self.relu = nn.ReLU() self.fc2 = nn.Linear(hidden_size, num_classes) def forward(self, x): out = self.fc1(x) out = self.relu(out) out = self.fc2(out) return out model = NeuralNet(input_size, hidden_size, num_classes).to(device) # Loss and optimizer criterion = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) # Train the model total_step = len(train_loader) for epoch in range(num_epochs): for i, (images, labels) in enumerate(train_loader): # Move tensors to the configured device images = images.reshape(-1, 4).to(device) labels = labels.to(device) # Forward pass outputs = model(images) loss = criterion(outputs, labels) # Backward and optimize optimizer.zero_grad() loss.backward() optimizer.step() if (i+1) % 100 == 0: print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' .format(epoch+1, num_epochs, i+1, total_step, loss.item())) # Test the model # In test phase, we don't need to compute gradients (for memory efficiency) with torch.no_grad(): correct = 0 total = 0 for images, labels in test_loader: images = images.reshape(-1, 4).to(device) labels = labels.to(device) outputs = model(images) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum().item() print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total)) # Save the model checkpoint torch.save(model.state_dict(), 'model.ckpt')",|python|machine-learning|neural-network|pytorch|,Tensors&Inputs,1
51586680,"Linear regression with pytorch. I tried to run linear regression on ForestFires dataset. Dataset is available on Kaggle and gist of my attempt is here: https://gist.github.com/Chandrak1907/747b1a6045bb64898d5f9140f4cf9a37 I am facing two problems: Output from prediction is of shape 32x1 and target data shape is 32. input and target shapes do not match: input [32 x 1], target [32] Using view I reshaped predictions tensor. y_pred = y_pred.view(inputs.shape[0]) Why there is a mismatch in shapes of predicted tensor and actual tensor? SGD in pytorch never converges. I tried to compute MSE manually using print(torch.mean((y_pred - labels)**2)) This value does not match loss = criterion(y_pred,labels) Can someone highlight where is the mistake in my code? Thank you.",|pytorch|,Tensors&Inputs,1
51746179,"ValueError: Cannot reshape a tensor with 99872 elements to shape [1,125,1,8]. I am trying to make a model with sample size 125*8 my input shape is (12484, 8) but i is giving me this error: ValueError: Cannot reshape a tensor with 99872 elements to shape [1,125,1,8] (1000 elements) for 'Reshape_9' (op: 'Reshape') with input shapes: [1,12484,1,8], [4] and with input tensors computed as partial shapes: input1 = [1,125,1,8]. model = Sequential() model.add(Convolution2D(64, kernel_size=(5, 5), strides=(1,1),padding=""same"", data_format=""channels_last"",input_shape= (8,125,1))) model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')) model.add(Convolution2D(64, (5, 5),padding=""same"", activation='relu')) model.add(MaxPooling2D(pool_size=(2, 2),padding='same')) model.add(Flatten()) model.add(Dense(5, activation='softmax')) model.compile(loss='mse', optimizer='adam', metrics=['accuracy']) print(model.summary()) _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_73 (Conv2D) (None, 8, 125, 64) 1664 _________________________________________________________________ max_pooling2d_23 (MaxPooling (None, 4, 63, 64) 0 _________________________________________________________________ conv2d_74 (Conv2D) (None, 4, 63, 64) 102464 _________________________________________________________________ max_pooling2d_24 (MaxPooling (None, 2, 32, 64) 0 _________________________________________________________________ flatten_5 (Flatten) (None, 4096) 0 _________________________________________________________________ dense_45 (Dense) (None, 5) 20485 ================================================================= Total params: 124,613 Trainable params: 124,613 Non-trainable params: 0 _________________________________________________________________ None x_train= tf.reshape(x_train,[1,125,1,8]) model.fit(x_train, y_train, epochs=150, batch_size= 125) score = model.evaluate(x_test, y_test, batch_size=125, verbose=0) print('Test loss:', score[0]) print('Test accuracy:', score[1]) Output --------------------------------------------------------------------------- InvalidArgumentError Traceback (most recent call last) ~\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in _create_c_op(graph, node_def, inputs, control_inputs) 1588 try: -> 1589 c_op = c_api.TF_FinishOperation(op_desc) 1590 except errors.InvalidArgumentError as e: InvalidArgumentError: Cannot reshape a tensor with 99872 elements to shape [1,125,1,8] (1000 elements) for 'Reshape_9' (op: 'Reshape') with input shapes: [1,12484,1,8], [4] and with input tensors computed as partial shapes: input[1] = [1,125,1,8]. During handling of the above exception, another exception occurred: ValueError Traceback (most recent call last) <ipython-input-89-4e8b18efb44b> in <module>() 1 #x_train= tf.reshape(x_train,[-1,288, 512, 3]) 2 ----> 3 x_train= tf.reshape(x_train,[1,125,1,8]) 4 5 ~\Anaconda3\lib\site-packages\tensorflow\python\ops\gen_array_ops.py in reshape(tensor, shape, name) 7429 if _ctx is None or not _ctx._eager_context.is_eager: 7430 _, _, _op = _op_def_lib._apply_op_helper( -> 7431 ""Reshape"", tensor=tensor, shape=shape, name=name) 7432 _result = _op.outputs[:] 7433 _inputs_flat = _op.inputs ~\Anaconda3\lib\site-packages\tensorflow\python\framework\op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords) 785 op = g.create_op(op_type_name, inputs, output_types, name=scope, 786 input_types=input_types, attrs=attr_protos, --> 787 op_def=op_def) 788 return output_structure, op_def.is_stateful, op 789 ~\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in create_op(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device) 3412 input_types=input_types, 3413 original_op=self._default_original_op, -> 3414 op_def=op_def) 3415 3416 # Note: shapes are lazily computed with the C API enabled. ~\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in __init__(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def) 1754 op_def, inputs, node_def.attr) 1755 self._c_op = _create_c_op(self._graph, node_def, grouped_inputs, -> 1756 control_input_ops) 1757 else: 1758 self._c_op = None ~\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in _create_c_op(graph, node_def, inputs, control_inputs) 1590 except errors.InvalidArgumentError as e: 1591 # Convert to ValueError for backwards compatibility. -> 1592 raise ValueError(str(e)) 1593 1594 return c_op ValueError: Cannot reshape a tensor with 99872 elements to shape [1,125,1,8] (1000 elements) for 'Reshape_9' (op: 'Reshape') with input shapes: [1,12484,1,8], [4] and with input tensors computed as partial shapes: input[1] = [1,125,1,8]. Data looks like this:",|python|keras|,Tensors&Inputs,1
51749207,"CNN not efficient on my dataset in Keras. I'm working on a classification of one dimensional data. I presented the data in the form of 549 arrays, each contains 600 samples. I've made a multi-layer perceptron, which showed an efficiency of about 80%. Now I'm trying to do CNN but for some reason it's accuracy doesn't exceed 31%. What can be wrong? My model: model = Sequential() model.add(Conv1D(filters=20, kernel_size=4,activation='relu',padding='same',input_shape=(600,1))) model.add(MaxPooling1D(pool_size = 2)) model.add(Dropout(0.3)) model.add(Flatten()) model.add(Dense(50, activation='relu', input_dim = 600)) model.add(Dense(1, activation='softmax')) model.compile(loss=""binary_crossentropy"", optimizer=""nadam"", metrics=['accuracy']) model.fit(np.array(X), np.array(Y), epochs = 100, batch_size=8, verbose=1, validation_data=(np.array(X1),np.array(Y1))) scores = model.evaluate(np.array(X1), np.array(Y1), verbose=0) Input data: X1 = X[:90] X = X[91:] Y1 = Y[:90] Y = Y[91:] X = np.expand_dims(X, axis=2) X1 =np.expand_dims(X1, axis=2) print(np.array(X).shape) Get the dimension (458, 600, 1) can there be something wrong with the dimensions?",|machine-learning|neural-network|keras|conv-neural-network|,Model,0
51763983,"Error when checking target: expected dense_1 to have 3 dimensions, but got array with shape (118, 1). I'm training a model to predict the stock price and input data is close price. I use 45 days data to predict the 46th day's close price and a economic Indicator to be second feature, here is the model: model = Sequential() model.add( LSTM( 512, input_shape=(45, 2), return_sequences=True)) model.add( LSTM( 512, return_sequences=True)) model.add( (Dense(1))) model.compile(loss='mse', optimizer='adam') history = model.fit( X_train, y_train, batch_size = batchSize, epochs=epochs, shuffle = False) When I run this I get the following error: ValueError: Error when checking target: expected dense_1 to have 3 dimensions, but got array with shape (118, 1) However, I print the shape of data and they are: X_train:(118, 45, 2) y_train:(118, 1) I have no idea why the model is expecting a 3 dimensional output when y_train is (118, 1). Where am I wrong and what should I do?",|python|keras|output|lstm|,Tensors&Inputs,1
51776802,"Keras Reshape layer adding an extra dimension?. The Reshape layer is not working how I would expect. In the example below, I think the last line should return a tensor object of shape [5,1]. However an error is thrown, stating that a shape [5] tensor cannot be reshaped into a size [5,5,1] tensor. >>> from keras.layers import Reshape >>> from keras import backend as K >>> import numpy as np >>> x = K.constant(np.array([1,2,3,4,5])) >>> K.eval(x) array([1., 2., 3., 4., 5.], dtype=float32) >>> Reshape(target_shape=(5,1))(x) ... ValueError: Cannot reshape a tensor with 5 elements to shape [5,5,1] (25 elements) for 'reshape_3/Reshape' (op: 'Reshape') with input shapes: [5], [3] and with input tensors computed as partial shapes: input[1] = [5,5,1]. Can someone kindly explain how the Reshape layer works (i.e. why it's adding the extra dim) and how to do the process of reshaping a vector into a matrix? Thanks",|keras|reshape|tensor|,Tensors&Inputs,1
51930566,"Input nodes in Keras NN. I am trying to create an neural network based on the iris dataset. I have an input of four dimensions. X = dataset[:,0:4].astype(float). Then, I create a neural network with four nodes. model = Sequential() model.add(Dense(4, input_dim=4, init='normal', activation='relu')) model.add(Dense(3, init='normal', activation='sigmoid')) model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) As I understand, I pass each dimension to the separate node. Four dimensions - four nodes. When I create a neural network with 8 input nodes, how does it work? Performance still is the same as with 4 nodes. model = Sequential() model.add(Dense(8, input_dim=4, init='normal', activation='relu')) model.add(Dense(3, init='normal', activation='sigmoid')) model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])",|python|machine-learning|neural-network|keras|,Model,0
52076815,"pytorch - use device inside 'with statement'. Is there a way of running pytorch inside the context of a specific (GPU) device (without having to specify the device for each new tensor, such as the .to option)? Something like an equivalent of the tensorflow with tf.device('/device:GPU:0'):.. It seems that the default device is the cpu (unless I'm doing it wrong): with torch.cuda.device('0'): a = torch.zeros(1) print(a.device) >>> cpu",|python|gpu|pytorch|,GPU Usage,3
52265049,"CuDNN -- Status Not Intitialized (Keras/TensorFlow + Nvidia P100 + Linux). I am having trouble converting my (working) LSTM model to leverage CuDNN via keras+tensorflow-backend. I am using: Tensorflow 1.10.1 Tensorflow-gpu 1.10.1 Keras 2.2.2 Cuda 9.2 CuDNN 7.2.1 (pretty sure) NVIDIA P100 GPU (driver 390.87). Code example: def build_lstm(num_neurons, dropout, recurent_dropout): model = Sequential() model.add(LSTM(num_neurons, input_shape=(12,1), dropout=dropout, recurrent_dropout=recurent_dropout, unroll=True)) model.add(Dense(1)) model.compile(loss='mean_squared_error', optimizer='adam') return model def build_cudnnlstm(num_neurons, dropout, recurent_dropout): model = Sequential() model.add(CuDNNLSTM(num_neurons, input_shape=(12,1))) model.add(Dropout(dropout)) model.add(Dense(1)) model.compile(loss='mean_squared_error', optimizer='adam') return model However, when I swap out build_cudnnlstm for build_lstm, I am presented with the following error: Epoch 1/5 2018-09-10 15:58:53.726819: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA 2018-09-10 15:58:54.001406: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties: name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285 pciBusID: 0000:17:00.0 totalMemory: 15.90GiB freeMemory: 15.61GiB 2018-09-10 15:58:54.001491: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0 2018-09-10 15:58:54.475955: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix: 2018-09-10 15:58:54.476019: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0 2018-09-10 15:58:54.476036: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0: N 2018-09-10 15:58:54.476408: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:17:00.0, compute capability: 6.0) 2018-09-10 15:58:55.098145: E tensorflow/stream_executor/cuda/cuda_dnn.cc:352] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED 2018-09-10 15:58:55.098409: E tensorflow/stream_executor/cuda/cuda_dnn.cc:360] Possibly insufficient driver version: 390.87.0 2018-09-10 15:58:55.098496: W tensorflow/core/framework/op_kernel.cc:1275] OP_REQUIRES failed at cudnn_rnn_ops.cc:1214 : Unknown: Fail to find the dnn implementation. Traceback (most recent call last): File ""<stdin>"", line 1, in <module> File ""/usr/local/lib64/python3.6/site-packages/keras/engine/training.py"", line 1037, in fit validation_steps=validation_steps) File ""/usr/local/lib64/python3.6/site-packages/keras/engine/training_arrays.py"", line 199, in fit_loop outs = f(ins_batch) File ""/usr/local/lib64/python3.6/site-packages/keras/backend/tensorflow_backend.py"", line 2666, in __call__ return self._call(inputs) File ""/usr/local/lib64/python3.6/site-packages/keras/backend/tensorflow_backend.py"", line 2636, in _call fetched = self._callable_fn(*array_vals) File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1382, in __call__ run_metadata_ptr) File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 519, in __exit__ c_api.TF_GetCode(self.status.status)) tensorflow.python.framework.errors_impl.UnknownError: Fail to find the dnn implementation. [[Node: cu_dnnlstm_1/CudnnRNN = CudnnRNN[T=DT_FLOAT, _class=[""loc:@training/Adam/gradients/cu_dnnlstm_1/CudnnRNN_grad/CudnnRNNBackprop""], direction=""unidirectional"", dropout=0, input_mode=""linear_input"", is_training=true, rnn_mode=""lstm"", seed=87654321, seed2=0, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](cu_dnnlstm_1/transpose, cu_dnnlstm_1/ExpandDims_1, cu_dnnlstm_1/ExpandDims_1, cu_dnnlstm_1/concat_1)]] [[Node: loss/mul/_79 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_782_loss/mul"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]] This error is printed during the fit process: model.fit(samples, targets_1q, epochs=epochs, shuffle=True, verbose=2) Any help is greatly appreciated!",|tensorflow|keras|gpu|lstm|cudnn|,GPU Usage,3
52324713,"Linear Regression with CNN using Pytorch: input and target shapes do not match: input [400 x 1], target [200 x 1]. Let me explain the objective first. Let's say I have 1000 images each with an associated quality score [in range of 0-10]. Now, I am trying to perform the image quality assessment using CNN with regression(in PyTorch). I have divided the images into equal size patches. Now, I have created a CNN network in order to perform the linear regression. Following is the code: class MultiLabelNN(nn.Module): def __init__(self): super(MultiLabelNN, self).__init__() self.conv1 = nn.Conv2d(1, 32, 5) self.pool = nn.MaxPool2d(2, 2) self.conv2 = nn.Conv2d(32, 64, 5) self.fc1 = nn.Linear(3200,1024) self.fc2 = nn.Linear(1024, 512) self.fc3 = nn.Linear(512, 1) def forward(self, x): x = self.conv1(x) x = F.relu(x) x = self.pool(x) x = self.conv2(x) x = F.relu(x) x = x.view(-1, 3200) x = self.fc1(x) x = F.relu(x) x = self.fc2(x) x = F.relu(x) x = self.fc3(x) return x While running this code of network I am getting following error input and target shapes do not match: input [400 x 1], target [200 x 1] the target shape is [200x1] is because I have taken the batch size of 200. I found the solution that if I change ""self.fc1 = nn.Linear(3200,1024)"" and ""x = x.view(-1, 3200)"" here from 3200 to 6400 my code runs without any error. Similarly, It will throw an error input and target shapes do not match: input [100 x 1], target [200 x 1] if I put 12800 instead of 6400 Now my doubt is that I am not able to understand the reason behind this. If I am giving 200 images as input to my network then why the input shape is getting affected while changing the parameters when I move from convolutional layer to fully connected layer. I hope I have clearly mentioned my doubt. Even though I anybody has any doubt please ask me. It will be a great help. Thanks in advance.",|python|linear-regression|pytorch|,Tensors&Inputs,1
52503695,"RuntimeError: expected stride to be a single integer value. I am new at Pytorch sorry for the basic question. The model gives me dimension mismatch error how to solve this ? Maybe more than one problems in it. Any help would be appriciated. Thanks class PR(nn.Module): def __init__(self): super(PR, self).__init__() self.conv1 = nn.Conv2d(3,6,kernel_size=5) self.conv2 = nn.Conv2d(6,1,kernel_size=2) self.dens1 = nn.Linear(300, 256) self.dens2 = nn.Linear(256, 256) self.dens3 = nn.Linear(512, 24) self.drop = nn.Dropout() def forward(self, x): out = self.conv1(x) out = self.conv2(x) out = self.dens1(x) out = self.dens2(x) out = self.dens3(x) return out model = PR() input = torch.rand(28,28,3) output = model(input)",|python|pytorch|,Tensors&Inputs,1
52595477,"Why is this Keras Conv2D layer not compatible with the input?. I am having trouble understanding what input shapes my first convolutional neural network expects. My training set is 500 grayscale images of 50x50 pixels. The network starts with a Conv2D layer. Documentation for the argument input_shape says: Input shape: 4D tensor with shape: `(samples, channels, rows, cols)` if data_format='channels_first' or 4D tensor with shape: `(samples, rows, cols, channels)` if data_format='channels_last'. So I expected that I need to supply my images (which are so far stored in a column of a pandas.DataFrame) as a numpy.array of the shape (500, 1, 50, 50), since I only have one ""color"" channel in the images. I reshaped it as follows: X = np.array([img for img in imgs[""img_res""]]) X = X.reshape(-1, 1, img_size, img_size) X.shape is now: (500, 1, 50, 50). I supplied that as an arguent to Conv2D. model = tf.keras.models.Sequential([ tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), input_shape=X.shape[1:], activation=""relu""), ]) This produces the following error. Can you point out what is wrong here? --------------------------------------------------------------------------- InvalidArgumentError Traceback (most recent call last) /usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in _create_c_op(graph, node_def, inputs, control_inputs) 1566 try: -> 1567 c_op = c_api.TF_FinishOperation(op_desc) 1568 except errors.InvalidArgumentError as e: InvalidArgumentError: Negative dimension size caused by subtracting 3 from 1 for 'conv2d/Conv2D' (op: 'Conv2D') with input shapes: [?,1,50,50], [3,3,50,64]. During handling of the above exception, another exception occurred: ValueError Traceback (most recent call last) <ipython-input-24-0b665136e60b> in <module>() 3 kernel_size=(3,3), 4 input_shape=X.shape[1:], ----> 5 activation=""relu""), 6 #tf.keras.layers.MaxPool2D(pool_size=(2,2)), 7 #tf.keras.layers.Conv2D(filters=64, /usr/local/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/sequential.py in __init__(self, layers, name) 99 if layers: 100 for layer in layers: --> 101 self.add(layer) 102 103 @property /usr/local/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/sequential.py in add(self, layer) 162 # and create the node connecting the current layer 163 # to the input layer we just created. --> 164 layer(x) 165 set_inputs = True 166 else: /usr/local/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs) 312 """""" 313 # Actually call the layer (optionally building it). --> 314 output = super(Layer, self).__call__(inputs, *args, **kwargs) 315 316 if args and getattr(self, '_uses_inputs_arg', True): /usr/local/lib/python3.6/site-packages/tensorflow/python/layers/base.py in __call__(self, inputs, *args, **kwargs) 715 716 if not in_deferred_mode: --> 717 outputs = self.call(inputs, *args, **kwargs) 718 if outputs is None: 719 raise ValueError('A layer\'s `call` method should return a Tensor ' /usr/local/lib/python3.6/site-packages/tensorflow/python/layers/convolutional.py in call(self, inputs) 166 167 def call(self, inputs): --> 168 outputs = self._convolution_op(inputs, self.kernel) 169 170 if self.use_bias: /usr/local/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py in __call__(self, inp, filter) 866 867 def __call__(self, inp, filter): # pylint: disable=redefined-builtin --> 868 return self.conv_op(inp, filter) 869 870 /usr/local/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py in __call__(self, inp, filter) 518 519 def __call__(self, inp, filter): # pylint: disable=redefined-builtin --> 520 return self.call(inp, filter) 521 522 /usr/local/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py in __call__(self, inp, filter) 202 padding=self.padding, 203 data_format=self.data_format, --> 204 name=self.name) 205 206 /usr/local/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py in conv2d(input, filter, strides, padding, use_cudnn_on_gpu, data_format, dilations, name) 954 ""Conv2D"", input=input, filter=filter, strides=strides, 955 padding=padding, use_cudnn_on_gpu=use_cudnn_on_gpu, --> 956 data_format=data_format, dilations=dilations, name=name) 957 _result = _op.outputs[:] 958 _inputs_flat = _op.inputs /usr/local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords) 785 op = g.create_op(op_type_name, inputs, output_types, name=scope, 786 input_types=input_types, attrs=attr_protos, --> 787 op_def=op_def) 788 return output_structure, op_def.is_stateful, op 789 /usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in create_op(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device) 3390 input_types=input_types, 3391 original_op=self._default_original_op, -> 3392 op_def=op_def) 3393 3394 # Note: shapes are lazily computed with the C API enabled. /usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in __init__(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def) 1732 op_def, inputs, node_def.attr) 1733 self._c_op = _create_c_op(self._graph, node_def, grouped_inputs, -> 1734 control_input_ops) 1735 else: 1736 self._c_op = None /usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in _create_c_op(graph, node_def, inputs, control_inputs) 1568 except errors.InvalidArgumentError as e: 1569 # Convert to ValueError for backwards compatibility. -> 1570 raise ValueError(str(e)) 1571 1572 return c_op ValueError: Negative dimension size caused by subtracting 3 from 1 for 'conv2d/Conv2D' (op: 'Conv2D') with input shapes: [?,1,50,50], [3,3,50,64].",|python|tensorflow|machine-learning|keras|tensor|,Tensors&Inputs,1
52761666,"PyTorch RuntimeError Invalid argument 2 of size. I am experimenting with a neural network (PyTorch) and I get this error. RuntimeError: invalid argument 2: size '[32 x 9216]' is invalid for input with 8192 elements at /pytorch/aten/src/TH/THStorage.cpp:84 My task is about image classification with AlexNet and I have backtracked the error to be the size of the images supplied to the neural network. My question is, given the network architecture with its parameters, how does one determine the correct image size required by the network? As per my code below, I first transform the training images before feeding into the neural network. But I noticed the neural network can only accept the size of 224 and or else it gives the error above. For instance, my instinct was to apply transforms.RandomResizedCrop of size 64 but apparently this is wrong. Is there a formula to determine the size required? Code # transformation to be done on images transform_train = transforms.Compose([ transforms.RandomResizedCrop(64), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]) class AlexNet(nn.Module): def __init__(self, num_classes=1000): super(AlexNet, self).__init__() self.features = nn.Sequential( nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), nn.Conv2d(64, 192, kernel_size=5, padding=2), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), nn.Conv2d(192, 384, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(256, 256, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), ) self.classifier = nn.Sequential( nn.Dropout(), nn.Linear(256 * 6 * 6, 4096), nn.ReLU(inplace=True), nn.Dropout(), nn.Linear(4096, 4096), nn.ReLU(inplace=True), nn.Linear(4096, num_classes), ) def forward(self, x): x = self.features(x) x = x.view(x.size(0), 256 * 6 * 6) x = self.classifier(x) return x",|python|neural-network|deep-learning|conv-neural-network|pytorch|,Tensors&Inputs,1
52815264,"TypeError: cannot unpack non-iterable int object. Im trying to make my first CNN using pyTorch and am following online help and code already people wrote. i am trying to reproduce their results. I'm using the Kaggle Dogs Breed Dataset for this and below is the error I get. The trainloader does not return my images and labels and any attempt to get them leads in an error: Traceback (most recent call last): File ""E:\Program Files\JetBrains\PyCharm Community Edition 2018.2.4\helpers\pydev\pydevd.py"", line 1664, in <module> main() File ""E:\Program Files\JetBrains\PyCharm Community Edition 2018.2.4\helpers\pydev\pydevd.py"", line 1658, in main globals = debugger.run(setup['file'], None, None, is_module) File ""E:\Program Files\JetBrains\PyCharm Community Edition 2018.2.4\helpers\pydev\pydevd.py"", line 1068, in run pydev_imports.execfile(file, globals, locals) # execute the script File ""E:\Program Files\JetBrains\PyCharm Community Edition 2018.2.4\helpers\pydev\_pydev_imps\_pydev_execfile.py"", line 18, in execfile exec(compile(contents+""\n"", file, 'exec'), glob, loc) File ""C:/Users/sbzfk/PycharmProjects/my_FCN_attempt/Kaggle_Dogs_Competition.py"", line 85, in <module> img, label = next(iter(train_loader)) File ""C:\Users\sbzfk\AppData\Local\Programs\Python\Python37\lib\site-packages\torch\utils\data\dataloader.py"", line 314, in __next__ batch = self.collate_fn([self.dataset[i] for i in indices]) File ""C:\Users\sbzfk\AppData\Local\Programs\Python\Python37\lib\site-packages\torch\utils\data\dataloader.py"", line 314, in <listcomp> batch = self.collate_fn([self.dataset[i] for i in indices]) File ""C:/Users/sbzfk/PycharmProjects/my_FCN_attempt/Kaggle_Dogs_Competition.py"", line 42, in __getitem__ img = self.transform(img) File ""C:\Users\sbzfk\AppData\Local\Programs\Python\Python37\lib\site-packages\torchvision\transforms.py"", line 34, in __call__ img = t(img) File ""C:\Users\sbzfk\AppData\Local\Programs\Python\Python37\lib\site-packages\torchvision\transforms.py"", line 187, in __call__ w, h = img.size TypeError: cannot unpack non-iterable int object Below is my code: class DogsDataset(Dataset): def __init__(self, filenames, labels, root_dir, transform=None): assert len(filenames) == len(labels) # if the two are not of equal length throw an error self.filenames = filenames self.labels = labels self.root_dir = root_dir self.transform = transform def __len__(self): return len(self.filenames) def __getitem__(self, idx): this_img = join(self.root_dir, 'train', self.filenames[idx]+'.jpg') print(this_img) img = io.imread(this_img) label = self.labels[idx] print(label) if self.transform: img = self.transform(img) return [img, label] batch_size = 64 device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"") dataset_root = expanduser(join('~', 'Documents', 'kaggle_dogs_dataset')) # join will intelligently join directories irrespective of OS, and expanduser will # replace with /home/ in linux or the username in Windows csv_file = pd.read_csv(join(dataset_root, 'labels.csv')) # csv file has two columns, id which are filenames and breed which are labels filenames = csv_file.id.values # convert that column to an array, id is the column name and values converty to numpy array # le = LabelEncoder() # labels = le.fit_transform(csv_file.breed) # this will just encode the names between 0 to models-1 , basically changing strings to integers labels = csv_file.breed.values filenames_train, filenames_eval, labels_train, labels_eval = train_test_split(filenames, labels, test_size=0.1, stratify=labels) # this is an import from sklearn as the name implies, it randomly splits data into train and eval, 10% of it to test and rest train data_transform = transforms.Compose([transforms.Scale(224), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))]) dog_train = DogsDataset(filenames_train, labels_train, dataset_root, transform=data_transform) train_loader = DataLoader(dog_train, batch_size, shuffle=True) dog_eval = DogsDataset(filenames_eval, labels_eval, dataset_root, transform=data_transform) eval_loader = DataLoader(dog_eval, batch_size, shuffle=True) def im_show(axis, inp): """"""Denormalize and show"""""" inp = inp.numpy().transpose((1, 2, 0)) mean = np.array([0.485, 0.456, 0.406]) std = np.array([0.229, 0.224, 0.225]) inp = std * inp + mean axis.imshow(inp) img, label = next(iter(train_loader)) print(img.size(), label.size()) fig = plt.figure(1, figsize=(16, 4)) grid = ImageGrid(fig, 111, nrows_ncols=(1, 4), axes_pad=0.05) for i in range(img.size()[0]): ax = grid[i] im_show(ax, img[i]) Ive tried debugging it line by line and with transform=none I seem to read all the images, only with transform=data_transform I seem to get this error.",|python|deep-learning|pytorch|,API,4
52800582,"Why do I fail to predict y=x**4 with Keras? (y=x**3 works). I manage to predict y=x**2 and y=x**3, but equations like y=x**4 or y=x**5 or y=x**7 converge only to inaccurate lines? What do I do wrong? What could I improve? import numpy as np from keras.layers import Dense, Activation from keras.models import Sequential import matplotlib.pyplot as plt import math import time x = np.arange(-100, 100, 0.5) y = x**4 model = Sequential() model.add(Dense(50, input_shape=(1,))) model.add(Activation('sigmoid')) model.add(Dense(50) ) model.add(Activation('elu')) model.add(Dense(1)) model.compile(loss='mse', optimizer='adam') t1 = time.clock() for i in range(100): model.fit(x, y, epochs=1000, batch_size=len(x), verbose=0) predictions = model.predict(x) print (i,"" "", np.mean(np.square(predictions - y)),"" t: "", time.clock()-t1) plt.hold(False) plt.plot(x, y, 'b', x, predictions, 'r--') plt.hold(True) plt.ylabel('Y / Predicted Value') plt.xlabel('X Value') plt.title([str(i),"" Loss: "",np.mean(np.square(predictions - y)),"" t: "", str(time.clock()-t1)]) plt.pause(0.001) #plt.savefig(""fig2.png"") plt.show()",|python|keras|deep-learning|,Training,2
52907593,"TypeError: unsupported operand type(s) for /: 'Dimension' and 'float' in TensorFlow. I have an code for implementing in deep learning in Tensorflow. I use the Keras module self.n_clusters = 10 self.alpha = 0.01 clustering_layer = ClusteringLayer(self.n_clusters, alpha=self.alpha, name='clustering')(hidden) my error is mainly from above, so I on attach it. It give me the following error: --> 118 clustering_layer = ClusteringLayer(self.n_clusters, alpha=self.alpha, name='clustering')(hidden) 119 self.model = Model(inputs=self.autoencoder.input, outputs=clustering_layer) 120 /usr/local/python/2.7-conda5.2/lib/python2.7/site-packages/tensorflow/python/keras/engine/base_layer.pyc in __call__(self, inputs, *args, **kwargs) 694 if all(hasattr(x, 'get_shape') for x in input_list): 695 input_shapes = nest.map_structure(lambda x: x.get_shape(), inputs) --> 696 self.build(input_shapes) 697 698 # Check input assumptions set after layer building, e.g. input shape. <ipython-input-13-8890754cc8a3> in build(self, input_shape) 73 input_dim = input_shape[1] 74 self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_dim)) ---> 75 self.clusters = self.add_weight(shape=(self.n_clusters, input_dim), initializer='glorot_uniform', name='clusters') 76 if self.initial_weights is not None: 77 self.set_weights(self.initial_weights) /usr/local/python/2.7-conda5.2/lib/python2.7/site-packages/tensorflow/python/keras/engine/base_layer.pyc in add_weight(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, getter) 532 trainable=trainable and self.trainable, 533 partitioner=partitioner, --> 534 use_resource=use_resource) 535 536 if regularizer is not None: /usr/local/python/2.7-conda5.2/lib/python2.7/site-packages/tensorflow/python/training/checkpointable/base.pyc in _add_variable_with_custom_getter(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter) 495 new_variable = getter( 496 name=name, shape=shape, dtype=dtype, initializer=initializer, --> 497 **kwargs_for_getter) 498 499 # If we set an initializer and the variable processed it, tracking will not /usr/local/python/2.7-conda5.2/lib/python2.7/site-packages/tensorflow/python/keras/engine/base_layer.pyc in make_variable(name, shape, dtype, initializer, partition_info, trainable, caching_device, validate_shape, constraint, use_resource, partitioner) 1871 validate_shape=validate_shape, 1872 constraint=constraint, -> 1873 use_resource=use_resource) 1874 return v /usr/local/python/2.7-conda5.2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc in variable(initial_value, trainable, collections, validate_shape, caching_device, name, dtype, constraint, use_resource) 2232 name=name, dtype=dtype, 2233 constraint=constraint, -> 2234 use_resource=use_resource) 2235 2236 /usr/local/python/2.7-conda5.2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc in <lambda>(**kwargs) 2222 constraint=None, 2223 use_resource=None): -> 2224 previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs) 2225 for getter in ops.get_default_graph()._variable_creator_stack: # pylint: disable=protected-access 2226 previous_getter = _make_getter(getter, previous_getter) /usr/local/python/2.7-conda5.2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc in default_variable_creator(next_creator, **kwargs) 2194 collections=collections, validate_shape=validate_shape, 2195 caching_device=caching_device, name=name, dtype=dtype, -> 2196 constraint=constraint) 2197 elif not use_resource and context.executing_eagerly(): 2198 raise RuntimeError( /usr/local/python/2.7-conda5.2/lib/python2.7/site-packages/tensorflow/python/ops/resource_variable_ops.pyc in __init__(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint) 310 name=name, 311 dtype=dtype, --> 312 constraint=constraint) 313 314 # pylint: disable=unused-argument /usr/local/python/2.7-conda5.2/lib/python2.7/site-packages/tensorflow/python/ops/resource_variable_ops.pyc in _init_from_args(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, constraint) 415 with ops.name_scope(""Initializer""), ops.device(None): 416 initial_value = ops.convert_to_tensor( --> 417 initial_value(), name=""initial_value"", dtype=dtype) 418 self._handle = _eager_safe_variable_handle( 419 shape=initial_value.get_shape(), /usr/local/python/2.7-conda5.2/lib/python2.7/site-packages/tensorflow/python/keras/engine/base_layer.pyc in <lambda>() 1858 initializer = initializer(dtype=dtype) 1859 init_val = lambda: initializer( # pylint: disable=g-long-lambda -> 1860 shape, dtype=dtype, partition_info=partition_info) 1861 variable_dtype = dtype.base_dtype 1862 if use_resource is None: /usr/local/python/2.7-conda5.2/lib/python2.7/site-packages/tensorflow/python/ops/init_ops.pyc in __call__(self, shape, dtype, partition_info) 466 scale /= max(1., fan_out) 467 else: --> 468 scale /= max(1., (fan_in + fan_out) / 2.) 469 if self.distribution == ""normal"": 470 stddev = math.sqrt(scale) TypeError: unsupported operand type(s) for /: 'Dimension' and 'float' line 118 is my code location. The error seems to occur in tensorflow package. it give me the TypeError: unsupported operand type(s) for /: 'Dimension' and 'float'. I try both python 2.7 and python 3.6 but with same problem. How to deal with this situation? An very similar situation is in github, its error can be addressed in its code, but my error seems to be happen in init_ops.pyc",|python|tensorflow|typeerror|,API,4
52950449,"valueError when using multi_gpu_model in keras. I am using google cloud VM with 4 Tesla K80 GPU's. I am running a keras model using multi_gpu_model with gpus=4(since i have 4 gpu's). But, i am getting the following error ValueError: To call multi_gpu_model with gpus=4, we expect the following devices to be available: ['/cpu:0', '/gpu:0', '/gpu:1', '/gpu:2', '/gpu:3']. However this machine only has: ['/cpu:0', '/xla_cpu:0', '/xla_gpu:0', '/gpu:0']. Try reducing gpus. I can see that there are only two gpu's here namely '/xla_gpu:0', '/gpu:0'. so, i tried with gpus = 2 and again got the following error ValueError: To call multi_gpu_model with gpus=2, we expect the following devices to be available: ['/cpu:0', '/gpu:0', '/gpu:1']. However this machine only has: ['/cpu:0', '/xla_cpu:0', '/xla_gpu:0', '/gpu:0']. Try reducing gpus. can anyone help me out with the error. Thanks!",|python|tensorflow|keras|google-cloud-platform|gpu|,GPU Usage,3
53082519,"Problem with tf.train.Saver() and GPU - TensorFlow. My code is structured as follows: with tf.device('/gpu:1'): ... model = get_model(input_pl) ... with tf.Session() as sess: saver = tf.train.Saver() sess.run(tf.global_variables_initializer()) for epoch in range(num_epochs): ... for n in range(num_batches): ... sess.run(...) # eval epoch saver.save(sess, ...) I want to save the model after the training phase. When I run it gives me this error: InvalidArgumentError (see above for traceback): Cannot assign a device for operation 'save/SaveV2': Could not satisfy explicit device specification '/device:GPU:1' because no supported kernel for GPU devices is available. Reading this question I changed the code in this way: saver = tf.train.Saver() with tf.device('/gpu:1'): ... model = get_model(pointcloud_pl) ... with tf.Session() as sess: sess.run(tf.global_variables_initializer()) for epoch in range(num_epochs): ... for n in range(num_batches): ... sess.run(...) # eval epoch saver.save(sess, ...) But now I get this error: ValueError: No variables to save I've tried also to do this way: with tf.Session() as sess: saver = tf.train.Saver() ... with tf.device('/gpu:1'): sess.run(tf.global_variables_initializer()) for epoch in range(num_epochs): ... for n in range(num_batches): ... sess.run() # eval epoch saver.save(sess, ...) And I still get the same error. The error is always in the saver = tf.train.Saver() line. How can I solve this problem?",|python|tensorflow|gpu|,GPU Usage,3
53119432,"Keras - CNN input shape incompatible. I am working on binary classification my code is working fine on Keras Lstm when I am working with CNN I am getting input shape incompatibility error. this is the value error i am getting ValueError: Error when checking target: expected dense_61 to have 3 dimensions, but got array with shape (24, 1) this is my cnn code using keras model=Sequential() inputBatch = inputBatch.reshape(24,30, 1) model.add(Conv1D(64, 3, activation='relu', input_shape=(30, 1))) model.add(Conv1D(64, 3, activation='relu')) model.add(MaxPooling1D(pool_size=4,strides=None, padding='valid')) model.add(Conv1D(128, 3, activation='relu')) model.add(Conv1D(128, 3, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(1, activation='sigmoid')) model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy']) model.fit(inputBatch,ponlabel,batch_size=24,epochs=20,validation_data=(inputBatch, ponlabel)) I am working on binary classification either it will be positive or either negative for reference this is my lstm code inputBatch =inputBatch.reshape(24,30,1) model=Sequential() model.add(LSTM(50, input_shape=(30, 1))) model.add(Dense(1, activation=""relu"")) model.compile(loss='mean_absolute_error',optimizer='adam') model.fit(inputBatch,ponlabel,batch_size=24,epochs=100,verbose=1) inputBatch is something like this it is working on LSTM code but not working on CNN this is the input which I have used for training on both codes separately [[ 0. 1288. 1288. 2214. 11266. 6923. 420. 0. 0. 8123. 0. 7619. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [ 0. 0. 0. 0. 0. 0. 11516. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [ 0. 9929. 11501. 6573. 11266. 7566. 9963. 4420. 10936. 3657. 7050. 0. 408. 11501. 9988. 9963. 8455. 2879. 9322. 2047. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [ 0. 11956. 5222. 0. 0. 12106. 6481. 0. 7093. 13756. 12152. 0. 0. 0. 0. 10173. 0. 5173. 13756. 9371. 0. 9956. 0. 0. 9716. 0. 0. 0. 0. 0.] [ 0. 0. 420. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [ 0. 11501. 1916. 2073. 10936. 6312. 0. 10193. 10322. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [ 0. 2879. 7852. 11501. 1934. 286. 11483. 0. 12004. 11118. 0. 12007. 9917. 12111. 1520. 10364. 0. 8840. 4195. 2910. 10773. 11386. 12117. 9321. 0. 0. 0. 0. 0. 0.] [ 0. 7885. 7171. 1034. 11501. 3103. 5842. 4395. 11871. 3328. 6719. 5407. 1087. 8935. 2937. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [ 0. 8894. 450. 11516. 7353. 11501. 11502. 11499. 0. 1319. 11693. 11501. 5735. 12111. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [ 0. 1087. 9565. 23. 0. 3045. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [ 0. 5015. 11501. 3306. 12111. 9307. 5050. 11501. 3306. 0. 3306. 12111. 1981. 11516. 615. 11516. 0. 3925. 11956. 9371. 9013. 4395. 12111. 5048. 0. 3925. 0. 0. 0. 0.] [ 0. 1287. 420. 4070. 11087. 7410. 12186. 2387. 12111. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [ 0. 128. 2073. 10936. 6312. 0. 10193. 10322. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [ 0. 10173. 9435. 1320. 9322. 12018. 1055. 8840. 6684. 12051. 2879. 0. 12018. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [ 0. 1570. 5466. 9322. 34. 11480. 1356. 11270. 420. 2153. 12006. 5157. 8840. 1055. 11516. 7387. 2356. 2163. 2879. 5541. 9443. 7441. 1295. 5473. 0. 0. 0. 0. 0. 0.] [ 0. 5014. 0. 0. 3651. 1087. 63. 6153. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [ 0. 10608. 10855. 9562. 0. 0. 0. 4202. 0. 0. 0. 10818. 10818. 5842. 0. 9963. 0. 11516. 10464. 7491. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [ 0. 5952. 6133. 450. 7520. 5842. 3412. 10400. 3412. 2149. 4891. 2979. 3456. 505. 9929. 11501. 9322. 1836. 11501. 12111. 3435. 11105. 11266. 420. 9322. 34. 0. 0. 0. 0.] [ 0. 1570. 5466. 9322. 34. 11480. 1356. 11270. 420. 2153. 12006. 5157. 8840. 1055. 11516. 7387. 2356. 2163. 2879. 5541. 9443. 7441. 1295. 5473. 0. 0. 0. 0. 0. 0.] [ 0. 7544. 0. 1709. 420. 10936. 5222. 5842. 10407. 6937. 11329. 2937. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [ 0. 7785. 8840. 0. 420. 8603. 12003. 2879. 1087. 2356. 2390. 12111. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [ 0. 8695. 8744. 420. 8840. 6697. 9267. 11516. 11203. 2260. 8840. 7309. 0. 11100. 6041. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [ 0. 9307. 12003. 2879. 6398. 9372. 4614. 5222. 0. 0. 2879. 10364. 6923. 4709. 4860. 11871. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [ 0. 0. 2844. 1287. 420. 11501. 610. 11501. 596. 0. 12111. 3690. 6343. 9963. 0. 0. 8840. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]",|python|tensorflow|keras|,Tensors&Inputs,1
53186736,"RuntimeError: Attempting to deserialize object on CUDA device 2 but torch.cuda.device_count() is 1. I've got a snippet of python code for training a model. The problem is that after running: loaded_state = torch.load(model_path+seq_to_seq_test_model_fname) to load a pretrained model,I'm getting: Traceback (most recent call last): File ""img_to_text.py"", line 480, in <module> main() File ""img_to_text.py"", line 475, in main r = setup_test() File ""img_to_text.py"", line 259, in setup_test s2s_data = s2s.setup_test() File ""/media/ahrzb/datasets/notebooks/mzh/SemStyle/semstyle/code/seq2seq_pytorch.py"", line 220, in setup_test loaded_state= torch.load(model_path+seq_to_seq_test_model_fname) File ""/home/ahrzb/.pyenv/versions/2.7.15/envs/mzh2.7/lib/python2.7/site-packages/torch/serialization.py"", line 358, in load return _load(f, map_location, pickle_module) File ""/home/ahrzb/.pyenv/versions/2.7.15/envs/mzh2.7/lib/python2.7/site-packages/torch/serialization.py"", line 542, in _load result = unpickler.load() File ""/home/ahrzb/.pyenv/versions/2.7.15/envs/mzh2.7/lib/python2.7/site-packages/torch/serialization.py"", line 508, in persistent_load data_type(size), location) File ""/home/ahrzb/.pyenv/versions/2.7.15/envs/mzh2.7/lib/python2.7/site-packages/torch/serialization.py"", line 372, in restore_location return default_restore_location(storage, location) File ""/home/ahrzb/.pyenv/versions/2.7.15/envs/mzh2.7/lib/python2.7/site-packages/torch/serialization.py"", line 104, in default_restore_location result = fn(storage, location) File ""/home/ahrzb/.pyenv/versions/2.7.15/envs/mzh2.7/lib/python2.7/site-packages/torch/serialization.py"", line 85, in _cuda_deserialize device, torch.cuda.device_count())) I think this is because they have trained the model on two GPUs and I need to load it in one GPU. I changed this line: loaded_state = torch.load(model_path+seq_to_seq_test_model_fname) to loaded_state = torch.load(model_path+seq_to_seq_test_model_fname, map_location={'cuda:1': 'cuda:0'} ) in order to map data of cuda 1 to cuda 0 but it did not work.",|python|pytorch|,GPU Usage,3
53194827,"cannot train Keras convolution network on GPU. I can train a Keras network with Dense layer using keras.datasets.fashion_mnist dataset. However, when I tried to train a convolutional network, I got an error. Here is some part of the code: from tensorflow.keras.layers import * model = keras.Sequential([ Convolution2D(16, (3,3), activation='relu', input_shape=(28,28,1)), MaxPooling2D(pool_size=(2,2)), Flatten(), Dense(16, activation='relu'), Dense(10, activation='softmax') ]) model.compile(optimizer=tf.train.AdamOptimizer(), loss='sparse_categorical_crossentropy', metrics=['accuracy']) model.fit(train_images, train_labels, epochs=5) and its error when I tried to fit. UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above. [[{{node conv2d/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=""NCHW"", dilations=[1, 1, 1, 1], padding=""VALID"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](training/TFOptimizer/gradients/conv2d/Conv2D_grad/Conv2DBackpropFilter-0-TransposeNHWCToNCHW-LayoutOptimizer, conv2d/Conv2D/ReadVariableOp)]] [[{{node loss/dense_1_loss/broadcast_weights/assert_broadcastable/AssertGuard/Assert/Switch_2/_69}} = _Recvclient_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_112_l...t/Switch_2"", tensor_type=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]] I have cudnn64_7.dll in C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\bin and the PATH is already contain that folder.",|tensorflow|keras|,API,4
53332663,"Torchvision 0.2.1 transforms.Normalize does not work as expected. I am trying a new code using Pytorch. In this code, to load the dataset (CIFAR10), I am using torchvision's datasets. I define two transform functions ToTensor() and Normalize(). After normalize I expect the data in the dataset should be between 0 and 1. But the max value is still 255. I also inserted a print statement inside the '__call__' function of Normalize class in transforms.py (Lib\site-packages\torchvision\transforms\transforms.py). This print is not printed while running the code too. Not sure what is happening. Every page I visited in the internet, mentions the usage almost the same way as I do. For example some sites I visited https://github.com/adventuresinML/adventures-in-ml-code/blob/master/pytorch_nn.py https://github.com/pytorch/tutorials/blob/master/beginner_source/blitz/cifar10_tutorial.py My code is given below. This reads the dataset with and without Normalize, then prints some stats. The min and max printed is an indicator of whether the data is normalized or not. import torchvision as tv import numpy as np dataDir = 'D:\\general\\ML_DL\\datasets\\CIFAR' trainTransform = tv.transforms.Compose([tv.transforms.ToTensor()]) trainSet = tv.datasets.CIFAR10(dataDir, train=True, download=False, transform=trainTransform) print (trainSet.train_data.mean(axis=(0,1,2))/255) print (trainSet.train_data.min()) print (trainSet.train_data.max()) print (trainSet.train_data.shape) trainTransform = tv.transforms.Compose([tv.transforms.ToTensor(), tv.transforms.Normalize((0.4914, 0.4822, 0.4466), (0.247, 0.243, 0.261))]) trainSet = tv.datasets.CIFAR10(dataDir, train=True, download=False, transform=trainTransform) print (trainSet.train_data.mean(axis=(0,1,2))/255) print (trainSet.train_data.min()) print (trainSet.train_data.max()) print (trainSet.train_data.shape) The output looks like, [ 0.49139968 0.48215841 0.44653091] 0 255 (50000, 32, 32, 3) [ 0.49139968 0.48215841 0.44653091] 0 255 (50000, 32, 32, 3) Please help me understand this better. As most of the functions I tried, ends up with similar results - for example Grayscale, CenterCrop too.",|python|deep-learning|pytorch|normalize|torchvision|,API,4
53367049,"F.relu(self.fc1(x)) is causing RuntimeError problem. I have implemented the following CNN for my training and validation data sets that contain 90 and 20 images respectively divided into 3 classes: def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.pool = nn.MaxPool2d(2, 2) self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 50) self.fc3 = nn.Linear(50, len(classes)) def forward(self, x): print(x.shape) x = self.pool(F.relu(self.conv1(x))) print(x.shape) x = self.pool(F.relu(self.conv2(x))) print(x.shape) x = x.view(x.size(0),-1) #x = x.view(-1,x.size(1)*x.size(2)*x.size(3)) #x = x.view(-1, 16 * 5 * 5) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x I am getting the following error while running: RuntimeError: size mismatch, m1: [1 x 214720], m2: [400 x 120] at /opt/conda/conda-bld/pytorch_1524584710464/work/aten/src/TH/generic/THTensorMath.c:2033 for x = F.relu(self.fc1(x)) Can someone please suggest what I should do to get rid of this problem? I changed the x.view(...) by following some threads. However, it did not help in this case.",|python|pytorch|,Tensors&Inputs,1
53380068,"Pytorch on google-colaboratory GPU - Illegal memory access. I am using pytorch(0.4.0) on google-colaboratory ( NVIDIA-SMI 396.44 Driver Version: 396.44) When running my code outside any function, I am able to send pytorch tensors and model to the GPU : ... model.cuda() data_tensor = data_tensor.cuda() ... And my CNN model is trained successfully with 98% accurancy. But when I put the same code in a function, def main(...): .... model.cuda() data_tensor= data_tensor.cuda() ... if __name__ == ""__main__"": main('...) I have the following error: cuda runtime error (77) : an illegal memory access was encountered at /pytorch/aten/src/THC/generic/THCTensorCopy.c:20 UPDATE(18/11/21): It turned out that being part or not of a function is irrelevant. Usually, I have first a CUDNN_STATUS_EXECUTION_FAILED error then the second time a cuda runtime error (77) as shown below. But it sometimes works a few times before failing. CUDNN_STATUS_EXECUTION_FAILED (first try) : RuntimeError Traceback (most recent call last) <ipython-input-27-53476e08e017> in <module>() 1 main('mnist', 'to', 'ndd', Xd=16, epo=5, bs=100, tXn=-1, vXn=300, ----> 2 lr=0.05, suf=""s1"", n_class=10, cuda=True) <ipython-input-23-918584456207> in main(ds, framework, format, Xd, epo, bs, tXn, vXn, lr, suf, n_class, cuda) 12 opt = torch.optim.SGD(net.parameters(), lr) 13 ---> 14 train(net, opt, Xd, epo, bs, cuda, tXn, tX, tT, vX, vT,lr) 15 <ipython-input-26-6b574a9e8af6> in train(model, optimizer, Xd, epo, bs, cuda, Xn, tX, tT, vX, vT, lr) 26 #t = t.cuda() 27 optimizer.zero_grad() ---> 28 z = model(x) 29 bat_loss = criterion(z, t) 30 bat_loss.backward() /usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs) 489 result = self._slow_forward(*input, **kwargs) 490 else: --> 491 result = self.forward(*input, **kwargs) 492 for hook in self._forward_hooks.values(): 493 hook_result = hook(self, input, result) <ipython-input-22-b4bc2e0b39b8> in forward(self, X) 10 H0 = torch.zeros(self.n_H, X.size(0), self.Wh) 11 C0 = torch.zeros(self.n_H, X.size(0), self.Wh) ---> 12 O, (Hn, Cn), = self.lstm1(X, (H0, C0)) 13 O = self.linear1(O[:, -1, :]) 14 return O /usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs) 489 result = self._slow_forward(*input, **kwargs) 490 else: --> 491 result = self.forward(*input, **kwargs) 492 for hook in self._forward_hooks.values(): 493 hook_result = hook(self, input, result) /usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py in forward(self, input, hx) 190 flat_weight=flat_weight 191 ) --> 192 output, hidden = func(input, self.all_weights, hx, batch_sizes) 193 if is_packed: 194 output = PackedSequence(output, batch_sizes) /usr/local/lib/python3.6/dist-packages/torch/nn/_functions/rnn.py in forward(input, *fargs, **fkwargs) 321 func = decorator(func) 322 --> 323 return func(input, *fargs, **fkwargs) 324 325 return forward /usr/local/lib/python3.6/dist-packages/torch/nn/_functions/rnn.py in forward(input, weight, hx, batch_sizes) 285 batch_first, dropout, train, bool(bidirectional), 286 list(batch_sizes.data) if variable_length else (), --> 287 dropout_ts) 288 289 if cx is not None: RuntimeError: CUDNN_STATUS_EXECUTION_FAILED cuda runtime error (77) (other tries): RuntimeError Traceback (most recent call last) <ipython-input-28-53476e08e017> in <module>() 1 main('mnist', 'to', 'ndd', Xd=16, epo=5, bs=100, tXn=-1, vXn=300, ----> 2 lr=0.05, suf=""s1"", n_class=10, cuda=True) <ipython-input-23-918584456207> in main(ds, framework, format, Xd, epo, bs, tXn, vXn, lr, suf, n_class, cuda) 12 opt = torch.optim.SGD(net.parameters(), lr) 13 ---> 14 train(net, opt, Xd, epo, bs, cuda, tXn, tX, tT, vX, vT,lr) 15 <ipython-input-26-6b574a9e8af6> in train(model, optimizer, Xd, epo, bs, cuda, Xn, tX, tT, vX, vT, lr) 4 if cuda and torch.cuda.is_available(): 5 print(""tX type (before):"", tX.type()) ----> 6 model.cuda() 7 tX = tX.cuda() 8 tT = tT.cuda() /usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in cuda(self, device) 247 Module: self 248 """""" --> 249 return self._apply(lambda t: t.cuda(device)) 250 251 def cpu(self): /usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in _apply(self, fn) 174 def _apply(self, fn): 175 for module in self.children(): --> 176 module._apply(fn) 177 178 for param in self._parameters.values(): /usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py in _apply(self, fn) 109 110 def _apply(self, fn): --> 111 ret = super(RNNBase, self)._apply(fn) 112 self.flatten_parameters() 113 return ret /usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in _apply(self, fn) 180 # Tensors stored in modules are graph leaves, and we don't 181 # want to create copy nodes, so we have to unpack the data. --> 182 param.data = fn(param.data) 183 if param._grad is not None: 184 param._grad.data = fn(param._grad.data) /usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in <lambda>(t) 247 Module: self 248 """""" --> 249 return self._apply(lambda t: t.cuda(device)) 250 251 def cpu(self): RuntimeError: cuda runtime error (77) : an illegal memory access was encountered at /pytorch/aten/src/THC/generic/THCTensorCopy.c:20",|gpu|pytorch|google-colaboratory|,GPU Usage,3
53416833,"RuntimeError: Given groups=1, weight of size [64, 3, 7, 7], expected input[3, 1, 224, 224] to have 3 channels, but got 1 channels instead. In the code below: model_ft.eval() test_data, test_target = image_datasets['train'][idx] test_data = test_data.cuda() #test_target = test_target.cuda() test_target = torch.tensor(test_target) test_target = test_target.cuda() test_data.unsqueeze_(1) test_target.unsqueeze_(0) print(test_data.shape) output = model_ft(test_data) I get the following error: Traceback (most recent call last): File ""test_loocv.py"", line 245, in <module> output = model_ft(test_data) File ""/scratch/sjn-p3/anaconda/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 477, in __call__ result = self.forward(*input, **kwargs) File ""/scratch/sjn-p3/anaconda/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/models/resnet.py"", line 139, in forward File ""/scratch/sjn-p3/anaconda/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 477, in __call__ result = self.forward(*input, **kwargs) File ""/scratch/sjn-p3/anaconda/anaconda3/lib/python3.6/site-packages/torch/nn/modules/conv.py"", line 301, in forward self.padding, self.dilation, self.groups) RuntimeError: Given groups=1, weight of size [64, 3, 7, 7], expected input[3, 1, 224, 224] to have 3 channels, but got 1 channels instead Also, test_data has the shape: torch.Size([3, 1, 224, 224]). How should I fix this?",|python|deep-learning|pytorch|tensor|,Tensors&Inputs,1
53442190,ImportError: cannot import name 'normalize_data_format'. I have read an article Here and its pretty nice enough to understand. Given its implementation on GitHub. When I am trying to train at my own using given code it gives me an Import Error in this file at line 117 like following. I am using google Colab environment. Having some search over the error i got that the following line is compatible to keras version==2.2.2. I have also installed that yet not solved with the error. Please help me to get over it. By default keras version installed in colab is 2.2.4 --------------------------------------------------------------------------- ImportError Traceback (most recent call last) <ipython-input-47-f8ce7e15cf87> in <module>() 9 from keras.layers.merge import Add 10 from keras.utils import conv_utils ---> 11 from keras.utils.conv_utils import normalize_data_format 12 13 from keras.layers.core import Dropout ImportError: cannot import name 'normalize_data_format' ---------------------------------------------------------------------------,|python-3.x|keras|deep-learning|conv-neural-network|keras-layer|,API,4
53538138,"Save and load checkpoint pytorch. i make a model and save the configuration as: def checkpoint(state, ep, filename='./Risultati/checkpoint.pth'): if ep == (n_epoch-1): print('Saving state...') torch.save(state,filename) checkpoint({'state_dict':rnn.state_dict()},ep) and then i want load this configuration : state_dict= torch.load('./Risultati/checkpoint.pth') rnn.state_dict(state_dict) when i try, this is the error: Traceback (most recent call last): File ""train.py"", line 288, in <module> rnn.state_dict(state_dict) File ""/home/marco/.local/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 593, in state_dict destination._metadata[prefix[:-1]] = dict(version=self._version) AttributeError: 'dict' object has no attribute '_metadata' where i do wrong? thx in advance",|python-3.x|pytorch|recurrent-neural-network|checkpointing|,API,4
53710313,"pytorch vgg model test on one image. I've trained a vgg model, this is how I transformed the test data test_transform_2= transforms.Compose([transforms.RandomResizedCrop(224), transforms.ToTensor()]) test_data = datasets.ImageFolder(test_dir, transform=test_transform_2) the model's finished training now I want to test it on a single image from scipy import misc test_image = misc.imread('flower_data/valid/1/image_06739.jpg') vgg16(torch.from_numpy(test_image)) Error --------------------------------------------------------------------------- RuntimeError Traceback (most recent call last) <ipython-input-60-b83587325fea> in <module> ----> 1 vgg16(torch.from_numpy(test_image)) c:\users\sam\mydocu~1\code\envs\data-science\lib\site-packages\torch\nn\modules\module.py in __call__(self, *input, **kwargs) 475 result = self._slow_forward(*input, **kwargs) 476 else: --> 477 result = self.forward(*input, **kwargs) 478 for hook in self._forward_hooks.values(): 479 hook_result = hook(self, input, result) c:\users\sam\mydocu~1\code\envs\data-science\lib\site-packages\torchvision\models\vgg.py in forward(self, x) 40 41 def forward(self, x): ---> 42 x = self.features(x) 43 x = x.view(x.size(0), -1) 44 x = self.classifier(x) c:\users\sam\mydocu~1\code\envs\data-science\lib\site-packages\torch\nn\modules\module.py in __call__(self, *input, **kwargs) 475 result = self._slow_forward(*input, **kwargs) 476 else: --> 477 result = self.forward(*input, **kwargs) 478 for hook in self._forward_hooks.values(): 479 hook_result = hook(self, input, result) c:\users\sam\mydocu~1\code\envs\data-science\lib\site-packages\torch\nn\modules\container.py in forward(self, input) 89 def forward(self, input): 90 for module in self._modules.values(): ---> 91 input = module(input) 92 return input 93 c:\users\sam\mydocu~1\code\envs\data-science\lib\site-packages\torch\nn\modules\module.py in __call__(self, *input, **kwargs) 475 result = self._slow_forward(*input, **kwargs) 476 else: --> 477 result = self.forward(*input, **kwargs) 478 for hook in self._forward_hooks.values(): 479 hook_result = hook(self, input, result) c:\users\sam\mydocu~1\code\envs\data-science\lib\site-packages\torch\nn\modules\conv.py in forward(self, input) 299 def forward(self, input): 300 return F.conv2d(input, self.weight, self.bias, self.stride, --> 301 self.padding, self.dilation, self.groups) 302 303 RuntimeError: Expected 4-dimensional input for 4-dimensional weight [64, 3, 3, 3], but got input of size [628, 500, 3] instead I can tell I need to shape the input, however I don't know how to based on the way it seems to expect the input to be inform of a batch.",|python|pytorch|,Tensors&Inputs,1
53767829,"Value Error problem with multicell Dimensions must be equal, but are 20 and 13. I am working with python 3.6.5 and tensorflow 1.8.0 Nr of neurons are 10 at the moment, input in this example is 3 I have already build a recurrent neuronal network and now wanted to improve it. I need some help! Here is a little excerpt of the code to reproduce my error: You can also replace BasicRNN by LSTM or GRU to get the other messages. import numpy as np import tensorflow as tf batch_size = 10 nr_inputs = 3 nr_outputs = 4 nr_steps = 4 nr_layers = 2 def mini_batch ( Xdata, ydata, batch_size ) : global global_counter result = None Xbatch = np.zeros( shape=[batch_size, nr_steps, nr_inputs], dtype = np.float32 ) ybatch = np.zeros( shape=[batch_size, nr_outputs], dtype = np.float32 ) return Xbatch, ybatch X = tf.placeholder( tf.float32, [ None, nr_steps, nr_inputs ] ) y = tf.placeholder( tf.float32, [ None, nr_outputs ] ) neurons = tf.contrib.rnn.BasicRNNCell(num_units = 10) neurons = tf.contrib.rnn.MultiRNNCell( [neurons] * nr_layers, state_is_tuple = True ) X_train = np.zeros( shape=[1000, nr_steps, nr_inputs], dtype = np.float32 ) y_train = np.zeros( shape=[1000, nr_outputs], dtype = np.float32 ) X_test = np.zeros( shape=[1000, nr_steps, nr_inputs], dtype = np.float32 ) y_test = np.zeros( shape=[1000, nr_outputs], dtype = np.float32 ) rnn_outputs, rnn_states = tf.nn.dynamic_rnn( neurons, X, dtype=tf.float32 ) logits = tf.contrib.layers.fully_connected( inputs = rnn_states, num_outputs = nr_outputs, activation_fn = None ) xentropy = tf.nn.sigmoid_cross_entropy_with_logits( labels = y, logits = logits ) loss = tf.reduce_mean( xentropy ) optimizer = tf.train.AdamOptimizer( learning_rate = 0.01 ) training_op = optimizer.minimize( loss ) init = tf.global_variables_initializer() with tf.Session() as sess : init.run() global_counter = 0 for epoch in range(100) : for iteration in range( 4) : X_batch, y_batch = mini_batch ( X_train, y_train, batch_size ) sess.run( training_op, feed_dict={ X : X_batch, y : y_batch } ) loss_train = loss.eval( feed_dict={ X : X_batch, y : y_batch } ) loss_test = loss.eval( feed_dict={ X : X_test, y : y_test } ) sess.close() I was trying this neurons = tf.contrib.rnn.MultiRNNCell([neurons]*nr_layers, state_ist_tuple = True) and received the error ValueError: Dimensions must be equal, but are 20 and 13 for 'rnn/.../MatMul1'(op 'MatMul') with input shapes [?,20], [13, 10] for a tf.contrib.rnn.BasicRNNCell(num_units = nr_neurons) with input shapes [?,20], [13, 20] for a tf.contrib.rnn.GRUCell(num_units = nr_neurons) and with input shapes [?,20], [13, 40] for a tf.contrib.rnn.BasicLSTMCell(num_units = nr_neurons, state_is_tuple = True) is there an error in the MatMul_1? Has anyone ever had similar problems? Thank you so much!",|python|tensorflow|multidimensional-array|valueerror|word-embedding|,Tensors&Inputs,1
53820175,"Error when converting PyTorch model to TorchScript. I'm trying to follow the PyTorch guide to load models in C++. The following sample code works: import torch import torchvision # An instance of your model. model = torchvision.models.resnet18() # An example input you would normally provide to your model's forward() method. example = torch.rand(1, 3, 224, 224) # Use torch.jit.trace to generate a torch.jit.ScriptModule via tracing. traced_script_module = torch.jit.trace(model, example) However, when trying other networks, such as squeezenet (or alexnet), my code fails: sq = torchvision.models.squeezenet1_0(pretrained=True) traced_script_module = torch.jit.trace(sq, example) >> traced_script_module = torch.jit.trace(sq, example) /home/fabio/.local/lib/python3.6/site-packages/torch/jit/__init__.py:642: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error: Not within tolerance rtol=1e-05 atol=1e-05 at input[0, 785] (3.1476082801818848 vs. 3.945478677749634) and 999 other locations (100.00%) _check_trace([example_inputs], func, executor_options, module, check_tolerance, _force_outplace)",|pytorch|torchscript|,API,4
53813636,"vgg probability doesn't add up to 1, pytorch. I've trained a vgg16 model to predict 102 classes of flowers. It works however now that I'm trying to understand one of it's predictions I feel it's not acting normally. model layout # Imports here import os import numpy as np import torch import torchvision from torchvision import datasets, models, transforms import matplotlib.pyplot as plt import json from pprint import pprint from scipy import misc %matplotlib inline data_dir = 'flower_data' train_dir = data_dir + '/train' test_dir = data_dir + '/valid' json_data=open('cat_to_name.json').read() main_classes = json.loads(json_data) main_classes = {int(k):v for k,v in classes.items()} train_transform_2 = transforms.Compose([transforms.RandomResizedCrop(224), transforms.RandomRotation(30), transforms.RandomHorizontalFlip(), transforms.ToTensor()]) test_transform_2= transforms.Compose([transforms.RandomResizedCrop(224), transforms.ToTensor()]) # TODO: Load the datasets with ImageFolder train_data = datasets.ImageFolder(train_dir, transform=train_transform_2) test_data = datasets.ImageFolder(test_dir, transform=test_transform_2) # define dataloader parameters batch_size = 20 num_workers=0 # TODO: Using the image datasets and the trainforms, define the dataloaders train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers, shuffle=True) test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers, shuffle=True) vgg16 = models.vgg16(pretrained=True) # Freeze training for all ""features"" layers for param in vgg16.features.parameters(): param.requires_grad = False import torch.nn as nn n_inputs = vgg16.classifier[6].in_features # add last linear layer (n_inputs -> 102 flower classes) # new layers automatically have requires_grad = True last_layer = nn.Linear(n_inputs, len(classes)) vgg16.classifier[6] = last_layer import torch.optim as optim # specify loss function (categorical cross-entropy) criterion = nn.CrossEntropyLoss() # specify optimizer (stochastic gradient descent) and learning rate = 0.001 optimizer = optim.SGD(vgg16.classifier.parameters(), lr=0.001) pre_trained_model=torch.load(""model.pt"") new=list(pre_trained_model.items()) my_model_kvpair=vgg16.state_dict() count=0 for key,value in my_model_kvpair.items(): layer_name, weights = new[count] my_model_kvpair[key] = weights count+=1 # number of epochs to train the model n_epochs = 6 # initialize tracker for minimum validation loss valid_loss_min = np.Inf # set initial ""min"" to infinity for epoch in range(1, n_epochs+1): # keep track of training and validation loss train_loss = 0.0 valid_loss = 0.0 ################### # train the model # ################### # model by default is set to train vgg16.train() for batch_i, (data, target) in enumerate(train_loader): # clear the gradients of all optimized variables optimizer.zero_grad() # forward pass: compute predicted outputs by passing inputs to the model output = vgg16(data) # calculate the batch loss loss = criterion(output, target) # backward pass: compute gradient of the loss with respect to model parameters loss.backward() # perform a single optimization step (parameter update) optimizer.step() # update training loss train_loss += loss.item() if batch_i % 20 == 19: # print training loss every specified number of mini-batches print('Epoch %d, Batch %d loss: %.16f' % (epoch, batch_i + 1, train_loss / 20)) train_loss = 0.0 ###################### # validate the model # ###################### vgg16.eval() # prep model for evaluation for data, target in test_loader: # forward pass: compute predicted outputs by passing inputs to the model output = vgg16(data) # calculate the loss loss = criterion(output, target) # update running validation loss valid_loss += loss.item() # print training/validation statistics # calculate average loss over an epoch train_loss = train_loss/len(train_loader.dataset) valid_loss = valid_loss/len(test_loader.dataset) print('Epoch: {} \tTraining Loss: {:.6f} \tValidation Loss: {:.6f}'.format( epoch+1, train_loss, valid_loss )) # save model if validation loss has decreased if valid_loss <= valid_loss_min: print('Validation loss decreased ({:.6f} --> {:.6f}). Saving model ...'.format( valid_loss_min, valid_loss)) torch.save(vgg16.state_dict(), 'model.pt') valid_loss_min = valid_loss testing on a single image tensor = torch.from_numpy(test_image) reshaped = tensor.permute(2, 0, 1).unsqueeze(0) floatified = reshaped.to(torch.float32) / 255 vgg16(floatified) >>> tensor([[ 2.5686, -1.1964, -0.0872, -1.7010, -1.6669, -1.0638, 0.4515, 0.1124, 0.0166, 0.3156, 1.1699, 1.5374, 1.8720, 2.5184, 2.9046, -0.8241, -1.1949, -0.5700, 0.8692, -1.0485, 0.0390, -1.3783, -3.4632, -0.0143, 1.0986, 0.2667, -1.1127, -0.8515, 0.7759, -0.7528, 1.6366, -0.1170, -0.4983, -2.6970, 0.7545, 0.0188, 0.1094, 0.5002, 0.8838, -0.0006, -1.7993, -1.3706, 0.4964, -0.3251, -1.7313, 1.8731, 2.4963, 1.1713, -1.5726, 1.5476, 3.9576, 0.7388, 0.0228, 0.3947, -1.7237, -1.8350, -2.0297, 1.4088, -1.3469, 1.6128, -1.0851, 2.0257, 0.5881, 0.7498, 0.0738, 2.0592, 1.8034, -0.5468, 1.9512, 0.4534, 0.7746, -1.0465, -0.7254, 0.3333, -1.6506, -0.4242, 1.9529, -0.4542, 0.2396, -1.6804, -2.7987, -0.6367, -0.3599, 1.0102, 2.6319, 0.8305, -1.4333, 3.3043, -0.4021, -0.4877, 0.9125, 0.0607, -1.0326, 1.3186, -2.5861, 0.1211, -2.3177, -1.5040, 1.0416, 1.4008, 1.4225, -2.7291]], grad_fn=<ThAddmmBackward>) sum([ 2.5686, -1.1964, -0.0872, -1.7010, -1.6669, -1.0638, 0.4515, 0.1124, 0.0166, 0.3156, 1.1699, 1.5374, 1.8720, 2.5184, 2.9046, -0.8241, -1.1949, -0.5700, 0.8692, -1.0485, 0.0390, -1.3783, -3.4632, -0.0143, 1.0986, 0.2667, -1.1127, -0.8515, 0.7759, -0.7528, 1.6366, -0.1170, -0.4983, -2.6970, 0.7545, 0.0188, 0.1094, 0.5002, 0.8838, -0.0006, -1.7993, -1.3706, 0.4964, -0.3251, -1.7313, 1.8731, 2.4963, 1.1713, -1.5726, 1.5476, 3.9576, 0.7388, 0.0228, 0.3947, -1.7237, -1.8350, -2.0297, 1.4088, -1.3469, 1.6128, -1.0851, 2.0257, 0.5881, 0.7498, 0.0738, 2.0592, 1.8034, -0.5468, 1.9512, 0.4534, 0.7746, -1.0465, -0.7254, 0.3333, -1.6506, -0.4242, 1.9529, -0.4542, 0.2396, -1.6804, -2.7987, -0.6367, -0.3599, 1.0102, 2.6319, 0.8305, -1.4333, 3.3043, -0.4021, -0.4877, 0.9125, 0.0607, -1.0326, 1.3186, -2.5861, 0.1211, -2.3177, -1.5040, 1.0416, 1.4008, 1.4225, -2.7291]) >>> 5.325799999999998 given this as how I test it on a single image (and the model as usual is trained and tested on batches it returns a prediction matrix that doesn't seem to be normalized or add up to 1. Is this normal?",|pytorch|vgg-net|,Model,0
53894900,"Why doesn't tensorflow on google deep learning VM use GPU?. I am using a google deep learning VM from google marketplace and I opted for a NvdiaK80 GPU. I am trying to train an object detection model using object detection API. However, I notice that tensorflow is not using GPU by default(code to check is below) My assumption here is that this instance comes with all the required NVIDIA drivers so it's not a driver related problem. Further investigation showed that I had 2 installations of Tensorflow (tensorflow 1.12.0 and tensorflow-GPU 1.12.0). So I uninstalled the CPU version. However it still does not help. I used the code below to check if tensorflow is using GPU from tensorflow.python.client import device_lib print(device_lib.list_local_devices()) For reference, I am using the below code for object detection training which is running fine on the deep learning VM but is not using GPU. python $Tensor_path/legacy/train.py --logtostderr -- train_dir=$Train_path/training/ -- pipeline_config_path=$Train_path/training/ ssd_inception_v2_pets.config Output(I would have expect the GPU device specifics that is being used) [name: ""/cpu:0"" device_type: ""CPU"" memory_limit: 268435456 locality { } incarnation: 18292259467280600161 ]",|tensorflow|gpu|google-dl-platform|,GPU Usage,3
53914450,"Problems with LSTM model. I try to realise LSTM model in PyTorch and got such problem: loss don't reduce. My task is so: I have sessions with different features. Session length is fixed and equals to 20. My goal is to predict will the last session been skipped or not. I tried to scale input features, I tried to pass target into features(maybe provided features are absolutely uninformative, I thought this should lead to overfitting and loss should be near 0), but always my loss reduction looks like this: print(X.shape) #(82770, 20, 31) where 82770 is count of sessions, 20 is seq_len, 31 is count of features print(y.shape) #(82770, 20) I defined also get_batches function. And yes, I know about problems with last batch in this generator def get_batches(X, y, batch_size): '''Create a generator that returns batches of size batch_size x seq_length from arr. ''' assert X.shape[0] == y.shape[0] assert X.shape[1] == y.shape[1] assert len(X.shape) == 3 assert len(y.shape) == 2 seq_len = X.shape[1] n_batches = X.shape[0]//seq_len for batch_number in range(n_batches): #print(batch_number*batch_size, ) batch_x = X[batch_number*batch_size:(batch_number+1)*batch_size, :, :] batch_y = y[batch_number*batch_size:(batch_number+1)*batch_size, :] if batch_x.shape[0] == batch_size: yield batch_x, batch_y else: print('batch_x shape: {}'.format(batch_x.shape)) break Here is my RNN class BaseRNN(nn.Module): def __init__(self, n_features, hidden_size, n_layers, drop_p=0.3, lr=0.001, last_items=10): super(BaseRNN, self).__init__() # constants self.n_features = n_features self.hidden_size = hidden_size self.n_layers = n_layers self.drop_p = drop_p self.lr = lr self.last_items = last_items # layers self.lstm = nn.LSTM( n_features, n_hidden, n_layers, dropout=drop_p, batch_first=True ) self.dropout = nn.Dropout(self.drop_p) self.linear_layer = nn.Linear(self.hidden_size, 1) self.sigm = nn.Sigmoid() def forward(self, x, hidden): out, hidden = self.lstm(x, hidden) batch_size = x.shape[0] out = self.dropout(out) out = out.contiguous().view(-1, self.hidden_size) out = self.linear_layer(out) out = self.sigm(out) # use only last elements out = out.view(batch_size, -1) out = out[:, -1] return out, hidden def init_hidden(self, batch_size): #initialize with zeros weight = next(self.parameters()).data hidden = (weight.new(self.n_layers, batch_size, self.hidden_size).zero_(), weight.new(self.n_layers, batch_size, self.hidden_size).zero_()) return hidden Here is my train function: def train(net, X, y, n_epochs=10, batch_size=10, clip=5): ''' pass ''' n_features = X.shape[2] seq_len = X.shape[1] net.train() opt = torch.optim.Adam(net.parameters(), lr=net.lr) criterion = nn.BCELoss() counter = 0 losses = [] for e in range(n_epochs): h = net.init_hidden(batch_size) for x, y in get_batches(X=X, y=y, batch_size=batch_size): counter += 1 h = net.init_hidden(batch_size) inputs, targets = torch.from_numpy(x).float(), torch.from_numpy(y.astype(int)) targets = targets[:,-net.last_items:].float().view(net.last_items*batch_size) h = tuple([each.data for each in h]) net.zero_grad() output, h = net(inputs, h) loss = criterion(output.view(net.last_items*batch_size), targets) losses.append(loss.item()) loss.backward() nn.utils.clip_grad_norm_(net.parameters(), clip) opt.step() return losses Run training: n_hidden = 100 n_layers = 1 n_features = X.shape[2] net = BaseRNN(n_features, n_hidden, n_layers, lr=0.01, drop_p=0.1, last_items=1) losses = train(net, X, y, n_epochs=5, batch_size=1000, lr=0.001, clip=5) plt.plot(losses) After all these steps I get plot like in the top of my question. I think I get a huge error somewhere because I put target variable in features, but still no loss reduction. Where I am wrong? PS.How to generate sample data? I will use real y data and add some noise. Y = np.array([[0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1], [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1], [0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0], [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1]]) print(Y.shape) #(10, 20) # add 5 features with random noise random_noise = np.random.randn(10*20*5).reshape(10,20,5) X = np.concatenate((Y.reshape(10,20,1), random_noise), axis=2) print(X.shape) #(10, 20, 6)",|python|python-3.x|lstm|pytorch|recurrent-neural-network|,Training,2
53972814,"CuDNNLSTM: Failed to call ThenRnnForward. I am facing an issue when trying to use CuDNNLSTM instead of keras.layers.LSTM. This is the error I am getting: Failed to call ThenRnnForward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, seq_length, batch_size]: [1, 300, 512, 1, 5521, 128] [[{{node bidirectional_1/CudnnRNN_1}} = CudnnRNN[T=DT_FLOAT, _class=[""loc:@train...NNBackprop""], direction=""unidirectional"", dropout=0, input_mode=""linear_input"", is_training=true, rnn_mode=""lstm"", seed=87654321, seed2=0, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](bidirectional_1/transpose_1, bidirectional_1/ExpandDims_1, bidirectional_1/ExpandDims_1, bidirectional_1/concat_1)]] [[{{node loss/mul/_75}} = _Recvclient_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_1209_loss/mul"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]] Also, I got this error in one of the runs: InternalError: GPU sync failed And the kernel kept dying after each run. I only started getting this error when I tried to run it on a VM instance on google cloud with CuDNNLSTM. my code is: MAX_LEN = max(len(article) for article in X_train_tokens) EMBEDDING_DIM=300 vocab_size = len(word_to_id) classes = 2 # Text input text_input = Input(shape=(MAX_LEN,)) embedding = Embedding(vocab_size, EMBEDDING_DIM, input_length=MAX_LEN)(text_input) x = Bidirectional(LSTM(512, return_sequences=False))(embedding) pred = Dense(2, activation='softmax')(x) model = Model(inputs=[text_input],outputs=pred) model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy']) batch_size = 128 generator = text_training_generator(batch_size) steps = len(X_train)/ batch_size model.fit_generator(generator, steps_per_epoch=steps, verbose=True, epochs=10) The model summary: _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) (None, 5521) 0 _________________________________________________________________ embedding_1 (Embedding) (None, 5521, 300) 8099100 _________________________________________________________________ bidirectional_1 (Bidirection (None, 1024) 3330048 _________________________________________________________________ dense_1 (Dense) (None, 2) 2050 ================================================================= Total params: 11,431,198 Trainable params: 11,431,198 Non-trainable params: 0 _________________________________________________________________",|tensorflow|keras|google-cloud-platform|gpu|lstm|,GPU Usage,3
54004948,"Consequences of Keras running out of memory. in case this question is off topic here, please feel free to refer to another StackExchange site. :-) I am working with Keras and have quite limited memory on my GPU (GeForce GTX 970, ~4G). So as a consequence I run out of memory (OOM) working with Keras having a batch size set above a certain level. Lowering the batch size I don't have this issue, but Keras outputs the following warnings: 2019-01-02 09:47:03.173259: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.57GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available. 2019-01-02 09:47:03.211139: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.68GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available. 2019-01-02 09:47:03.268074: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.95GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available. 2019-01-02 09:47:03.685032: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.39GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available. 2019-01-02 09:47:03.732304: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.56GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available. 2019-01-02 09:47:03.850711: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.39GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available. 2019-01-02 09:47:03.879135: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.48GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available. 2019-01-02 09:47:03.963522: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.42GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available. 2019-01-02 09:47:03.984897: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.47GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available. 2019-01-02 09:47:04.058733: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.08GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available. What do these warnings mean for me as a user? What are those performance gains? Does it mean that it simply computes faster or do I even get better results in terms of a better validation loss? In my setup I use Keras with Tensorflow backend and tensorflow-gpu==1.8.0.",|tensorflow|keras|out-of-memory|gpu|,GPU Usage,3
53976618,"Tensorflow can find right cudnn in one python file but fail in another. I am trying to use tensorflow gpu version to train and test my deep learning model. But here comes the problem. When I train my model in one python file things go on well. Tensorflow-gpu can be used properly. Then I save my model as a pretrained on as grapg.pb format and try to reuse it in another python file. Then I got the following error messages. E tensorflow/stream_executor/cuda/cuda_dnn.cc:363] Loaded runtime CuDNN library: 7.1.4 but source was compiled with: 7.2.1. CuDNN library major and minor version needs to match or have higher minor version in case of CuDNN 7.0 or later version. If using a binary install, upgrade your CuDNN library. If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration. I checked my cudnn version, in fact it is version 7.4.2. I also checked my environment path settings /cuda/v9.0/bin, cuda/v9.0/lib/x64, /cuda/v9.0/include are in there. So why this happens? And how can I solve this? -- cuda:v9.0 cudnn:7.4.2 (I think, I copy those cudnn files manually) windows 10 python: 3.5",|tensorflow|gpu|nvidia|,GPU Usage,3
54054163,"Index pytorch tensor with different dimension index array. I have the following function, which does what I want using numpy.array, but breaks when feeding a torch.Tensor due to indexing errors. import torch import numpy as np def combination_matrix(arr): idxs = np.arange(len(arr)) idx = np.ix_(idxs, idxs) mesh = np.stack(np.meshgrid(idxs, idxs)) def np_combination_matrix(): output = np.zeros((len(arr), len(arr), 2, *arr.shape[1:]), dtype=arr.dtype) num_dims = len(output.shape) output[idx] = arr[mesh].transpose((2, 1, 0, *np.arange(3, num_dims))) return output def torch_combination_matrix(): output = torch.zeros(len(arr), len(arr), 2, *arr.shape[1:], dtype=arr.dtype) num_dims = len(output.shape) print(arr[mesh].shape) # <-- This is wrong/different to numpy! output[idx] = arr[mesh].permute(2, 1, 0, *np.arange(3, num_dims)) return output if isinstance(arr, np.ndarray): return np_combination_matrix() elif isinstance(arr, torch.Tensor): return torch_combination_matrix() The problem is that arr[mesh] results in different dimensions, depending on numpy and torch. Apparently, pytorch does not support indexing with index arrays of different dimensionality than the array being indexed. Ideally, the following should work: features = np.arange(9).reshape(3, 3) np_combs = combination_matrix(features) features = torch.from_numpy(features) torch_combs = combination_matrix(features) assert np.array_equal(np_combs, torch_combs.numpy()) But the dimensions are different: (2, 3, 3, 3) torch.Size([3, 3]) Which results in an error (logically): Traceback (most recent call last): File ""/home/XXX/util.py"", line 226, in <module> torch_combs = combination_matrix(features) File ""/home/XXX/util.py"", line 218, in combination_matrix return torch_combination_matrix() File ""/home/XXX/util.py"", line 212, in torch_combination_matrix output[idx] = arr[mesh].permute(2, 1, 0, *np.arange(3, num_dims)) RuntimeError: number of dims don't match in permute How do I match the torch behavior to numpy? I've read various questions on the torch forums (e.g. this one with only one dimension), but could find how to apply this here. Similarly, index_select only works for one dimension, but I need it to work for at least 2 dimensions.",|python|numpy|indexing|pytorch|,Tensors&Inputs,1
54064299,"CNN Keras Object Localization - Bad predictions. I'm a beginner in machine learning and I currently am trying to predict the position of an object within an image that is part of a dataset I created. This dataset contains about 300 images in total and contains 2 classes (Ace and Two). I created a CNN that predicts whether it's an Ace or a two with about 88% accuracy. Since this dataset was doing a great job, I decided to try and predict the position of the card (instead of the class). I read up some articles and from what I understood, all I had to do was to take the same CNN that I used to predict the class and to change the last layer for a Dense layer of 4 nodes. That's what I did, but apparently this isn't working. Here is my model: model = Sequential() model.add(Conv2D(64,(3,3),input_shape = (150,150,1))) model.add(Activation(""relu"")) model.add(MaxPooling2D(pool_size=2)) model.add(Conv2D(32,(3,3))) model.add(Activation(""relu"")) model.add(MaxPooling2D(pool_size=2)) model.add(Dense(64)) model.add(Activation(""relu"")) model.add(Flatten()) model.add(Dense(4)) model.compile(loss=""mean_squared_error"",optimizer='adam',metrics=[]) model.fit(X,y,batch_size=1,validation_split=0, epochs=30,verbose=1,callbacks=[TENSOR_BOARD]) What I feed to my model: X: a grayscale Image of 150x150 pixels. Each pixels are rescaled between [0-1] y: Smallest X coordinate, Highest Y coordinate, Width and Height of the object (each of those values are between [0-1]. And here's an example of predictions it gives me: [array([ 28.66145 , 41.278576, -9.568813, -13.520659], dtype=float32)] but what I really wanted was: [0.32, 0.38666666666666666, 0.4, 0.43333333333333335] I knew something was wrong here so I decided to train and test my CNN on a single image (so it should overfit and predict the right bounding box for this single image if it worked). Even after overfitting on this single image, the predicted values were ridiculously high. So my question is: What am I doing wrong ? EDIT 1 After trying @Matias's solution which was to add a sigmoid activation function to the last layer, all of the output's values are now between [0,1]. But, even with this, the model still produces bad outputs. For example, after training it 10 epochs on the same image, it predicted this: [array([0.0000000e+00, 0.0000000e+00, 8.4378130e-18, 4.2288357e-07],dtype=float32)] but what I expected was: [0.2866666666666667, 0.31333333333333335, 0.44666666666666666, 0.5] EDIT 2 Okay, so, after experimenting for quite a while, I've come to a conclusion that the problem was either my model (the way it is built) or the lack of training data. But even if it was caused by a lack of training data, I should have been able to overfit it on 1 image in order to get the right predictions for this one, right? I created another post which asks about my last question since the original one has been answered and I don't want to completely re-edit the post since it would make the first answers kind of pointless.",|tensorflow|machine-learning|keras|conv-neural-network|object-detection|,Model,0
54196469,"Tensorflow predict the class of output. I have tried the example with keras but was not with LSTM. My model is with LSTM in Tensorflow and I am willing to predict the output in the form of classes as the keras model thus with predict_classes. The Tensorflow model I am trying is something like this: seq_len=10 n_steps = seq_len-1 n_inputs = x_train.shape[2] n_neurons = 50 n_outputs = y_train.shape[1] n_layers = 2 learning_rate = 0.0001 batch_size =100 n_epochs = 1000 train_set_size = x_train.shape[0] test_set_size = x_test.shape[0] tf.reset_default_graph() X = tf.placeholder(tf.float32, [None, n_steps, n_inputs]) y = tf.placeholder(tf.float32, [None, n_outputs]) layers = [tf.contrib.rnn.LSTMCell(num_units=n_neurons,activation=tf.nn.sigmoid, use_peepholes = True) for layer in range(n_layers)] multi_layer_cell = tf.contrib.rnn.MultiRNNCell(layers) rnn_outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32) stacked_rnn_outputs = tf.reshape(rnn_outputs, [-1, n_neurons]) stacked_outputs = tf.layers.dense(stacked_rnn_outputs, n_outputs) outputs = tf.reshape(stacked_outputs, [-1, n_steps, n_outputs]) outputs = outputs[:,n_steps-1,:] loss = tf.reduce_mean(tf.square(outputs - y)) optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) training_op = optimizer.minimize(loss) I am encoding the with sklearn LabelEncoder as: encoder_train = LabelEncoder() encoder_train.fit(y_train) encoded_Y_train = encoder_train.transform(y_train) y_train = np_utils.to_categorical(encoded_Y_train) The data is converted to sparse matrix kinda thing in binary format. When I tried to predict the output I got the following: actual==> [[0. 0. 1.] [1. 0. 0.] [1. 0. 0.] [0. 0. 1.] [1. 0. 0.] [1. 0. 0.] [1. 0. 0.] [0. 1. 0.] [0. 1. 0.]] predicted==> [[0.3112209 0.3690182 0.31357136] [0.31085992 0.36959863 0.31448898] [0.31073445 0.3703295 0.31469804] [0.31177694 0.37011752 0.3145326 ] [0.31220382 0.3692756 0.31515726] [0.31232828 0.36947766 0.3149037 ] [0.31190437 0.36756667 0.31323162] [0.31339088 0.36542615 0.310322 ] [0.31598282 0.36328828 0.30711085]] What I was expecting for the label based on the encoding done. As the Keras model thus. See the following: predictions = model.predict_classes(X_test, verbose=True) print(""REAL VALUES:"",reverse_category(Y_test,axis=1)) print(""PRED VALUES:"",predictions) print(""REAL COLORS:"") print(encoder.inverse_transform(reverse_category(Y_test,axis=1))) print(""PREDICTED COLORS:"") print(encoder.inverse_transform(predictions)) The output is something like the following: REAL VALUES: [1 1 1 ... 1 2 1] PRED VALUES: [2 1 1 ... 1 2 2] REAL COLORS: ['ball' 'ball' 'ball' ... 'ball' 'bat' 'ball'] PREDICTED COLORS: ['bat' 'ball' 'ball' ... 'ball' 'bat' 'bat'] Kindly, let me know what I can do in the tensorflow model that will get me the result with respect to the encoding done. I am using Tensorflow 1.12.0 and Windows 10",|python|python-3.x|tensorflow|keras|categorical-data|,Training,2
54194884,"Tensorflow segmentation fault with single machine multiple GPUs training. Recently, I am trying to learn how to use Tensorflow to do the data parallel training and I found a toy example here https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/6_MultiGPU/multigpu_cnn.py. However, I cannot run this example successfully and I got the following error. WARNING:tensorflow:From /usr/local/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version. Instructions for updating: Please use alternatives such as official/mnist/dataset.py from tensorflow/models. WARNING:tensorflow:From test_tensorflow.py:138: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version. Instructions for updating: Future major versions of TensorFlow will allow gradients to flow into the labels input on backprop by default. See @{tf.nn.softmax_cross_entropy_with_logits_v2}. 2019-01-15 16:08:51.603247: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA 2019-01-15 16:09:01.674855: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties: name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.545 pciBusID: 0000:1b:00.0 totalMemory: 10.73GiB freeMemory: 10.53GiB 2019-01-15 16:09:01.971847: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 1 with properties: name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.545 pciBusID: 0000:1c:00.0 totalMemory: 10.73GiB freeMemory: 10.53GiB 2019-01-15 16:09:01.972036: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0, 1 2019-01-15 16:09:02.728988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix: 2019-01-15 16:09:02.729020: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0 1 2019-01-15 16:09:02.729025: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0: N N 2019-01-15 16:09:02.729027: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 1: N N 2019-01-15 16:09:02.729458: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10166 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:1b:00.0, compute capability: 7.5) 2019-01-15 16:09:02.875709: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10166 MB memory) -> physical GPU (device: 1, name: GeForce RTX 2080 Ti, pci bus id: 0000:1c:00.0, compute capability: 7.5) Segmentation fault (core dumped) I believe that what I really do is to collect and average gradients from multiple GPUs and then, update the parameters in my model. I am not sure what causes this problem. The complete code is as the following, which is the official code. from __future__ import division, print_function, absolute_import import numpy as np import tensorflow as tf import time # Import MNIST data from tensorflow.examples.tutorials.mnist import input_data mnist = input_data.read_data_sets(""/tmp/data/"", one_hot=True) # Training Parameters num_gpus = 2 num_steps = 200 learning_rate = 0.001 batch_size = 1024 display_step = 10 # Network Parameters num_input = 784 # MNIST data input (img shape: 28*28) num_classes = 10 # MNIST total classes (0-9 digits) dropout = 0.75 # Dropout, probability to keep units # Build a convolutional neural network def conv_net(x, n_classes, dropout, reuse, is_training): # Define a scope for reusing the variables with tf.variable_scope('ConvNet', reuse=reuse): # MNIST data input is a 1-D vector of 784 features (28*28 pixels) # Reshape to match picture format [Height x Width x Channel] # Tensor input become 4-D: [Batch Size, Height, Width, Channel] x = tf.reshape(x, shape=[-1, 28, 28, 1]) # Convolution Layer with 64 filters and a kernel size of 5 x = tf.layers.conv2d(x, 64, 5, activation=tf.nn.relu) # Max Pooling (down-sampling) with strides of 2 and kernel size of 2 x = tf.layers.max_pooling2d(x, 2, 2) # Convolution Layer with 256 filters and a kernel size of 5 x = tf.layers.conv2d(x, 256, 3, activation=tf.nn.relu) # Convolution Layer with 512 filters and a kernel size of 5 x = tf.layers.conv2d(x, 512, 3, activation=tf.nn.relu) # Max Pooling (down-sampling) with strides of 2 and kernel size of 2 x = tf.layers.max_pooling2d(x, 2, 2) # Flatten the data to a 1-D vector for the fully connected layer x = tf.contrib.layers.flatten(x) # Fully connected layer (in contrib folder for now) x = tf.layers.dense(x, 2048) # Apply Dropout (if is_training is False, dropout is not applied) x = tf.layers.dropout(x, rate=dropout, training=is_training) # Fully connected layer (in contrib folder for now) x = tf.layers.dense(x, 1024) # Apply Dropout (if is_training is False, dropout is not applied) x = tf.layers.dropout(x, rate=dropout, training=is_training) # Output layer, class prediction out = tf.layers.dense(x, n_classes) # Because 'softmax_cross_entropy_with_logits' loss already apply # softmax, we only apply softmax to testing network out = tf.nn.softmax(out) if not is_training else out return out def average_gradients(tower_grads): average_grads = [] for grad_and_vars in zip(*tower_grads): # Note that each grad_and_vars looks like the following: # ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN)) grads = [] for g, _ in grad_and_vars: # Add 0 dimension to the gradients to represent the tower. expanded_g = tf.expand_dims(g, 0) # Append on a 'tower' dimension which we will average over below. grads.append(expanded_g) # Average over the 'tower' dimension. grad = tf.concat(grads, 0) grad = tf.reduce_mean(grad, 0) # Keep in mind that the Variables are redundant because they are shared # across towers. So .. we will just return the first tower's pointer to # the Variable. v = grad_and_vars[0][1] grad_and_var = (grad, v) average_grads.append(grad_and_var) return average_grads # By default, all variables will be placed on '/gpu:0' # So we need a custom device function, to assign all variables to '/cpu:0' # Note: If GPUs are peered, '/gpu:0' can be a faster option PS_OPS = ['Variable', 'VariableV2', 'AutoReloadVariable'] def assign_to_device(device, ps_device='/cpu:0'): def _assign(op): node_def = op if isinstance(op, tf.NodeDef) else op.node_def if node_def.op in PS_OPS: return ""/"" + ps_device else: return device return _assign # Place all ops on CPU by default with tf.device('/cpu:0'): tower_grads = [] reuse_vars = False # tf Graph input X = tf.placeholder(tf.float32, [None, num_input]) Y = tf.placeholder(tf.float32, [None, num_classes]) # Loop over all GPUs and construct their own computation graph for i in range(num_gpus): with tf.device(assign_to_device('/gpu:{}'.format(i), ps_device='/cpu:0')): # Split data between GPUs _x = X[i * batch_size: (i+1) * batch_size] _y = Y[i * batch_size: (i+1) * batch_size] # Because Dropout have different behavior at training and prediction time, we # need to create 2 distinct computation graphs that share the same weights. # Create a graph for training logits_train = conv_net(_x, num_classes, dropout, reuse=reuse_vars, is_training=True) # Create another graph for testing that reuse the same weights logits_test = conv_net(_x, num_classes, dropout, reuse=True, is_training=False) # Define loss and optimizer (with train logits, for dropout to take effect) loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits( logits=logits_train, labels=_y)) optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) grads = optimizer.compute_gradients(loss_op) # Only first GPU compute accuracy if i == 0: # Evaluate model (with test logits, for dropout to be disabled) correct_pred = tf.equal(tf.argmax(logits_test, 1), tf.argmax(_y, 1)) accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32)) reuse_vars = True tower_grads.append(grads) tower_grads = average_gradients(tower_grads) train_op = optimizer.apply_gradients(tower_grads) # Initialize the variables (i.e. assign their default value) init = tf.global_variables_initializer() # Start Training with tf.Session() as sess: # Run the initializer sess.run(init) # Keep training until reach max iterations for step in range(1, num_steps + 1): # Get a batch for each GPU batch_x, batch_y = mnist.train.next_batch(batch_size * num_gpus) # Run optimization op (backprop) ts = time.time() sess.run(train_op, feed_dict={X: batch_x, Y: batch_y}) te = time.time() - ts if step % display_step == 0 or step == 1: # Calculate batch loss and accuracy loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x, Y: batch_y}) print(""Step "" + str(step) + "": Minibatch Loss= "" + \ ""{:.4f}"".format(loss) + "", Training Accuracy= "" + \ ""{:.3f}"".format(acc) + "", %i Examples/sec"" % int(len(batch_x)/te)) step += 1 print(""Optimization Finished!"") # Calculate accuracy for MNIST test images print(""Testing Accuracy:"", \ np.mean([sess.run(accuracy, feed_dict={X: mnist.test.images[i:i+batch_size], Y: mnist.test.labels[i:i+batch_size]}) for i in range(0, len(mnist.test.images), batch_size)]))",|python|tensorflow|gpu|,GPU Usage,3
54255431,"InvalidArgumentError: cannot compute MatMul as input #0(zero-based) was expected to be a float tensor but is a double tensor [Op:MatMul]. Can somebody explain, how does TensorFlow's eager mode work? I am trying to build a simple regression as follows: import tensorflow as tf import numpy as np tfe = tf.contrib.eager tf.enable_eager_execution() def make_model(): net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(4, activation='relu')) net.add(tf.keras.layers.Dense(1)) return net def compute_loss(pred, actual): return tf.reduce_mean(tf.square(tf.subtract(pred, actual))) def compute_gradient(model, pred, actual): """"""compute gradients with given noise and input"""""" with tf.GradientTape() as tape: loss = compute_loss(pred, actual) grads = tape.gradient(loss, model.variables) return grads, loss def apply_gradients(optimizer, grads, model_vars): optimizer.apply_gradients(zip(grads, model_vars)) model = make_model() optimizer = tf.train.AdamOptimizer(1e-4) x = np.linspace(0,1,1000) y = x + np.random.normal(0,0.3,1000) y = y.astype('float32') train_dataset = tf.data.Dataset.from_tensor_slices((y.reshape(-1,1))) epochs = 2# 10 batch_size = 25 itr = y.shape[0] # batch_size for epoch in range(epochs): for data in tf.contrib.eager.Iterator(train_dataset.batch(25)): preds = model(data) grads, loss = compute_gradient(model, preds, data) apply_gradients(optimizer, grads, model.variables) # Gradient output: [None, None, None, None, None, None] The error is following: ---------------------------------------------------------------------- ValueError Traceback (most recent call last) <ipython-input-3-a589b9123c80> in <module> 35 grads, loss = compute_gradient(model, preds, data) 36 print(grads) ---> 37 apply_gradients(optimizer, grads, model.variables) 38 # with tf.GradientTape() as tape: 39 # loss = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(preds, data)))) <ipython-input-3-a589b9123c80> in apply_gradients(optimizer, grads, model_vars) 17 18 def apply_gradients(optimizer, grads, model_vars): ---> 19 optimizer.apply_gradients(zip(grads, model_vars)) 20 21 model = make_model() ~/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py in apply_gradients(self, grads_and_vars, global_step, name) 589 if not var_list: 590 raise ValueError(""No gradients provided for any variable: %s."" % --> 591 ([str(v) for _, v, _ in converted_grads_and_vars],)) 592 with ops.init_scope(): 593 self._create_slots(var_list) ValueError: No gradients provided for any variable: Edit I updated my code. Now, the problem comes in gradients calculation, it is returning zero. I have checked the loss value that is non-zero.",|python|tensorflow|machine-learning|keras|eager-execution|,Training,2
54233276,"What is the reason that TensorFlow does not detect GPU on Windows. I have installed CUDA 9.0 on my machine which has the NVIDIA GTX 1080 graphics cards. When I run the command nvcc --version then I get: nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2017 NVIDIA Corporation Built on Fri_Sep__1_21:08:32_Central_Daylight_Time_2017 Cuda compilation tools, release 9.0, V9.0.176 But I have tried the steps from the TensorFlow official site to install TF with GPU support, but it still using the CPU. I have tried pip install and Anaconda install, all was the same result. No one was able to detect GPU, then I have tried many other tutorials on the web, which they were able to detect the GPU, but I am not. What can be the reason, is there any changing in the new GPU version of TF? If yes, then what is the latest documentation to install TF with GPU support, if not, then where I am doing wrong. Thanks! Update1: Tensorflow really wastes my time. Very annoying, at the first I decided to build TF from source, to use it with CUDA 10, but on both OS Windows 10 and Ubuntu 18.04 I was unable to build it successfully. So I gave up, then I decided to use with CUDA 9.0, which is not supported in Ubuntu 18.04, so I came back to windows, but even still the prebuilt library of TF not working, really annoying. I don't know why TF still using CUDA 9.0 which CUDA 10.0 already officially released, and TF still not supporting Python 3.7? amazing not? and the same thing with MS Build Tools 2015, which 2017 already exist, and many more tools. TF relays on old versions of the tools which make a lot of problem for some people that they must uninstall their new versions which still using, it is very annoying... Update2: nvidia-smi output: +-----------------------------------------------------------------------------+ | NVIDIA-SMI 417.71 Driver Version: 417.71 CUDA Version: 9.0 | |-------------------------------+----------------------+----------------------+ | GPU Name TCC/WDDM | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 1080 WDDM | 00000000:01:00.0 On | N/A | | 27% 35C P8 8W / 180W | 498MiB / 8192MiB | 1% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1264 C+G Insufficient Permissions N/A | | 0 2148 C+G ...0108.0_x64__8wekyb3d8bbwe\HxOutlook.exe N/A | | 0 4360 C+G ...mmersiveControlPanel\SystemSettings.exe N/A | | 0 7332 C+G C:\Windows\explorer.exe N/A | | 0 7384 C+G ...t_cw5n1h2txyewy\ShellExperienceHost.exe N/A | | 0 8488 C+G ...dows.Cortana_cw5n1h2txyewy\SearchUI.exe N/A | | 0 9704 C+G ...osoft.LockApp_cw5n1h2txyewy\LockApp.exe N/A | | 0 10588 C+G ...al\Google\Chrome\Application\chrome.exe N/A | | 0 10904 C+G ...x64__8wekyb3d8bbwe\Microsoft.Photos.exe N/A | | 0 12608 C+G ...DIA GeForce Experience\NVIDIA Share.exe N/A | | 0 13000 C+G ...241.0_x64__8wekyb3d8bbwe\Calculator.exe N/A | | 0 14668 C+G ...ng4wbp0\app\DellMobileConnectClient.exe N/A | | 0 17628 C+G ...2.0_x64__8wekyb3d8bbwe\WinStore.App.exe N/A | | 0 18060 C+G ...oftEdge_8wekyb3d8bbwe\MicrosoftEdge.exe N/A | +-----------------------------------------------------------------------------+",|tensorflow|gpu|,GPU Usage,3
54282951,"single layer net in keras with imagedatagenerator, but loss is always negative. I have tried many kinds of net, but even in basic net(single layer), loss which set as binary_crossentropy is always negative here is the code from __future__ import print_function import numpy as np import keras from keras.preprocessing.image import ImageDataGenerator from keras.models import Sequential from keras.layers import Dense, Dropout, Activation, Flatten from keras.layers import Conv2D, MaxPooling2D import os import cv2 from PIL import ImageFile ImageFile.LOAD_TRUNCATED_IMAGES = True train_path = 'D:/rectangle' val_path = 'D:/rectang' model = Sequential() model.add(Conv2D(32, 1, 1, input_shape=(230, 230, 3))) model.add(Flatten()) model.add(Dense(64)) model.add(Dropout(0.5)) model.add(Dense(1)) model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy']) train_datagen = ImageDataGenerator( samplewise_center=True, samplewise_std_normalization=True) test_datagen = ImageDataGenerator() train_generator = train_datagen.flow_from_directory( train_path, target_size=(230, 230), batch_size=32, class_mode='binary') validation_generator = test_datagen.flow_from_directory( val_path, target_size=(230, 230), batch_size=32, class_mode='binary') model.fit_generator( train_generator, steps_per_epoch=200, epochs=50, validation_data=validation_generator, nb_val_samples=800 ) here is the processing: 1/200 [..............................] - ETA: 20:17 - loss: 12.9030 - acc: 0.1250 2/200 [..............................] - ETA: 10:22 - loss: -2.0179 - acc: 0.0625 3/200 [..............................] - ETA: 7:03 - loss: -6.3273 - acc: 0.0417 4/200 [..............................] - ETA: 5:23 - loss: -7.8592 - acc: 0.0312 5/200 [..............................] - ETA: 4:24 - loss: -8.6776 - acc: 0.0250 6/200 [..............................] - ETA: 3:44 - loss: -9.5563 - acc: 0.0208 7/200 [>.............................] - ETA: 3:15 - loss: -9.3298 - acc: 0.0179 8/200 [>.............................] - ETA: 2:54 - loss: -9.3455 - acc: 0.0156 9/200 [>.............................] - ETA: 2:37 - loss: -10.2439 - acc: 0.0139 10/200 [>.............................] - ETA: 2:24 - loss: -10.5647 - acc: 0.0125 11/200 [>.............................] - ETA: 2:13 - loss: -10.8719 - acc: 0.0114 12/200 [>.............................] - ETA: 2:04 - loss: -11.3775 - acc: 0.0104 13/200 [>.............................] - ETA: 1:56 - loss: -11.3066 - acc: 0.0096 14/200 [=>............................] - ETA: 1:49 - loss: -11.4598 - acc: 0.0089 15/200 [=>............................] - ETA: 1:48 - loss: -11.4930 - acc: 0.0083 16/200 [=>............................] - ETA: 1:47 - loss: -11.6465 - acc: 0.0078 17/200 [=>............................] - ETA: 1:51 - loss: -11.6061 - acc: 0.0074 the input image is the photo of breast cancer hispological images, with 460*460 size and 20000 pics in PNG format. I would appreciate it if it will be solved!",|image|keras|neural-network|deep-learning|loss-function|,Model,0
54250552,"AttributeError: 'tuple' object has no attribute 'ndims', while using tensorflow eager execution mode. I am using tf eager mode, and trying to create a GAN model. To made this, i created a class as follows. I tried sending array specificly, found in keras issues, but that also didn't worked? class vanillaGAN(tf.keras.Model): """"""Vanilla GAN"""""" def __init__(self, noise_dims, input_dims): """"""Define all layer used in network"""""" super(vanillaGAN, self).__init__() self.disc1 = tf.keras.layers.Dense(128, activation='relu') self.disc2 = tf.keras.layers.Dense(1)#, activation='sigmoid') self.gen1 = tf.keras.layers.Dense(128, activation='relu') self.gen2 = tf.keras.layers.Dense(784)#, activation='sigmoid') def gen_forward(self, x): """"""Forward Pass for Generator"""""" x = self.gen1(x) x = self.gen2(x) return x def dis_forward(self, x): """"""Forward Pass for Discriminator"""""" x = self.disc1(x) x = self.disc2(x) return x Now, on using following script: def sample(batch_size, dims): return np.random.uniform(size=(batch_size, dims)) gan = vanillaGAN(noise_dims=40, input_dims=784) noise = sample(32,40) #gan.gen_forward(np.array(noise)) gan.gen_forward(noise)} I am getting following error ---------------------------------------------------------------------- AttributeError Traceback (most recent call last) <ipython-input-43-11c01bb2233d> in <module> 1 noise = sample(32,40) ----> 2 gan.gen_forward(np.array(noise)) <ipython-input-20-22ce18fda8ff> in gen_forward(self, x) 12 def gen_forward(self, x): 13 """"""Forward Pass for Generator"""""" ---> 14 x = self.gen1(x) 15 x = self.gen2(x) 16 return x ~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs) 728 729 # Check input assumptions set before layer building, e.g. input rank. --> 730 self._assert_input_compatibility(inputs) 731 if input_list and self._dtype is None: 732 try: ~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py in _assert_input_compatibility(self, inputs) 1461 spec.min_ndim is not None or 1462 spec.max_ndim is not None): -> 1463 if x.shape.ndims is None: 1464 raise ValueError('Input ' + str(input_index) + ' of layer ' + 1465 self.name + ' is incompatible with the layer: ' AttributeError: 'tuple' object has no attribute 'ndims' please, if someone can help.",|python|tensorflow|deep-learning|attributeerror|,API,4
54417736,"PyTorch runtime error : invalid argument 0: Sizes of tensors must match except in dimension 1. I have a PyTorch model and I'm trying to test it by performing a forward pass. Here is the code: class ResBlock(nn.Module): def __init__(self, inplanes, planes, stride=1): super(ResBlock, self).__init__() self.conv1x1 = nn.Conv2d(inplanes, planes, kernel_size=1, stride=1, bias=False) self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False) #batch normalization self.bn1 = nn.BatchNorm2d(planes) self.relu = nn.ReLU() self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False) self.bn2 = nn.BatchNorm2d(planes) self.stride = stride def forward(self, x): residual = self.conv1x1(x) out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) #adding the skip connection out += residual out = self.relu(out) return out class ResUnet (nn.Module): def __init__(self, in_shape, num_classes): super(ResUnet, self).__init__() in_channels, height, width = in_shape # #self.L1 = IncResBlock(in_channels,64) self.e1 = nn.Sequential( nn.Conv2d(in_channels, 64, kernel_size=4, stride=2,padding=1), ResBlock(64,64)) self.e2 = nn.Sequential( nn.LeakyReLU(0.2,), nn.Conv2d(64, 128, kernel_size=4, stride=2,padding=1), nn.BatchNorm2d(128), ResBlock(128,128)) # self.e2add = nn.Sequential( nn.Conv2d(128, 128, kernel_size=3, stride=1,padding=1), nn.BatchNorm2d(128)) # ## self.e3 = nn.Sequential( nn.LeakyReLU(0.2,inplace=True), nn.Conv2d(128, 128, kernel_size=3, stride=1,padding=1), nn.BatchNorm2d(128), nn.LeakyReLU(0.2,), nn.Conv2d(128,256, kernel_size=4, stride=2,padding=1), nn.BatchNorm2d(256), ResBlock(256,256)) self.e4 = nn.Sequential( nn.LeakyReLU(0.2,), nn.Conv2d(256,512, kernel_size=4, stride=2,padding=1), nn.BatchNorm2d(512), ResBlock(512,512)) # self.e4add = nn.Sequential( nn.Conv2d(512,512, kernel_size=3, stride=1,padding=1), nn.BatchNorm2d(512)) # self.e5 = nn.Sequential( nn.LeakyReLU(0.2,inplace=True), nn.Conv2d(512,512, kernel_size=3, stride=1,padding=1), nn.BatchNorm2d(512), nn.LeakyReLU(0.2,), nn.Conv2d(512,512, kernel_size=4, stride=2,padding=1), nn.BatchNorm2d(512), ResBlock(512,512)) # # self.e6 = nn.Sequential( nn.LeakyReLU(0.2,), nn.Conv2d(512,512, kernel_size=4, stride=2,padding=1), nn.BatchNorm2d(512), ResBlock(512,512)) # self.e6add = nn.Sequential( nn.Conv2d(512,512, kernel_size=3, stride=1,padding=1), nn.BatchNorm2d(512)) # self.e7 = nn.Sequential( nn.LeakyReLU(0.2,inplace=True), nn.Conv2d(512,512, kernel_size=3, stride=1,padding=1), nn.BatchNorm2d(512), nn.LeakyReLU(0.2,), nn.Conv2d(512,512, kernel_size=4, stride=2,padding=1), nn.BatchNorm2d(512), ResBlock(512,512)) # self.e8 = nn.Sequential( nn.LeakyReLU(0.2,), nn.Conv2d(512,512, kernel_size=4, stride=2,padding=1)) #nn.BatchNorm2d(512)) self.d1 = nn.Sequential( nn.ReLU(), nn.ConvTranspose2d(512, 512, kernel_size=4, stride=2,padding=1), nn.BatchNorm2d(512), nn.Dropout(p=0.5), ResBlock(512,512)) # self.d2 = nn.Sequential( nn.ReLU(), nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2,padding=1), nn.BatchNorm2d(512), nn.Dropout(p=0.5), ResBlock(512,512)) # self.d3 = nn.Sequential( nn.ReLU(), nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2,padding=1), nn.BatchNorm2d(512), nn.Dropout(p=0.5), ResBlock(512,512)) # self.d4 = nn.Sequential( nn.ReLU(), nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2,padding=1), nn.BatchNorm2d(512), ResBlock(512,512)) # self.d5 = nn.Sequential( nn.ReLU(), nn.ConvTranspose2d(1024, 256, kernel_size=4, stride=2,padding=1), nn.BatchNorm2d(256), ResBlock(256,256)) # self.d6 = nn.Sequential( nn.ReLU(), nn.ConvTranspose2d(512, 128, kernel_size=4, stride=2,padding=1), nn.BatchNorm2d(128), ResBlock(128,128)) # self.d7 = nn.Sequential( nn.ReLU(), nn.ConvTranspose2d(256, 64, kernel_size=4, stride=2,padding=1), nn.BatchNorm2d(64), ResBlock(64,64)) # self.d8 = nn.Sequential( nn.ReLU(), nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2,padding=1)) #nn.BatchNorm2d(64), #nn.ReLU()) self.out_l = nn.Sequential( nn.Conv2d(64,num_classes,kernel_size=1,stride=1)) #nn.ReLU()) def forward(self, x): #Image Encoder #### Encoder ##### en1 = self.e1(x) en2 = self.e2(en1) en2add = self.e2add(en2) en3 = self.e3(en2add) en4 = self.e4(en3) en4add = self.e4add(en4) en5 = self.e5(en4add) en6 = self.e6(en5) en6add = self.e6add(en6) en7 = self.e7(en6add) en8 = self.e8(en7) #### Decoder #### de1_ = self.d1(en8) de1 = torch.cat([en7,de1_],1) de2_ = self.d2(de1) de2 = torch.cat([en6add,de2_],1) de3_ = self.d3(de2) de3 = torch.cat([en5,de3_],1) de4_ = self.d4(de3) de4 = torch.cat([en4add,de4_],1) de5_ = self.d5(de4) de5 = torch.cat([en3,de5_],1) de6_ = self.d6(de5) de6 = torch.cat([en2add,de6_],1) de7_ = self.d7(de6) de7 = torch.cat([en1,de7_],1) de8 = self.d8(de7) out_l_mask = self.out_l(de8) return out_l_mask Here is how I attempt to test it: modl = ResUnet((1,512,512), 1) x = torch.rand(1, 1, 512, 512) modl(x) This works fine, as does for any size that are multiples of 64. If I try: modl = ResUnet((1,320,320), 1) x = torch.rand(1, 1, 320, 320) modl(x) It throws an error --------------------------------------------------------------------------- RuntimeError Traceback (most recent call last) <ipython-input-46-4ddc821c365b> in <module> ----> 1 modl(x) ~/.conda/envs/torch0.4/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs) 475 result = self._slow_forward(*input, **kwargs) 476 else: --> 477 result = self.forward(*input, **kwargs) 478 for hook in self._forward_hooks.values(): 479 hook_result = hook(self, input, result) <ipython-input-36-f9eeefa3c0b8> in forward(self, x) 221 de2_ = self.d2(de1) 222 #print de2_.size() --> 223 de2 = torch.cat([en6add,de2_],1) 224 #print de2.size() 225 RuntimeError: invalid argument 0: Sizes of tensors must match except in dimension 1. Got 5 and 4 in dimension 2 at /opt/conda/conda-bld/pytorch_1535491974311/work/aten/src/TH/generic/THTensorMath.cpp:3616 I figure the problem is caused by the input size not being a power of 2 but I am not sure how to rectify it for the given input dimenstions (320, 320).",|deep-learning|pytorch|,Tensors&Inputs,1
54479547,"Can we use pytorch scatter_ on GPU. I'm trying to do one hot encoding on some data with pyTorch on GPU mode, however, it keeps giving me an exception. Can anybody help me? Here's one example: def char_OneHotEncoding(x): coded = torch.zeros(x.shape[0], x.shape[1], 101) for i in range(x.shape[1]): coded[:,i] = scatter(x[:,i]) return coded def scatter(x): return torch.zeros(x.shape[0], 101).scatter_(1, x.view(-1,1), 1) So if I give it an tensor on GPU, it shows like this: x_train = [[ 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0], [14, 13, 83, 18, 14], [ 0, 0, 0, 0, 0]] print(char_OneHotEncoding(torch.tensor(x_train, dtype=torch.long).cuda()).shape) --------------------------------------------------------------------------- RuntimeError Traceback (most recent call last) <ipython-input-62-95c0c4ade406> in <module>() 4 [14, 13, 83, 18, 14], 5 [ 0, 0, 0, 0, 0]] ----> 6 print(char_OneHotEncoding(torch.tensor(x_train, dtype=torch.long).cuda()).shape) 7 x_train[:5, maxlen:maxlen+5] <ipython-input-53-055f1bf71306> in char_OneHotEncoding(x) 2 coded = torch.zeros(x.shape[0], x.shape[1], 101) 3 for i in range(x.shape[1]): ----> 4 coded[:,i] = scatter(x[:,i]) 5 return coded 6 <ipython-input-53-055f1bf71306> in scatter(x) 7 8 def scatter(x): ----> 9 return torch.zeros(x.shape[0], 101).scatter_(1, x.view(-1,1), 1) RuntimeError: Expected object of backend CPU but got backend CUDA for argument #3 'index' BTW, if we simply remove the .cuda() here, everything goes one well print(char_OneHotEncoding(torch.tensor(x_train, dtype=torch.long)).shape) torch.Size([5, 5, 101])",|gpu|pytorch|one-hot-encoding|,GPU Usage,3
54590442,"Remove operation graph tensorflow to run on CPU. I have trained a network (using GPU) and now I want to run it (for inference) on a CPU. To do so, I use the following code which loads the meta graph and then the parameters of the network. config = tf.ConfigProto( device_count = {'GPU': 0} ) sess = tf.Session(config=config) meta_graph="".../graph-0207-190023.meta"" model="".../model.data-00000-of-00001"" new_saver = tf.train.import_meta_graph(meta_graph) new_saver.restore(sess, model) Problem is that since the graph has been defined for training, I have used some specific operations that do not run on CPU. For example ""MaxBytesInUse"" https://www.tensorflow.org/api_docs/python/tf/contrib/memory_stats/MaxBytesInUse which records the GPU activity. Thats is why, when I try to run this code, I get the following error : InvalidArgumentError: No OpKernel was registered to support Op 'MaxBytesInUse' with these attrs. Registered devices: [CPU], Registered kernels: device='GPU' [[Node: PeakMemoryTracker/MaxBytesInUse = MaxBytesInUse[_device=""/device:GPU:0""]()]] Is there a simple way to remove the specific GPU related operations and to run the graph on a CPU ?",|python|tensorflow|gpu|,GPU Usage,3
54706146,"Moving member tensors with module.to() in PyTorch. I am building a Variational Autoencoder (VAE) in PyTorch and have a problem writing device agnostic code. The Autoencoder is a child of nn.Module with an encoder and decoder network, which are too. All weights of the network can be moved from one device to another by calling net.to(device). The problem I have is with the reparametrization trick: encoding = mu + noise * sigma The noise is a tensor of the same size as mu and sigma and saved as a member variable of the autoencoder module. It is initialized in the constructor and resampled in-place each training step. I do it that way to avoid constructing a new noise tensor each step and pushing it to the desired device. Additionally, I want to fix the noise in the evaluation. Here is the code: class VariationalGenerator(nn.Module): def __init__(self, input_nc, output_nc): super(VariationalGenerator, self).__init__() self.input_nc = input_nc self.output_nc = output_nc embedding_size = 128 self._train_noise = torch.randn(batch_size, embedding_size) self._eval_noise = torch.randn(1, embedding_size) self.noise = self._train_noise # Create encoder self.encoder = Encoder(input_nc, embedding_size) # Create decoder self.decoder = Decoder(output_nc, embedding_size) def train(self, mode=True): super(VariationalGenerator, self).train(mode) self.noise = self._train_noise def eval(self): super(VariationalGenerator, self).eval() self.noise = self._eval_noise def forward(self, inputs): # Calculate parameters of embedding space mu, log_sigma = self.encoder.forward(inputs) # Resample noise if training if self.training: self.noise.normal_() # Reparametrize noise to embedding space inputs = mu + self.noise * torch.exp(0.5 * log_sigma) # Decode to image inputs = self.decoder(inputs) return inputs, mu, log_sigma When I now move the autoencoder to the GPU with net.to('cuda:0') I get an error in forwarding because the noise tensor is not moved. I don't want to add a device parameter to the constructor, because then it is still not possible to move it to another device later. I also tried to wrap the noise into nn.Parameter so that it is affected by net.to(), but that gives an error from the optimizer, as the noise is flagged as requires_grad=False. Anyone has a solution to move all of the modules with net.to()?",|python|deep-learning|gpu|pytorch|autoencoder|,GPU Usage,3
54738769,"Why is my loss in Keras not changing when training my model?. I have been trying to make a Keras model to find patterns within a numerical dataset of mine. I have changed loss and optimizers many times without any change in the loss. I also have removed/added layers and changed the number of neurons in layers and still get no change in the loss. The model is: from keras.models import Sequential from keras.layers import Dense import numpy numpy.random.seed(7) dataset = numpy.loadtxt(""data.csv"", delimiter="","") X = dataset[:, :-1] Y = dataset[:, -1] print(X) # create model model = Sequential() model.add(Dense(18, input_dim=18, activation='tanh')) model.add(Dense(36, activation='relu')) model.add(Dense(72, activation='relu')) model.add(Dense(72, activation='relu')) model.add(Dense(32, activation='relu')) model.add(Dense(16, activation='relu')) model.add(Dense(1, activation='softmax')) # Compile model model.compile(loss='mean_squared_error', optimizer='adam') # Fit the model model.fit(X, Y, epochs=100, batch_size=35) # save model model.save('tried.h5') I have also changed the epochs and batch size with no effect on the loss. Here are the logs: Using TensorFlow backend. Printing X Data [[1.19539070e+01 1.72686310e+01 2.24426384e+01 ... 1.73570000e-04 4.35710000e-04 9.55710000e-04] [1.20239086e+01 1.45762539e+01 2.13278122e+01 ... 1.78570000e-04 4.06430000e-04 9.17860000e-04] [2.30696812e+01 1.82697601e+01 2.13278122e+01 ... 1.15000000e-04 3.75710000e-04 9.17860000e-04] ... [2.83583431e+01 2.38079319e+01 2.81154442e+01 ... 1.13570000e-04 3.20710000e-04 6.65000000e-04] [4.34185066e+01 2.17990398e+01 2.81154442e+01 ... 1.12860000e-04 3.37140000e-04 6.65000000e-04] [5.71823807e+01 2.19225960e+01 3.02071724e+01 ... 6.42900000e-05 3.56430000e-04 6.45000000e-04]] Epoch 1/100 342420/342420 [==============================] - 15s 45us/step - loss: 0.4945 Epoch 2/100 342420/342420 [==============================] - 15s 44us/step - loss: 0.4945 Epoch 3/100 342420/342420 [==============================] - 15s 43us/step - loss: 0.4945 Epoch 4/100 342420/342420 [==============================] - 15s 44us/step - loss: 0.4945 Epoch 5/100 342420/342420 [==============================] - 15s 44us/step - loss: 0.4945 Epoch 6/100 342420/342420 [==============================] - 15s 44us/step - loss: 0.4945 Epoch 7/100 342420/342420 [==============================] - 14s 42us/step - loss: 0.4945 Epoch 8/100 234500/342420 [===================>..........] - ETA: 4s - loss: 0.4946 The data most certainly does have a slight/decent pattern which the model should be able to pick up on. Can anyone recommend any changes in order to make the model actually fit the data or spot any errors? Thank you!",|tensorflow|machine-learning|keras|deep-learning|,Model,0
54811239,"LSTM with multiple input sequences and corresponding multiple output sequences. I have an issue related to reshaping input/output data for LSTM. While there are a lot of posts considering these issues, I couldn't come across to find a proper solution for this. My apologies if the mistake is quite obvious - I am rather new to the field of Deep Learning. My issue is as follows: I performed a simulation which resulted in several sequences of time dependent data which I'd like to feed into an LSTM-network. The data (very much simplified) looks as follows: X=[[[8, 0, 18, 10] [9, 0, 20, 7] [7, 0, 17, 12]] [[7, 0, 31, 8] [5, 0, 22, 9] [7, 0, 17, 12]]] That is I have two sequences with three time steps each and 4 features per time step. Hence, the shape of X is (2,3,4). Correspondingly, what I would like to predict looks as follows y=[[[10] [7] [12]] [[8] [9] [12]]] and has shape (2,3,1). That is, the data point [8,0,18,10] is supposed to predict [10], followed by point [9,0,20,7] which should predict 7 and so on. My model then looks as follows: model.add(LSTM(input_shape=(X.shape[1], X.shape[2]), return_sequences=True)) model.add(Dense(50, activation='tanh')) model.add(Dense(1, activation='tanh')) While this seems to work without errors, my result is quite bad. Most likely, I think this is related to reshaping the output vector correctly. Also, I am not quite sure about whether or not return_sequences has to be true or not. If it is set to False, I get the error message 'Expected dense_2 to have 2 dimensions, but got an array with shape (2,3,1). Not quite sure about this. I was also looking into Seq2Seq modelling since I am trying to predict a sequence based on a sequence, but I couldn't find a workaround. Can anybody help?",|python|tensorflow|keras|lstm|,Model,0
54888906,"scalar custom loss function in keras for end-to-end time series prediction resulting in NaN loss and predictions. I am working on a denoising autoencoder for audio, feeding raw time-series audio to the network and receiving time-series audio as output from the network. The mean_square_error loss objective function returns values of shape (batch_size, audio_sequence_length), which (I hope I understood correctly) is further processed by Keras internally to reach the final single-valued loss used for backprop by computing the mean over time bins and batches. My current efforts are focused on creating a custom loss function using signal power instead of the error of individual samples, returning values of shape (batch_size, ). The model compiles nicely but returns only NaN loss at training time. Trying to predict anything using such a model results in output vectors consisting of NaN as well. This is the loss function: def SI_SNR(yTrue,yPred): yTarget = K.batch_dot(yTrue,yPred,axes=0) yTarget = K.batch_dot(yTrue,yTarget,axes=None) yNorm = K.batch_dot(yTrue,yTrue, axes = 0) yTarget = yTarget/yNorm eNoise = yPred - yTarget losses = -(10.*K.log(K.batch_dot(yTarget,yTarget,axes=0)/ K.batch_dot(eNoise,eNoise,axes=0))/K.log(10.)) return K.reshape(losses,([-1])) When using the function on actual numbers (either using a subset of the training data or randomly filled arrays) I do get non NaN results: x=K.variable(np.random.rand(8,1024,1)) y=K.variable(np.random.rand(8,1024,1)) K.eval(SI_SNR(y,x)) Is the training behavior due to the shape of the loss or is there perhaps some other problem with the internal structure of the loss function?",|keras|conv-neural-network|nan|autoencoder|loss-function|,Training,2
55142951,"Tensorflow 2.0 - AttributeError: module 'tensorflow' has no attribute 'Session'. When I am executing the command sess = tf.Session() in Tensorflow 2.0 environment, I am getting an error message as below: Traceback (most recent call last): File ""<stdin>"", line 1, in <module> AttributeError: module 'tensorflow' has no attribute 'Session' System Information: OS Platform and Distribution: Windows 10 Python Version: 3.7.1 Tensorflow Version: 2.0.0-alpha0 (installed with pip) Steps to reproduce: Installation: pip install --upgrade pip pip install tensorflow==2.0.0-alpha0 pip install keras pip install numpy==1.16.2 Execution: Execute command: import tensorflow as tf Execute command: sess = tf.Session()",|python|tensorflow|keras|tensorflow2.0|,API,4
55198221,"Sudden 50% accuracy drop while training convolutional NN. Training convolutional neural network from scratch on my own dataset with Keras and Tensorflow. learning rate = 0.0001, 5 classes to sort, no Dropout used, dataset checked twice, no wrong labels found Model: model = models.Sequential() model.add(layers.Conv2D(16,(2,2),activation='relu',input_shape=(75,75,3))) model.add(layers.MaxPooling2D((2,2))) model.add(layers.Conv2D(16,(2,2),activation='relu')) model.add(layers.MaxPooling2D((2,2))) model.add(layers.Conv2D(32,(2,2),activation='relu')) model.add(layers.MaxPooling2D((2,2))) model.add(layers.Flatten()) model.add(layers.Dense(128,activation='relu')) model.add(layers.Dense(5,activation='sigmoid')) model.compile(optimizer=optimizers.adam(lr=0.0001), loss='categorical_crossentropy', metrics=['acc']) history = model.fit_generator(train_generator, steps_per_epoch=100, epochs=50, validation_data=val_generator, validation_steps=25) Everytime when model achieves 25-35 epochs (80-90% accuracy) this happens: Epoch 31/50 100/100 [==============================] - 3s 34ms/step - loss: 0.3524 - acc: 0.8558 - val_loss: 0.4151 - val_acc: 0.7992 Epoch 32/50 100/100 [==============================] - 3s 34ms/step - loss: 0.3393 - acc: 0.8700 - val_loss: 0.4384 - val_acc: 0.7951 Epoch 33/50 100/100 [==============================] - 3s 34ms/step - loss: 0.3321 - acc: 0.8702 - val_loss: 0.4993 - val_acc: 0.7620 Epoch 34/50 100/100 [==============================] - 3s 33ms/step - loss: 1.5444 - acc: 0.3302 - val_loss: 1.6062 - val_acc: 0.1704 Epoch 35/50 100/100 [==============================] - 3s 34ms/step - loss: 1.6094 - acc: 0.2935 - val_loss: 1.6062 - val_acc: 0.1724 There is some similar problems with answers, but mostly they recommend to lower learning rate, but it doesnt help at all. UPD: almost all weights and biases in network became nan. Network somehow died inside",|python|tensorflow|keras|neural-network|,Model,0
55277192,"ValueError: operands could not be broadcast together with shapes (50,50,512) (3,) (50,50,512) while converting tensor to image in pytorch. I'm doing a neural style transfer. I'm trying to reconstruct the output of the convolutional layer conv4_2 of the VGG19 network. def get_features(image, model): layers = {'0': 'conv1_1', '5': 'conv2_1', '10': 'conv3_1', '19': 'conv4_1', '21': 'conv4_2', '28': 'conv5_1'} x = image features = {} for name, layer in model._modules.items(): x = layer(x) if name in layers: features[layers[name]] = x return features content_img_features = get_features(content_img, vgg) style_img_features = get_features(style_img, vgg) target_content = content_img_features['conv4_2'] content_img_features is a dict that contains the output of every layer. target_content is a tensor of shape torch.Size([1, 512, 50, 50]) This is the method I use to plot the image using the tensor. It works fine for the input image as well as the final output. def tensor_to_image(tensor): image = tensor.clone().detach() image = image.numpy().squeeze() image = image.transpose(1, 2, 0) image *= np.array((0.22, 0.22, 0.22))+ np.array((0.44, 0.44, 0.44)) image = image.clip(0, 1) return image image = tensor_to_image(target_content) fig = plt.figure() plt.imshow(image) But this throws the error, --------------------------------------------------------------------------- ValueError Traceback (most recent call last) <ipython-input-188-a75a5f0743bb> in <module>() 1 ----> 2 image = tensor_to_image(target_content) 3 fig = plt.figure() 4 plt.imshow(image) <ipython-input-186-e9385dbc4a85> in tensor_to_image(tensor) 3 image = image.numpy().squeeze() 4 image = image.transpose(1, 2, 0) ----> 5 image *= np.array((0.22, 0.22, 0.22))+ np.array((0.44, 0.44, 0.44)) 6 image = image.clip(0, 1) 7 return image ValueError: operands could not be broadcast together with shapes (50,50,512) (3,) (50,50,512) This is the initial transformation I apply to the image before passing to the cnn layers, def transformation(img): tasks = tf.Compose([tf.Resize(400), tf.ToTensor(), tf.Normalize((0.44,0.44,0.44),(0.22,0.22,0.22))]) img = tasks(img)[:3,:,:].unsqueeze(0) return img How do I fix this? Is there another way to reconstruct the image from the convolution layer?",|conv-neural-network|pytorch|tensor|vgg-net|style-transfer|,Tensors&Inputs,1
55261785,"NVidia drivers stopped working on AWS EC2 instance with Ubuntu 16.04 and Tesla K80 GPU. I've been using an AWS EC2 instance, with a Tesla K80 GPU, for a while to run TensorFlow code. I have CUDA 9.0 and cuDNN 7.1.4 installed, and I'm using TF 1.12, all of this on Ubuntu 16.04 Everything worked well up to yesterday, but today it seems that the NVidia drivers have stopped running for some reason : ubuntu@ip-10-0-0-13:~$ nvidia-smi NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running. I checked the drivers: ubuntu@ip-10-0-0-13:~$ dpkg -l | grep nvidia rc nvidia-367 367.48-0ubuntu1 amd64 NVIDIA binary driver - version 367.48 ii nvidia-396 396.37-0ubuntu1 amd64 NVIDIA binary driver - version 396.37 ii nvidia-396-dev 396.37-0ubuntu1 amd64 NVIDIA binary Xorg driver development files ii nvidia-machine-learning-repo-ubuntu1604 1.0.0-1 amd64 nvidia-machine-learning repository configuration files ii nvidia-modprobe 396.37-0ubuntu1 amd64 Load the NVIDIA kernel driver and create device files rc nvidia-opencl-icd-367 367.48-0ubuntu1 amd64 NVIDIA OpenCL ICD ii nvidia-opencl-icd-396 396.37-0ubuntu1 amd64 NVIDIA OpenCL ICD ii nvidia-prime 0.8.2 amd64 Tools to enable NVIDIA's Prime ii nvidia-settings 396.37-0ubuntu1 amd64 Tool for configuring the NVIDIA graphics driver It seems that there are 2 different versions present, could that be a problem ? (But I couldn't see why as everything worked before). Finding this thread, I checked my kernel, which is appearently different from the ones mentionned in the thread: ubuntu@ip-10-0-0-13:~$ uname -a Linux ip-10-0-0-13 4.4.0-143-generic #169-Ubuntu SMP Thu Feb 7 07:56:38 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux Has anyone run into this problem and know how to fix it ? Thanks in advance for your help ! EDIT: When trying to upgrade the drivers with @Dehydrated_Mud 's method, I got the following error: ERROR: The installation was canceled due to the availability or presence of an alternate driver installation. Please see /var/log/nvidia-installer.log for more details. And the content of the log file: nvidia-installer log file '/var/log/nvidia-installer.log' creation time: Thu Mar 21 10:56:46 2019 installer version: 384.183 PATH: /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin nvidia-installer command line: ./nvidia-installer --no-drm --disable-nouveau --dkms --silent --install-libglvnd Using built-in stream user interface -> Detected 4 CPUs online; setting concurrency level to 4. -> Installing NVIDIA driver version 384.183. -> The NVIDIA driver appears to have been installed previously using a different installer. To prevent potential conflicts, it is recommended either to update the existing installation using the same mechanism by which it was originally installed, or to uninstall the existing installation before installing this driver. Please review the message provided by the maintainer of this alternate installation method and decide how to proceed: The package that is already installed is named nvidia-396. You can upgrade the driver by running: `apt-get install nvidia-396 nvidia-modprobe nvidia-settings` You can remove nvidia-396, and all related packages, by running: `apt-get remove --purge nvidia-396 nvidia-modprobe nvidia-settings` This package is maintained by NVIDIA (cudatools@nvidia.com). (Answer: Abort installation) ERROR: The installation was canceled due to the availability or presence of an alternate driver installation. Please see /var/log/nvidia-installer.log for more details. Running apt-cache search nvidia | grep -P '^nvidia-[0-9]+\s' gives: nvidia-331 - Transitional package for nvidia-331 nvidia-346 - Transitional package for nvidia-346 nvidia-304 - NVIDIA legacy binary driver - version 304.135 nvidia-340 - NVIDIA binary driver - version 340.107 nvidia-361 - Transitional package for nvidia-367 nvidia-352 - Transitional package for nvidia-375 nvidia-367 - Transitional package for nvidia-387 nvidia-375 - Transitional package for nvidia-418 nvidia-387 - NVIDIA binary driver - version 387.26 nvidia-418 - NVIDIA binary driver - version 418.39 nvidia-384 - NVIDIA binary driver - version 384.183 nvidia-390 - NVIDIA binary driver - version 390.116 nvidia-410 - NVIDIA binary driver - version 410.104 nvidia-396 - NVIDIA binary driver - version 396.82",|amazon-web-services|tensorflow|amazon-ec2|gpu|nvidia|,GPU Usage,3
55328966,"tf.keras loss becomes NaN. I'm programming a neural network in tf.keras, with 3 layers. My dataset is the MNIST dataset. I decreased the number of examples in the dataset, so the runtime is lower. This is my code: import tensorflow as tf from tensorflow.keras import layers import numpy as np import pandas as pd !git clone https://github.com/DanorRon/data %cd data !ls batch_size = 32 epochs = 10 alpha = 0.0001 lambda_ = 0 h1 = 50 train = pd.read_csv('/content/first-repository/mnist_train.csv.zip') test = pd.read_csv('/content/first-repository/mnist_test.csv.zip') train = train.loc['1':'5000', :] test = test.loc['1':'2000', :] train = train.sample(frac=1).reset_index(drop=True) test = test.sample(frac=1).reset_index(drop=True) x_train = train.loc[:, '1x1':'28x28'] y_train = train.loc[:, 'label'] x_test = test.loc[:, '1x1':'28x28'] y_test = test.loc[:, 'label'] x_train = x_train.values y_train = y_train.values x_test = x_test.values y_test = y_test.values nb_classes = 10 targets = y_train.reshape(-1) y_train_onehot = np.eye(nb_classes)[targets] nb_classes = 10 targets = y_test.reshape(-1) y_test_onehot = np.eye(nb_classes)[targets] model = tf.keras.Sequential() model.add(layers.Dense(784, input_shape=(784,))) model.add(layers.Dense(h1, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(lambda_))) model.add(layers.Dense(10, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2(lambda_))) model.compile(optimizer=tf.train.GradientDescentOptimizer(alpha), loss = 'categorical_crossentropy', metrics = ['accuracy']) model.fit(x_train, y_train_onehot, epochs=epochs, batch_size=batch_size) Whenever I run it, one of 3 things happens: The loss decreases and the accuracy increases for a few epochs, until the loss becomes NaN for no apparent reason and the accuracy plummets. The loss and accuracy stay the same for each epoch. Usually the loss is 2.3025 and the accuracy is 0.0986. The loss starts at NaN(and stays that way), while the accuracy stays low. Most of the time, the model does one of these things, but sometimes it does something random. It seems like the type of erratic behavior that occurs is completely random. I have no idea what the problem is. How do I fix this problem? Edit: Sometimes, the loss decreases, but the accuracy stays the same. Also, sometimes the loss decreases and the accuracy increases, then after a while the accuracy decreases while the loss still decreases. Or, the loss decreases and the accuracy increases, then it switches and the loss goes up fast while the accuracy plummets, eventually ending with loss: 2.3025 acc: 0.0986. Edit 2: This is an example of something that sometimes happens: Epoch 1/100 49999/49999 [==============================] - 5s 92us/sample - loss: 1.8548 - acc: 0.2390 Epoch 2/100 49999/49999 [==============================] - 5s 104us/sample - loss: 0.6894 - acc: 0.8050 Epoch 3/100 49999/49999 [==============================] - 4s 90us/sample - loss: 0.4317 - acc: 0.8821 Epoch 4/100 49999/49999 [==============================] - 5s 104us/sample - loss: 2.2178 - acc: 0.1345 Epoch 5/100 49999/49999 [==============================] - 5s 90us/sample - loss: 2.3025 - acc: 0.0986 Epoch 6/100 49999/49999 [==============================] - 4s 90us/sample - loss: 2.3025 - acc: 0.0986 Epoch 7/100 49999/49999 [==============================] - 4s 89us/sample - loss: 2.3025 - acc: 0.0986 Edit 3: I changed the loss to mean squared error and the network works well now. Is there a way to keep it in cross entropy without it converging to a local minimum?",|python|machine-learning|neural-network|mnist|tf.keras|,Training,2
55496289,"How to fix ""AttributeError: module 'tensorflow' has no attribute 'get_default_graph'""?. I am trying to run some code to create an LSTM model but i get an error: AttributeError: module 'tensorflow' has no attribute 'get_default_graph' My code is as follows: from keras.models import Sequential model = Sequential() model.add(Dense(32, input_dim=784)) model.add(Activation('relu')) model.add(LSTM(17)) model.add(Dense(1, activation='sigmoid')) model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) I have found someone else with a similar problem and they updated tensorflow and it works; but mine is up to date and still does not work. I am new to using keras and machine learning so I apologise if this is something silly!",|python|tensorflow|keras|keras-layer|tf.keras|,API,4
55731589,"CNN Keras: ValueError: Negative dimension size caused by subtracting 3 from 2 for 'conv2d. I got this error when using Keras: Is it because input_size not larger than the filter? If input_shape=(64,64,3))), there will be no error. ``ValueError: Negative dimension size caused by subtracting 3 from 2 for 'conv2d_24/convolution' (op: 'Conv2D') with input shapes: [?,2,2,128], [3,3,128,128]. My code are here: from keras import layers from keras import models model = models.Sequential() model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3))) model.add(layers.MaxPooling2D((2, 2))) model.add(layers.Conv2D(64, (3, 3), activation='relu')) model.add(layers.MaxPooling2D((2, 2))) model.add(layers.Conv2D(128, (3, 3), activation='relu')) model.add(layers.MaxPooling2D((2, 2))) model.add(layers.Conv2D(128, (3, 3), activation='relu')) model.add(layers.MaxPooling2D((2, 2))) model.add(layers.Flatten()) model.add(layers.Dense(512, activation='relu')) model.add(layers.Dense(1, activation='sigmoid'))",|python|tensorflow|keras|deep-learning|conv-neural-network|,Tensors&Inputs,1
55780923,"What does ""RuntimeError: CUDA error: device-side assert triggered"" in PyTorch mean?. I have seen a lot of specific posts to particular case-specific problems, but no fundamental motivating explanation. What does this error: RuntimeError: CUDA error: device-side assert triggered mean? Specifically, what is the assert that is being triggered, why is the assert there, and how do we work backwards to debug the problem? As-is, this error message is near useless in diagnosing any problem because of the generality that it seems to say ""some code somewhere that touches the GPU"" has a problem. The documentation of Cuda also does not seem helpful in this regard, though I could be wrong. https://docs.nvidia.com/cuda/cuda-gdb/index.html",|python|gpu|pytorch|,GPU Usage,3
55833353,"Getting ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory - downgrading TF version does not work. I know this question may have already been asked before, but I haven't found a solution that works for my case. I'm trying to install tensorflow-gpu, but it keeps returning the error: ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory Past answers suggest it might have something to do with the compatability of tensorflow and CUDA. When I run nvcc --version, I get: nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2018 NVIDIA Corporation Built on Tue_Jun_12_23:07:04_CDT_2018 Cuda compilation tools, release 9.2, V9.2.148 I've already tried installing previous versions of tensorflow-gpu, i.e. tensorflow-gpu==1.12.0, tensorflow-gpu==1.8.0, and tensorflow-gpu==1.4.0. But what happens is the error becomes libcublas.so.9.0or libcublas.so.8.0:, depending on the version of tensorflow-gpu. Note that I've set LD_LIBRARY_PATH environment variable to the following: LD_LIBRARY_PATH=/usr/local/cuda-9.2/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda-9.2/lib64/:/usr/local/cuda/lib64/ I'm not quite sure how to fix this. I have absolutely no trouble getting pytorch to work with GPU, but with tensorflow and keras, it's a lot of hassle. Any ideas? Full error here: Using TensorFlow backend. --------------------------------------------------------------------------- ImportError Traceback (most recent call last) ~/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py in <module> 57 ---> 58 from tensorflow.python.pywrap_tensorflow_internal import * 59 from tensorflow.python.pywrap_tensorflow_internal import __version__ ~/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py in <module> 27 return _mod ---> 28 _pywrap_tensorflow_internal = swig_import_helper() 29 del swig_import_helper ~/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py in swig_import_helper() 23 try: ---> 24 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description) 25 finally: /usr/lib/python3.5/imp.py in load_module(name, file, filename, details) 241 else: --> 242 return load_dynamic(name, filename, file) 243 elif type_ == PKG_DIRECTORY: /usr/lib/python3.5/imp.py in load_dynamic(name, path, file) 341 name=name, loader=loader, origin=path) --> 342 return _load(spec) 343 ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory During handling of the above exception, another exception occurred: ImportError Traceback (most recent call last) <ipython-input-1-3d00d838479b> in <module> ----> 1 from keras import backend as K 2 K.tensorflow_backend._get_available_gpus() ~/.local/lib/python3.5/site-packages/keras/__init__.py in <module> 1 from __future__ import absolute_import 2 ----> 3 from . import utils 4 from . import activations 5 from . import applications ~/.local/lib/python3.5/site-packages/keras/utils/__init__.py in <module> 4 from . import data_utils 5 from . import io_utils ----> 6 from . import conv_utils 7 8 # Globally-importable utils. ~/.local/lib/python3.5/site-packages/keras/utils/conv_utils.py in <module> 7 from six.moves import range 8 import numpy as np ----> 9 from .. import backend as K 10 11 ~/.local/lib/python3.5/site-packages/keras/backend/__init__.py in <module> 87 elif _BACKEND == 'tensorflow': 88 sys.stderr.write('Using TensorFlow backend.\n') ---> 89 from .tensorflow_backend import * 90 else: 91 # Try and load external backend. ~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py in <module> 3 from __future__ import print_function 4 ----> 5 import tensorflow as tf 6 from tensorflow.python.framework import ops as tf_ops 7 from tensorflow.python.training import moving_averages ~/.local/lib/python3.5/site-packages/tensorflow/__init__.py in <module> 22 23 # pylint: disable=g-bad-import-order ---> 24 from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import 25 26 try: ~/.local/lib/python3.5/site-packages/tensorflow/python/__init__.py in <module> 47 import numpy as np 48 ---> 49 from tensorflow.python import pywrap_tensorflow 50 51 from tensorflow.python.tools import component_api_helper ~/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py in <module> 72 for some common reasons and solutions. Include the entire stack trace 73 above this error message when asking for help."""""" % traceback.format_exc() ---> 74 raise ImportError(msg) 75 76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long ImportError: Traceback (most recent call last): File ""/home/jack/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module> from tensorflow.python.pywrap_tensorflow_internal import * File ""/home/jack/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module> _pywrap_tensorflow_internal = swig_import_helper() File ""/home/jack/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description) File ""/usr/lib/python3.5/imp.py"", line 242, in load_module return load_dynamic(name, filename, file) File ""/usr/lib/python3.5/imp.py"", line 342, in load_dynamic return _load(spec) ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common reasons and solutions. Include the entire stack trace above this error message when asking for help.enter code here EDIT: I think I may have found the root cause of the problem here. It's that the cuda folder is named cuda-9.2 and tensorflow is looking for cuda-9.0. I'm not quite sure how to fix this though.",|python|tensorflow|keras|gpu|,GPU Usage,3
55955130,"Keras giving error - ValueError: Error when checking target: expected dense_3 to have 4 dimensions, but got array with shape (10000, 1). I have dataset of 28x28 pictures. Datapoints array x has shape (10000, 28, 28), labels array y has shape (10000,). The following code: x = x.reshape(-1, 28, 28, 1) model = Sequential([ Conv2D(8, kernel_size=(3, 3), padding=""same"", activation=tf.nn.relu, input_shape=(28, 28, 1)), Dense(64, activation=tf.nn.relu), Dense(64, activation=tf.nn.relu), Dense(10, activation=tf.nn.softmax) ]) model.compile( optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'] ) model.fit(x, y, epochs=5) #error gives: ValueError: Error when checking target: expected dense_3 to have 4 dimensions, but got array with shape (10000, 1) model.summary() output: Layer (type) Output Shape Param # ================================================================= conv2d_1 (Conv2D) (None, 28, 28, 8) 80 _________________________________________________________________ dense_1 (Dense) (None, 28, 28, 64) 576 _________________________________________________________________ dense_2 (Dense) (None, 28, 28, 64) 4160 _________________________________________________________________ dense_3 (Dense) (None, 28, 28, 10) 650 ================================================================= Total params: 5,466 Trainable params: 5,466 Non-trainable params: 0 _________________________________________________________________",|python|tensorflow|keras|conv-neural-network|,Tensors&Inputs,1
56008764,"Tensorflow Dataset API not using GPU. 1. Problem : I have a tf.data.Dataset that I give to a Keras model (tf.python.keras) with train_on_batch. My dataset looks like this : Generate TFRecord path > tf.data.TFRecordDataset > Parse single example > Batch(2) > Map(merge) > Map(normalize) > Map(split to inputs,labels) > Batch(batch_size) > Prefetch(1) I used RunMetadata to output a Timeline readable with Chrome. Looks like IteratorGetNext is only ran on the CPU and is eating a significant amount of time. (I can't post images, IteratorGetNext took 617ms, MEMCPYHtoD took 58ms and training took 500ms) I can't seem to find a way to get IteratorGetNext to run on the GPU, even partially. Currently, CPU is used at 100% and GPU at 40-60% at most. I would expect something like : Read from disk > Move from CPU to GPU > Preprocess. I am currently using only one GPU, but I plan to use more GPUs later so a scalable solution would be perfect ! By the way, I am using tensorflow-gpu 1.13.1 on Windows 10 with CUDA 10.0 and python 3.6.7. I am not using eager mode. I haven't tried on Ubuntu but it is a possibility. 2. What I tried : I tried using prefetch_to_device and copy_to_device from tf.data.experimental, in several places in the pipeline. When using copy_to_device, IteratorGetNext took twice as long. It looked like it was copying on the GPU to only copy back to the CPU because the MEMCPYHtoD was still present after IteratorGetNext. I tried replacing Keras' train_on_batch with session.run(train_op) but it did not really improve, the only change I noticed was that some prefetching actually happened, reducing IteratorGetNext time for a few samples (independent of the amount I put in ""prefetch""). By the way, prefetch(1) or prefetch(tf.data.experimental.AUTOTUNE) did not seem to have any impact. I tried session.run both with and without copy_to_device. I also tried to put the building of the dataset in with tf.device(""/gpu:0""). 3. Some code : dataset = tf.data.Dataset.from_generator(self.random_shard_filepath_generator, output_types=tf.string, output_shapes=()) dataset = tf.data.TFRecordDataset(dataset) dataset = dataset.map(lambda serialized_shard: self.parse_shard(serialized_shard, output_labels)) dataset = dataset.batch(self.shards_per_sample) dataset = dataset.map(self.join_shards_randomly) dataset = dataset.map(self.normalize_batch) dataset = dataset.map(self.split_batch_io) dataset = dataset.batch(batch_size).prefetch(1) autoencoder.train_on_batch(dataset) Finally, I would add that my model may just not be big enough and I could improve the ratio by just making it ""bigger"", but it does not feel like a great solution. -- Edit : I had : ... dataset = dataset.batch(batch_size).prefetch(1) autoencoder.train_on_batch(dataset) Which I changed to : ... dataset = dataset.batch(batch_size).prefetch(1) dataset_iterator = dataset.make_initializable_iterator() dataset_initializer = dataset_iterator.initializer session.run(dataset_initializer) x, y = dataset_iterator autoencoder.train_on_batch(x, y) Thanks to EdoardoG for making me try MultiDeviceIterator which made me create an Iterator outside of Keras' train_on_batch. Now IteratorGetNext only takes about 0.05ms where it took previously about 600ms.",|python|tensorflow|dataset|gpu|,GPU Usage,3
56014360,"Keras does not use my Nvidia GPU when training a neural network. My GPU is not used by Keras/TensorFlow. To try to make my GPU working with tensorflow, I tried to install tensorflow-gpu (I am using Python 3.6.8 on Windows): pip3 install tensorflow-gpu --user python -m notebook import tensorflow as tf I got then the following errors: ImportError ... Traceback (most recent call last), ImportError: DLL load failed: Le module spcifi est introuvable. ImportError ... Traceback (most recent call last) Then I do pip3 install tensorflow, python - notebook, and then import tensorflow as tf works but when I continue with: from tensorflow.python.client import device_lib print(device_lib.list_local_devices()) print('Tensorflow: ', tf._ _ version _ _) And I obtain: [name: ""/device:CPU:0"" device_type: ""CPU"" memory_limit: 268435456 locality { } incarnation: 587921620497715868 ] Tensorflow: 1.13.1 It means that no GPU has been found (and I have an Intel UHD Graphics P630 and a Nvidia Quadro P5200 on my Lenovo Thinkpad P72). Also, note that when I do !nvidia-smi I see that the Nvidia is detected (as GPU [0]), so I do not understand why Tensorflow/Keras do not use it when I train neural networks. Thank your for your help.",|python|tensorflow|keras|gpu|,GPU Usage,3
56034040,"Tensorflow can't find GPU. My apologies as there's a million questions out there like this, but none of them seem to answer mine. I'm trying to re-install Tensorflow so that it uses my GPU. I'm running: Windows 10 Python 3.6.5 Keras 2.2.4 TF 1.13.1 Nvidia Quadro M1000M (driver 412.16, compute capability 5.0) CUDA 10.0.130 Have CUPTI installed and CuDNN files copied into CUDA folders. The CUDA v10.0 folder, the \extras\CUPTI\libx64 and the \include folder are all in my PATH, also CUDAPATH is as well as CUDA_PATH is defined) Yet, with pip install tensorflow it only finds my CPU (using: from tensorflow.python.client import device_lib device_lib.list_local_devices() And I can't seem to be able to import tensorflow if I pip uninstall tensorflow pip install tensorflow-gpu python import tensorflow ModuleNotFoundError: No module named 'tensorflow'",|python|tensorflow|gpu|,GPU Usage,3
56103207,"Keras CNN overfitting for more than four classes. I'm trying to train a classifier on Google QuickDraw drawings using Keras: import numpy as np from tensorflow.keras.layers import Conv2D, Dense, Flatten, MaxPooling2D from tensorflow.keras.models import Sequential model = Sequential() model.add(Conv2D(filters=32, kernel_size=5, data_format=""channels_last"", activation=""relu"", input_shape=(28, 28, 1))) model.add(MaxPooling2D(data_format=""channels_last"")) model.add(Conv2D(filters=16, kernel_size=3, data_format=""channels_last"", activation=""relu"")) model.add(MaxPooling2D(data_format=""channels_last"")) model.add(Flatten(data_format=""channels_last"")) model.add(Dense(units=128, activation=""relu"")) model.add(Dense(units=64, activation=""relu"")) model.add(Dense(units=4, activation=""softmax"")) model.compile(optimizer=""adam"", loss=""categorical_crossentropy"", metrics=[""accuracy""]) x = np.load(""./x.npy"") y = np.load(""./y.npy"") model.fit(x=x, y=y, batch_size=100, epochs=40, validation_split=0.2) The input data is a 4d array with 12000 normalized images (28 x 28 x 1) per class. The output data is an array of one hot encoded vectors. If I train this model on four classes, it produces convincing results: (red is training data, blue is validation data) I know the model is slightly overfitted. However, I want to keep the architecture as simple as possible, so I accepted that. My problem is that as soon as I add just one arbitrary class, the model starts to overfit extremely: I tried many different things to prevent it from overfitting such as Batch Normalization, Dropout, Kernel Regularizers, much more training data and different batch sizes, none of which caused any significant improvement. What could be the reason why my CNN overfits so much? EDIT: This is the code I used to create x.npy and y.npy: import numpy as np from tensorflow.keras.utils import to_categorical files = ['cat.npy', 'dog.npy', 'apple.npy', 'banana.npy', 'flower.npy'] SAMPLES = 12000 x = np.concatenate([np.load(f'./data/{f}')[:SAMPLES] for f in files]) / 255.0 y = np.concatenate([np.full(SAMPLES, i) for i in range(len(files))]) # (samples, rows, cols, channels) x = x.reshape(x.shape[0], 28, 28, 1).astype('float32') y = to_categorical(y) np.save('./x.npy', x) np.save('./y.npy', y) The .npy files come from here.",|tensorflow|machine-learning|keras|deep-learning|conv-neural-network|,Training,2
56218256,"nan values in loss in keras model. I have following data shapes X_Train.shape,Y_Train.shape Out[52]: ((983, 19900), (983,)) X_Test.shape,Y_Test.shape Out[53]: ((52, 19900), (52,)) I am running a simple binary classifier as Y_train and Y_test could be either 1 or 2 import keras import tensorflow as tf from keras import layers from keras.layers import Input, Dense from keras.models import Model,Sequential import numpy as np from keras.optimizers import Adam myModel = keras.Sequential([ keras.layers.Dense(1000,activation=tf.nn.relu,input_shape=(19900,)), keras.layers.Dense(64, activation=tf.nn.relu), keras.layers.Dense(32, activation=tf.nn.relu), keras.layers.Dense(1, activation=tf.nn.softmax) ]) myModel.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics=['accuracy']) myModel.fit(X_Train, Y_Train, epochs=100,batch_size=1000) test_loss,test_acc=myModel.evaluate(X_Test,Y_Test) Output of the Code Training Loss and Accuracy Epoch 1/100 983/983 [==============================] - 1s 1ms/step - loss: nan - acc: 0.4608 Epoch 2/100 983/983 [==============================] - 0s 206us/step - loss: nan - acc: 0.4873 Epoch 3/100 983/983 [==============================] - 0s 200us/step - loss: nan - acc: 0.4883 Epoch 4/100 983/983 [==============================] - 0s 197us/step - loss: nan - acc: 0.4883 Epoch 5/100 983/983 [==============================] - 0s 194us/step - loss: nan - acc: 0.4873 Epoch 6/100 983/983 [==============================] - 0s 202us/step - loss: nan - acc: 0.4863 Epoch 7/100 983/983 [==============================] - 0s 198us/step - loss: nan - acc: 0.4863 Epoch 8/100 983/983 [==============================] - 0s 194us/step - loss: nan - acc: 0.4883 Epoch 9/100 983/983 [==============================] - 0s 196us/step - loss: nan - acc: 0.4873 Epoch 10/100 983/983 [==============================] - 0s 198us/step - loss: nan - acc: 0.4873 Epoch 11/100 983/983 [==============================] - 0s 200us/step - loss: nan - acc: 0.4893 Epoch 12/100 983/983 [==============================] - 0s 198us/step - loss: nan - acc: 0.4873 Epoch 13/100 983/983 [==============================] - 0s 194us/step - loss: nan - acc: 0.4873 Epoch 14/100 983/983 [==============================] - 0s 197us/step - loss: nan - acc: 0.4883 Epoch 97/100 983/983 [==============================] - 0s 196us/step - loss: nan - acc: 0.4893 Epoch 98/100 983/983 [==============================] - 0s 199us/step - loss: nan - acc: 0.4883 Epoch 99/100 983/983 [==============================] - 0s 193us/step - loss: nan - acc: 0.4883 Epoch 100/100 983/983 [==============================] - 0s 196us/step - loss: nan - acc: 0.4863 Testing Loss and Accuracy test_loss,test_acc Out[58]: (nan, 0.4615384661234342) I also checked if there is any nan value in my data np.isnan(X_Train).any() Out[5]: False np.isnan(Y_Train).any() Out[6]: False np.isnan(X_Test).any() Out[7]: False np.isnan(Y_Test).any() Out[8]: False My Question is why my training accuracy is not improving and why loss is nan also why without one-hot encoding the softmax in the output is working fine? Note1: I apologize that my data is big so I cannot share it here but if there are some way to share it here then I am ready to do that. Note2 There are lot of zero values in my training data",|python|tensorflow|keras|,Model,0
56315726,"Cannot import name 'Merge' from 'keras.layers'. I have try run a code but I find a problem with merge layers of Keras. I'm using python 3 and keras 2.2.4 This is de code part of code import numpy as np import pandas as pd from keras.models import Sequential from keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector, Merge, Activation from keras.preprocessing import image, sequence import cPickle as pickle def create_model(self, ret_model = False): image_model = Sequential() image_model.add(Dense(EMBEDDING_DIM, input_dim = 4096, activation='relu')) image_model.add(RepeatVector(self.max_length)) lang_model = Sequential() lang_model.add(Embedding(self.vocab_size, 256, input_length=self.max_length)) lang_model.add(LSTM(256,return_sequences=True)) lang_model.add(TimeDistributed(Dense(EMBEDDING_DIM))) model = Sequential() model.add(Merge([image_model, lang_model], mode='concat')) model.add(LSTM(1000,return_sequences=False)) model.add(Dense(self.vocab_size)) model.add(Activation('softmax')) print (""Model created!"") This is the message of error from keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector, Merge, Activation ImportError: cannot import name 'Merge' from 'keras.layers'",|python|keras|keras-layer|,API,4
56341753,"""Too early"" early stopping in Keras. I'm training a neural network with Keras, using early stopping. However, when training, the network very early hits a point where the validation loss is unnaturally low, which flattens out after a while, like this. When using early stopping with patience = 50 the validation loss flattens out, but never goes below the validation loss in the beginning. I've trained the network multiple times with the same result, with both the rmsprop (with learning rates from 0.1 to 1e-4) and adam optimizers. Does anyone know if there is a way to set a ""burn in period"" (like in a Markov Chain Monte Carlo model) for the network, before monitoring the validation loss for choosing the best model?",|python|keras|loss-function|,Training,2
56380303,"Keras: Making a neural network to find a number's modulus. I'm an experienced Python developer, but a complete newbie in machine learning. This is my first attempt to use Keras. Can you tell what I'm doing wrong? I'm trying to make a neural network that takes a number in binary form, and outputs its modulo when dividing by 7. (My goal was to take a very simple task just to see that everything works.) In the code below I define the network and I train it on 10,000 random numbers. Then I test it on 500 random numbers. For some reason the accuracy that I get is around 1/7, which is the accuracy you'd expect from a completely random algorithm, i.e. my neural network isn't doing anything. Can anyone help me figure out what's wrong? import keras.models import numpy as np from python_toolbox import random_tools RADIX = 7 def _get_number(vector): return sum(x * 2 ** i for i, x in enumerate(vector)) def _get_mod_result(vector): return _get_number(vector) % RADIX def _number_to_vector(number): binary_string = bin(number)[2:] if len(binary_string) > 20: raise NotImplementedError bits = (((0,) * (20 - len(binary_string))) + tuple(map(int, binary_string)))[::-1] assert len(bits) == 20 return np.c_[bits] def get_mod_result_vector(vector): return _number_to_vector(_get_mod_result(vector)) def main(): model = keras.models.Sequential( ( keras.layers.Dense( units=20, activation='relu', input_dim=20 ), keras.layers.Dense( units=20, activation='relu' ), keras.layers.Dense( units=20, activation='softmax' ) ) ) model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy']) data = np.random.randint(2, size=(10000, 20)) labels = np.vstack(map(get_mod_result_vector, data)) model.fit(data, labels, epochs=10, batch_size=50) def predict(number): foo = model.predict(_number_to_vector(number)) return _get_number(tuple(map(round, foo[0]))) def is_correct_for_number(x): return bool(predict(x) == x % RADIX) predict(7) sample = random_tools.shuffled(range(2 ** 20))[:500] print('Total accuracy:') print(sum(map(is_correct_for_number, sample)) / len(sample)) print(f'(Accuracy of random algorithm is {1/RADIX:.2f}') if __name__ == '__main__': main()",|python|tensorflow|machine-learning|keras|neural-network|,Training,2
56745316,"Tensorflow slower on GPU than on CPU. Using Keras with Tensorflow backend, I am trying to train an LSTM network and it is taking much longer to run it on a GPU than a CPU. I am training an LSTM network using the fit_generator function. It takes CPU ~250 seconds per epoch while it takes GPU ~900 seconds per epoch. The packages in my GPU environment include keras-applications 1.0.8 py_0 anaconda keras-base 2.2.4 py36_0 anaconda keras-gpu 2.2.4 0 anaconda keras-preprocessing 1.1.0 py_1 anaconda ... tensorflow 1.13.1 gpu_py36h3991807_0 anaconda tensorflow-base 1.13.1 gpu_py36h8d69cac_0 anaconda tensorflow-estimator 1.13.0 py_0 anaconda tensorflow-gpu 1.13.1 pypi_0 pypi My Cuda compilation tools are of version 9.1.85 and my CUDA and Driver version are +-----------------------------------------------------------------------------+ | NVIDIA-SMI 418.67 Driver Version: 418.67 CUDA Version: 10.1 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce RTX 2080 On | 00000000:0A:00.0 Off | N/A | | 0% 39C P8 5W / 225W | 7740MiB / 7952MiB | 0% Default | +-------------------------------+----------------------+----------------------+ | 1 GeForce RTX 2080 On | 00000000:42:00.0 Off | N/A | | 0% 33C P8 19W / 225W | 142MiB / 7951MiB | 0% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 49251 C .../whsu014/.conda/envs/whsuphd/bin/python 7729MiB | | 1 1354 G /usr/lib/xorg/Xorg 16MiB | | 1 49251 C .../whsu014/.conda/envs/whsuphd/bin/python 113MiB | +-----------------------------------------------------------------------------+ When I insert this line of code tf.Session(config = tf.configProto(log_device_placement = True)): I see the below in my terminal ... ining_1/Adam/Const_10: (Const)/job:localhost/replica:0/task:0/device:GPU:0 training_1/Adam/Const_11: (Const): /job:localhost/replica:0/task:0/device:GPU:0 2019-06-25 11:27:31.720653: I tensorflow/core/common_runtime/placer.cc:1059] training_1/Adam/Const_11: (Const)/job:localhost/replica:0/task:0/device:GPU:0 training_1/Adam/add_15/y: (Const): /job:localhost/replica:0/task:0/device:GPU:0 2019-06-25 11:27:31.720666: I tensorflow/core/common_runtime/placer.cc:1059] training_1/Adam/add_15/y: (Const)/job:localhost/replica:0/task:0/device:GPU:0 ... So it seems that Tensorflow is using GPU. When I profile the code, on GPU this is the first 10 lines 10852017 function calls (10524203 primitive calls) in 184.768 seconds Ordered by: internal time ncalls tottime percall cumtime percall filename:lineno(function) 16200 173.827 0.011 173.827 0.011 {built-in method _pywrap_tensorflow_internal.TF_SessionRunCallable} 6 0.926 0.154 0.926 0.154 {built-in method _pywrap_tensorflow_internal.TF_SessionMakeCallable} 62 0.813 0.013 0.813 0.013 {built-in method _pywrap_tensorflow_internal.TF_SessionRun_wrapper} 156954 0.414 0.000 0.415 0.000 {built-in method numpy.array} 16200 0.379 0.000 1.042 0.000 training.py:643(_standardize_user_data) 24300 0.338 0.000 0.338 0.000 {method 'partition' of 'numpy.ndarray' objects} 68 0.301 0.004 0.301 0.004 {built-in method _pywrap_tensorflow_internal.ExtendSession} 32458 0.223 0.000 2.122 0.000 tensorflow_backend.py:156(get_session) 3206 0.212 0.000 0.238 0.000 tf_stack.py:31(extract_stack) 76024 0.210 0.000 0.702 0.000 ops.py:5246(get_controller) ... on CPU this is the first 10 lines 22123473 function calls (21647174 primitive calls) in 60.173 seconds Ordered by: internal time ncalls tottime percall cumtime percall filename:lineno(function) 16269 42.491 0.003 42.491 0.003 {built-in method tensorflow.python._pywrap_tensorflow_internal.TF_Run} 16269 0.568 0.000 48.964 0.003 session.py:1042(_run) 56 0.532 0.010 0.532 0.010 {built-in method time.sleep} 153641 0.458 0.000 0.460 0.000 {built-in method numpy.core.multiarray.array} 183148/125354 0.447 0.000 1.316 0.000 python_message.py:469(init) 1226659 0.362 0.000 0.364 0.000 {built-in method builtins.getattr} 2302110/2301986 0.339 0.000 0.358 0.000 {built-in method builtins.isinstance} 8 0.285 0.036 0.285 0.036 {built-in method tensorflow.python._pywrap_tensorflow_internal.TF_ExtendGraph} 12150 0.267 0.000 0.271 0.000 callbacks.py:211(on_batch_end) 147026/49078 0.264 0.000 1.429 0.000 python_message.py:1008(ByteSize) ... This is my code. def train_generator(x_list, y_list): # 0.1 validatioin split train_length = (len(x_list)//10)*9 while True: for i in range(train_length): train_x = np.array([x_list[i]]) train_y = np.array([y_list[i]]) yield train_x, train_y def val_generator(x_list, y_list): # 0.1 validation split val_length = len(x_list)//10 while True: for i in range(-val_length, 0, 1): val_x = np.array([x_list[i]]) val_y = np.array([y_list[i]]) yield val_x, val_y with tf.Session(config = tf.ConfigProto(log_device_placement = True)): model = Sequential() model.add(LSTM(64, return_sequences=False, input_shape=(None, 24))) model.add(Dense(1)) model.compile(loss='mae', optimizer='adam') checkpointer = ModelCheckpoint(filepath=""weights.hdf5"", monitor='val_loss', verbose=1, save_best_only=True) history = model.fit_generator(generator=train_generator(train_x, train_y), steps_per_epoch=(len(train_x)//10)*9, epochs=5, validation_data=val_generator(train_x, train_y), validation_steps=len(train_x)//10, callbacks=[checkpointer], verbose=2, shuffle=False) # plot history pyplot.plot(history.history['loss'], label='train') pyplot.plot(history.history['val_loss'], label='validation') pyplot.legend() pyplot.show() I expect a significant speed up when using GPU for training. How can I fix this? Can someone help me to understand what is causing the slowdown? Thank you.",|tensorflow|keras|gpu|tensorflow2.x|,GPU Usage,3
56774954,"Approximation of funtion with multi-dimensional output using a keras neural network. As part of a project for my studies I want to try and approximate a function f:R^m -> R^n using a Keras neural network (to which I am completely new). The network seems to be learning to some (indeed unsatisfactory) point. But the predictions of the network don't resemble the expected results in the slightest. I have two numpy-arrays containing the training-data (the m-dimensional input for the function) and the training-labels (the n-dimensional expected output of the function). I use them for training my Keras model (see below), which seems to be learning on the provided data. inputs = Input(shape=(m,)) hidden = Dense(100, activation='sigmoid')(inputs) hidden = Dense(80, activation='sigmoid')(hidden) outputs = Dense(n, activation='softmax')(hidden) opti = tf.keras.optimizers.Adam(lr=0.001) model = Model(inputs=inputs, outputs=outputs) model.compile(optimizer=opti, loss='poisson', metrics=['accuracy']) model.fit(training_data, training_labels, verbose = 2, batch_size=32, epochs=30) When I call the evaluate-method on my model with a set of test-data and a set of test-labels, I get an apparent accuracy of more than 50%. However, when I use the predict method, the predictions of the network do not resemble the expected results in the slightest. For example, the first ten entries of the expected output are: [0., 0.08193582, 0.13141066, 0.13495408, 0.16852582, 0.2154705 , 0.30517559, 0.32567417, 0.34073457, 0.37453226] whereas the first ten entries of the predicted results are: [3.09514281e-09, 2.20849714e-03, 3.84095078e-03, 4.99367528e-03, 6.06226595e-03, 7.18442770e-03, 8.96730460e-03, 1.03423093e-02, 1.16029680e-02, 1.31887039e-02] Does this have something to do with the metrics I use? Could the results be normalized by Keras in some intransparent way? Have I just used the wrong kind of model for the problem I want to solve? What does 'accuracy' mean anyway? Thank you in advance for your help, I am new to neural networks and have been stuck with this issue for several days.",|python|keras|neural-network|deep-learning|approximation|,Model,0
56999493,"Tensorflow: GPU Acceleration only happens after first run. I've installed CUDA and CUDNN on my machine (Ubuntu 16.04) alongside tensorflow-gpu. Versions used: CUDA 10.0, CUDNN 7.6, Python 3.6, Tensorflow 1.14 This is the output from nvidia-smi, showing the video card configuration. | NVIDIA-SMI 410.78 Driver Version: 410.78 CUDA Version: 10.0 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 960M On | 00000000:02:00.0 Off | N/A | | N/A 44C P8 N/A / N/A | 675MiB / 4046MiB | 0% E. Process | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1502 G /usr/lib/xorg/Xorg 363MiB | | 0 3281 G compiz 96MiB | | 0 4375 G ...uest-channel-token=14359313252217012722 69MiB | | 0 5157 C ...felipe/proj/venv/bin/python3.6 141MiB | +-----------------------------------------------------------------------------+ This is the output from device_lib.list_local_devices() (tensorflow helper method to show what devices it can see), showing that my GPU is visible to tensorflow: [name: ""/device:CPU:0"" device_type: ""CPU"" memory_limit: 268435456 locality { } incarnation: 5096693727819965430, name: ""/device:XLA_GPU:0"" device_type: ""XLA_GPU"" memory_limit: 17179869184 locality { } incarnation: 13415556283266501672 physical_device_desc: ""device: XLA_GPU device"", name: ""/device:XLA_CPU:0"" device_type: ""XLA_CPU"" memory_limit: 17179869184 locality { } incarnation: 14339781620792127180 physical_device_desc: ""device: XLA_CPU device"", name: ""/device:GPU:0"" device_type: ""GPU"" memory_limit: 3464953856 locality { bus_id: 1 links { } } incarnation: 13743207545082600644 physical_device_desc: ""device: 0, name: GeForce GTX 960M, pci bus id: 0000:02:00.0, compute capability: 5.0"" ] Now as for actually using the GPU for computations. I've used a small piece of code to run some dummy matrix multiplications on the CPUs and on the GPUs, to compare the performance: shapes = [(50, 50), (100, 100), (500, 500), (1000, 1000), (10000,10000), (15000,15000)] devices = ['/device:CPU:0', '/device:XLA_GPU:0'] for device in devices: for shape in shapes: with tf.device(device): random_matrix = tf.random_uniform(shape=shape, minval=0, maxval=1) dot_operation = tf.matmul(random_matrix, tf.transpose(random_matrix)) sum_operation = tf.reduce_sum(dot_operation) # Time the actual runtime of the operations start_time = datetime.now() with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as session: result = session.run(sum_operation) elapsed_time = datetime.now() - start_time # PRINT ELAPSED TIME, SHAPE AND DEVICE USED Here is the surprise. The first time I run the cell containing this block of code (I'm on a jupyter notebook), the GPU computations take much longer than the CPU: # output of first run: CPU is faster ---------------------------------------- Input shape: (50, 50) using Device: /device:CPU:0 took: 0.01 Input shape: (100, 100) using Device: /device:CPU:0 took: 0.01 Input shape: (500, 500) using Device: /device:CPU:0 took: 0.01 Input shape: (1000, 1000) using Device: /device:CPU:0 took: 0.02 Input shape: (10000, 10000) using Device: /device:CPU:0 took: 6.22 Input shape: (15000, 15000) using Device: /device:CPU:0 took: 21.23 ---------------------------------------- Input shape: (50, 50) using Device: /device:XLA_GPU:0 took: 2.82 Input shape: (100, 100) using Device: /device:XLA_GPU:0 took: 0.17 Input shape: (500, 500) using Device: /device:XLA_GPU:0 took: 0.18 Input shape: (1000, 1000) using Device: /device:XLA_GPU:0 took: 0.20 Input shape: (10000, 10000) using Device: /device:XLA_GPU:0 took: 28.36 Input shape: (15000, 15000) using Device: /device:XLA_GPU:0 took: 93.73 ---------------------------------------- Surprise #2: When I rerun the cell containing the dummy matrix multiplication code, the GPU version is much faster (as expected): # output of reruns: GPU is faster ---------------------------------------- Input shape: (50, 50) using Device: /device:CPU:0 took: 0.02 Input shape: (100, 100) using Device: /device:CPU:0 took: 0.02 Input shape: (500, 500) using Device: /device:CPU:0 took: 0.02 Input shape: (1000, 1000) using Device: /device:CPU:0 took: 0.04 Input shape: (10000, 10000) using Device: /device:CPU:0 took: 6.78 Input shape: (15000, 15000) using Device: /device:CPU:0 took: 24.65 ---------------------------------------- Input shape: (50, 50) using Device: /device:XLA_GPU:0 took: 0.14 Input shape: (100, 100) using Device: /device:XLA_GPU:0 took: 0.12 Input shape: (500, 500) using Device: /device:XLA_GPU:0 took: 0.13 Input shape: (1000, 1000) using Device: /device:XLA_GPU:0 took: 0.14 Input shape: (10000, 10000) using Device: /device:XLA_GPU:0 took: 1.64 Input shape: (15000, 15000) using Device: /device:XLA_GPU:0 took: 5.29 ---------------------------------------- So my question is: Why is it that only after I run the code once does GPU acceleration actually occur? I can see the GPU is correctly set up (otherwise no acceleration would happen at all). Is it due to some sort of initial overhead? Do GPUs need to warm-up before we can actually use them? P.S.: On both runs (i.e. the one where the GPU was slower and the next ones, where the GPU was faster), I could see GPU Usage was 100%, so it was definitely being used. P.S.: Only in the very first run does it seem the GPU isn't get picked up. If I then run it two, three or multiple times, all runs after the first one are successful (i.e. GPU computation is faster).",|python|tensorflow|gpu|nvidia|,GPU Usage,3
57082306,"tensor dot product in keras. I am new to keras, and got some problems understanding the keras.layers.Dot() layer. I am trying to calculate a dot product of two vectors. from keras.layers import Input, Dot from keras.models import Model import numpy as np x1 = Input(shape=(4,)) x2 = Input(shape=(4,)) y1 = Dot(axes=1)([x1,x2]) model = Model(inputs=[x1, x2], outputs=y1) a1 = np.arange(4) a2=np.arange(4) model.predict([a1,a2]) I expect the output to be 14=0+1^2+2^2+3^2. However, I got error message like below: ValueError: Error when checking input: expected input_46 to have shape (4,) but got array with shape (1,) I tried to run model.get_config(), and below is the corresponding information about the graph of the model. As you can see input_46 is x1, and input_47 is x2. {'name': 'model_19', 'layers': [{'name': 'input_46', 'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 4), 'dtype': 'float32', 'sparse': False, 'name': 'input_46'}, 'inbound_nodes': []}, {'name': 'input_47', 'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 4), 'dtype': 'float32', 'sparse': False, 'name': 'input_47'}, 'inbound_nodes': []}, {'name': 'dot_20', 'class_name': 'Dot', 'config': {'name': 'dot_20', 'trainable': True, 'axes': 1, 'normalize': False}, 'inbound_nodes': [[['input_46', 0, 0, {}], ['input_47', 0, 0, {}]]]}], 'input_layers': [['input_46', 0, 0], ['input_47', 0, 0]], 'output_layers': [['dot_20', 0, 0]]} Is there anything I didn't do right? Thanks! UPDATE The following code worked: x1 = Input(shape=(4,)) x2 = Input(shape=(4,)) y1 = Dot(axes=1)([x1,x2]) model = Model(inputs=[x1, x2], outputs=y1) a1 = np.arange(4).reshape(1,4) a2=np.arange(4).reshape(1,4) model.predict([a1,a2]) or from keras.layers import Input, Dot from keras.models import Model import numpy as np x1 = Input(shape=(4,)) x2 = Input(shape=(4,)) y1 = Dot(axes=1)([x1,x2]) model = Model(inputs=[x1, x2], outputs=y1) a1 = np.arange(4) a2=np.arange(4) model.predict([[a1],[a2]])",|python|keras|tensor|dot-product|,Tensors&Inputs,1
57188409,"Assigning a parameter to the GPU sets is_leaf as false. If I create a Parameter in PyTorch, then it is automatically assigned as a leaf variable: x = torch.nn.Parameter(torch.Tensor([0.1])) print(x.is_leaf) This prints out True. From what I understand, if x is a leaf variable, then it will be updated by the optimiser. But if I then assign x to the GPU: x = torch.nn.Parameter(torch.Tensor([0.1])) x = x.cuda() print(x.is_leaf) This prints out False. So now I cannot assign x to the GPU and keep it as a leaf node. Why does this happen?",|pytorch|cuda|gpu|autograd|,GPU Usage,3
57253395,"Tensorflow only sees XLA_GPUs and cannot use them. I have a machine with 8 GPUS (4x GPU GTX 1080 Ti of 11 Gb de RAM and 4x RTX 1080) and cannot get tensorflow to use them correctly (or at all). When I do from tensorflow.python.client import device_lib print(device_lib.list_local_devices()) It prints [name: ""/device:CPU:0"" device_type: ""CPU"" memory_limit: 268435456 locality { } incarnation: 5295519098812813462 , name: ""/device:XLA_GPU:0"" device_type: ""XLA_GPU"" memory_limit: 17179869184 locality { } incarnation: 12186007115805339517 physical_device_desc: ""device: XLA_GPU device"" , name: ""/device:XLA_GPU:1"" device_type: ""XLA_GPU"" memory_limit: 17179869184 locality { } incarnation: 17706271046686153881 physical_device_desc: ""device: XLA_GPU device"" , name: ""/device:XLA_GPU:2"" device_type: ""XLA_GPU"" memory_limit: 17179869184 locality { } incarnation: 14710290295129432533 physical_device_desc: ""device: XLA_GPU device"" , name: ""/device:XLA_GPU:3"" device_type: ""XLA_GPU"" memory_limit: 17179869184 locality { } incarnation: 1381213064943868400 physical_device_desc: ""device: XLA_GPU device"" , name: ""/device:XLA_GPU:4"" device_type: ""XLA_GPU"" memory_limit: 17179869184 locality { } incarnation: 12093982778662340719 physical_device_desc: ""device: XLA_GPU device"" , name: ""/device:XLA_GPU:5"" device_type: ""XLA_GPU"" memory_limit: 17179869184 locality { } incarnation: 682960671898108683 physical_device_desc: ""device: XLA_GPU device"" , name: ""/device:XLA_GPU:6"" device_type: ""XLA_GPU"" memory_limit: 17179869184 locality { } incarnation: 9901240111105546679 physical_device_desc: ""device: XLA_GPU device"" , name: ""/device:XLA_GPU:7"" device_type: ""XLA_GPU"" memory_limit: 17179869184 locality { } incarnation: 8442134369143872649 physical_device_desc: ""device: XLA_GPU device"" , name: ""/device:XLA_CPU:0"" device_type: ""XLA_CPU"" memory_limit: 17179869184 locality { } incarnation: 1687638086072792879 physical_device_desc: ""device: XLA_CPU device"" ]. If I try to use the GPUs for anything, nvidia-smi says they are occupied, but running at 0%, and the speed of the task shows tensorflow is just using the CPU. In other machines, with the same setup, it prints too '/device:GPU:2' along with '/device:XLA_GPU:2' (for instance), and tensorflow is able to use them with no problem. I have already seen similar problems and solutions but none seems to work.",|python|tensorflow|gpu|,GPU Usage,3
57266256,"ImportError: cannot import name 'warmup_linear'. While trying to import warmup_linear, I'm getting this error ImportError: cannot import name 'warmup_linear' Import - from pytorch_pretrained_bert.optimization import BertAdam, warmup_linear Requirements file boto3==1.9.198 botocore==1.12.198 certifi==2019.6.16 chardet==3.0.4 docutils==0.14 h5py==2.9.0 idna==2.8 jmespath==0.9.4 Keras==2.2.4 Keras-Applications==1.0.8 Keras-Preprocessing==1.1.0 numpy==1.17.0 Pillow==6.1.0 python-dateutil==2.8.0 pytorch-pretrained-bert==0.6.2 PyYAML==5.1.1 regex==2019.6.8 requests==2.22.0 s3transfer==0.2.1 scipy==1.3.0 seqeval==0.0.12 six==1.12.0 torch==1.1.0 torchvision==0.3.0 tqdm==4.32.2 urllib3==1.25.3 What needs to be done to import 'warmup_linear'?",|pytorch|torch|bert-language-model|,API,4
57326611,"Tensorflow accuracy is 0.5. I'm working on an application that should predict interesting moments in audio files with a length of 10 seconds. I converted each 50ms of audio to a note, so each my record has 1 label (1,0 - interesting or not) and 200 note features. Then I created 200 train examples: from __future__ import absolute_import, division, print_function, unicode_literals from google.colab import drive import functools import tensorflow as tf import tensorflow_datasets as tfds from google.colab import drive drive.mount('/content/gdrive') def get_dataset(file_path): dataset = tf.data.experimental.make_csv_dataset( file_path, batch_size=12 label_name='label', na_value='?', num_epochs=1, ignore_errors=False) return dataset train = get_dataset('/content/gdrive/My Drive/myProject/train.csv') test = get_dataset('/content/gdrive/My Drive/myProject/test.csv') feature_columns = [] for number in range(200): feature_columns.append(tf.feature_column.numeric_column('note' + str(number + 1) )) preprocessing_layer = tf.keras.layers.DenseFeatures(feature_columns) model = tf.keras.Sequential([ preprocessing_layer, tf.keras.layers.Dense(50, activation=tf.nn.relu), tf.keras.layers.Dense(2, activation=tf.nn.softmax) ]) model.compile( loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) model.fit(train, epochs=20) Then my model returns such output on 20 epoch: 17/17 [==============================] - 0s 7ms/step - loss: 0.6959 - acc: 0.5000 What am I doing wrong?",|python|tensorflow|keras|neural-network|,Model,0
57328548,"Running Tensorflow on CPU is faster than running it on GPU. I have an ASUS n552vw laptop that has a 4GB dedicated Geforce GTX 960M graphic card. I put these lines of code in the beginning of my code to compare training speed using GPU or CPU, and I saw it seems using the CPU wins! For GPU: import os os.environ['CUDA_VISIBLE_DEVICES'] = '0' For CPU: import os os.environ['CUDA_VISIBLE_DEVICES'] = '-1' I have installed CUDA, cuDNN, tensorflow-gpu, etc to increase my training speed but seems inverse thing happened! When I try the first code, it says(before execution start): Train on 2128 samples, validate on 22 samples Epoch 1/1 2019-08-02 18:49:41.828287: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 2019-08-02 18:49:42.457662: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: name: GeForce GTX 960M major: 5 minor: 0 memoryClockRate(GHz): 1.176 pciBusID: 0000:01:00.0 totalMemory: 4.00GiB freeMemory: 3.34GiB 2019-08-02 18:49:42.458819: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0 2019-08-02 18:49:43.776498: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix: 2019-08-02 18:49:43.777007: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0 2019-08-02 18:49:43.777385: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0: N 2019-08-02 18:49:43.777855: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3050 MB memory) -> physical GPU (device: 0, name: GeForce GTX 960M, pci bus id: 0000:01:00.0, compute capability: 5.0) 2019-08-02 18:49:51.834610: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library cublas64_100.dll locally And it's really slow [Finished in 263.2s], But when I try the second code it says: Train on 2128 samples, validate on 22 samples Epoch 1/1 2019-08-02 18:51:43.021867: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 2019-08-02 18:51:43.641123: E tensorflow/stream_executor/cuda/cuda_driver.cc:300] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected 2019-08-02 18:51:43.645072: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:161] retrieving CUDA diagnostic information for host: DESKTOP-UQ8B9FK 2019-08-02 18:51:43.645818: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:168] hostname: DESKTOP-UQ8B9FK And it's much faster than the first code [Finished in 104.7s] ! How is it possible?? EDIT: This is the part of code that is related to Tensorflow : model = Sequential() model.add((LSTM(un , return_sequences = True))) model.add(Dropout(dp)) model.add((LSTM(un , return_sequences = True))) model.add(Dropout(dp)) model.add((LSTM(un , return_sequences = True))) model.add(Dropout(dp)) model.add((LSTM(un , return_sequences = True))) model.add(Dropout(dp)) model.add((LSTM(un , return_sequences = False))) model.add(Dropout(dp)) model.add(RepeatVector(rp)) model.add((LSTM(un , return_sequences= True))) model.add(Dropout(dp)) model.add((LSTM(un , return_sequences= True))) model.add(Dropout(dp)) model.add((LSTM(un , return_sequences= True))) model.add(Dropout(dp)) model.add((LSTM(un , return_sequences= True))) model.add(Dropout(dp)) model.add((LSTM(un , return_sequences= True))) model.add(Dropout(dp)) model.add(TimeDistributed(Dense(ds)))",|tensorflow|keras|windows-10|gpu|cpu|,GPU Usage,3
57397440,"Keras loss is not decreasing. I tried to learn my NN with breast Cancer Wisconsin (I add ""id"" column as an index and changed ""diagnosis"" column to 0 and 1 with sklearn.preprocessing.LabelEncoder), but my NN is not reducing Loss. I tried other optimizers and losses but this isn't working. That's my NN: from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, BatchNormalization, InputLayer import tensorflow.nn as tfnn model = Sequential() model.add(Dense(30, activation = tfnn.relu, input_dim = 30)) model.add(BatchNormalization(axis = 1)) model.add(Dense(60, activation = tfnn.relu)) model.add(BatchNormalization(axis = 1)) model.add(Dense(1, activation = tfnn.softmax)) model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy']) model.fit(data, target, epochs = 6) And my output: Epoch 1/6 569/569 [==============================] - 2s 3ms/sample - loss: 10.0025 - acc: 0.3726 Epoch 2/6 569/569 [==============================] - 0s 172us/sample - loss: 10.0025 - acc: 0.3726 Epoch 3/6 569/569 [==============================] - 0s 176us/sample - loss: 10.0025 - acc: 0.3726 Epoch 4/6 569/569 [==============================] - 0s 167us/sample - loss: 10.0025 - acc: 0.3726 Epoch 5/6 569/569 [==============================] - 0s 163us/sample - loss: 10.0025 - acc: 0.3726 Epoch 6/6 569/569 [==============================] - 0s 169us/sample - loss: 10.0025 - acc: 0.3726 I seems that NN after a few iterations stops learning (look at the time of epochs learning, in the first epoch it's 2s and in others it's 0s and in first epoch speed of processing the data is ms/sample, but in other epochs iits us/sample) Thank you for your time!",|python|tensorflow|machine-learning|keras|neural-network|,Model,0
57407973,"neural network validation accuracy doesn't change sometimes. I am using a neural network for a binary classification problem but I am running into some trouble. Sometimes when running my model, my validation accuracy doesn't change at all and sometimes it works just fine. My dataset has 1200 samples with 28 features and I have a class imbalance (200 class a 1000 class b).All my features are normalized and are between 1 and 0. As I stated before this problem doesn't always happen but I want to know why and fix it I have tried changing the optimisation function and the activation function but that did me no good. I have also noticed that when I increased the number of neurons in my network this problem occurs less often but it wasn't fixed.I also tried increasing the number of epochs but the problem keeps occuring sometimes model = Sequential() model.add(Dense(28, input_dim=28,kernel_initializer='normal', activation='sigmoid')) model.add(Dense(200, kernel_initializer='normal',activation='sigmoid')) model.add(Dropout(0.5)) model.add(Dense(300, kernel_initializer='normal',activation='sigmoid')) model.add(Dropout(0.5)) model.add(Dense(300, kernel_initializer='normal',activation='sigmoid')) model.add(Dropout(0.5)) model.add(Dense(150, kernel_initializer='normal',activation='sigmoid')) model.add(Dropout(0.4)) model.add(Dense(1,kernel_initializer='normal')) model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy']) history = model.fit(X_train, y_train, epochs=34, batch_size=32, validation_data=(X_val, y_val), verbose=1) This is the result I get sometimes from training my model Epoch 1/34 788/788 [==============================] - 1s 2ms/step - loss: 1.5705 - acc: 0.6865 - val_loss: 0.6346 - val_acc: 0.7783 Epoch 2/34 788/788 [==============================] - 0s 211us/step - loss: 1.0262 - acc: 0.6231 - val_loss: 0.5310 - val_acc: 0.7783 Epoch 3/34 788/788 [==============================] - 0s 194us/step - loss: 1.7575 - acc: 0.7221 - val_loss: 0.5431 - val_acc: 0.7783 Epoch 4/34 788/788 [==============================] - 0s 218us/step - loss: 0.9113 - acc: 0.5774 - val_loss: 0.5685 - val_acc: 0.7783 Epoch 5/34 788/788 [==============================] - 0s 199us/step - loss: 1.0987 - acc: 0.6688 - val_loss: 0.6435 - val_acc: 0.7783 Epoch 6/34 788/788 [==============================] - 0s 201us/step - loss: 0.9777 - acc: 0.5343 - val_loss: 0.5643 - val_acc: 0.7783 Epoch 7/34 788/788 [==============================] - 0s 204us/step - loss: 1.0603 - acc: 0.5914 - val_loss: 0.6266 - val_acc: 0.7783 Epoch 8/34 788/788 [==============================] - 0s 197us/step - loss: 0.7580 - acc: 0.5939 - val_loss: 0.6615 - val_acc: 0.7783 Epoch 9/34 788/788 [==============================] - 0s 206us/step - loss: 0.8950 - acc: 0.6650 - val_loss: 0.5291 - val_acc: 0.7783 Epoch 10/34 788/788 [==============================] - 0s 230us/step - loss: 0.8114 - acc: 0.6701 - val_loss: 0.5428 - val_acc: 0.7783 Epoch 11/34 788/788 [==============================] - 0s 281us/step - loss: 0.7235 - acc: 0.6624 - val_loss: 0.5275 - val_acc: 0.7783 Epoch 12/34 788/788 [==============================] - 0s 264us/step - loss: 0.7237 - acc: 0.6485 - val_loss: 0.5473 - val_acc: 0.7783 Epoch 13/34 788/788 [==============================] - 0s 213us/step - loss: 0.6902 - acc: 0.7056 - val_loss: 0.5265 - val_acc: 0.7783 Epoch 14/34 788/788 [==============================] - 0s 217us/step - loss: 0.6726 - acc: 0.7145 - val_loss: 0.5285 - val_acc: 0.7783 Epoch 15/34 788/788 [==============================] - 0s 197us/step - loss: 0.6656 - acc: 0.7132 - val_loss: 0.5354 - val_acc: 0.7783 Epoch 16/34 788/788 [==============================] - 0s 216us/step - loss: 0.6083 - acc: 0.7259 - val_loss: 0.5262 - val_acc: 0.7783 Epoch 17/34 788/788 [==============================] - 0s 218us/step - loss: 0.6188 - acc: 0.7310 - val_loss: 0.5271 - val_acc: 0.7783 Epoch 18/34 788/788 [==============================] - 0s 210us/step - loss: 0.6642 - acc: 0.6142 - val_loss: 0.5676 - val_acc: 0.7783 Epoch 19/34 788/788 [==============================] - 0s 200us/step - loss: 0.6017 - acc: 0.7221 - val_loss: 0.5256 - val_acc: 0.7783 Epoch 20/34 788/788 [==============================] - 0s 209us/step - loss: 0.6188 - acc: 0.7157 - val_loss: 0.8090 - val_acc: 0.2217 Epoch 21/34 788/788 [==============================] - 0s 201us/step - loss: 1.1724 - acc: 0.4061 - val_loss: 0.5448 - val_acc: 0.7783 Epoch 22/34 788/788 [==============================] - 0s 205us/step - loss: 0.5724 - acc: 0.7424 - val_loss: 0.5293 - val_acc: 0.7783 Epoch 23/34 788/788 [==============================] - 0s 234us/step - loss: 0.5829 - acc: 0.7538 - val_loss: 0.5274 - val_acc: 0.7783 Epoch 24/34 788/788 [==============================] - 0s 209us/step - loss: 0.5815 - acc: 0.7525 - val_loss: 0.5274 - val_acc: 0.7783 Epoch 25/34 788/788 [==============================] - 0s 220us/step - loss: 0.5688 - acc: 0.7576 - val_loss: 0.5274 - val_acc: 0.7783 Epoch 26/34 788/788 [==============================] - 0s 210us/step - loss: 0.5715 - acc: 0.7525 - val_loss: 0.5273 - val_acc: 0.7783 Epoch 27/34 788/788 [==============================] - 0s 206us/step - loss: 0.5584 - acc: 0.7576 - val_loss: 0.5274 - val_acc: 0.7783 Epoch 28/34 788/788 [==============================] - 0s 215us/step - loss: 0.5728 - acc: 0.7563 - val_loss: 0.5272 - val_acc: 0.7783 Epoch 29/34 788/788 [==============================] - 0s 281us/step - loss: 0.5735 - acc: 0.7576 - val_loss: 0.5275 - val_acc: 0.7783 Epoch 30/34 788/788 [==============================] - 0s 272us/step - loss: 0.5773 - acc: 0.7614 - val_loss: 0.5272 - val_acc: 0.7783 Epoch 31/34 788/788 [==============================] - 0s 225us/step - loss: 0.5847 - acc: 0.7525 - val_loss: 0.5272 - val_acc: 0.7783 Epoch 32/34 788/788 [==============================] - 0s 239us/step - loss: 0.5739 - acc: 0.7551 - val_loss: 0.5272 - val_acc: 0.7783 Epoch 33/34 788/788 [==============================] - 0s 216us/step - loss: 0.5632 - acc: 0.7525 - val_loss: 0.5269 - val_acc: 0.7783 Epoch 34/34 788/788 [==============================] - 0s 240us/step - loss: 0.5672 - acc: 0.7576 - val_loss: 0.5267 - val_acc: 0.7783",|python|machine-learning|keras|neural-network|deep-learning|,Model,0
57618482,"Higher loss penalty for true non-zero predictions. I am building a deep regression network (CNN) to predict a (1000,1) target vector from images (7,11). The target usually consists of about 90 % zeros and only 10 % non-zero values. The distribution of (non-) zero values in the targets vary from sample to sample (i.e. there is no global class imbalance). Using mean sqaured error loss, this led to the network predicting only zeros, which I don't find surprising. My best guess is to write a custom loss function that penalizes errors regarding non-zero values more than the prediction of zero-values. I have tried this loss function with the intend to implement what I have guessed could work above. It is a mean squared error loss in which the predictions of non-zero targets are penalized less (w=0.1). def my_loss(y_true, y_pred): # weights true zero predictions less than true nonzero predictions w = 0.1 y_pred_of_nonzeros = tf.where(tf.equal(y_true, 0), y_pred-y_pred, y_pred) return K.mean(K.square(y_true-y_pred_of_nonzeros)) + K.mean(K.square(y_true-y_pred))*w The network is able to learn without getting stuck with only-zero predictions. However, this solution seems quite unclean. Is there a better way to deal with this type of problem? Any advice on improving the custom loss function? Any suggestions are welcome, thank you in advance! Best, Lukas",|tensorflow|keras|deep-learning|loss-function|loss|,Training,2
57849535,"Would it possible to use all memory of GPUs with one model?. There is a model and two GPUs. I put the model on GPU with model.cuda(). If I passed a big image to the model, it allocated all memory of GPU0 and then it raised CUDA out of memory error without allocating any memory of GPU1. Because there is only one image every forward(), I can not use such torch.nn.DataParallel things to split input. Is there any way to use all the memory of GPUs when passing a image to the model? I'm using Python3.7 and Pytorch1.1.",|python|gpu|pytorch|,GPU Usage,3
57943425,"CNN architecture: classifying ""good"" and ""bad"" images. I'm researching the possibility of implementing a CNN in order to classify images as ""good"" or ""bad"" but am having no luck with my current architecture. Characteristics that denote a ""bad"" image: Overexposure Oversaturation Incorrect white balance Blurriness Would it be feasible to implement a neural network to classify images based on these characteristics or is it best left to a traditional algorithm that simply looks at the variance in brightness/contrast throughout an image and classifies it that way? I have attempted training a CNN using the VGGNet architecture but I always seem to get a biased and unreliable model, regardless of the number of epochs or number of steps. Examples: My current model's architecture is very simple (as I am new to the whole machine learning world) but seemed to work fine with other classification problems, and I have modified it slightly to work better with this binary classification problem: # CONV => RELU => POOL layer set # define convolutional layers, use ""ReLU"" activation function # and reduce the spatial size (width and height) with pool layers model.add(Conv2D(32, (3, 3), padding=""same"", input_shape=input_shape)) # 32 3x3 filters (height, width, depth) model.add(Activation(""relu"")) model.add(BatchNormalization(axis=channel_dimension)) model.add(MaxPooling2D(pool_size=(2,2))) model.add(Dropout(0.25)) # helps prevent overfitting (25% of neurons disconnected randomly) # (CONV => RELU) * 2 => POOL layer set (increasing number of layers as you go deeper into CNN) model.add(Conv2D(64, (3, 3), padding=""same"", input_shape=input_shape)) # 64 3x3 filters model.add(Activation(""relu"")) model.add(BatchNormalization(axis=channel_dimension)) model.add(Conv2D(64, (3, 3), padding=""same"", input_shape=input_shape)) # 64 3x3 filters model.add(Activation(""relu"")) model.add(BatchNormalization(axis=channel_dimension)) model.add(MaxPooling2D(pool_size=(2,2))) model.add(Dropout(0.25)) # helps prevent overfitting (25% of neurons disconnected randomly) # (CONV => RELU) * 3 => POOL layer set (input volume size becoming smaller and smaller) model.add(Conv2D(128, (3, 3), padding=""same"", input_shape=input_shape)) # 128 3x3 filters model.add(Activation(""relu"")) model.add(BatchNormalization(axis=channel_dimension)) model.add(Conv2D(128, (3, 3), padding=""same"", input_shape=input_shape)) # 128 3x3 filters model.add(Activation(""relu"")) model.add(BatchNormalization(axis=channel_dimension)) model.add(Conv2D(128, (3, 3), padding=""same"", input_shape=input_shape)) # 128 3x3 filters model.add(Activation(""relu"")) model.add(BatchNormalization(axis=channel_dimension)) model.add(MaxPooling2D(pool_size=(2,2))) model.add(Dropout(0.25)) # helps prevent overfitting (25% of neurons disconnected randomly) # only set of FC => RELU layers model.add(Flatten()) model.add(Dense(512)) model.add(Activation(""relu"")) model.add(BatchNormalization()) model.add(Dropout(0.5)) # sigmoid classifier (output layer) model.add(Dense(classes)) model.add(Activation(""sigmoid"")) Is there any glaring omissions or mistakes with this model or can I simply not solve this problem using deep learning (with my current GPU, a GTX 970)? Thanks for your time and experience, Josh EDIT: Here is my code for compiling/training the model: # initialise the model and optimiser print(""[INFO] Training network..."") opt = SGD(lr=initial_lr, decay=initial_lr / epochs) model.compile(loss=""sparse_categorical_crossentropy"", optimizer=opt, metrics=[""accuracy""]) # set up checkpoints model_name = ""output/50_epochs_{epoch:02d}_{val_acc:.2f}.model"" checkpoint = ModelCheckpoint(model_name, monitor='val_acc', verbose=1, save_best_only=True, mode='max') reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001) tensorboard = TensorBoard(log_dir=""logs/{}"".format(time())) callbacks_list = [checkpoint, reduce_lr, tensorboard] # train the network H = model.fit_generator(training_set, steps_per_epoch=500, epochs=50, validation_data=test_set, validation_steps=150, callbacks=callbacks_list)",|python|machine-learning|keras|deep-learning|conv-neural-network|,Model,0
58043002,"Training and Testing accuracy not increasing for a CNN followed by a RNN for signature verification. I'm currently working on online signature verification. The dataset has a variable shape of (x, 7) where x is the number of points a person used to sign their signature. I have the following model: model = Sequential() #CNN model.add(Conv1D(filters=64, kernel_size=3, activation='sigmoid', input_shape=(None, 7))) model.add(MaxPooling1D(pool_size=3)) model.add(Conv1D(filters=64, kernel_size=2, activation='sigmoid')) #RNN model.add(Masking(mask_value=0.0)) model.add(LSTM(8)) model.add(Dense(2, activation='softmax')) opt = Adam(lr=0.0001) model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy']) model.summary() print(model.fit(x_train, y_train, epochs=100, verbose=2, batch_size=50)) score, accuracy = model.evaluate(x_test,y_test, verbose=2) print(score, accuracy) I know it may not be the best model but this is the first time I'm building a neural network. I have to use a CNN and RNN as it is required for my honours project. At the moment, I achieve 0.5142 as the highest training accuracy and 0.54 testing accuracy. I have tried increasing the number of epochs, changing the activation function, add more layers, moving the layers around, changing the learning rate and changing the optimizer. Please share some advice on changing my model or dataset. Any help is much appreciated.",|python|keras|conv-neural-network|recurrent-neural-network|,Model,0
58064701,"How to optimize accuracy of ANN. I have 50 target classes of 300 datasets. This is my sample dataset, with 98 features: import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt %matplotlib inline dataset = pd.read_csv(root_path + 'pima-indians-diabetes.data.csv', header=None) X= dataset.iloc[:,0:8] y= dataset.iloc[:,8] from sklearn.preprocessing import StandardScaler sc = StandardScaler() X = sc.fit_transform(X) from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3) from keras import Sequential from keras.layers import Dense classifier = Sequential() #First Hidden Layer classifier.add(Dense(units = 10, activation='relu',kernel_initializer='random_normal', input_dim=8)) #Second Hidden Layer classifier.add(Dense(units = 10, activation='relu',kernel_initializer='random_normal')) #Output Layer classifier.add(Dense(units = 1, activation='sigmoid',kernel_initializer='random_normal')) #Compiling the neural network classifier.compile(optimizer ='adam',loss='binary_crossentropy', metrics =['accuracy']) #Fitting the data to the training dataset classifier.fit(X_train,y_train, batch_size=2, epochs=10) I get 19% accuracy here, and I don't know how to optimize my prediction result.",|python|keras|neural-network|,Model,0
58051767,"Keras - how to predict two values instead of one value?. I'm learning machine learning and my dataset consists of 7 columns: home_team, away_team, home_odds, away_odds, home_score, away_score, 1_if_home_wins_else_0 To be able to feed the Tensorflow with teams, I converted every team to integer so the first two columns are integers (like database ids) There is a 10k rows in the csv. example I modified the code for pima indians diabetes to predict winnings of home team. So now it ""predicts"" whether the home team win (1) otherwise 0. Now I would like to modify the algorithm to predict exact score home_score, away_score. I know the outputs will be wrong it's just learning. code # load the dataset dataset = loadtxt('football_data.csv', delimiter=',') # split into input (X) and output (y) variables X = dataset[:, 0:4] y = dataset[:, 6] # define the keras model model = Sequential() model.add(Dense(12, input_dim=4, activation='relu')) model.add(Dense(8, activation='relu')) model.add(Dense(1, activation='sigmoid')) # compile the keras model model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) # fit the keras model on the dataset model.fit(X, y, epochs=150, batch_size=10) # evaluate the keras model _, accuracy = model.evaluate(X, y) print('Accuracy: %.2f' % (accuracy * 100)) # make class predictions with the model predictions = model.predict_classes(X) # summarize the first 5 cases for i in range(50): print('%s => %d (expected %d)' % (X[i].tolist(), predictions[i], y[i])) Do you know how to do that?",|python|machine-learning|keras|neural-network|,Model,0
58055105,"Keras model producing same output. I've seen a couple questions that have a similar problem, but none of them solved mine. I'm trying to fit a neural network in Keras to a dataset with 22 input features for binary classification. The problem is that I only have 195 training samples. I know it's a small dataset, but I don't know if it's possible to fit a model with reasonable accuracy (I'm aiming for >95% accuracy). The problem I'm having is that my model is only outputting 1 and getting 75% accuracy because my dataset is 75% positive cases. Here's the code I have: data = pd.read_csv("""") #filename omitted, but it loads properly scaler = MinMaxScaler() X = scaler.fit_transform(X) Y = data['status'] X = data.drop(['status', 'name'], axis = 1) xTrain, xTest, yTrain, yTest = train_test_split(X, Y, train_size = 0.8) model = Sequential() model.add(Dense(48, input_shape=(22,), activation = 'relu')) model.add(Dropout(0.5)) model.add(Dense(1, activation = 'softmax')) optim = keras.optimizers.adam(lr=0.0001) model.compile(optimizer = optim, loss = 'binary_crossentropy', metrics = ['accuracy']) model.fit(xTrain, yTrain, epochs = 20, batch_size = 5, validation_data = (xTest, yTest)) I've tried adding more hidden layers, increasing the number of training epochs, and increased and lowered the optimizer's learning rate, but the accuracy stays the same. Here's the link to the dataset: https://www.dropbox.com/s/c4td650b4z7aizc/fixed.xlsx?dl=0",|python|machine-learning|keras|,Model,0
58136592,"For a given condition, get indices of values in 2D tensor A, use those to index a 3D tensor B. For a given 2D tensor I want to retrieve all indices where the value is 1. I expected to be able to simply use torch.nonzero(a == 1).squeeze(), which would return tensor([1, 3, 2]). However, instead, torch.nonzero(a == 1) returns a 2D tensor (that's okay), with two values per row (that's not what I expected). The returned indices should then be used to index the second dimension (index 1) of a 3D tensor, again returning a 2D tensor. import torch a = torch.Tensor([[12, 1, 0, 0], [4, 9, 21, 1], [10, 2, 1, 0]]) b = torch.rand(3, 4, 8) print('a_size', a.size()) # a_size torch.Size([3, 4]) print('b_size', b.size()) # b_size torch.Size([3, 4, 8]) idxs = torch.nonzero(a == 1) print('idxs_size', idxs.size()) # idxs_size torch.Size([3, 2]) print(b.gather(1, idxs)) Evidently, this does not work, leading to aRunTimeError: RuntimeError: invalid argument 4: Index tensor must have same dimensions as input tensor at C:\w\1\s\windows\pytorch\aten\src\TH/generic/THTensorEvenMoreMath.cpp:453 It seems that idxs is not what I expect it to be, nor can I use it the way I thought. idxs is tensor([[0, 1], [1, 3], [2, 2]]) but reading through the documentation I don't understand why I also get back the row indices in the resulting tensor. Now, I know I can get the correct idxs by slicing idxs[:, 1] but then still, I cannot use those values as indices for the 3D tensor because the same error as before is raised. Is it possible to use the 1D tensor of indices to select items across a given dimension?",|python|multidimensional-array|pytorch|tensor|tensor-indexing|,Tensors&Inputs,1
58237726,"Keras model fails to decrease loss. I propose a example in which a tf.keras model fails to learn from very simple data. I'm using tensorflow-gpu==2.0.0, keras==2.3.0 and Python 3.7. At the end of my post, I give the Python code to reproduce the problem I observed. Data The samples are Numpy arrays of shape (6, 16, 16, 16, 3). To make things very simple, I only consider arrays full of 1s and 0s. Arrays with 1s are given the label 1 and arrays with 0s are given the label 0. I can generate some samples (in the following, n_samples = 240) with this code: def generate_fake_data(): for j in range(1, 240 + 1): if j < 120: yield np.ones((6, 16, 16, 16, 3)), np.array([0., 1.]) else: yield np.zeros((6, 16, 16, 16, 3)), np.array([1., 0.]) In order to input this data in a tf.keras model, I create an instance of tf.data.Dataset using the code below. This will essentially create shuffled batches of BATCH_SIZE = 12 samples. def make_tfdataset(for_training=True): dataset = tf.data.Dataset.from_generator(generator=lambda: generate_fake_data(), output_types=(tf.float32, tf.float32), output_shapes=(tf.TensorShape([6, 16, 16, 16, 3]), tf.TensorShape([2]))) dataset = dataset.repeat() if for_training: dataset = dataset.shuffle(buffer_size=1000) dataset = dataset.batch(BATCH_SIZE) dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE) return dataset Model I propose the following model to classify my samples: def create_model(in_shape=(6, 16, 16, 16, 3)): input_layer = Input(shape=in_shape) reshaped_input = Lambda(lambda x: K.reshape(x, (-1, *in_shape[1:])))(input_layer) conv3d_layer = Conv3D(filters=64, kernel_size=8, strides=(2, 2, 2), padding='same')(reshaped_input) relu_layer_1 = ReLU()(conv3d_layer) pooling_layer = GlobalAveragePooling3D()(relu_layer_1) reshape_layer_1 = Lambda(lambda x: K.reshape(x, (-1, in_shape[0] * 64)))(pooling_layer) expand_dims_layer = Lambda(lambda x: K.expand_dims(x, 1))(reshape_layer_1) conv1d_layer = Conv1D(filters=1, kernel_size=1)(expand_dims_layer) relu_layer_2 = ReLU()(conv1d_layer) reshape_layer_2 = Lambda(lambda x: K.squeeze(x, 1))(relu_layer_2) out = Dense(units=2, activation='softmax')(reshape_layer_2) return Model(inputs=[input_layer], outputs=[out]) The model is optimized using Adam (with default parameters) and with the binary_crossentropy loss: clf_model = create_model() clf_model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy', 'categorical_crossentropy']) The output of clf_model.summary() is: Model: ""model"" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) [(None, 6, 16, 16, 16, 3) 0 _________________________________________________________________ lambda (Lambda) (None, 16, 16, 16, 3) 0 _________________________________________________________________ conv3d (Conv3D) (None, 8, 8, 8, 64) 98368 _________________________________________________________________ re_lu (ReLU) (None, 8, 8, 8, 64) 0 _________________________________________________________________ global_average_pooling3d (Gl (None, 64) 0 _________________________________________________________________ lambda_1 (Lambda) (None, 384) 0 _________________________________________________________________ lambda_2 (Lambda) (None, 1, 384) 0 _________________________________________________________________ conv1d (Conv1D) (None, 1, 1) 385 _________________________________________________________________ re_lu_1 (ReLU) (None, 1, 1) 0 _________________________________________________________________ lambda_3 (Lambda) (None, 1) 0 _________________________________________________________________ dense (Dense) (None, 2) 4 ================================================================= Total params: 98,757 Trainable params: 98,757 Non-trainable params: 0 Training The model is trained for 500 epochs as follows: train_ds = make_tfdataset(for_training=True) history = clf_model.fit(train_ds, epochs=500, steps_per_epoch=ceil(240 / BATCH_SIZE), verbose=1) The problem! During the 500 epochs, the model loss stays around 0.69 and never goes below 0.69. This is also true if I set the learning rate to 1e-2 instead of 1e-3. The data is very simple (just 0s and 1s). Naively, I would expect the model to have a better accuracy than just 0.6. In fact, I would expect it to reach 100% accuracy quickly. What I am doing wrong? The full code... import numpy as np import tensorflow as tf import tensorflow.keras.backend as K from math import ceil from tensorflow.keras.layers import Input, Dense, Lambda, Conv1D, GlobalAveragePooling3D, Conv3D, ReLU from tensorflow.keras.models import Model from tensorflow.keras.optimizers import Adam BATCH_SIZE = 12 def generate_fake_data(): for j in range(1, 240 + 1): if j < 120: yield np.ones((6, 16, 16, 16, 3)), np.array([0., 1.]) else: yield np.zeros((6, 16, 16, 16, 3)), np.array([1., 0.]) def make_tfdataset(for_training=True): dataset = tf.data.Dataset.from_generator(generator=lambda: generate_fake_data(), output_types=(tf.float32, tf.float32), output_shapes=(tf.TensorShape([6, 16, 16, 16, 3]), tf.TensorShape([2]))) dataset = dataset.repeat() if for_training: dataset = dataset.shuffle(buffer_size=1000) dataset = dataset.batch(BATCH_SIZE) dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE) return dataset def create_model(in_shape=(6, 16, 16, 16, 3)): input_layer = Input(shape=in_shape) reshaped_input = Lambda(lambda x: K.reshape(x, (-1, *in_shape[1:])))(input_layer) conv3d_layer = Conv3D(filters=64, kernel_size=8, strides=(2, 2, 2), padding='same')(reshaped_input) relu_layer_1 = ReLU()(conv3d_layer) pooling_layer = GlobalAveragePooling3D()(relu_layer_1) reshape_layer_1 = Lambda(lambda x: K.reshape(x, (-1, in_shape[0] * 64)))(pooling_layer) expand_dims_layer = Lambda(lambda x: K.expand_dims(x, 1))(reshape_layer_1) conv1d_layer = Conv1D(filters=1, kernel_size=1)(expand_dims_layer) relu_layer_2 = ReLU()(conv1d_layer) reshape_layer_2 = Lambda(lambda x: K.squeeze(x, 1))(relu_layer_2) out = Dense(units=2, activation='softmax')(reshape_layer_2) return Model(inputs=[input_layer], outputs=[out]) train_ds = make_tfdataset(for_training=True) clf_model = create_model(in_shape=(6, 16, 16, 16, 3)) clf_model.summary() clf_model.compile(optimizer=Adam(lr=1e-3), loss='categorical_crossentropy', metrics=['accuracy', 'categorical_crossentropy']) history = clf_model.fit(train_ds, epochs=500, steps_per_epoch=ceil(240 / BATCH_SIZE), verbose=1)",|python|tensorflow|keras|deep-learning|tensorflow-datasets|,Training,2
58333926,"Keras Image Classification Loss. I am trying to make a simple CNN model which can recognize Pokemons. For the first try, I created by myself a very small dataset, composed of 100 pictures of 10 different Pokemons. Using this code in Python, it seems like it is working well. import tensorflow as tf import numpy as np model = tf.keras.models.Sequential() model.add(tf.keras.layers.Conv2D(32, (3,3), input_shape=(200,200,3), activation='relu')) model.add(tf.keras.layers.MaxPooling2D((2,2))) model.add(tf.keras.layers.Conv2D(32, (3,3), activation='relu')) model.add(tf.keras.layers.MaxPooling2D((2,2))) model.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Dense(units=400, activation='relu')) model.add(tf.keras.layers.Dense(units=10, activation='sigmoid')) model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) train = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True) test = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255) training_set= train.flow_from_directory('datasets/starter/train', target_size=(200,200), class_mode='categorical') val_set= test.flow_from_directory('datasets/starter/test', target_size=(200,200), class_mode='categorical') history=model.fit_generator(training_set, steps_per_epoch=32, epochs=3, validation_data=val_set, validation_steps=32) test_image = tf.keras.preprocessing.image.load_img('datasets/starter/val/attempt.png', target_size=(200, 200)) test_image = tf.keras.preprocessing.image.img_to_array(test_image) test_image = np.expand_dims(test_image, axis=0) result = model.predict(test_image) print(training_set.class_indices) print(result) test_image2 = tf.keras.preprocessing.image.load_img('datasets/starter/val/attempt2.png', target_size=(200, 200)) test_image2 = tf.keras.preprocessing.image.img_to_array(test_image2) test_image2 = np.expand_dims(test_image2, axis=0) result2 = model.predict(test_image2) print(training_set.class_indices) print(result2) The training accuracy at the last epoch is fixed at 1. When I try to predict the example images: attempt.png is a Charmander picture, its label is 1, so I am getting this vector: [[0. 1. 0. 0. ... 0.]] attempt2.png is a Torchic picture, its label is 7, so I am getting: [[0. 0. ... 1. 0. 0. ]] But I noticed that the loss in the 'model.compile' should be 'categorical_crossentropy' and not 'binary_crossentropy'. Using the categorical one, my program will not work anymore. Can someone help me to understand?",|python|tensorflow|keras|conv-neural-network|image-recognition|,Model,0
58343317,"Why does logical_gpus = tf.config.experimental.list_logical_devices('GPU') lead to an empty list?. I use Tensorflow version 2.0 and will like to configure the GPU's with it. for Tensorflow 1.x, it was done in following way # GPU configuration from keras.backend.tensorflow_backend import set_session import keras configtf = tf.compat.v1.ConfigProto() configtf.gpu_options.allow_growth = True configtf.gpu_options.visible_device_list = ""0"" sess = tf.compat.v1.Session(config=configtf) set_session(sess) However, set_session is not longer available in Tensorflow 2.0, so to use access GPU's, I tried following this guide. Both the codes below lead to empty list of available GPUs, which means tensorflow is not using them. gpus = tf.config.experimental.list_physical_devices(""GPU"") gpus logical_gpus = tf.config.experimental.list_logical_devices('GPU') logical_gpus I do have Tesla K80 access available. What will be the right way to configure tf for it to use the available GPUs? Any help will be appreciated.",|gpu|tensorflow2.0|,GPU Usage,3
58392266,"My binary classification model's accuracy seems stuck: Where did i go wrong?. I have attempted training a model for binary classification problem using a dataset made of medical pictures(LIDC dataset to be exact), which, too my understanding, shouldnt be too far from the ""dog vs cat"" classification problem. ( distinguishing between benign and malignant nodules ) Problem is, the accuracy seems to stucks, from the beginning to about 65% and doesnt seem to change at all. Am i doing something wrong ? The example that was provided by my friend easily reaches more than 80% and the accuracy is improving epoch after epoch, but not mine :/ I am training on already extracted patches, all of the same size and separated into two classes. The model i am using a VGG16 fine-tuned for this task ( i replaced the FC layers with new ones, freezed the previous layer and attempted training ) I have tried changing to binary_crossentropy, applying to_categorical, change the last layer from 1 to 2. At his point, i am confused on the correct combination of parameters for my problems. Sorry if i sound like an absolute beginner... i tried skipping the less informative parts of the code, hope its readable training_data_dir = ""flow/training"" validation_data_dir = ""flow/validation"" test_data_dir = ""flow/test"" from keras.preprocessing.image import ImageDataGenerator training_data_generator = ImageDataGenerator(rescale=1./255) validation_data_generator = ImageDataGenerator(rescale=1./255) test_data_generator = ImageDataGenerator(rescale=1./255) training_generator = training_data_generator.flow_from_directory( training_data_dir, target_size=(IMAGE_WIDTH, IMAGE_HEIGHT), batch_size=BATCH_SIZE, class_mode=""binary"") validation_generator = validation_data_generator.flow_from_directory( validation_data_dir, target_size=(IMAGE_WIDTH, IMAGE_HEIGHT), batch_size=BATCH_SIZE, class_mode=""binary"") test_generator = test_data_generator.flow_from_directory( test_data_dir, target_size=(IMAGE_WIDTH, IMAGE_HEIGHT), batch_size=1, class_mode=""binary"", shuffle=False) vgg16_model = VGG16(weights=""imagenet"", include_top=False,input_tensor=Input(shape=(57, 57, 3))) vgg16_model.summary() model = Sequential() for layer in vgg16_model.layers: layer.trainable = False model.add(layer) model.add(Flatten()) model.add(Dense(4096, activation='relu')) model.add(Dense(1024, activation='relu')) model.add(Dense(1, activation='softmax')) model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(lr=1e-5), metrics=['accuracy']) model.fit_generator(training_generator, steps_per_epoch=len(training_generator.filenames) // BATCH_SIZE, epochs=EPOCHS, validation_data=validation_generator, validation_steps=len(validation_generator.filenames) // BATCH_SIZE, verbose=2)",|python|machine-learning|keras|,Model,0
58572274,"Why Can't I train the ANN for XNOR?. I have made a simple NN for deciding the XNOR values with the Two Binary values in the Input layer. I have the Numpy array of all the possible combinations with the lables. Code : from keras.models import Sequential from keras.layers import Dense import numpy data = numpy.array([[0.,0.,1.],[0.,1.,0.],[1.,0.,0.],[1.,1.,1.]]) train = data[:,:-1] # Taking The same and All data for training test = data[:,:-1] train_l = data[:,-1] test_l = data[:,-1] train_label = [] test_label = [] for i in train_l: train_label.append([i]) for i in test_l: test_label.append([i]) # Just made Labels Single element... train_label = numpy.array(train_label) test_label = numpy.array(test_label) # Numpy Conversion model = Sequential() model.add(Dense(2,input_dim = 2,activation = 'relu')) model.add(Dense(2,activation = 'relu')) model.add(Dense(1,activation = 'relu')) model.compile(loss = ""binary_crossentropy"" , metrics = ['accuracy'], optimizer = 'adam') model.fit(train,train_label, epochs = 10, verbose=2) model.predict_classes(test) Even if taking the Same dataset to train and to test... It doesn't predict properly ... Where was I wrong ? I have taken whole dataset deliberately as it wasn't predicting with 2 values...",|python|numpy|tensorflow|machine-learning|keras|,Model,0
58581464,"PyTorch embedding layer raises ""expected...cuda...but got...cpu"" error. I'm working on translating a PyTorch model from CPU (where it works) to GPU (where it so far doesn't). The error message (clipped to the important bits) is as follows: --------------------------------------------------------------------------- RuntimeError Traceback (most recent call last) <ipython-input-12-a7bb230c924c> in <module> 1 model = FeedforwardTabularModel() 2 model.cuda() ----> 3 model.fit(X_train_sample.values, y_train_sample.values) <ipython-input-11-40b1edae7417> in fit(self, X, y) 100 for epoch in range(self.n_epochs): 101 for i, (X_batch, y_batch) in enumerate(batches): --> 102 y_pred = model(X_batch).squeeze() 103 # scheduler.batch_step() # Disabled due to a bug, see above. 104 loss = self.loss_fn(y_pred, y_batch) [...] /opt/conda/lib/python3.6/site-packages/torch/nn/functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse) 1482 # remove once script supports set_grad_enabled 1483 _no_grad_embedding_renorm_(weight, input, max_norm, norm_type) -> 1484 return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse) 1485 1486 RuntimeError: Expected object of device type cuda but got device type cpu for argument #1 'self' in call to _th_index_select Here is the full model definition: import torch from torch import nn import torch.utils.data # ^ https://discuss.pytorch.org/t/attributeerror-module-torch-utils-has-no-attribute-data/1666 class FeedforwardTabularModel(nn.Module): def __init__(self): super().__init__() self.batch_size = 512 self.base_lr, self.max_lr = 0.001, 0.003 self.n_epochs = 5 self.cat_vars_embedding_vector_lengths = [ (1115, 80), (7, 4), (3, 3), (12, 6), (31, 10), (2, 2), (25, 10), (26, 10), (4, 3), (3, 3), (4, 3), (23, 9), (8, 4), (12, 6), (52, 15), (22, 9), (6, 4), (6, 4), (3, 3), (3, 3), (8, 4), (8, 4) ] self.loss_fn = torch.nn.MSELoss() self.score_fn = torch.nn.MSELoss() # Layer 1: embeddings. self.embeddings = [] for (in_size, out_size) in self.cat_vars_embedding_vector_lengths: emb = nn.Embedding(in_size, out_size) self.embeddings.append(emb) # Layer 1: dropout. self.embedding_dropout = nn.Dropout(0.04) # Layer 1: batch normalization (of the continuous variables). self.cont_batch_norm = nn.BatchNorm1d(16, eps=1e-05, momentum=0.1) # Layers 2 through 9: sequential feedforward model. self.seq_model = nn.Sequential(*[ nn.Linear(in_features=215, out_features=1000, bias=True), nn.ReLU(), nn.BatchNorm1d(1000, eps=1e-05, momentum=0.1), nn.Dropout(p=0.001), nn.Linear(in_features=1000, out_features=500, bias=True), nn.ReLU(), nn.BatchNorm1d(500, eps=1e-05, momentum=0.1), nn.Dropout(p=0.01), nn.Linear(in_features=500, out_features=1, bias=True) ]) def forward(self, x): # Layer 1: embeddings. inp_offset = 0 embedding_subvectors = [] for emb in self.embeddings: index = torch.tensor(inp_offset, dtype=torch.int64).cuda() inp = torch.index_select(x, dim=1, index=index).long().cuda() out = emb(inp) out = out.view(out.shape[2], out.shape[0], 1).squeeze() embedding_subvectors.append(out) inp_offset += 1 out_cat = torch.cat(embedding_subvectors) out_cat = out_cat.view(out_cat.shape[::-1]) # Layer 1: dropout. out_cat = self.embedding_dropout(out_cat) # Layer 1: batch normalization (of the continuous variables). out_cont = self.cont_batch_norm(x[:, inp_offset:]) out = torch.cat((out_cat, out_cont), dim=1) # Layers 2 through 9: sequential feedforward model. out = self.seq_model(out) return out def fit(self, X, y): self.train() # TODO: set a random seed to invoke determinism. # cf. https://github.com/pytorch/pytorch/issues/11278 X = torch.tensor(X, dtype=torch.float32).cuda() y = torch.tensor(y, dtype=torch.float32).cuda() # The build of PyTorch on Kaggle has a blog that prevents us from using # CyclicLR with ADAM. Cf. GH#19003. # optimizer = torch.optim.Adam(model.parameters(), lr=max_lr) # scheduler = torch.optim.lr_scheduler.CyclicLR( # optimizer, base_lr=base_lr, max_lr=max_lr, # step_size_up=300, step_size_down=300, # mode='exp_range', gamma=0.99994 # ) optimizer = torch.optim.Adam(model.parameters(), lr=(self.base_lr + self.max_lr) / 2) batches = torch.utils.data.DataLoader( torch.utils.data.TensorDataset(X, y), batch_size=self.batch_size, shuffle=True ) for epoch in range(self.n_epochs): for i, (X_batch, y_batch) in enumerate(batches): y_pred = model(X_batch).squeeze() # scheduler.batch_step() # Disabled due to a bug, see above. loss = self.loss_fn(y_pred, y_batch) optimizer.zero_grad() loss.backward() optimizer.step() print( f""Epoch {epoch + 1}/{self.n_epochs}, Loss {loss.detach().numpy()}"" ) def predict(self, X): self.eval() with torch.no_grad(): y_pred = model(torch.tensor(X, dtype=torch.float32).cuda()) return y_pred.squeeze() def score(self, X, y): y_pred = self.predict(X) y = torch.tensor(y, dtype=torch.float32).cuda() return self.score_fn(y, y_pred) model = FeedforwardTabularModel() model.cuda() model.fit(X_train_sample.values, y_train_sample.values) This type of error typically occurs when there is a tensor in the model that should be on GPU but is on CPU instead. But as far as I can tell, I've already placed .cuda() calls at all of the necessary places: every time a torch.tensor is declared, and running model.cuda() before model.fit. What is causing this error?",|python|gpu|pytorch|,GPU Usage,3
58609115,"Manual predictions of neural net go wrong. I have a dataset (csv) with the format shown bellow: First column: random integers Second column: The class of each integer (called bins) Bins have been made after preprocessing,for exampe integers between 1000 and 1005 belong in bin number 0 , 1006 and 1011 beongs in bin number 1 and go on. Target column for my neural network is the column of bins (second column). I use OneHotEncoding for my target column and transform every bin number to a binary vector. I have 3557 different bins (classes). I trained it and evaluate it with accurancy 99,7% as a result. import numpy as np import pandas as pd import tensorflow as tf from sklearn.preprocessing import OneHotEncoder from keras import Sequential from keras.layers import Dense from sklearn.model_selection import train_test_split df = pd.read_csv(""/dbfs/FileStore/tables/export78.csv"") onehotencoder = OneHotEncoder(categorical_features = [1]) data2 = onehotencoder.fit_transform(df).toarray() dataset = pd.DataFrame(data2) X= dataset.iloc[:,3557].astype(float) y= dataset.iloc[:,0:3557].astype(int) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) classifier = Sequential() #First Hidden Layer classifier.add(Dense(3557, activation='sigmoid', kernel_initializer='random_normal', input_dim=1)) #Second Hidden Layer classifier.add(Dense(3557, activation='sigmoid', kernel_initializer='random_normal')) #Output Layer classifier.add(Dense(3557, activation='sigmoid', kernel_initializer='random_normal')) #Compiling the neural network classifier.compile(optimizer ='adam',loss='binary_crossentropy', metrics=['accuracy']) #Fitting the data to the training dataset classifier.fit(X_train,y_train, batch_size=50, epochs=10) accr = classifier.evaluate(X_test, y_test) print('Test set\n Loss: {:0.3f}\n Accuracy: {:0.3f}'.format(accr[0] ,accr[1])) classifier.save(""model.h67"") data1 = np.array(X_test) List = [data1] model = tf.keras.models.load_model(""model.h67"") prediction = model.predict([(data1)]) target = (np.argmax(prediction, axis=0)) dataset1 = pd.DataFrame(target) display(dataset1) THE PROBLEM: When I try to predict manually using my model I cant take right results. As prediction input a give a csv with only one column with random integers and I want bins that belong as a result.",|python|machine-learning|keras|neural-network|classification|,Model,0
58693786,"AttributeError: 'function' object has no attribute 'predict'. Keras. I am working on an RL problem and I created a class to initialize the model and other parameters. The code is as follows: class Agent: def __init__(self, state_size, is_eval=False, model_name=""""): self.state_size = state_size self.action_size = 20 # measurement, CNOT, bit-flip self.memory = deque(maxlen=1000) self.inventory = [] self.model_name = model_name self.is_eval = is_eval self.done = False self.gamma = 0.95 self.epsilon = 1.0 self.epsilon_min = 0.01 self.epsilon_decay = 0.995 def model(self): model = Sequential() model.add(Dense(units=16, input_dim=self.state_size, activation=""relu"")) model.add(Dense(units=32, activation=""relu"")) model.add(Dense(units=8, activation=""relu"")) model.add(Dense(self.action_size, activation=""softmax"")) model.compile(loss=""categorical_crossentropy"", optimizer=Adam(lr=0.003)) return model def act(self, state): options = self.model.predict(state) return np.argmax(options[0]), options I want to run it for only one iteration, hence I create an object and I pass a vector of length 16 like this: agent = Agent(density.flatten().shape) state = density.flatten() action, probs = agent.act(state) However, I get the following error: AttributeError Traceback (most recent call last) <ipython-input-14-4f0ff0c40f49> in <module> ----> 1 action, probs = agent.act(state) <ipython-input-10-562aaf040521> in act(self, state) 39 # return random.randrange(self.action_size) 40 # model = self.model() ---> 41 options = self.model.predict(state) 42 return np.argmax(options[0]), options 43 AttributeError: 'function' object has no attribute 'predict' What's the issue? I checked some other people's codes as well, like this and I think mine is also very similar. Let me know. EDIT: I changed the argument in Dense from input_dim to input_shape and self.model.predict(state) to self.model().predict(state). Now when I run the NN for one input data of shape (16,1), I get the following error: ValueError: Error when checking input: expected dense_1_input to have 3 dimensions, but got array with shape (16, 1) And when I run it with shape (1,16), I get the following error: ValueError: Error when checking input: expected dense_1_input to have 3 dimensions, but got array with shape (1, 16) What should I do in this case?",|python-3.x|keras|deep-learning|reinforcement-learning|attributeerror|,API,4
58730110,"Training Keras models in two seperate juypter notebooks on CPU and GPU. I am training Keras CNN models for two different applications on the Jupyter Notebook. Given that I want to utilize the full resources of my PC, can I use Keras-GPU in one notebook and another notebook using CPU. I learned that Keras uses GPU by default - if available- and I can force Keras to use CPU as in Can Keras with Tensorflow backend be forced to use CPU or GPU at will?. My question is that by running this line of code, os.environ['CUDA_VISIBLE_DEVICES'] = '-1' will the default settings change in all the running notebooks or in that particular notebook only?",|python|keras|gpu|cpu|,GPU Usage,3
58844149,"Accuracy Equals 0 CNN Python Keras. I'm working on a binary classification problem. I was getting 69% accuracy at first, but kept running out of memory so I shrunk certain parameters, now it's coming up 0. Any idea whats going on? model = Sequential() from keras.layers import Dropout model.add(Conv2D(96, kernel_size=11, padding=""same"", input_shape=(300, 300, 1), activation = 'relu')) model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2))) model.add(Conv2D(128, kernel_size=3, padding=""same"", activation = 'relu')) model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2))) model.add(Conv2D(128, kernel_size=3, padding=""same"", activation = 'relu')) model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2))) from keras.layers.core import Activation model.add(Flatten()) # model.add(Dense(units=1000, activation='relu' )) model.add(Dense(units= 300, activation='relu')) model.add(Dropout(0.2)) model.add(Dense(1)) model.add(Activation(""softmax"")) model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) from keras.preprocessing.image import ImageDataGenerator datagen = ImageDataGenerator( featurewise_center=True, rotation_range=90, fill_mode='nearest', validation_split = 0.2 ) datagen.fit(train) train_generator = datagen.flow(train, train_labels, batch_size=8) # # fits the model on batches with real-time data augmentation: history = model.fit_generator(generator=train_generator, use_multiprocessing=True, steps_per_epoch = len(train_generator) / 8, epochs = 5, workers=20)",|python|tensorflow|machine-learning|keras|,Model,0
59078187,"can't train Tensorflow on a simple dataset. I'm trying to learn a bit about Tensorflow/Machine Learning. As a starting point, I'm trying to create a model that is trained on a simple 1-D function (y=x^2) and see how it behaves for other inputs outside of the training range. The problem I'm having is that the training function doesn't really ever improve. I'm sure it's due to a lack of understanding and/or misconfiguration on my part, but there really doesn't seem to be any sort of ""baby's first machine learning"" out there that deals with a dataset of a known form. My code is pretty simple, and is borrowed from TensorFlow's introduction notebook here import tensorflow as tf import numpy as np # Load the dataset x_train = np.linspace(0,10,1000) y_train = np.power(x_train,2.0) x_test = np.linspace(8,12,100) y_test = np.power(x_test,2.0) # (x_train, y_train), (x_test, y_test) = mnist.load_data() # x_train, x_test = x_train / 255.0, x_test / 255.0 """"""Build the `tf.keras.Sequential` model by stacking layers. Choose an optimizer and loss function for training:"""""" from tensorflow.keras import layers model = tf.keras.models.Sequential([ tf.keras.layers.Dense(128, activation='relu'), tf.keras.layers.Dropout(0.2), tf.keras.layers.Dense(10, activation='softmax') ]) model.compile(optimizer='adam', loss='mse', metrics=['mae']) """"""Train and evaluate the model:"""""" model.fit(x_train, y_train, epochs=5) model.evaluate(x_test, y_test, verbose=2) and I get output like this: Train on 1000 samples Epoch 1/5 1000/1000 [==============================] - 0s 489us/sample - loss: 1996.3631 - mae: 33.2543 Epoch 2/5 1000/1000 [==============================] - 0s 36us/sample - loss: 1996.3540 - mae: 33.2543 Epoch 3/5 1000/1000 [==============================] - 0s 36us/sample - loss: 1996.3495 - mae: 33.2543 Epoch 4/5 1000/1000 [==============================] - 0s 33us/sample - loss: 1996.3474 - mae: 33.2543 Epoch 5/5 1000/1000 [==============================] - 0s 38us/sample - loss: 1996.3450 - mae: 33.2543 100/1 - 0s - loss: 15546.3655 - mae: 101.2603 Like I said, I'm positive that this is a misconfiguration/lack of understanding on my part. I really learn best when I can take something this simple and incrementally make it more complex rather than starting on something whose patterns I can't readily identify, but I can't find any tutorials, etc that take this approach. Can anyone recommend either a good tutorial source, or just educate me on what I am doing wrong here?",|python|tensorflow|machine-learning|keras|deep-learning|,Model,0
59278771,"Super low accuracy for neural network model. I followed a tutorial on neural network model evaluation using cross-validation with code: # Multiclass Classification with the Iris Flowers Dataset import numpy import pandas from keras.models import Sequential from keras.layers import Dense from keras.wrappers.scikit_learn import KerasClassifier from keras.utils import np_utils from sklearn.model_selection import cross_val_score from sklearn.model_selection import KFold from sklearn.preprocessing import LabelEncoder from sklearn.pipeline import Pipeline # fix random seed for reproducibility seed = 7 numpy.random.seed(seed) # load dataset dataframe = pandas.read_csv(""/content/drive/My Drive/iris.data"", header=None) dataset = dataframe.values X = dataset[:,0:4].astype(float) Y = dataset[:,4] # encode class values as integers encoder = LabelEncoder() encoder.fit(Y) encoded_Y = encoder.transform(Y) # convert integers to dummy variables (i.e. one hot encoded) dummy_y = np_utils.to_categorical(encoded_Y) # define baseline model def baseline_model(): # create model model = Sequential() model.add(Dense(4, input_dim=4, activation=""relu"", kernel_initializer=""normal"")) model.add(Dense(3, activation=""sigmoid"", kernel_initializer=""normal"")) # Compile model model.compile(loss= 'categorical_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ]) return model estimator = KerasClassifier(build_fn=baseline_model, nb_epoch=200, batch_size=5, verbose=0) kfold = KFold(n_splits=10, shuffle=True, random_state=seed) results = cross_val_score(estimator, X, dummy_y, cv=kfold) print(""Accuracy: %.2f%% (%.2f%%)"" % (results.mean()*100, results.std()*100)) The accuracy was supposed to be around 95.33% (4.27%) but I got ~Accuracy: 34.00% (13.15%) on a few attempts. The model code seems exactly the same. I downloaded the data from here as instructed. What could go wrong? Thanks",|python|machine-learning|keras|neural-network|cross-validation|,Model,0
59282996,"Zero predictions despite masking support for zero-padded mini batch LSTM training in keras. Problem Statement Im training a many-to-many LSTM in keras with tensorflow backend (tf version 1.13.1) on tagged text sequences to predict the tag of each element in the sequence using pretrained GloVe embeddings. My training regime involves mini batch stochastic gradient descent, with each mini batch matrix zero-padded column-wise to ensure equal length input to the network. Crucially, because of custom constrains on my mini batches due to the nature of the task and the data, I am not using the keras embedding layer. My goal is to implement a masking mechanism for my zero-padded cells to ensure the loss computation does not spuriously treat these cells as genuine data points. Approach As explained in the keras documentation, keras has three ways in which a masking layer can be set up: Configuring a keras.layers.Embedding layer with mask_zero set to True. Adding a keras.layers.Masking layer; Passing a mask argument manually when calling recurrent layers. Because I am not using an embedding layer to encode my data for training, option (1) with a masked embedding layer is not available to me. So instead, I chose (2) and added a masking layer right after initializing my model. This change, however, does not seem to have had an effect. In fact, not only has the accuracy of my model not improved, at the prediction stage the model still generates zero predictions. Why does my masking layer not mask zero-padded cells? Could it have to do with the fact that in my dense layer I'm specifying 3 classes rather than 2 (thus including 0 as a separate class)? Limitations of Existing Resources Similar questions have been asked and answered, but I wasn't able to use them to resolve my issue. While this post received no direct response, a linked post mentioned in a comment focuses on how to preprocess data to assign mask value, which is uncontroversial here. The masking layer initializtion, however, is identical to the one used here. This post mentions the same problem - a masking layer has no effect on performance - and the answer defines the masking layer in the same way as I do, but again focuses on converting specific values to mask values. Finally, the answer in this post provides the same layer initialization without elaborating further. Toy Data Generation To reproduce my problem, I have generated a toy 10-batch dataset with two classes (1,2). A batch is a variable-length sequence post-padded with zeros to a maximum length of 20 embeddings, with each embedding vector consisting of 5 cells, so input_shape=(20,5). Embedding values for the two classes were generated from different but partially-overlapping truncated normal distributions to create a learnable but not trivial problem for the network. I've included the toy data below so you can reproduce the problem. import pandas as pd from keras.models import Sequential from keras.layers import LSTM, Dense, TimeDistributed, Bidirectional, Dropout, Masking from keras import optimizers # *** model initialization *** model = Sequential() model.add(Masking(mask_value=0., input_shape=(20, 5))) # <- masking layer here model.add(Bidirectional(LSTM(20, return_sequences=True), input_shape=(20, 5))) model.add(Dropout(0.2)) model.add(TimeDistributed(Dense(3, activation='sigmoid'))) sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True) model.compile(loss='sparse_categorical_crossentropy', optimizer=sgd, metrics=['mse']) # *** model training *** for epoch in range(10): for X,y in data_train: X = X.reshape(1, 20, 5) y = y.reshape(1, 20, 1) history = model.fit(X, y, epochs=1, batch_size=20, verbose=0) # *** model prediction *** preds = pd.DataFrame(columns=['true', 'pred']) for index, (X,y) in enumerate(data_test): X = X.reshape(1, 20, 5) y = y.reshape(1, 20, 1) y_pred = model.predict_classes(X, verbose=0) df = pd.DataFrame(columns=['true', 'pred']) df['true'] = [y[0, i][0] for i in range(20)] df['pred'] = [y_pred[0, i] for i in range(20)] preds = preds.append(df, ignore_index=True) # convert true labels to int & drop padded rows (where y_true=0) preds['true'] = [int(label) for label in preds['true']] preds = preds[preds['true']!=0] This is the summary of the model with masking: _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= masking_2 (Masking) (None, 20, 5) 0 _________________________________________________________________ bidirectional_4 (Bidirection (None, 20, 40) 4160 _________________________________________________________________ dropout_4 (Dropout) (None, 20, 40) 0 _________________________________________________________________ time_distributed_4 (TimeDist (None, 20, 3) 123 ================================================================= Total params: 4,283 Trainable params: 4,283 Non-trainable params: 0 I trained one model with and one without the masking layer and calculated accuracy using: np.round(sum(preds['true']==preds['pred'])/len(preds)*100,1) I got 53.3% accuracy for the model without masking and 33.3% for the model with masking. More surprisingly, I kept on getting zero as a predicted label in both models. Why does the masking layer fail to ignore zero-padded cells? Data for reproducing issue: data_train = list(zip(X_batches_train, y_batches_train)) data_test = list(zip(X_batches_test, y_batches_test)) X_batches_train [array([[-1.00612917, 1.47313952, 2.68021318, 1.54875809, 0.98385996, 1.49465265, 0.60429106, 1.12396908, -0.24041602, 1.77266187, 0.1961381 , 1.28019637, 1.78803092, 2.05151245, 0.93606708, 0.51554755, 0. , 0. , 0. , 0. ], [-0.97596563, 2.04536053, 0.88367922, 1.013342 , -0.16605355, 3.02994344, 2.04080806, -0.25153046, -0.5964068 , 2.9607247 , -0.49722121, 0.02734492, 2.16949987, 2.77367066, 0.15628842, 2.19823207, 0. , 0. , 0. , 0. ], [ 0.31546283, 3.27420503, 3.23550769, -0.63724013, 0.89150128, 0.69774266, 2.76627308, -0.58408384, -0.45681779, 1.98843041, -0.31850477, 0.83729882, 0.45471165, 3.61974147, -1.45610756, 1.35217453, 0. , 0. , 0. , 0. ], [ 1.03329532, 1.97471646, 1.33949611, 1.22857243, -1.46890642, 1.74105506, 1.40969261, 0.52465603, -0.18895266, 2.81025597, 2.64901037, -0.83415186, 0.76956826, 1.48730868, -0.16190164, 2.24389007, 0. , 0. , 0. , 0. ], [-1.0676654 , 3.08429323, 1.7601179 , 0.85448051, 1.15537064, 2.82487842, 0.27891413, 0.57842569, -0.62392063, 1.00343057, 1.15348843, -0.37650332, 3.37355345, 2.22285473, 0.43444434, 0.15743873, 0. , 0. , 0. , 0. ]]), array([[ 1.05258873, -0.17897376, -0.99932932, -1.02854121, 0.85159208, 2.32349131, 1.96526709, -0.08398597, -0.69474809, 1.32820222, 1.19514151, 1.56814867, 0.86013263, 1.48342922, 0. , 0. , 0. , 0. , 0. , 0. ], [ 0.1920635 , -0.48702788, 1.24353985, -1.3864121 , 0.16713229, 3.10134683, 0.61658271, -0.63360643, 0.86000807, 2.74876157, 2.87604877, 0.16339724, 2.87595396, 3.2846962 , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0.1380241 , -0.76783029, 0.18814436, -1.18165209, -0.02981728, 1.49908113, 0.61521007, -0.98191097, 0.31250199, 1.39015803, 3.16213211, -0.70891214, 3.83881766, 1.92683533, 0. , 0. , 0. , 0. , 0. , 0. ], [ 1.39080778, -0.59179216, 0.80348201, 0.64638205, -1.40144268, 1.49751413, 3.0092166 , 1.33099666, 1.43714841, 2.90734268, 3.09688943, 0.32934884, 1.14592787, 1.58152023, 0. , 0. , 0. , 0. , 0. , 0. ], [-0.77164353, 0.50293096, 0.0717377 , 0.14487556, -0.90246591, 2.32612179, 1.98628857, 1.29683166, -0.12399569, 2.60184685, 3.20136653, 0.44056647, 0.98283455, 1.79026663, 0. , 0. , 0. , 0. , 0. , 0. ]]), array([[-0.93359914, 2.31840281, 0.55691601, 1.90930758, -1.58260431, -1.05801881, 3.28012523, 3.84105406, -1.2127093 , 0.00490079, 1.28149304, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [-1.03105486, 2.7703693 , 0.16751813, 1.12127987, -0.44070271, -0.0789227 , 2.79008301, 1.11456745, 1.13982551, -1.10128658, 0.87430834, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [-0.69710668, 1.72702833, -2.62599502, 2.34730002, 0.77756661, 0.16415884, 3.30712178, 1.67331828, -0.44022431, 0.56837829, 1.1566811 , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [-0.71845983, 1.79908544, 0.37385522, 1.3870915 , -1.48823234, -1.487419 , 3.0879945 , 1.74617784, -0.91538815, -0.24244522, 0.81393954, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [-1.38501563, 3.73330047, -0.52494265, 2.37133716, -0.24546709, -0.28360782, 2.89384717, 2.42891743, 0.40144022, -1.21850571, 2.00370751, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ]]), array([[ 1.27989188, 1.16254538, -0.06889142, 1.84133355, 1.3234908 , 1.29611702, 2.0019294 , -0.03220116, 1.1085194 , 1.96495985, 1.68544302, 1.94503544, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 1.3004439 , 2.48768923, 0.59809607, 2.38155155, 2.78705889, 1.67018683, 0.21731778, -0.59277191, 2.87427207, 2.63950475, 2.39211459, 0.93083423, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 2.39239371, 0.30900383, -0.97307155, 1.98100711, 0.30613735, 1.12827171, 0.16987791, 0.31959096, 1.30366416, 1.45881023, 2.45668401, 0.5218711 , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0.0826574 , 2.05100254, 0.013161 , 2.95120798, 1.15730011, 0.75537024, 0.13708569, -0.44922143, 0.64834001, 2.50640862, 2.00349347, 3.35573624, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0.47135124, 2.10258532, 0.70212032, 2.56063126, 1.62466971, 2.64026892, 0.21309489, -0.57752813, 2.21335957, 0.20453233, 0.03106993, 3.01167822, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ]]), array([[-0.42125521, 0.54016939, 1.63016057, 2.01555253, -0.10961255, -0.42549555, 1.55793753, -0.0998756 , 0.36417335, 3.37126414, 1.62151191, 2.84084192, 0.10831384, 0.89293054, -0.08671363, 0.49340353, 0. , 0. , 0. , 0. ], [-0.37615411, 2.00581062, 2.30426605, 2.02205839, 0.65871664, 1.34478836, -0.55379752, -1.42787727, 0.59732227, 0.84969282, 0.54345723, 0.95849568, -0.17131602, -0.70425277, -0.5337757 , 1.78207229, 0. , 0. , 0. , 0. ], [-0.13863276, 1.71490034, 2.02677925, 2.60608619, 0.26916522, 0.35928298, -1.26521844, -0.59859219, 1.19162219, 1.64565259, 1.16787165, 2.95245196, 0.48681084, 1.66621053, 0.918077 , -1.10583747, 0. , 0. , 0. , 0. ], [ 0.87763797, 2.38740754, 2.9111822 , 2.21184069, 0.78091173, -0.53270909, 0.40100338, -0.83375593, 0.9860009 , 2.43898437, -0.64499989, 2.95092003, -1.52360727, 0.44640918, 0.78131922, -0.24401283, 0. , 0. , 0. , 0. ], [ 0.92615066, 3.45437746, 3.28808981, 2.87207404, -1.60027223, -1.14164941, -1.63807699, 0.33084805, 2.92963629, 3.51170824, -0.3286093 , 2.19108385, 0.97812366, -1.82565766, -0.34034678, -2.0485913 , 0. , 0. , 0. , 0. ]]), array([[ 1.96438618e+00, 1.88104784e-01, 1.61114494e+00, 6.99567690e-04, 2.55271963e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00], [ 2.41578815e+00, -5.70625661e-01, 2.15545894e+00, -1.80948908e+00, 1.62049331e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00], [ 1.97017040e+00, -1.62556528e+00, 2.49469152e+00, 4.18785985e-02, 2.61875866e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00], [ 3.14277819e+00, 3.01098398e-02, 7.40376369e-01, 1.76517344e+00, 2.68922918e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00], [ 2.06250296e+00, 4.67605528e-01, 1.55927230e+00, 1.85788889e-01, 1.30359922e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]]), array([[ 1.22152427, 3.74926839, 0.64415552, 2.35268329, 1.98754653, 2.89384829, 0.44589817, 3.94228743, 2.72405657, 0.86222004, 0.68681903, 3.89952458, 1.43454512, 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [-0.02203262, 0.95065123, 0.71669023, 0.02919391, 2.30714524, 1.91843002, 0.73611294, 1.20560482, 0.85206836, -0.74221506, -0.72886308, 2.39872927, -0.95841402, 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0.55775319, 0.33773314, 0.79932151, 1.94966883, 3.2113281 , 2.70768249, -0.69745554, 1.23208345, 1.66199957, 1.69894081, 0.13124461, 1.93256147, -0.17787952, 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0.45089205, 2.62430534, -1.9517961 , 2.24040577, 1.75642049, 1.94962325, 0.26796497, 2.28418304, 1.44944487, 0.28723885, -0.81081633, 1.54840214, 0.82652939, 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 1.27678173, 1.17204606, -0.24738322, 1.02761617, 1.81060444, 2.37830861, 0.55260134, 2.50046334, 1.04652821, 0.03467176, -2.07336654, 1.2628897 , 0.61604732, 0. , 0. , 0. , 0. , 0. , 0. , 0. ]]), array([[ 3.86138405, 2.35068317, -1.90187438, 0.600788 , 0.18011722, 1.3469559 , -0.54708828, 1.83798823, -0.01957845, 2.88713217, 3.1724991 , 2.90802072, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 1.26785642, 0.51076756, 0.32070756, 2.33758816, 2.08146669, -0.60796736, 0.93777509, 2.70474711, 0.44785738, 1.61720609, 1.52890594, 3.03072971, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 3.30219394, 3.1515445 , 1.16550716, 2.07489374, 0.66441859, 0.97529244, 0.35176367, 1.22593639, -1.80698271, 1.19936482, 3.34017172, 2.15960657, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 2.34839018, 2.24827352, -1.61070856, 2.81044265, -1.21423372, 0.24633846, -0.82196609, 2.28616568, 0.033922 , 2.7557593 , 1.16178372, 3.66959512, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 1.32913219, 1.63231852, 0.58642744, 1.55873546, 0.86354741, 2.06654246, -0.44036504, 3.22723595, 1.33279468, 0.05975892, 2.48518999, 3.44690602, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ]]), array([[ 0.61424344, -1.03068819, -1.47929328, 2.91514641, 2.06867196, 1.90384921, -0.45835234, 1.22054782, 0.67931536, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 2.76480464, 1.12442631, -2.36004758, 2.91912726, 1.67891181, 3.76873596, -0.93874096, -0.32397781, -0.55732374, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0.39953353, -1.26828104, 0.44482517, 2.85604975, 3.08891062, 2.60268725, -0.15785176, 1.58549879, -0.32948578, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 1.65156484, -1.56545168, -1.42771206, 2.74216475, 1.8758154 , 3.51169147, 0.18353058, -0.14704149, 0.00442783, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 1.27736372, 0.37407608, -1.25713475, 0.53171176, 1.53714914, 0.21015523, -1.06850669, -0.09755327, -0.92373834, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ]]), array([[-1.39160433, 0.21014669, -0.89792475, 2.6702794 , 1.54610601, 0.84699037, 2.96726482, 1.84236946, 0.02211578, 0.32842575, 1.02718924, 1.78447936, -1.20056829, 2.26699318, -0.23156537, 2.50124959, 1.93372501, 0.10264369, -1.70813962, 0. ], [ 0.38823591, -1.30348049, -0.31599117, 2.60044143, 2.32929389, 1.40348483, 3.25758736, 1.92210728, -0.34150988, -1.22336921, 2.3567069 , 1.75456835, 0.28295694, 0.68114898, -0.457843 , 1.83372069, 2.10177851, -0.26664178, -0.26549595, 0. ], [ 0.08540346, 0.71507504, 1.78164285, 3.04418137, 1.52975256, 3.55159169, 3.21396003, 3.22720346, 0.68147142, 0.12466013, -0.4122895 , 1.97986653, 1.51671949, 2.06096825, -0.6765908 , 2.00145086, 1.73723014, 0.50186043, -2.27525744, 0. ], [ 0.00632717, 0.3050794 , -0.33167875, 1.48109172, 0.19653696, 1.97504239, 2.51595821, 1.74499313, -1.65198805, -1.04424953, -0.23786945, 1.18639347, -0.03568057, 3.82541131, 2.84039446, 2.88325909, 1.79827675, -0.80230291, 0.08165052, 0. ], [ 0.89980086, 0.34690991, -0.60806566, 1.69472308, 1.38043417, 0.97139487, 0.21977176, 1.01340944, -1.69946943, -0.01775586, -0.35851919, 1.81115864, 1.15105661, 1.21410373, 1.50667558, 1.70155313, 3.1410754 , -0.54806167, -0.51879299, 0. ]])] y_batches_train [array([1., 2., 2., 1., 1., 2., 2., 1., 1., 2., 1., 1., 2., 2., 1., 2., 0., 0., 0., 0.]), array([1., 1., 1., 1., 1., 2., 2., 1., 1., 2., 2., 1., 2., 2., 0., 0., 0., 0., 0., 0.]), array([1., 2., 1., 2., 1., 1., 2., 2., 1., 1., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([2., 2., 1., 2., 2., 2., 1., 1., 2., 2., 2., 2., 0., 0., 0., 0., 0., 0., 0., 0.]), array([1., 2., 2., 2., 1., 1., 1., 1., 2., 2., 1., 2., 1., 1., 1., 1., 0., 0., 0., 0.]), array([2., 1., 2., 1., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([1., 2., 1., 2., 2., 2., 1., 2., 2., 1., 1., 2., 1., 0., 0., 0., 0., 0., 0., 0.]), array([2., 2., 1., 2., 1., 1., 1., 2., 1., 2., 2., 2., 0., 0., 0., 0., 0., 0., 0., 0.]), array([2., 1., 1., 2., 2., 2., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([1., 1., 1., 2., 2., 2., 2., 2., 1., 1., 1., 2., 1., 2., 1., 2., 2., 1., 1., 0.])] X_batches_test [array([[ 0.74119496, 1.97273418, 1.76675805, 0.51484268, 1.39422086, 2.97184667, -1.35274514, 2.08825434, -1.2521965 , 1.11556387, 0.19776789, 2.38259223, -0.57140597, -0.79010112, 0.17038974, 1.28075761, 0.696398 , 3.0920007 , -0.41138503, 0. ], [-1.39081797, 0.41079718, 3.03698894, -2.07333633, 2.05575621, 2.73222939, -0.98182787, 1.06741172, -1.36310914, 0.20174856, 0.35323654, 2.70305775, 0.52549713, -0.7786237 , 1.80857093, 0.96830907, -0.23610863, 1.28160768, 0.7026651 , 0. ], [ 1.16357113, 0.43907935, 3.40158623, -0.73923043, 1.484668 , 1.52809569, -0.02347205, 1.65349967, 1.79635118, -0.46647772, -0.78400883, 0.82695404, -1.34932627, -0.3200281 , 2.84417045, 0.01534261, 0.10047148, 2.70769609, -1.42669461, 0. ], [-1.05475682, 3.45578027, 1.58589338, -0.55515227, 2.13477478, 1.86777473, 0.61550335, 1.05781415, -0.45297406, -0.04317595, -0.15255388, 0.74669395, -1.43621979, 1.06229278, 0.99792794, 1.24391783, -1.86484584, 1.92802343, 0.56148011, 0. ], [-0.0835337 , 1.89593955, 1.65769335, -0.93622246, 1.05002869, 1.49675624, -0.00821712, 1.71541053, 2.02408452, 0.59011484, 0.72719784, 3.44801858, -0.00957537, 0.37176007, 1.93481168, 2.23125062, 1.67910471, 2.80923862, 0.34516993, 0. ]]), array([[ 0.40691415, 2.31873444, -0.83458005, -0.17018249, -0.39177831, 1.90353251, 2.98241467, 0.32808584, 3.09429553, 2.27183083, 3.09576659, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 1.6862473 , 1.0690102 , -0.07415598, -0.09846767, 1.14562424, 2.52211963, 1.71911351, 0.41879894, 1.62787544, 3.50533394, 2.69963456, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 3.27824216, 2.25067953, 0.40017321, -1.36011162, -1.41010106, 0.98956203, 2.30881584, -0.29496046, 2.29748247, 3.24940966, 1.06431776, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 2.80167214, 3.88324559, -0.6984172 , 0.81889567, 1.86945352, 3.07554419, 3.10357189, 1.31426767, 0.28163147, 2.75559628, 2.00866885, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 1.54574419, 1.00720596, -1.55418837, 0.70823839, 0.14715209, 1.03747262, 0.82988672, -0.54006372, 1.4960777 , 0.34578788, 1.10558132, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ]])] y_batches_test [array([1., 2., 2., 1., 2., 2., 1., 2., 1., 1., 1., 2., 1., 1., 2., 2., 1., 2., 1., 0.]), array([2., 2., 1., 1., 1., 2., 2., 1., 2., 2., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0.])]",|python|keras|deep-learning|lstm|mini-batch|,Training,2
59325381,"Low accuracy after training a CNN. I try to train a CNN model that classifies the handwritten digit using Keras, but I am getting low accuracy in the training (lower than 10%) and a big error. I tried a simple neural network without concolutions and it didn't work as well. This is my code. import tensorflow as tf from tensorflow import keras import numpy as np import matplotlib.pyplot as plt (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data() #Explore data print(y_train[12]) print(np.shape(x_train)) print(np.shape(x_test)) #we have 60000 imae for the training and 10000 for testing # Scaling data x_train = x_train/255 y_train = y_train/255 #reshape the data x_train = x_train.reshape(60000,28,28,1) x_test = x_test.reshape(10000,28,28,1) y_train = y_train.reshape(60000,1) y_test = y_test.reshape(10000,1) #Create a model model = keras.Sequential([ keras.layers.Conv2D(64,(3,3),(1,1),padding = ""same"",input_shape=(28,28,1)), keras.layers.MaxPooling2D(pool_size = (2,2),padding = ""valid""), keras.layers.Conv2D(32,(3,3),(1,1),padding = ""same""), keras.layers.MaxPooling2D(pool_size = (2,2),padding = ""valid""), keras.layers.Flatten(), keras.layers.Dense(128,activation = ""relu""), keras.layers.Dense(10,activation = ""softmax"")]) model.compile(optimizer = ""adam"", loss = ""sparse_categorical_crossentropy"", metrics = ['accuracy']) model.fit(x_train,y_train,epochs=10) test_loss,test_acc = model.evaluate(x_test,y_test) print(""\ntest accuracy:"",test_acc) Could anyone advice me on how to improve my model?",|python|tensorflow|keras|conv-neural-network|mnist|,Training,2
59416289,"ValueError: Error when checking target: expected (keras Sequence model layer) to have n dimensions, but got array with shape. I have loaded images to train my model on recognizing one feature in those images. Xtrain is a numpy ndarray of shape (1380,200,200,3 ) containing 1380 images sized 200 by 200pixels in RGB format Ytrain has targets. shape (1380,2) When I train my model (model.fit(Xtrain,Ytrain)) I seem to get a value error on everyone of the layers. As if the input was both Xtrain then Ytrain... ValueError: Error when checking target: expected batch_normalization_24 to have 4 dimensions, but got array with shape (1380, 2) Image:",|python|tensorflow|keras|valueerror|,Tensors&Inputs,1
59473949,"PyTorch Object Detection with GPU on Ubuntu 18.04 - RuntimeError: CUDA out of memory. Tried to allocate xx.xx MiB. I'm attempting to get this PyTorch person detection example: https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html running locally with a GPU, either in a Jupyter Notebook or a regular python file. I get the error in the title either way. I'm using Ubuntu 18.04. Here is a summary of the steps I've performed: 1) Stock Ubuntu 18.04 install on a Lenovo ThinkPad X1 Extreme Gen 2 with a GTX 1650 GPU. 2) Perform a standard CUDA 10.0 / cuDNN 7.4 install. I'd rather not restate all the steps as this post is going to be more than long enough already. This is a standard procedure, pretty much any link found via googling is what I followed. 3) Install torch and torchvision pip3 install torch torchvision 4) From this link on the PyTorch site: https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html I've both saved the linked notebook: https://colab.research.google.com/github/pytorch/vision/blob/temp-tutorial/tutorials/torchvision_finetuning_instance_segmentation.ipynb And Also tried the link at the bottom that has the regular Python file: https://pytorch.org/tutorials/_static/tv-training-code.py 5) Before running either the notebook or the regular Python way, I did the following (found at the top of the above linked notebook): Install the CoCo API into Python: cd ~ git clone https://github.com/cocodataset/cocoapi.git cd cocoapi/PythonAPI open Makefile in gedit, change the two instances of ""python"" to ""python3"", then: python3 setup.py build_ext --inplace sudo python3 setup.py install Get the necessary files the above linked files need to run: cd ~ git clone https://github.com/pytorch/vision.git cd vision git checkout v0.5.0 from ~/vision/references/detection, copy coco_eval.py, coco_utils.py, engine.py, transforms.py, and utils.py to whichever directory the above linked notebook or tv-training-code.py file are being ran from. 6) Download the Penn Fudan Pedestrian dataset from the link on the above page: https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip then unzip and put in the same directory as the notebook or tv-training-code.py In case the above link ever breaks or just for easier reference, here is tv-training-code.py as I have downloaded it at this time: # Sample code from the TorchVision 0.3 Object Detection Finetuning Tutorial # http://pytorch.org/tutorials/intermediate/torchvision_tutorial.html import os import numpy as np import torch from PIL import Image import torchvision from torchvision.models.detection.faster_rcnn import FastRCNNPredictor from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor from engine import train_one_epoch, evaluate import utils import transforms as T class PennFudanDataset(object): def __init__(self, root, transforms): self.root = root self.transforms = transforms # load all image files, sorting them to # ensure that they are aligned self.imgs = list(sorted(os.listdir(os.path.join(root, ""PNGImages"")))) self.masks = list(sorted(os.listdir(os.path.join(root, ""PedMasks"")))) def __getitem__(self, idx): # load images ad masks img_path = os.path.join(self.root, ""PNGImages"", self.imgs[idx]) mask_path = os.path.join(self.root, ""PedMasks"", self.masks[idx]) img = Image.open(img_path).convert(""RGB"") # note that we haven't converted the mask to RGB, # because each color corresponds to a different instance # with 0 being background mask = Image.open(mask_path) mask = np.array(mask) # instances are encoded as different colors obj_ids = np.unique(mask) # first id is the background, so remove it obj_ids = obj_ids[1:] # split the color-encoded mask into a set # of binary masks masks = mask == obj_ids[:, None, None] # get bounding box coordinates for each mask num_objs = len(obj_ids) boxes = [] for i in range(num_objs): pos = np.where(masks[i]) xmin = np.min(pos[1]) xmax = np.max(pos[1]) ymin = np.min(pos[0]) ymax = np.max(pos[0]) boxes.append([xmin, ymin, xmax, ymax]) boxes = torch.as_tensor(boxes, dtype=torch.float32) # there is only one class labels = torch.ones((num_objs,), dtype=torch.int64) masks = torch.as_tensor(masks, dtype=torch.uint8) image_id = torch.tensor([idx]) area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]) # suppose all instances are not crowd iscrowd = torch.zeros((num_objs,), dtype=torch.int64) target = {} target[""boxes""] = boxes target[""labels""] = labels target[""masks""] = masks target[""image_id""] = image_id target[""area""] = area target[""iscrowd""] = iscrowd if self.transforms is not None: img, target = self.transforms(img, target) return img, target def __len__(self): return len(self.imgs) def get_model_instance_segmentation(num_classes): # load an instance segmentation model pre-trained pre-trained on COCO model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True) # get number of input features for the classifier in_features = model.roi_heads.box_predictor.cls_score.in_features # replace the pre-trained head with a new one model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) # now get the number of input features for the mask classifier in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels hidden_layer = 256 # and replace the mask predictor with a new one model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes) return model def get_transform(train): transforms = [] transforms.append(T.ToTensor()) if train: transforms.append(T.RandomHorizontalFlip(0.5)) return T.Compose(transforms) def main(): # train on the GPU or on the CPU, if a GPU is not available device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') # our dataset has two classes only - background and person num_classes = 2 # use our dataset and defined transformations dataset = PennFudanDataset('PennFudanPed', get_transform(train=True)) dataset_test = PennFudanDataset('PennFudanPed', get_transform(train=False)) # split the dataset in train and test set indices = torch.randperm(len(dataset)).tolist() dataset = torch.utils.data.Subset(dataset, indices[:-50]) dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:]) # define training and validation data loaders data_loader = torch.utils.data.DataLoader( dataset, batch_size=2, shuffle=True, num_workers=4, collate_fn=utils.collate_fn) data_loader_test = torch.utils.data.DataLoader( dataset_test, batch_size=1, shuffle=False, num_workers=4, collate_fn=utils.collate_fn) # get the model using our helper function model = get_model_instance_segmentation(num_classes) # move model to the right device model.to(device) # construct an optimizer params = [p for p in model.parameters() if p.requires_grad] optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005) # and a learning rate scheduler lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1) # let's train it for 10 epochs num_epochs = 10 for epoch in range(num_epochs): # train for one epoch, printing every 10 iterations train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10) # update the learning rate lr_scheduler.step() # evaluate on the test dataset evaluate(model, data_loader_test, device=device) print(""That's it!"") if __name__ == ""__main__"": main() Here is an exmaple run of tv-training-code.py $ python3 tv-training-code.py Epoch: [0] [ 0/60] eta: 0:01:17 lr: 0.000090 loss: 4.1717 (4.1717) loss_classifier: 0.8903 (0.8903) loss_box_reg: 0.1379 (0.1379) loss_mask: 3.0632 (3.0632) loss_objectness: 0.0700 (0.0700) loss_rpn_box_reg: 0.0104 (0.0104) time: 1.2864 data: 0.1173 max mem: 1865 Traceback (most recent call last): File ""tv-training-code.py"", line 165, in <module> main() File ""tv-training-code.py"", line 156, in main train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10) File ""/xxx/PennFudanExample/engine.py"", line 46, in train_one_epoch losses.backward() File ""/usr/local/lib/python3.6/dist-packages/torch/tensor.py"", line 166, in backward torch.autograd.backward(self, gradient, retain_graph, create_graph) File ""/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py"", line 99, in backward allow_unreachable=True) # allow_unreachable flag File ""/usr/local/lib/python3.6/dist-packages/torch/autograd/function.py"", line 77, in apply return self._forward_cls.backward(self, *args) File ""/usr/local/lib/python3.6/dist-packages/torch/autograd/function.py"", line 189, in wrapper outputs = fn(ctx, *args) File ""/usr/local/lib/python3.6/dist-packages/torchvision/ops/roi_align.py"", line 38, in backward output_size[0], output_size[1], bs, ch, h, w, sampling_ratio) RuntimeError: CUDA out of memory. Tried to allocate 132.00 MiB (GPU 0; 3.81 GiB total capacity; 2.36 GiB already allocated; 132.69 MiB free; 310.59 MiB cached) (malloc at /pytorch/c10/cuda/CUDACachingAllocator.cpp:267) frame #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x33 (0x7fdfb6c9b813 in /usr/local/lib/python3.6/dist-packages/torch/lib/libc10.so) frame #1: <unknown function> + 0x1ce68 (0x7fdfb6edce68 in /usr/local/lib/python3.6/dist-packages/torch/lib/libc10_cuda.so) frame #2: <unknown function> + 0x1de6e (0x7fdfb6edde6e in /usr/local/lib/python3.6/dist-packages/torch/lib/libc10_cuda.so) frame #3: at::native::empty_cuda(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0x279 (0x7fdf59472789 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch.so) [many more frame lines omitted] Clearly the line: RuntimeError: CUDA out of memory. Tried to allocate 132.00 MiB (GPU 0; 3.81 GiB total capacity; 2.36 GiB already allocated; 132.69 MiB free; 310.59 MiB cached) (malloc at /pytorch/c10/cuda/CUDACachingAllocator.cpp:267) is the critical error. If I run an nvidia-smi before a run: $ nvidia-smi Tue Dec 24 14:32:49 2019 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 440.44 Driver Version: 440.44 CUDA Version: 10.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 1650 Off | 00000000:01:00.0 On | N/A | | N/A 47C P8 5W / N/A | 296MiB / 3903MiB | 3% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1190 G /usr/lib/xorg/Xorg 142MiB | | 0 1830 G /usr/bin/gnome-shell 72MiB | | 0 3711 G ...uest-channel-token=14371934934688572948 78MiB | +-----------------------------------------------------------------------------+ It seems pretty clear there is plenty of GPU memory available (this GPU is 4GB). Moreover, I'm confident my CUDA/cuDNN install and GPU hardware are good b/c I train and inference the TensorFlow object detection API on this computer frequently, and as long as I use the allow_growth option I never have GPU related errors. From Googling on this error it seems to be relatively common. The most common solutions are: 1) Try a smaller batch size (not really applicable in this case since the training and testing batch sizes are 2 and 1 respectively, and I tried with 1 and 1 and still got the same error) 2) Update to the latest version of PyTorch (but I'm already at the latest version). Some other suggestions involve reworking the training script. I'm very familiar with TensorFlow but I'm new to PyTorch so I'm not sure how to go about that. Also, most of the rework suggestions I can find for this error do not pertain to object detection and therefore I'm not able to relate them to this training script specifically. Has anybody else gotten this script to run locally with an NVIDIA GPU? Do you suspect a OS/CUDA/PyTorch configuration concern, or is there someway the script can be reworked to prevent this error? Any assistance would be greatly appreciated.",|gpu|pytorch|,GPU Usage,3
59486265,"Multi GPU training slower than single GPU on Tensorflow. I have created 3 virtual GPU's (have 1 GPU) and try to speedup vectorization on images. However, using provided below code with manual placement from off docs (here) I got strange results: training on all GPU two times slower than on a single one. Also check this code(and remove virtual device initialization) on machine with 3 physical GPU's - work the same. Environment: Python 3.6, Ubuntu 18.04.3, tensorflow-gpu 1.14.0. Code(this example create 3 virtual devices and you could test it on a PC with one GPU): import os import time import numpy as np import tensorflow as tf start = time.time() def load_graph(frozen_graph_filename): # We load the protobuf file from the disk and parse it to retrieve the # unserialized graph_def with tf.gfile.GFile(frozen_graph_filename, ""rb"") as f: graph_def = tf.GraphDef() graph_def.ParseFromString(f.read()) # Then, we import the graph_def into a new Graph and returns it with tf.Graph().as_default() as graph: # The name var will prefix every op/nodes in your graph # Since we load everything in a new graph, this is not needed tf.import_graph_def(graph_def, name="""") return graph path_to_graph = '/imagenet/' # Path to imagenet folder where graph file is placed GRAPH = load_graph(os.path.join(path_to_graph, 'classify_image_graph_def.pb')) # Create Session config = tf.ConfigProto() config.gpu_options.per_process_gpu_memory_fraction = 0.9 config.gpu_options.allow_growth = True session = tf.Session(graph=GRAPH, config=config) output_dir = '/vectors/' # where to saved vectors from images # Single GPU vectorization for image_index, image in enumerate(selected_list): with Image.open(image) as f: image_data = f.convert('RGB') feature_tensor = session.graph.get_tensor_by_name('pool_3:0') feature_vector = session.run(feature_tensor, {'DecodeJpeg:0': image_data}) feature_vector = np.squeeze(feature_vector) outfile_name = os.path.basename(image) + "".vc"" out_path = os.path.join(output_dir, outfile_name) # Save vector np.savetxt(out_path, feature_vector, delimiter=',') print(f""Single GPU: {time.time() - start}"") start = time.time() print(""Start calculation on multiple GPU"") gpus = tf.config.experimental.list_physical_devices('GPU') if gpus: # Create 3 virtual GPUs with 1GB memory each try: tf.config.experimental.set_virtual_device_configuration( gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024), tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024), tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)]) logical_gpus = tf.config.experimental.list_logical_devices('GPU') print(len(gpus), ""Physical GPU,"", len(logical_gpus), ""Logical GPUs"") except RuntimeError as e: # Virtual devices must be set before GPUs have been initialized print(e) print(""Create prepared ops"") start1 = time.time() gpus = logical_gpus # comment this line to use physical GPU devices for calculations image_list = ['1.jpg', '2.jpg', '3.jpg'] # list with images to vectorize (tested on 100 and 1000 examples) # Assign chunk of list to each GPU # image_list1, image_list2, image_list3 = image_list[:len(image_list)],\ # image_list[len(image_list):2*len(image_list)],\ # image_list[2*len(image_list):] selected_list = image_list # commit this line if you want to try to assign chunk of list manually to each GPU output_vectors = [] if gpus: # Replicate your computation on multiple GPUs feature_vectors = [] for gpu in gpus: # iterating on a virtual GPU devices, not physical with tf.device(gpu.name): print(f""Assign list of images to {gpu.name.split(':', 4)[-1]}"") # Try to assign chunk of list with images to each GPU - work the same time as single GPU # if gpu.name.split(':', 4)[-1] == ""GPU:0"": # selected_list = image_list1 # if gpu.name.split(':', 4)[-1] == ""GPU:1"": # selected_list = image_list2 # if gpu.name.split(':', 4)[-1] == ""GPU:2"": # selected_list = image_list3 for image_index, image in enumerate(selected_list): with Image.open(image) as f: image_data = f.convert('RGB') feature_tensor = session.graph.get_tensor_by_name('pool_3:0') feature_vector = session.run(feature_tensor, {'DecodeJpeg:0': image_data}) feature_vectors.append(feature_vector) print(""All images has been assigned to GPU's"") print(f""Time spend on prep ops: {time.time() - start1}"") print(""Start calculation on multiple GPU"") start1 = time.time() for image_index, image in enumerate(image_list): feature_vector = np.squeeze(feature_vectors[image_index]) outfile_name = os.path.basename(image) + "".vc"" out_path = os.path.join(output_dir, outfile_name) # Save vector np.savetxt(out_path, feature_vector, delimiter=',') # Close session session.close() print(f""Calc on GPU's spend: {time.time() - start1}"") print(f""All time, spend on multiple GPU: {time.time() - start}"") Provide view of output(from list with 100 images): 1 Physical GPU, 3 Logical GPUs Single GPU: 18.76301646232605 Start calculation on multiple GPU Create prepared ops Assign list of images to GPU:0 Assign list of images to GPU:1 Assign list of images to GPU:2 All images has been assigned to GPU's Time spend on prep ops: 18.263537883758545 Start calculation on multiple GPU Calc on GPU's spend: 11.697082042694092 All time, spend on multiple GPU: 29.960679531097412 What I tried: split list with images into 3 chunks and assign each chunk to GPU(see commited lines of code). This reduce multiGPU time to 17 seconds, which a little bit faster than single GPU run 18 seconds (~5%). Expected results: MultiGPU version is faster than singleGPU version (at least 1.5x speedup). Ideas, why it maybe happens: I wrote calculation in a wrong way",|python|python-3.x|tensorflow|multi-gpu|,GPU Usage,3
59732129,"Using `DataParallel` when network needs a shared (constant) `Tensor`. I would like to use DataParallel to distribute my computations across multiple GPUs along the batch dimension. My network requires a Tensor (let's call it A) internally, which is constant and doens't change through the optimization. It seems that DataParallel does not automatically copy this Tensor to all the GPUs in question, and the network will thus complain that the chunk of the input data x that it sees resides on a different GPU than A. Is there a way DataParallel can handle this situation automatically? Alternatively, is there a way to copy a Tensor to all GPUs? Or should I just keep one Tensor for each GPU and manually figure out which copy to use depending on where the chunk seen by forward resides?",|gpu|pytorch|,GPU Usage,3
59758722,"Keras NN loss is 1. Getting started with simple NN but my loss remains one at each iteration. Can somebody point out what I'm doing wrong here. This is from a Kaggle introductory course and my modified training set contains shop id, category id, item id, month and revenue. I'm basically trying to predict revenue per shop per category for the following month. I've scaled revenue and trained on a simple NN with 2 hidden layers; however, it doesn't seem like the training is working as the loss remains constant. I haven't done anything with the labels (ie shop ids, category ids) but I would still think the loss would change on each iteration. If you have some comments on coding practice, I would be interested as well. Thanks. X_train = grouped_train.drop('revenue', axis=1) y_train = grouped_train['revenue'] print('X & y trains') print(X_train.head()) print(y_train.head()) scaler = StandardScaler() y_train = pd.DataFrame(scaler.fit_transform(y_train.values.reshape(-1,1))) print('Scaled y train') print(y_train.head()) keras.backend.clear_session() model = Sequential() model.add(Dense(30, activation='relu', input_shape=(4,))) model.add(Dense(30, activation='relu')) model.add(Dense(1, activation='relu')) model.summary() print('Compile & fit') model.compile(loss='mean_squared_error', optimizer='RMSprop') model.fit(X_train, scaled_data, batch_size=128, epochs=13) predictions = pd.DataFrame(model.predict(test)) print('Scaled predictions') print(predictions.head()) print('Unscaled predictions') print(pd.DataFrame(scaler.inverse_transform(predictions)).head()) IN OUT",|keras|neural-network|,Model,0
60030329,"Tensorboard import error in keras.callbacks. I'm following sentdex's video on Neural Networks with Keras. My tensorflow version is 2.0.0. I'm on Windows 10 and running this code in an Anaconda Jupyter environment. I have tried searching up or this error but I have got no results. Code: import tensorflow as tf from tensorflow.keras.datasets import cifar10 from tensorflow.keras.preprocessing.image import ImageDataGenerator from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten from tensorflow.keras.layers import Conv2D, MaxPooling2D from tensorflow.keras.callbacks import Tensorboard import pickle import numpy as np import os import time os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' NAME = ""CATS-VS-DOGS-CNN-{}"".format(int(time.time())) tensorboard = TensorBoard(log_dir=""logs/{}"".format(NAME)) pickle_in = open(""X.pickle"",""rb"") X = pickle.load(pickle_in) pickle_in = open(""y.pickle"",""rb"") y = pickle.load(pickle_in) X = X/255.0 model = Sequential() model.add(Conv2D(256, (3, 3), input_shape=X.shape[1:])) model.add(Activation('relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Conv2D(256, (3, 3))) model.add(Activation('relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Flatten()) # this converts our 3D feature maps to 1D feature vectors model.add(Dense(64)) model.add(Activation('relu')) model.add(Dense(1)) model.add(Activation('sigmoid')) model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) model.fit(X, y, batch_size=8, epochs=10, validation_split=0.3, callbacks = [tensorboard]) Error: ImportError Traceback (most recent call last) <ipython-input-2-a32bf5fe83d7> in <module> 5 from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten 6 from tensorflow.keras.layers import Conv2D, MaxPooling2D ----> 7 from tensorflow.keras.callbacks import Tensorboard 8 9 import pickle ImportError: cannot import name 'Tensorboard' from 'tensorflow.keras.callbacks' (C:\Users\anves\Anaconda3\lib\site-packages\tensorflow_core\python\keras\api\_v2\keras\callbacks\__init__.py)",|python|tensorflow|keras|importerror|tensorboard|,API,4
60067039,"TypeError: `Conv2D` can accept only 2 positional arguments ('filters', 'kernel_size'), but you passed the following positional arguments:. I'm trying to run a CNN, but I get this message: TypeError: `Conv2D` can accept only 2 positional arguments ('filters', 'kernel_size'), but you passed the following positional arguments: [64, (3, 3), (1, 1)] My code is: num_filters_conv1 = 64 kernel_size_conv1 = (3,3) stride_conv1 = (1,1) padding_conv1 = 'valid' input_shape = (rows, cols, 1) model.add(Conv2D(num_filters_conv1, kernel_size_conv1, stride_conv1, padding_conv1, activation='relu', input_shape=input_shape)) Anyone knows what's going on? Why are the kernel and padding not supported?",|keras|conv-neural-network|typeerror|,API,4
60187732,"JupyterHub singleuser not able to use tensorflow gpu support using systemdspawner. (this is a crossposting to SO, the jupyterhub issue tracker and the jupyterhub/systemdspawner issue tracker) I have a private JupyterHub Setup using a SystemdSpawner where I try to run tensorflow with gpu support. I followed the tensorflow instructions and alternatively tried a already provisioned AWS AMI (Deep Learning Base AMI (Ubuntu 18.04) Version 21.0) with NDVIDIA, both on AWS EC2 g4 instances. On both setups I'm able to use tensorflow with gpu support in an (i)python 3.6 shell >>> import tensorflow as tf >>> tf.config.list_physical_devices('GPU') 2020-02-12 10:57:13.670937: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1 2020-02-12 10:57:13.698230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-02-12 10:57:13.699066: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: pciBusID: 0000:00:1e.0 name: Tesla T4 computeCapability: 7.5 coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s 2020-02-12 10:57:13.699286: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1 2020-02-12 10:57:13.700918: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10 2020-02-12 10:57:13.702512: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10 2020-02-12 10:57:13.702814: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10 2020-02-12 10:57:13.704561: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10 2020-02-12 10:57:13.705586: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10 2020-02-12 10:57:13.709171: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7 2020-02-12 10:57:13.709278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-02-12 10:57:13.710120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-02-12 10:57:13.710893: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0 [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')] (some warnings about NUMA node, but the gpu is found) Also using nvidia-smi and deviceQuery shows the gpu: $ nvidia-smi Wed Feb 12 10:39:44 2020 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 418.87.01 Driver Version: 418.87.01 CUDA Version: 10.1 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 Tesla T4 On | 00000000:00:1E.0 Off | 0 | | N/A 33C P8 9W / 70W | 0MiB / 15079MiB | 0% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ $ /usr/local/cuda/extras/demo_suite/deviceQuery /usr/local/cuda/extras/demo_suite/deviceQuery Starting... CUDA Device Query (Runtime API) version (CUDART static linking) Detected 1 CUDA Capable device(s) Device 0: ""Tesla T4"" CUDA Driver Version / Runtime Version 10.1 / 10.0 CUDA Capability Major/Minor version number: 7.5 Total amount of global memory: 15080 MBytes (15812263936 bytes) (40) Multiprocessors, ( 64) CUDA Cores/MP: 2560 CUDA Cores GPU Max Clock rate: 1590 MHz (1.59 GHz) Memory Clock rate: 5001 Mhz Memory Bus Width: 256-bit L2 Cache Size: 4194304 bytes Maximum Texture Dimension Size (x,y,z) 1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384) Maximum Layered 1D Texture Size, (num) layers 1D=(32768), 2048 layers Maximum Layered 2D Texture Size, (num) layers 2D=(32768, 32768), 2048 layers Total amount of constant memory: 65536 bytes Total amount of shared memory per block: 49152 bytes Total number of registers available per block: 65536 Warp size: 32 Maximum number of threads per multiprocessor: 1024 Maximum number of threads per block: 1024 Max dimension size of a thread block (x,y,z): (1024, 1024, 64) Max dimension size of a grid size (x,y,z): (2147483647, 65535, 65535) Maximum memory pitch: 2147483647 bytes Texture alignment: 512 bytes Concurrent copy and kernel execution: Yes with 3 copy engine(s) Run time limit on kernels: No Integrated GPU sharing Host Memory: No Support host page-locked memory mapping: Yes Alignment requirement for Surfaces: Yes Device has ECC support: Enabled Device supports Unified Addressing (UVA): Yes Device supports Compute Preemption: Yes Supports Cooperative Kernel Launch: Yes Supports MultiDevice Co-op Kernel Launch: Yes Device PCI Domain ID / Bus ID / location ID: 0 / 0 / 30 Compute Mode: < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) > deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 10.1, CUDA Runtime Version = 10.0, NumDevs = 1, Device0 = Tesla T4 Result = PASS Now I start the JupyterHub, login, and open a terminal, there I get: $ nvidia-smi NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running. and $ /usr/local/cuda/extras/demo_suite/deviceQuery cuda/extras/demo_suite/deviceQuery Starting... CUDA Device Query (Runtime API) version (CUDART static linking) cudaGetDeviceCount returned 38 -> no CUDA-capable device is detected Result = FAIL and also I suspect some kind of ""sandbox"", missing ENV vars, etc. due to that the gpu drivers are not found inside the singleuser environment and subsequently the tensorflow gpu support does not work. Any ideas on this? Probably it is either a small configuration tweak or due to the architecture not solvable at all ;)",|python|tensorflow|gpu|jupyter|jupyterhub|,GPU Usage,3
60355380,"Validation accurary doesn't get better. I'm doing a project on Neural network and was trying a python code using keras and tensorflow package. Currently, I'm experiencing a problem of not getting the validation accurary to going up at all. I have a training set of 9815 images and 200 test set images. I'm really stuck here please help. Right now, the validation result is at exactly 0.5000 for almost all 100 epoch and not going up at all. #Image Processing Stage train_data = ImageDataGenerator(rescale = 1./255, shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True) test_data = ImageDataGenerator(rescale = 1./255) training_set = train_data.flow_from_directory('dataset/train_data', target_size = (128, 128), batch_size = 42, class_mode = 'binary') test_set = test_data.flow_from_directory('dataset/test_data', target_size = (128, 128), batch_size = 42, class_mode = 'binary') # Starting Convolutional Neural Network start_cnn = load_model('CNN.h5') start_cnn.get_weights() start_cnn = Sequential() start_cnn.add(Conv2D(32, (3, 3), input_shape = (128, 128, 3), activation = 'relu', padding='same')) #3*3*3*32+32 start_cnn.add(Conv2D(32, (3, 3), activation = 'relu')) start_cnn.add(MaxPooling2D(pool_size = (2, 2))) for i in range(0,2): start_cnn.add(Conv2D(128, (3, 3), activation = 'relu', padding='same')) start_cnn.add(MaxPooling2D(pool_size = (2, 2))) for i in range(0,2): start_cnn.add(Conv2D(128, (3, 3), activation = 'relu', padding='same')) start_cnn.add(MaxPooling2D(pool_size = (2, 2))) # Flattening start_cnn.add(Flatten()) # Step 4 - Full connection start_cnn.add(Dense(activation=""relu"", units=128)) start_cnn.add(Dense(activation=""relu"", units=64)) start_cnn.add(Dense(activation=""relu"", units=32)) start_cnn.add(Dense(activation=""softmax"", units=1)) start_cnn.summary() # Compiling the CNN start_cnn.compile(Adam(learning_rate=0.001), loss = 'binary_crossentropy', metrics = ['accuracy']) start_cnn.fit(training_set, steps_per_epoch=234, epochs = 100, validation_data = test_set) start_cnn.save('CNN.h5')",|python|tensorflow|keras|conv-neural-network|,Model,0
60351164,"keras: 98% of accuracy but the NN always predicts the same. What could be the cause?. We have the following problem training a DL model to predict loan scoring (classifing into 0,1 or 3). These are the steps: step1: Create new column ""scoring"" (the output) conditions = [ (df2['Credit Score'] >= 0) & (df2['Credit Score'] < 1000), (df2['Credit Score'] >= 1000) & (df2['Credit Score'] < 6000), (df2['Credit Score'] >= 6000) & (df2['Credit Score'] <= 7000)] choices = [0,1,2] df2['Scoring'] = np.select(conditions, choices) step 2:preparing training array = df2.values X = np.vstack((array[:,2:3].T, array[:,5:15].T)).T Y = array[:,15:] N = Y.shape[0] T = np.zeros((N, np.max(Y)+1)) for i in range(N): T[i,Y[i]] = 1 x_train, x_test, y_train, y_test = train_test_split(X, T, test_size=0.2, random_state=42) step 3: Topology model = Sequential() model.add(Dense(80, input_shape=(11,), activation='tanh')) model.add(Dropout(0.2)) model.add(Dense(80, activation='tanh')) model.add(Dropout(0.1)) model.add(Dense(40, activation='relu')) model.add(Dense(3, activation='softmax')) epochs =200 learning_rate = 0.00001 decay_rate = learning_rate / epochs momentum = 0.002 sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False) ad = Adamax(lr=learning_rate) step 4: training epochs = 200 batch_size = 16 history = model.fit(x_train, y_train, validation_data=(x_test, y_test), nb_epoch=epochs, batch_size=batch_size,validation_split=0.1) print ('fit done!') metrics 365/365 [==============================] - 0s 60us/sample - loss: 0.0963 - acc: 0.9808 Test set Loss: 0.096 Accuracy: 0.981 accuracy step5:prediction text1 = [1358,1555,1,3,1741,8,0,1596,1518,0,0] #scoring 0 text2 = [1454,1601,3,11,1763,10,0,685,1044,0,0] #scoring 1 text3 = [1209,1437,3,11,199,18,1,761,1333,1,0] #scoring 2 tmp = np.vstack(text1).T textA = tmp.reshape(1,-1) tmp = np.vstack(text2).T textB = tmp.reshape(1,-1) tmp = np.vstack(text3).T print(tmp) textC = tmp.reshape(1,-1) p = model.predict(textA) t = p[0] print(textA,np.argmax(t)) p = model.predict(textB) t = p[0] print(textB,np.argmax(t)) p = model.predict(textC) t = p[0] print(textC,np.argmax(t)) Problem : The output is always the same in the prediction !!! [9.9205679e-01 3.8634153e-04 7.5568780e-03] [[1358 1555 1 3 1741 8 0 1596 1518 0 0]] 0 --- scoring 0 [0.9862417 0.00205712 0.01170125] [[1454 1601 3 11 1763 10 0 685 1044 0 0]] 0 --- scoring 0 [9.9251783e-01 2.5733517e-04 7.2247880e-03] [[1209 1437 3 11 199 18 1 761 1333 1 0]] 0 ---- scoring 0 what can be the reason of this behavior? Thanks in advance!",|python|machine-learning|keras|deep-learning|tensor|,Training,2
60421221,"PyTorch: Convolving a single channel image using torch.nn.Conv2d. I am trying to use a convolution layer to convolve a grayscale (single layer) image (stored as a numpy array). Here is the code: conv1 = torch.nn.Conv2d(in_channels = 1, out_channels = 1, kernel_size = 33) tensor1 = torch.from_numpy(img_gray) out_2d_np = conv1(tensor1) out_2d_np = np.asarray(out_2d_np) I want my kernel to be 33x33 and the number of output layers should be equal to the number of input layers, which is 1 as the image's RGB channels are summed. Whenout_2d_np = conv1(tensor1) is run it yields the following runtime error: RuntimeError: Expected 4-dimensional input for 4-dimensional weight 1 1 33 33, but got 2-dimensional input of size [246, 248] instead Any idea on how I can solve this? I specifically want to use the torch.nn.Conv2d() class/function. Thanks in advance for any help!",|pytorch|conv-neural-network|convolution|tensor|,Tensors&Inputs,1
60472292,My keras backend tensorflow does not use the gpu?. I install the tensorflow gpu first pip install tensorflow-gpu. pip install keras but when I;m running the gpu task. it does not run with the gpu. It run with CPU. import keras import tensorflow as tf print(keras.__version__) print(tf.__version__) 2.3.1 2.1.0,|tensorflow|keras|gpu|tensorflow2.0|,GPU Usage,3
60508218,"Keras CNN Model accuracy remaining relatively the same and val_accuracy not improving. I am trying to train a model to identify between malignant and benign images using Keras, however I am not achieving the results I had hoped for. The dataset is categorized well and gathered from the ISIC - Archive (https://www.isic-archive.com/). I have tried to change the learning rate multiple times but to no avail...results from one of the training intervals below is the code I am using to train my model using the Adam Optimizer: # In[1]: from keras.callbacks import ModelCheckpoint from keras.models import Sequential from keras.layers import Dropout, Flatten, Dense from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D from PIL import ImageFile from tqdm import tqdm from keras.preprocessing import image from sklearn.datasets import load_files from keras.utils import np_utils import numpy as np from glob import glob import keras # define function to load train, test, and validation datasets def load_dataset(path): data = load_files(path) condition_files = np.array(data['filenames']) condition_targets = np_utils.to_categorical(np.array(data['target']), 2) print(condition_targets) return condition_files, condition_targets # load train, test, and validation datasets train_files, train_targets = load_dataset( '/Users/Grampun/Desktop/ISIC-Archive-Downloader-master/data_set/training_data') valid_files, valid_targets = load_dataset( '/Users/Grampun/Desktop/ISIC-Archive-Downloader-master/data_set/valid_data') test_files, test_targets = load_dataset( '/Users/Grampun/Desktop/ISIC-Archive-Downloader-master/data_set/test_data') # load list of labels condition_names = [item[58:-1] for item in sorted( glob(""/Users/Grampun/Desktop/ISIC-Archive-Downloader-master/data_set/training_data/*/""))] print(condition_names) # print statistics about the dataset print('There are %d total categories.' % len(condition_names)) print('There are %s total images.\n' % len(np.hstack([train_files, valid_files, test_files]))) print('There are %d training images.' % len(train_files)) print('There are %d validation images.' % len(valid_files)) print('There are %d test images.' % len(test_files)) # In[2]: def path_to_tensor(img_path): # loads RGB image as PIL.Image.Image type img = image.load_img(img_path, target_size=(224, 224)) # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3) x = image.img_to_array(img) # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor return np.expand_dims(x, axis=0) def paths_to_tensor(img_paths): list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)] return np.vstack(list_of_tensors) ImageFile.LOAD_TRUNCATED_IMAGES = True # pre-process the data for Keras train_tensors = paths_to_tensor(train_files).astype('float32')/255 valid_tensors = paths_to_tensor(valid_files).astype('float32')/255 test_tensors = paths_to_tensor(test_files).astype('float32')/255 # (IMPLEMENTATION) Model Architecture # # In[4]: model = Sequential() model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(224, 224, 3))) model.add(Conv2D(64, (3, 3), activation='relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Dropout(0.25)) model.add(Flatten()) model.add(Dense(128, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(2, activation='sigmoid')) model.summary() # Compile the Model # In[ ]: # opt = keras.optimizers.Adadelta() opt = keras.optimizers.Adam(lr=0.00003, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0) model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy']) # ### Train the Model # # In[ ]: # TODO: specify the number of epochs that you would like to use to train the model. epochs = 20 checkpointer = ModelCheckpoint(filepath='weights.best.from_scratch.6.hdf5', verbose=1, save_best_only=True) model.fit(train_tensors, train_targets, validation_data=(valid_tensors, valid_targets), epochs=epochs, batch_size=10, callbacks=[checkpointer], verbose=1) # ### Load the Model with the Best Validation Loss # In[5]: model.load_weights('weights.best.from_scratch.6.hdf5') # ### Test the Model # # In[6]: # get index of predicted label for each image in test set condition_predictions = [np.argmax(model.predict( np.expand_dims(tensor, axis=0))) for tensor in test_tensors] # report test accuracy test_accuracy = 100*np.sum(np.array(condition_predictions) == np.argmax(test_targets, axis=1))/len(condition_predictions) print('Test accuracy: %.4f%%' % test_accuracy) # confusion matrix Any help on this would be greatly appreciated (this is my first ML project and am still learning). Thanks!",|python|tensorflow|machine-learning|image-processing|keras|,Model,0
60574843,"Machine Learning Model overfitting. So I build a GRU model and I'm comparing 3 different datasets on the same model. I was just running the first dataset and set the number of epochs to 25, but I have noticed that my validation loss is increasing just after the 6th epoch, doesn't that indicate overfitting, am I doing something wrong? import pandas as pd import tensorflow as tf from keras.layers.core import Dense from keras.layers.recurrent import GRU from keras.models import Sequential import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from sklearn.preprocessing import MinMaxScaler from google.colab import files from tensorboardcolab import TensorBoardColab, TensorBoardColabCallback tbc=TensorBoardColab() # Tensorboard df10=pd.read_csv('/content/drive/My Drive/Isolation Forest/IF 10 PERCENT.csv',index_col=None) df2_10= pd.read_csv('/content/drive/My Drive/2019 Dataframe/2019 10minutes IF 10 PERCENT.csv',index_col=None) X10_train= df10[['WindSpeed_mps','AmbTemp_DegC','RotorSpeed_rpm','RotorSpeedAve','NacelleOrientation_Deg','MeasuredYawError','Pitch_Deg','WindSpeed1','WindSpeed2','WindSpeed3','GeneratorTemperature_DegC','GearBoxTemperature_DegC']] X10_train=X10_train.values y10_train= df10['Power_kW'] y10_train=y10_train.values X10_test= df2_10[['WindSpeed_mps','AmbTemp_DegC','RotorSpeed_rpm','RotorSpeedAve','NacelleOrientation_Deg','MeasuredYawError','Pitch_Deg','WindSpeed1','WindSpeed2','WindSpeed3','GeneratorTemperature_DegC','GearBoxTemperature_DegC']] X10_test=X10_test.values y10_test= df2_10['Power_kW'] y10_test=y10_test.values # scaling values for model x_scale = MinMaxScaler() y_scale = MinMaxScaler() X10_train= x_scale.fit_transform(X10_train) y10_train= y_scale.fit_transform(y10_train.reshape(-1,1)) X10_test= x_scale.fit_transform(X10_test) y10_test= y_scale.fit_transform(y10_test.reshape(-1,1)) X10_train = X10_train.reshape((-1,1,12)) X10_test = X10_test.reshape((-1,1,12)) # creating model using Keras model10 = Sequential() model10.add(GRU(units=512, return_sequences=True, input_shape=(1,12))) model10.add(GRU(units=256, return_sequences=True)) model10.add(GRU(units=256)) model10.add(Dense(units=1, activation='sigmoid')) model10.compile(loss=['mse'], optimizer='adam',metrics=['mse']) model10.summary() history10=model10.fit(X10_train, y10_train, batch_size=256, epochs=25,validation_split=0.20, verbose=1, callbacks=[TensorBoardColabCallback(tbc)]) score = model10.evaluate(X10_test, y10_test) print('Score: {}'.format(score)) y10_predicted = model10.predict(X10_test) y10_predicted = y_scale.inverse_transform(y10_predicted) y10_test = y_scale.inverse_transform(y10_test) plt.plot( y10_predicted, label='Predicted') plt.plot( y10_test, label='Measurements') plt.legend() plt.savefig('/content/drive/My Drive/Figures/Power Prediction 10 Percent.png') plt.show()",|python|pandas|tensorflow|machine-learning|keras|,Model,0
60690327,"TypeError: ('Keyword argument not understood:', 'inputs'). The following code is for disease detection with a CNN model using Tensorflow and Keras. For some reason, I keep getting an error. This is a TypeError with parameter 'inputs'. I don't understand why this error is being raised. Here is my code: from __future__ import absolute_import, division, print_function, unicode_literals import numpy as np # linear algebra import pandas as pd # data processing CSV file import tensorflow as tf from tensorflow.keras.layers import Dense, Flatten, Conv2D from tensorflow.keras import Model import cv2 import matplotlib.pyplot as plt import seaborn as sns # seaborn is a data visualization library for python graphs from PIL import Image import os #file path interacting with operating system thisFolder = os.path.dirname(os.path.realpath(__file__)) print(thisFolder) print(tf.__version__) infected = os.listdir(thisFolder + '/cell_images/cell_images/Parasitized/') uninfected = os.listdir(thisFolder +'/cell_images/cell_images/Uninfected/') data = [] labels = [] for i in infected: try: image = cv2.imread(thisFolder + ""/cell_images/cell_images/Parasitized/""+i) image_array = Image.fromarray(image , 'RGB') resize_img = image_array.resize((50 , 50)) rotated45 = resize_img.rotate(45) rotated75 = resize_img.rotate(75) blur = cv2.blur(np.array(resize_img) ,(10, 10)) data.append(np.array(resize_img)) data.append(np.array(rotated45)) data.append(np.array(rotated75)) data.append(np.array(blur)) labels.append(1) labels.append(1) labels.append(1) labels.append(1) except AttributeError: print('') for u in uninfected: try: image = cv2.imread(""../input/cell_images/cell_images/Uninfected/""+u) image_array = Image.fromarray(image , 'RGB') resize_img = image_array.resize((50 , 50)) rotated45 = resize_img.rotate(45) rotated75 = resize_img.rotate(75) data.append(np.array(resize_img)) data.append(np.array(rotated45)) data.append(np.array(rotated75)) labels.append(0) labels.append(0) labels.append(0) except AttributeError: print('') cells = np.array(data) labels = np.array(labels) np.save('Cells' , cells) np.save('Labels' , labels) print('Cells : {} | labels : {}'.format(cells.shape , labels.shape)) # plt.figure(1 , figsize = (15, 9)) # all graphs and displays n = 0 for i in range(49): n += 1 r = np.random.randint(0 , cells.shape[0] , 1) plt.subplot(7 , 7, n) plt.subplots_adjust(hspace = 0.5 , wspace = 0.5) plt.imshow(cells[r[0]]) plt.title('{} : {}'.format('Infected' if labels[r[0]] == 1 else 'Uninfected', labels[r[0]])) plt.xticks([]) , plt.yticks([]) plt.figure(1, figsize = (15 , 7)) plt.subplot(1 , 2 , 1) plt.imshow(cells[0]) plt.title('Infected Cell') plt.xticks([]) , plt.yticks([]) n = np.arange(cells.shape[0]) np.random.shuffle(n) cells = cells[n] labels = labels[n] cells = cells.astype(np.float32) labels = labels.astype(np.int32) cells = cells/255 from sklearn.model_selection import train_test_split train_x , x , train_y , y = train_test_split(cells , labels , test_size = 0.2 , random_state = 111) eval_x , test_x , eval_y , test_y = train_test_split(x , y , test_size = 0.5 , random_state = 111) plt.figure(1 , figsize = (15 ,5)) n = 0 for z , j in zip([train_y , eval_y , test_y] , ['train labels','eval labels','test labels']): n += 1 plt.subplot(1 , 3 , n) sns.countplot(x = z ) plt.title(j) # plt.show() print('train data shape {} ,eval data shape {} , test data shape {}'.format(train_x.shape, eval_x.shape , test_x.shape)) from tensorflow.python.framework import ops ops.reset_default_graph() def cnn_model_fn(features , labels , mode): input_layers = tf.reshape(features['x'] , [-1 , 50 , 50 ,3]) conv1 = tf.compat.v1.layers.Conv2D( inputs = input_layers , filters = 50 , kernel_size = [7 , 7], padding = 'same', activation = tf.nn.relu ) conv2 = tf.layers.conv2d( inputs = conv1, filters = 90, kernel_size = [3 , 3], padding = 'valid', activation = tf.nn.relu ) conv3 = tf.layers.conv2d( inputs = conv2 , filters = 10, kernel_size = [5 , 5], padding = 'same', activation = tf.nn.relu ) pool1 = tf.layers.max_pooling2d(inputs = conv3 , pool_size = [2 , 2] , strides = 2 ) conv4 = tf.layers.conv2d( inputs = pool1 , filters = 5, kernel_size = [3 , 3], padding = 'same', activation = tf.nn.relu ) pool2 = tf.layers.max_pooling2d(inputs = conv4 , pool_size = [2 , 2] , strides = 2 , padding = 'same') pool2_flatten = tf.layers.flatten(pool2) fc1 = tf.layers.dense( inputs = pool2_flatten, units = 2000, activation = tf.nn.relu ) fc2 = tf.layers.dense( inputs = fc1, units = 1000, activation = tf.nn.relu ) fc3 = tf.layers.dense( inputs = fc2 , units = 500 , activation = tf.nn.relu ) logits = tf.layers.dense( inputs = fc3 , units = 2 ) predictions = { 'classes': tf.argmax(input = logits , axis = 1), 'probabilities': tf.nn.softmax(logits , name = 'softmax_tensor') } if mode == tf.estimator.ModeKeys.PREDICT: return tf.estimator.EstimatorSpec(mode = mode , predictions = predictions) loss = tf.losses.sparse_softmax_cross_entropy(labels = labels , logits = logits) if mode == tf.estimator.ModeKeys.TRAIN: optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.001) train_op = optimizer.minimize(loss = loss , global_step = tf.train.get_global_step()) return tf.estimator.EstimatorSpec(mode = mode , loss = loss , train_op = train_op ) eval_metric_op = {'accuracy' : tf.metrics.accuracy(labels = labels , predictions = predictions['classes'])} logging_hook = tf.train.LoggingTensorHook( tensors = tensors_to_log , every_n_iter = 50 ) return tf.estimator.EstimatorSpec(mode = mode , loss = loss , eval_metric_ops = eval_metric_op) # Checkpoint saving training values malaria_detector = tf.estimator.Estimator(model_fn = cnn_model_fn , model_dir = '/tmp/modelchkpt') tensors_to_log = {'probabilities':'softmax_tensor'} logging_hook = tf.estimator.LoggingTensorHook( tensors = tensors_to_log , every_n_iter = 50 ) train_input_fn = tf.compat.v1.estimator.inputs.numpy_input_fn( x = {'x': train_x}, y = train_y, batch_size = 100 , num_epochs = None , shuffle = True ) malaria_detector.train(input_fn = train_input_fn , steps = 1 , hooks = [logging_hook]) malaria_detector.train(input_fn = train_input_fn , steps = 10000) eval_input_fn = tf.estimator.inputs.numpy_input_fn( x = {'x': eval_x}, y = eval_y , num_epochs = 1 , shuffle = False ) eval_results = malaria_detector.evaluate(input_fn = eval_input_fn) print(eval_results) pred_input_fn = tf.estimator.inputs.numpy_input_fn( x = {'x' : test_x}, y = test_y, num_epochs = 1, shuffle = False ) y_pred = malaria_detector.predict(input_fn = pred_input_fn) classes = [p['classes'] for p in y_pred] from sklearn.metrics import confusion_matrix , classification_report , accuracy_score print('{} \n{} \n{}'.format(confusion_matrix(test_y , classes) , classification_report(test_y , classes) , accuracy_score(test_y , classes))) plt.figure(1 , figsize = (15 , 9)) n = 0 for i in range(49): n += 1 r = np.random.randint( 0 , test_x.shape[0] , 1) plt.subplot(7 , 7 , n) plt.subplots_adjust(hspace = 0.5 , wspace = 0.5) plt.imshow(test_x[r[0]]) plt.title('true {} : pred {}'.format(test_y[r[0]] , classes[r[0]]) ) plt.xticks([]) , plt.yticks([]) plt.show() print(""done"") And here is the error: File ""CNN.py"", line 240, in <module> malaria_detector.train(input_fn = train_input_fn , steps = 1 , hooks = [logging_hook]) File ""/usr/local/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 374, in train loss = self._train_model(input_fn, hooks, saving_listeners) File ""/usr/local/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1164, in _train_model return self._train_model_default(input_fn, hooks, saving_listeners) File ""/usr/local/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1194, in _train_model_default features, labels, ModeKeys.TRAIN, self.config) File ""/usr/local/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1152, in _call_model_fn model_fn_results = self._model_fn(features=features, **kwargs) File ""CNN.py"", line 136, in cnn_model_fn activation = tf.nn.relu File ""/usr/local/lib/python2.7/site-packages/tensorflow_core/python/layers/convolutional.py"", line 314, in __init__ name=name, **kwargs) File ""/usr/local/lib/python2.7/site-packages/tensorflow_core/python/keras/layers/convolutional.py"", line 527, in __init__ **kwargs) File ""/usr/local/lib/python2.7/site-packages/tensorflow_core/python/keras/layers/convolutional.py"", line 122, in __init__ **kwargs) File ""/usr/local/lib/python2.7/site-packages/tensorflow_core/python/layers/base.py"", line 213, in __init__ **kwargs) File ""/usr/local/lib/python2.7/site-packages/tensorflow_core/python/training/tracking/base.py"", line 457, in _method_wrapper result = method(self, *args, **kwargs) File ""/usr/local/lib/python2.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 186, in __init__ generic_utils.validate_kwargs(kwargs, allowed_kwargs) File ""/usr/local/lib/python2.7/site-packages/tensorflow_core/python/keras/utils/generic_utils.py"", line 718, in validate_kwargs raise TypeError(error_message, kwarg) TypeError: ('Keyword argument not understood:', 'inputs') How can I fix this TypeError? I have installed tensorflow 2.1 and upgraded keras. Not sure if that would relate to this error - this seems like a syntax error. Thanks! - Satya",|python|tensorflow|keras|typeerror|,API,4
60753759,"Keras model: Input shape dimension error for RL agent. My goal is to develop a DQN-agent that will choose its action based on a certain strategy/policy. I previously worked with OpenAi gym-environments, but now I wanted to create my own RL environment. At this stage, the agent shall either choose a random action or choose his action based on the predictions given by a deep neural network (defined in the class DQN). So far, I have setup both the neural net model and my environment. The NN shall receive states as its input. These states represent 11 possible scalar values ranging from 9.5 to 10.5 (9.5, 9.6, ..., 10.4, 10.5). Since we're dealing with RL, the agent generates its data during the training process. The output should be either 0 and 1 corresponding to the recommended action. Now, I would like to feed my agent a scalar value: e.g. a sample state of x = 10 and let him decide upon the action to take (Agent.select_action() is called), I encounter an issue related to the input shape/input dimension. Here's the code: 1. DQN Class: class DQN(): def __init__(self, state_size, action_size, lr): self.state_size = state_size self.action_size = action_size self.lr = lr self.model = Sequential() self.model.add(Dense(128, input_dim=self.state_size, activation='relu')) self.model.add(Dense(128, activation='relu')) self.model.add(Dense(self.action_size, activation='linear')) self.model.compile(optimizer=Adam(lr=self.lr), loss='mse') self.model.summary() def model_info(self): model_description = '\n\n---Model_INFO Summary: The model was passed {} state sizes,\ \n {} action sizes and a learning rate of {} -----'\ .format(self.state_size, self.action_size, self.lr) return model_description def predict(self, state): return self.model.predict(state) def train(self, state, q_values): self.state = state self.q_values = q_values return self.model.fit(state, q_values, verbose=0) def load_weights(self, path): self.model.load_weights(path) def save_weights(self, path): self.model.save_weights(path) 2. Agent Class: NUM_EPISODES = 100 MAX_STEPS_PER_EPISODE = 100 EPSILON = 0.5 EPSILON_DECAY_RATE = 0.001 EPSILON_MIN = 0.01 EPSILON_MAX = 1 DISCOUNT_FACTOR = 0.99 REPLAY_MEMORY_SIZE = 50000 BATCH_SIZE = 50 TRAIN_START = 100 ACTION_SPACE = [0, 1] STATE_SIZE = 11 LEARNING_RATE = 0.01 class Agent(): def __init__(self, num_episodes, max_steps_per_episode, epsilon, epsilon_decay_rate, \ epsilon_min, epsilon_max, discount_factor, replay_memory_size, batch_size, train_start): self.num_episodes = NUM_EPISODES self.max_steps_per_episode = MAX_STEPS_PER_EPISODE self.epsilon = EPSILON self.epsilon_decay_rate = EPSILON_DECAY_RATE self.epsilon_min = EPSILON_MIN self.epsilon_max = EPSILON_MAX self.discount_factor = DISCOUNT_FACTOR self.replay_memory_size = REPLAY_MEMORY_SIZE self.replay_memory = deque(maxlen=self.replay_memory_size) self.batch_size = BATCH_SIZE self.train_start = TRAIN_START self.action_space = ACTION_SPACE self.action_size = len(self.action_space) self.state_size = STATE_SIZE self.learning_rate = LEARNING_RATE self.model = DQN(self.state_size, self.action_size, self.learning_rate) def select_action(self, state): random_value = np.random.rand() if random_value < self.epsilon: print('random_value = ', random_value) chosen_action = random.choice(self.action_space) # = EXPLORATION Strategy print('Agent randomly chooses the following EXPLORATION action:', chosen_action) else: print('random_value = {} is greater than epsilon'.format(random_value)) state = np.float32(state) # Transforming passed state into numpy array prediction_by_model = self.model.predict(state) chosen_action = np.argmax(prediction_by_model[0]) # = EXPLOITATION strategy print('NN chooses the following EXPLOITATION action:', chosen_action) return chosen_action if __name__ == ""__main__"": agent_test = Agent(NUM_EPISODES, MAX_STEPS_PER_EPISODE, EPSILON, EPSILON_DECAY_RATE, \ EPSILON_MIN, EPSILON_MAX, DISCOUNT_FACTOR, REPLAY_MEMORY_SIZE, BATCH_SIZE, \ TRAIN_START) # Test of select_action function: state = 10 state = np.array(state) print(state.shape) print(agent_test.select_action(state)) Here's the traceback error I get when running this code: **ValueError**: Error when checking input: expected dense_209_input to have 2 dimensions, but got array with shape () I am unsure why the error regarding 2 dimensions occurs since I have configured the NN in the DQN class to receive only 1 dimension. I have already read through similar questions on stackoverflow (Keras Sequential model input shape, Keras model input shape wrong, Keras input explanation: input_shape, units, batch_size, dim, etc). However, I was not yet able to adapt the suggestions to my use case. Do you have any suggestions or hints? Thank you for your help!",|python|machine-learning|keras|reinforcement-learning|valueerror|,Tensors&Inputs,1
60766775,"Neural network loss value not changing. I am new in deep learning, so I made this model, to train my Data, I tried many combinations, adding layers, changing the activation function, changing the loss function, but the loss is not decreasing. Seeking for your help guys. my training_data contain 1000 samples: 1000 raws, and 20 columns all numbers, outputs: a list of 4 numbers here is my model: from keras import models from keras.models import Sequential from keras import layers from keras.layers import Dense from keras.layers import Flatten , Dropout from keras.optimizers import SGD from keras.callbacks import EarlyStopping from sklearn.preprocessing import StandardScaler from keras import optimizers scaler = StandardScaler() input_shape = x_train[0].shape x_train_std = scaler.fit_transform(x_train) model = Sequential() model.add(layers.Dense(32, activation='sigmoid' , input_shape=input_shape)) model.add(Dropout(0.1)) model.add(layers.Dense(20, activation='sigmoid' )) model.add(Dropout(0.1)) model.add(layers.Dense(15, activation='sigmoid' )) model.add(Dropout(0.1)) model.add(layers.Dense(4, activation='softmax')) #sgd = optimizers.SGD(lr=0.00001, decay=1e-6, momentum=0.85, nesterov=True) #opt = SGD(lr=0.1, nesterov=True) sgd = optimizers.SGD(lr=0.01, momentum=0.87, nesterov=True) model.compile(loss='mean_squared_error', optimizer=sgd) es = EarlyStopping(monitor='val_loss', patience=10) history = model.fit(x_train_std, y_train , validation_split=0.1, epochs=100, batch_size=1 , callbacks = [es])#,",|python|machine-learning|keras|neural-network|,Model,0
60801900,"Model predicts negative values as zeros. I am training a keras autoencoder model with the following structure: model = Sequential() model.add(Dense(128, activation='relu', input_shape=(MAX_CONTEXTS, 3))) model.add(Dense(64, activation='relu')) model.add(Dense(32, activation='relu')) model.add(Dense(64, activation='relu')) model.add(Dense(128, activation='relu')) model.add(Dense(3, activation='relu')) model.compile(optimizer='adam', loss='mse', metrics=['accuracy']) My data is in the shape of (number_of_samples, 430, 3) and contains values from [-1.9236537371711413, 1.9242677998256246]. This data is already normalized. I then train this model: history = model.fit(X, X, epochs=15, batch_size=2, verbose=1, shuffle=True, validation_split=0.2) and get an accuracy of 95.03% (suspiciously high, but my problem now is something else). Now when I predict a sample of my data, the positive values are relatively good, close to what they are in the input, but the negative values are all rounded to 0. Is this a fault of the loss function that I chose? And if so which other loss function should I choose? Or do I have to scale my data differently?",|python|tensorflow|keras|autoencoder|,Model,0
60831731,"Neural Network model not improving accuracy. Scaling problem or model problem?. I am trying to create a simple neural network to see how it works. A second degree ecuation has the form (x-x1)*(x-x2)=0, and if you rearrange it, it will become ax^2+bx+c=0, where a=1, b=-2*x1*x2, c=x1*x2. I want to create a neural network where the inputs are (a,b), and the outputs are (x1,x2). In order to do this, I have created 2 functions that create the data, and stored them in matrices named input and output. I have created a neural network with layers 2x2x2 (including inputs and output), and tested it out with bad results, even after tweaking it. I guess the issue that I have is regarding the data, because the neural network works and spits out a result, but its not good. I don't know where the issue is, but my guess is that it has to do with the data scaling. I have tried to introduce the data without scaling it, but I get the same bad results. The idea is that I provide enough training, so the weights and biases are such that provided any input data, the outcome will be very close to the desired output. This is the code of the whole program import keras from keras import backend as K from keras.models import Sequential from keras.models import load_model from keras.layers import Dense, Activation from keras.layers.core import Dense from keras.optimizers import SGD from keras.metrics import categorical_crossentropy from sklearn.metrics import confusion_matrix import itertools import os os.environ['TF_CPP_MIN_LOG_LEVEL']='2' from random import randint from sklearn.preprocessing import MinMaxScaler import numpy as np def abc(x1, x2): b=-2*x1*x2 c=x1*x2 sol=[b,c] return sol a=10 b=10 c=a*b def Nx2(N, M): matrix=[] n = N+ 1 m= M + 1 for i in range(1,n): for j in range(1,m): temp=[i,j] matrix.append(temp) final_matrix = np.array(matrix) return final_matrix output=Nx2(a, b) # print(output) input=[] for i in range(0,c): temp2=abc(output[i,0],output[i,1]) input.append(temp2) input=np.array(input) print(input) train_labels = output train_samples = input scaler = MinMaxScaler(feature_range=(0,1)) scaled_train_samples = scaler.fit_transform((train_samples).reshape(-1,1)) scaled_train_samples=scaled_train_samples.reshape(-1,2) scaler = MinMaxScaler(feature_range=(0,1)) scaled_train_labels = scaler.fit_transform((train_labels).reshape(-1,1)) scaled_train_labels=scaled_train_labels.reshape(-1,2) print(scaled_train_samples) print(scaled_train_labels) model = Sequential([ Dense(2, input_shape=(2,), activation='sigmoid'), Dense(2, activation='sigmoid'), ]) print(model.weights) model.compile(SGD(lr=0.01), loss='mean_squared_error', metrics=['accuracy']) model.fit(scaled_train_labels, scaled_train_labels, validation_split=0.2, batch_size=10, epochs=20, shuffle=True, verbose=2) print(model.summary()) print(model.weights) These are the kind of results that I am getting. Epoch 1/20 - 0s - loss: 0.1456 - accuracy: 0.5500 - val_loss: 0.3715 - val_accuracy: 0.0500 Epoch 2/20 - 0s - loss: 0.1449 - accuracy: 0.5500 - val_loss: 0.3704 - val_accuracy: 0.0500 Epoch 3/20 - 0s - loss: 0.1443 - accuracy: 0.5500 - val_loss: 0.3692 - val_accuracy: 0.0500 Epoch 4/20 - 0s - loss: 0.1437 - accuracy: 0.5500 - val_loss: 0.3681 - val_accuracy: 0.0500 Epoch 5/20 - 0s - loss: 0.1431 - accuracy: 0.5500 - val_loss: 0.3670 - val_accuracy: 0.0500 Epoch 6/20 - 0s - loss: 0.1425 - accuracy: 0.5500 - val_loss: 0.3658 - val_accuracy: 0.0500 Epoch 7/20 - 0s - loss: 0.1419 - accuracy: 0.5500 - val_loss: 0.3647 - val_accuracy: 0.0500 Epoch 8/20 - 0s - loss: 0.1413 - accuracy: 0.5500 - val_loss: 0.3636 - val_accuracy: 0.0500 Epoch 9/20 - 0s - loss: 0.1407 - accuracy: 0.5500 - val_loss: 0.3625 - val_accuracy: 0.0500 Epoch 10/20 - 0s - loss: 0.1401 - accuracy: 0.5500 - val_loss: 0.3613 - val_accuracy: 0.0500 Epoch 11/20 - 0s - loss: 0.1395 - accuracy: 0.5500 - val_loss: 0.3602 - val_accuracy: 0.0500 Epoch 12/20 - 0s - loss: 0.1389 - accuracy: 0.5500 - val_loss: 0.3591 - val_accuracy: 0.0500 Epoch 13/20 - 0s - loss: 0.1383 - accuracy: 0.5500 - val_loss: 0.3580 - val_accuracy: 0.0500 Epoch 14/20 - 0s - loss: 0.1377 - accuracy: 0.5500 - val_loss: 0.3568 - val_accuracy: 0.0500 Epoch 15/20 - 0s - loss: 0.1372 - accuracy: 0.5500 - val_loss: 0.3557 - val_accuracy: 0.0500 Epoch 16/20 - 0s - loss: 0.1366 - accuracy: 0.5500 - val_loss: 0.3546 - val_accuracy: 0.0500 Epoch 17/20 - 0s - loss: 0.1360 - accuracy: 0.5500 - val_loss: 0.3535 - val_accuracy: 0.0500 Epoch 18/20 - 0s - loss: 0.1354 - accuracy: 0.5500 - val_loss: 0.3524 - val_accuracy: 0.0500 Epoch 19/20 - 0s - loss: 0.1348 - accuracy: 0.5500 - val_loss: 0.3513 - val_accuracy: 0.0500 Epoch 20/20 - 0s - loss: 0.1342 - accuracy: 0.5500 - val_loss: 0.3502 - val_accuracy: 0.0500 Can someone point me into the right direction? Thank you",|python|machine-learning|keras|neural-network|scaling|,Model,0
60874661,"How to increase the accurancy of an image classifier?. I made a model that can classify 82 number with a dataset of images (around 10500 images) the dataset is in Two folders : the first folder the train folder has 8000 images in 82 folders the Second folder the test folder has 2000 images in 82 folders I have tested the model on 2 other folders before going to the main dataset folder and it worked fine but here I don't know why the acc won't get better Please note that not all folders in my dataset has the same number of images neither the resolution of images is the same, but all around 210x50 also please note that in my first try when i used the model to test it on the two folders i made the small dataset of two classe with the same number of images in the folders (same for the validation folder) bellow the code that I used to create the model: from keras.preprocessing.image import ImageDataGenerator from keras.models import Sequential from keras.layers import Conv2D, MaxPooling2D from keras.layers import Activation, Dropout, Flatten, Dense from keras import backend as K # dimensions of our images. img_width, img_height = 251, 54 #img_width, img_height = 150, 33 train_data_dir = 'C:/Users/ADEM/Desktop/msi_youssef/PFE/test/numbers/data/train' validation_data_dir = 'C:/Users/ADEM/Desktop/msi_youssef/PFE/test/numbers/data/valid' nb_train_samples = 10435 nb_validation_samples = 2051 epochs = 20 # how much time you want to train your model on the data batch_size = 16 if K.image_data_format() == 'channels_first': input_shape = (3, img_width, img_height) else: input_shape = (img_width, img_height, 3) model = Sequential() model.add(Conv2D(32, (3, 3), input_shape=input_shape)) model.add(Activation('relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Conv2D(32, (3, 3))) model.add(Activation('relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Conv2D(64, (3, 3))) model.add(Activation('relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Flatten()) model.add(Dense(64)) model.add(Activation('relu')) model.add(Dropout(0.5)) model.add(Dense(1)) model.add(Activation('sigmoid')) model.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy']) # this is the augmentation configuration we will use for training train_datagen = ImageDataGenerator( rescale=1. / 255, shear_range=0.1, zoom_range=0.05, horizontal_flip=False) # this is the augmentation configuration we will use for testing: # only rescaling test_datagen = ImageDataGenerator(rescale=1. / 255) train_generator = train_datagen.flow_from_directory( train_data_dir, target_size=(img_width, img_height), batch_size=batch_size, class_mode='binary') validation_generator = test_datagen.flow_from_directory( validation_data_dir, target_size=(img_width, img_height), batch_size=batch_size, class_mode='binary') model.fit_generator( train_generator, steps_per_epoch=nb_train_samples // batch_size, epochs=epochs, validation_data=validation_generator, validation_steps=nb_validation_samples // batch_size) model.save('first_try.h5') and here the result: WARNING:tensorflow:From C:\Users\ADEM\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead. WARNING:tensorflow:From C:\Users\ADEM\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead. WARNING:tensorflow:From C:\Users\ADEM\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead. WARNING:tensorflow:From C:\Users\ADEM\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead. WARNING:tensorflow:From C:\Users\ADEM\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead. WARNING:tensorflow:From C:\Users\ADEM\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version. Instructions for updating: Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`. WARNING:tensorflow:From C:\Users\ADEM\Anaconda3\lib\site-packages\keras\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead. WARNING:tensorflow:From C:\Users\ADEM\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead. WARNING:tensorflow:From C:\Users\ADEM\Anaconda3\lib\site-packages\tensorflow_core\python\ops\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.where in 2.0, which has the same broadcast rule as np.where Found 10435 images belonging to 82 classes. Found 2051 images belonging to 82 classes. WARNING:tensorflow:From C:\Users\ADEM\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead. WARNING:tensorflow:From C:\Users\ADEM\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead. WARNING:tensorflow:From C:\Users\ADEM\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead. Epoch 1/20 WARNING:tensorflow:From C:\Users\ADEM\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead. WARNING:tensorflow:From C:\Users\ADEM\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead. WARNING:tensorflow:From C:\Users\ADEM\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead. WARNING:tensorflow:From C:\Users\ADEM\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead. WARNING:tensorflow:From C:\Users\ADEM\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead. 652/652 [==============================] - 43s 65ms/step - loss: -625.7214 - acc: 0.0143 - val_loss: -632.8458 - val_acc: 0.0112 Epoch 2/20 652/652 [==============================] - 47s 72ms/step - loss: -627.1426 - acc: 0.0143 - val_loss: -632.6816 - val_acc: 0.0113 Epoch 3/20 652/652 [==============================] - 42s 65ms/step - loss: -627.8743 - acc: 0.0143 - val_loss: -633.1438 - val_acc: 0.0113 Epoch 4/20 652/652 [==============================] - 45s 69ms/step - loss: -627.0466 - acc: 0.0142 - val_loss: -632.6816 - val_acc: 0.0108 Epoch 5/20 652/652 [==============================] - 47s 73ms/step - loss: -628.4401 - acc: 0.0143 - val_loss: -632.7599 - val_acc: 0.0118 Epoch 6/20 652/652 [==============================] - 45s 69ms/step - loss: -626.8264 - acc: 0.0143 - val_loss: -631.9844 - val_acc: 0.0108 Epoch 7/20 652/652 [==============================] - 55s 85ms/step - loss: -627.8007 - acc: 0.0141 - val_loss: -636.2931 - val_acc: 0.0103 Epoch 8/20 652/652 [==============================] - 46s 71ms/step - loss: -626.7282 - acc: 0.0144 - val_loss: -633.0968 - val_acc: 0.0123 Epoch 9/20 652/652 [==============================] - 47s 72ms/step - loss: -628.2569 - acc: 0.0143 - val_loss: -633.8959 - val_acc: 0.0113 Epoch 10/20 652/652 [==============================] - 46s 71ms/step - loss: -627.1006 - acc: 0.0144 - val_loss: -629.7360 - val_acc: 0.0113 Epoch 11/20 652/652 [==============================] - 54s 83ms/step - loss: -627.1028 - acc: 0.0142 - val_loss: -636.8650 - val_acc: 0.0098 Epoch 12/20 652/652 [==============================] - 45s 70ms/step - loss: -627.8524 - acc: 0.0143 - val_loss: -627.5894 - val_acc: 0.0118 Epoch 13/20 652/652 [==============================] - 46s 70ms/step - loss: -627.1357 - acc: 0.0142 - val_loss: -631.9687 - val_acc: 0.0118 Epoch 14/20 652/652 [==============================] - 48s 73ms/step - loss: -627.5105 - acc: 0.0146 - val_loss: -638.9724 - val_acc: 0.0118 Epoch 15/20 652/652 [==============================] - 46s 70ms/step - loss: -629.0591 - acc: 0.0136 - val_loss: -630.7622 - val_acc: 0.0103 Epoch 16/20 652/652 [==============================] - 46s 71ms/step - loss: -625.9115 - acc: 0.0147 - val_loss: -630.3392 - val_acc: 0.0098 Epoch 17/20 652/652 [==============================] - 45s 70ms/step - loss: -627.0184 - acc: 0.0144 - val_loss: -636.2304 - val_acc: 0.0123 Epoch 18/20 652/652 [==============================] - 47s 72ms/step - loss: -626.8828 - acc: 0.0144 - val_loss: -634.5618 - val_acc: 0.0118 Epoch 19/20 652/652 [==============================] - 45s 70ms/step - loss: -627.3642 - acc: 0.0140 - val_loss: -629.8300 - val_acc: 0.0118 Epoch 20/20 652/652 [==============================] - 46s 71ms/step - loss: -627.4297 - acc: 0.0142 - val_loss: -637.6797 - val_acc: 0.0108",|python|tensorflow|machine-learning|keras|deep-learning|,Model,0
61122561,"training by batches leads to more over-fitting. I'm training a sequence to sequence (seq2seq) model and I have different values to train on for the input_sequence_length. For values 10 and 15, I get acceptable results but when I try to train with 20, I get memory errors so I switched the training to train by batches but the model over-fit and the validation loss explodes, and even with the accumulated gradient I get the same behavior, so I'm looking for hints and leads to more accurate ways to do the update. Here is my training function (only with batch section) : if batch_size is not None: k=len(list(np.arange(0,(X_train_tensor_1.size()[0]//batch_size-1), batch_size ))) for epoch in range(num_epochs): optimizer.zero_grad() epoch_loss=0 for i in list(np.arange(0,(X_train_tensor_1.size()[0]//batch_size-1), batch_size )): # by using equidistant batch till the last one it becomes much faster than using the X.size()[0] directly sequence = X_train_tensor[i:i+batch_size,:,:].reshape(-1, sequence_length, input_size).to(device) labels = y_train_tensor[i:i+batch_size,:,:].reshape(-1, sequence_length, output_size).to(device) # Forward pass outputs = model(sequence) loss = criterion(outputs, labels) epoch_loss+=loss.item() # Backward and optimize loss.backward() optimizer.step() epoch_loss=epoch_loss/k model.eval validation_loss,_= evaluate(model,X_test_hard_tensor_1,y_test_hard_tensor_1) model.train() training_loss_log.append(epoch_loss) print ('Epoch [{}/{}], Train MSELoss: {}, Validation : {} {}'.format(epoch+1, num_epochs,epoch_loss,validation_loss)) EDIT: here are the parameters that I'm training with : batch_size = 1024 num_epochs = 25000 learning_rate = 10e-04 optimizer=torch.optim.Adam(model.parameters(), lr=learning_rate) criterion = nn.MSELoss(reduction='mean')",|python|machine-learning|neural-network|pytorch|training-data|,Training,2
61234347,"Why I'm getting bad result with Keras vs random forest or knn?. I'm learning deep learning with keras and trying to compare the results (accuracy) with machine learning algorithms (sklearn) (i.e random forest, k_neighbors) It seems that with keras I'm getting the worst results. I'm working on simple classification problem: iris dataset My keras code looks: samples = datasets.load_iris() X = samples.data y = samples.target df = pd.DataFrame(data=X) df.columns = samples.feature_names df['Target'] = y # prepare data X = df[df.columns[:-1]] y = df[df.columns[-1]] # hot encoding encoder = LabelEncoder() y1 = encoder.fit_transform(y) y = pd.get_dummies(y1).values # split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3) # build model model = Sequential() model.add(Dense(1000, activation='tanh', input_shape = ((df.shape[1]-1),))) model.add(Dense(500, activation='tanh')) model.add(Dense(250, activation='tanh')) model.add(Dense(125, activation='tanh')) model.add(Dense(64, activation='tanh')) model.add(Dense(32, activation='tanh')) model.add(Dense(9, activation='tanh')) model.add(Dense(y.shape[1], activation='softmax')) model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) model.fit(X_train, y_train) score, acc = model.evaluate(X_test, y_test, verbose=0) #results: #score = 0.77 #acc = 0.711 I have tired to add layers and/or change number of units per layer and/or change the activation function (to relu) by it seems that the result are not higher than 0.85. With sklearn random forest or k_neighbors I'm getting result (on same dataset) above 0.95. What am I missing ? With sklearn I did little effort and got good results, and with keras, I had a lot of upgrades but not as good as sklearn results. why is that ? How can I get same results with keras ?",|tensorflow|machine-learning|keras|scikit-learn|neural-network|,Model,0
61274792,"Model suddenly 'forgets' all it has learned and stops working at around 110 epochs. NaN loss and 50% accuracy. I've tested over 20 different LSTM network architectures on my dataset and up until now have had no problems whatsoever. But this one model I've tried gets to around the 110th epoch and it suddenly wipes all it has learned and gets NaN loss (See attached screenshot). It happens every time with this model. The model looks like this: model = keras.Sequential([ keras.kayers.Embedding(numberOfWords, embedding_vector_length, input_length=1000), keras.layers.Dropout(0.5), keras.layers.LSTM(256, dropout = 0.6), keras.layers.Dropout(0.6), keras.layers.Dense(128), keras.layers.Dropout(0.5), keras.layers.Dense(1, activation='sigmoid') ]) model.compile(optimizers.RMSprop(learning_rate=0.001, rho=0.9), loss='binary_crossentropy', metrics=['accuracy']) history = model.fit(x_train, y_train, epochs=700, callbacks=callbacks_list, batch_size=32, validation_data=(x_test, y_test)) I presume I'm doing something wrong here but I'm not experienced enough to spot it. Can someone help me out?",|python|tensorflow|machine-learning|keras|lstm|,Model,0
61482878,"My Keras network with multi_gpu_model uses only 1 GPU. I have a problem when trying to use Keras with three GPUs. My psuedocode is as follows: import keras import keras.models as M from keras.utils import multi_gpu_model i = M.Input(None,None,6) o1,o2,o3 = my_Network(i) net = M.Model(inputs = i, outputs = [o1,o2,o3]) net = multi_gpu_model(net,gpus = 3) net.compile( ~~~~~ ) net.fit(~~~~~ ) My code is training my network, however, only one GPU is utilised. My configuration is as follows: keras : 2.3.1 tensorflow : 2.1.0 Cuda : 10.0 windows : 10 GPU : Tesla 100 x 3 (VRAM : 32GB x 3 ) What is the mistake?",|tensorflow|keras|multi-gpu|,GPU Usage,3
61498304,"Model accuracy and loss not improving in CNN. I am using the below LeNet architecture to train my image classification model , I have noticed that both train , val accuracy not improving for each iteration . Can any one expertise in this area explain what might have gone wrong ? training samples - 110 images belonging to 2 classes. validation - 50 images belonging to 2 classes. #LeNet import keras from keras.models import Sequential from keras.layers import Conv2D from keras.layers import MaxPooling2D from keras.layers import Flatten from keras.layers import Dense #import dropout class if needed from keras.layers import Dropout from keras import regularizers model = Sequential() #Layer 1 #Conv Layer 1 model.add(Conv2D(filters = 6, kernel_size = 5, strides = 1, activation = 'relu', input_shape = (32,32,3))) #Pooling layer 1 model.add(MaxPooling2D(pool_size = 2, strides = 2)) #Layer 2 #Conv Layer 2 model.add(Conv2D(filters = 16, kernel_size = 5, strides = 1, activation = 'relu', input_shape = (14,14,6))) #Pooling Layer 2 model.add(MaxPooling2D(pool_size = 2, strides = 2)) #Flatten model.add(Flatten()) #Layer 3 #Fully connected layer 1 model.add(Dense(units=128,activation='relu',kernel_initializer='uniform' ,kernel_regularizer=regularizers.l2(0.01))) model.add(Dropout(rate=0.2)) #Layer 4 #Fully connected layer 2 model.add(Dense(units=64,activation='relu',kernel_initializer='uniform' ,kernel_regularizer=regularizers.l2(0.01))) model.add(Dropout(rate=0.2)) #layer 5 #Fully connected layer 3 model.add(Dense(units=64,activation='relu',kernel_initializer='uniform' ,kernel_regularizer=regularizers.l2(0.01))) model.add(Dropout(rate=0.2)) #layer 6 #Fully connected layer 4 model.add(Dense(units=64,activation='relu',kernel_initializer='uniform' ,kernel_regularizer=regularizers.l2(0.01))) model.add(Dropout(rate=0.2)) #Layer 7 #Output Layer model.add(Dense(units = 2, activation = 'softmax')) model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy']) from keras.preprocessing.image import ImageDataGenerator #Image Augmentation train_datagen = ImageDataGenerator( rescale=1./255, #rescaling pixel value bw 0 and 1 shear_range=0.2, zoom_range=0.2, horizontal_flip=True) #Just Feature scaling test_datagen = ImageDataGenerator(rescale=1./255) training_set = train_datagen.flow_from_directory( '/Dataset/Skin_cancer/training', target_size=(32, 32), batch_size=32, class_mode='categorical') test_set = test_datagen.flow_from_directory( '/Dataset/Skin_cancer/testing', target_size=(32, 32), batch_size=32, class_mode='categorical') model.fit_generator( training_set, steps_per_epoch=50, #number of input (image) epochs=25, validation_data=test_set, validation_steps=10) # number of training sample Epoch 1/25 50/50 [==============================] - 52s 1s/step - loss: 0.8568 - accuracy: 0.4963 - val_loss: 0.7004 - val_accuracy: 0.5000 Epoch 2/25 50/50 [==============================] - 50s 1s/step - loss: 0.6940 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000 Epoch 3/25 50/50 [==============================] - 48s 967ms/step - loss: 0.6932 - accuracy: 0.5065 - val_loss: 0.6932 - val_accuracy: 0.5000 Epoch 4/25 50/50 [==============================] - 50s 1s/step - loss: 0.6932 - accuracy: 0.4824 - val_loss: 0.6933 - val_accuracy: 0.5000 Epoch 5/25 50/50 [==============================] - 49s 974ms/step - loss: 0.6932 - accuracy: 0.4949 - val_loss: 0.6932 - val_accuracy: 0.5000 Epoch 6/25 50/50 [==============================] - 51s 1s/step - loss: 0.6932 - accuracy: 0.4854 - val_loss: 0.6931 - val_accuracy: 0.5000 Epoch 7/25 50/50 [==============================] - 49s 976ms/step - loss: 0.6931 - accuracy: 0.5015 - val_loss: 0.6918 - val_accuracy: 0.5000 Epoch 8/25 50/50 [==============================] - 51s 1s/step - loss: 0.6932 - accuracy: 0.4986 - val_loss: 0.6932 - val_accuracy: 0.5000 Epoch 9/25 50/50 [==============================] - 49s 973ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6929 - val_accuracy: 0.5000 Epoch 10/25 50/50 [==============================] - 50s 1s/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6932 - val_accuracy: 0.5000 Epoch 11/25 50/50 [==============================] - 49s 976ms/step - loss: 0.6931 - accuracy: 0.5022 - val_loss: 0.6932 - val_accuracy: 0.5000 Epoch 12/25",|python|tensorflow|keras|deep-learning|conv-neural-network|,Model,0
61568463,"Keras: Validation accuracy stays the exact same but validation loss decreases. I know that the problem can't be with the dataset because I've seen other projects use the same dataset. Here is my data preprocessing code: import pandas as pd dataset = pd.read_csv('political_tweets.csv') dataset.head() dataset = pd.read_csv('political_tweets.csv')[""tweet""].values y_train = pd.read_csv('political_tweets.csv')[""dem_or_rep""].values from sklearn.model_selection import train_test_split x_train, x_test, y_train, y_test = train_test_split(dataset, y_train, test_size=0.1) max_words = 10000 print(max_words) max_len = 25 tokenizer = Tokenizer(num_words = max_words, filters='!""#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n1234567890', lower=False,oov_token=""<OOV>"") tokenizer.fit_on_texts(x_train) x_train = tokenizer.texts_to_sequences(x_train) x_train = pad_sequences(x_train, max_len, padding='post', truncating='post') tokenizer.fit_on_texts(x_test) x_test = tokenizer.texts_to_sequences(x_test) x_test = pad_sequences(x_test, max_len, padding='post', truncating='post') And my model: model = Sequential([ Embedding(max_words+1,64,input_length=max_len), Bidirectional(GRU(64, return_sequences = True), merge_mode='concat'), GlobalMaxPooling1D(), Dense(64,kernel_regularizer=regularizers.l2(0.02)), Dropout(0.5), Dense(1, activation='sigmoid'), ]) model.summary() model.compile(loss='binary_crossentropy', optimizer=RMSprop(learning_rate=0.0001), metrics=['accuracy']) model.fit(x_train,y_train, batch_size=128, epochs=500, verbose=1, shuffle=True, validation_data=(x_test, y_test)) Both of my losses decrease, my training accuracy increases, but the validation accuracy stays at 50% (which is awful considering I am doing a binary classification model). Epoch 1/500 546/546 [==============================] - 35s 64ms/step - loss: 1.7385 - accuracy: 0.5102 - val_loss: 1.2458 - val_accuracy: 0.5102 Epoch 2/500 546/546 [==============================] - 34s 62ms/step - loss: 0.9746 - accuracy: 0.5137 - val_loss: 0.7886 - val_accuracy: 0.5102 Epoch 3/500 546/546 [==============================] - 34s 62ms/step - loss: 0.7235 - accuracy: 0.5135 - val_loss: 0.6943 - val_accuracy: 0.5102 Epoch 4/500 546/546 [==============================] - 34s 62ms/step - loss: 0.6929 - accuracy: 0.5135 - val_loss: 0.6930 - val_accuracy: 0.5102 Epoch 5/500 546/546 [==============================] - 34s 62ms/step - loss: 0.6928 - accuracy: 0.5135 - val_loss: 0.6931 - val_accuracy: 0.5102 Epoch 6/500 546/546 [==============================] - 34s 62ms/step - loss: 0.6927 - accuracy: 0.5135 - val_loss: 0.6931 - val_accuracy: 0.5102 Epoch 7/500 546/546 [==============================] - 37s 68ms/step - loss: 0.6925 - accuracy: 0.5136 - val_loss: 0.6932 - val_accuracy: 0.5106 Epoch 8/500 546/546 [==============================] - 34s 63ms/step - loss: 0.6892 - accuracy: 0.5403 - val_loss: 0.6958 - val_accuracy: 0.5097 Epoch 9/500 546/546 [==============================] - 35s 63ms/step - loss: 0.6815 - accuracy: 0.5633 - val_loss: 0.7013 - val_accuracy: 0.5116 Epoch 10/500 546/546 [==============================] - 34s 63ms/step - loss: 0.6747 - accuracy: 0.5799 - val_loss: 0.7096 - val_accuracy: 0.5055 I've seen other posts on this topic and they say to add dropout, crossentropy, decrease the learning rate, etc. I have done all of this and none of it works. Any help is greatly appreciated. Thanks in advance!",|tensorflow|machine-learning|keras|deep-learning|loss|,Training,2
61561253,"AttributeError: module 'tensorflow' has no attribute 'RunOptions'. I'm a beginner. I'm working with python - TensorFlow '2.2.0' on python IDLE. run_opts = tf.RunOptions(report_tensor_allocations_upon_oom = True) I got the following error while running the previous code.: AttributeError: module 'tensorflow' has no attribute 'RunOptions'"" however, according to example 18 from this link on the official page on Tensorflow, there's no error! what's wrong in my case? How should I resolve this issue?",|python|tensorflow|neural-network|attributeerror|,API,4
61704401,"Expected input batch_size (1) to match target batch_size (11). I know this seems to be a common problem but I wasn't able to find a solution. I'm running a multi-label classification model and having issues with tensor sizing. My full code looks like this: from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification import torch # Instantiating tokenizer and model tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased') model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-cased') # Instantiating quantized model quantized_model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8) # Forming data tensors input_ids = torch.tensor(tokenizer.encode(x_train[0], add_special_tokens=True)).unsqueeze(0) labels = torch.tensor(Y[0]).unsqueeze(0) # Train model outputs = quantized_model(input_ids, labels=labels) loss, logits = outputs[:2] Which yields the error: ValueError: Expected input batch_size (1) to match target batch_size (11) Input_ids looks like: tensor([[ 101, 789, 160, 1766, 1616, 1110, 170, 1205, 7727, 1113, 170, 2463, 1128, 1336, 1309, 1138, 112, 119, 11882, 11545, 119, 108, 15710, 108, 3645, 108, 3994, 102]]) with shape: torch.Size([1, 28]) and labels looks like: tensor([[0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1]]) with shape: torch.Size([1, 11]) The size of input_ids will vary as the strings to be encoded vary in size. I also noticed that when feeding in 5 values of Y to produce 5 labels, it yields the error: ValueError: Expected input batch_size (1) to match target batch_size (55). with labels shape: torch.Size([1, 5, 11]) (Note that I didn't feed 5 input_ids, which is presumably why input size remains constant) I've tried a few different approaches to getting these to work, but I'm currently at a loss. I'd really appreciate some guidance. Thanks!",|python|tensorflow|pytorch|classification|tensor|,Tensors&Inputs,1
61706535,"Keras - Validation Loss and Accuracy stuck at 0. I am trying to train a simple 2 layer Fully Connected neural net for Binary Classification in Tensorflow keras. I have split my data into Training and Validation sets with a 80-20 split using sklearn's train_test_split(). When I call model.fit(X_train, y_train, validation_data=[X_val, y_val]), it shows 0 validation loss and accuracy for all epochs, but it trains just fine. Also, when I try to evaluate it on the validation set, the output is non-zero. Can someone please explain why I am facing this 0 loss 0 accuracy error on validation. Thanks for your help. Here is the complete sample code (MCVE) for this error: https://colab.research.google.com/drive/1P8iCUlnD87vqtuS5YTdoePcDOVEKpBHr?usp=sharing",|python|tensorflow|machine-learning|keras|tf.keras|,API,4
61763616,"Why the probabilities of CNN output don't match?. I'm training a CNN model with two classes to predict. I know it gives me a probability for one class and for the other one, and I also know I can get the predicted label, but I don't the results given. Isn't the sum of the output for each evaluated input supposed to be equal 1.0? For instance: [[0.2858745 0.85059494] [0.2858745 0.85059494] [0.6040499 0.5927084 ] [0.8403308 0.291448 ] [0.04195209 0.95504093] [0.79433376 0.21279709] [0.79433376 0.21279709] [0.01326967 0.9891382 ] [0.0153821 0.9867737 ] [0.79433376 0.21279709] [0.01617167 0.98520505] [0.01351487 0.98596036] [0.01473185 0.9846144 ] [0.00896762 0.9899838 ] [0.00936404 0.9893628 ]] Is there something I didn't get? my code: model_05_01 = Sequential() model_05_01.add(Conv1D(filters=16, kernel_size=12, input_shape=(x_train.shape[1], 1))) model_05_01.add(MaxPooling1D(pool_size=4)) model_05_01.add(Conv1D(filters=32, kernel_size=12)) model_05_01.add(MaxPooling1D(pool_size=4)) model_05_01.add(Conv1D(filters=16, kernel_size=12)) model_05_01.add(MaxPooling1D(pool_size=4)) model_05_01.add(Flatten()) model_05_01.add(Dense(16, activation='relu')) model_05_01.add(Dense(2, activation='sigmoid')) model_05_01.compile(loss='logcosh', optimizer='adam', metrics=['accuracy'])",|machine-learning|keras|deep-learning|conv-neural-network|,Model,0
61828821,"""Function call stack: keras_scratch_graph"" - Glove embedded text sentiment analysis. A relatively simple block of code running in Jupyter notebook (because I cannot for the life of me get it working in Google Colab) but I'm getting the below error when running the model. The linked post shows some more of the detail around the inputs I'm working with. I think it may be a GPU related error. I've followed several similarly titled posts and CUDA/Visual Studio installation tutorials, to no avail. import gensim import os from gensim.models import KeyedVectors from gensim.test.utils import datapath, get_tmpfile from gensim.scripts.glove2word2vec import glove2word2vec glove_file = datapath('glove.6B.50d.txt') word2vec_glove_file = get_tmpfile(""glove.6B.50d.word2vec.txt"") glove2word2vec(glove_file, word2vec_glove_file) model = KeyedVectors.load_word2vec_format(word2vec_glove_file, binary=False) # get the vector for each word in the glove file emb_dict = {} glove = open(glove_file, encoding=""utf8"") for line in glove: values = line.split() word = values[0] vector = numpy.asarray(values[1:], dtype='float32') emb_dict[word] = vector glove.close() # get the top 10k words in the tweets, and find their vector from the glove files num_words = 10000 glove_dims = 50 emb_matrix = numpy.zeros((num_words, glove_dims)) for w, i in token.word_index.items(): if i < num_words: vect = emb_dict.get(w) if vect is not None: emb_matrix[i] = vect else: break from keras import models, layers from keras.layers import Embedding, Flatten, Dense glove_model = models.Sequential() glove_model.add(Embedding(num_words, glove_dims, input_length=max_tweet_len)) glove_model.add(layers.LSTM(100)) #glove_model.add(Flatten()) glove_model.add(Dense(3, activation='softmax')) glove_model.layers[0].set_weights([emb_matrix]) glove_model.layers[0].trainable = False glove_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) history = glove_model.fit(train_seq_x, y_train, epochs=10, batch_size=64) The error returned is: Epoch 1/10 --------------------------------------------------------------------------- InvalidArgumentError Traceback (most recent call last) <ipython-input-11-224ee1a00f0e> in <module> 13 glove_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) 14 ---> 15 history = glove_model.fit(train_seq_x, y_train, epochs=10, batch_size=64) ~\anaconda3\lib\site-packages\keras\engine\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs) 1237 steps_per_epoch=steps_per_epoch, 1238 validation_steps=validation_steps, -> 1239 validation_freq=validation_freq) 1240 1241 def evaluate(self, ~\anaconda3\lib\site-packages\keras\engine\training_arrays.py in fit_loop(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq) 194 ins_batch[i] = ins_batch[i].toarray() 195 --> 196 outs = fit_function(ins_batch) 197 outs = to_list(outs) 198 for l, o in zip(out_labels, outs): ~\anaconda3\lib\site-packages\tensorflow\python\keras\backend.py in __call__(self, inputs) 3790 value = math_ops.cast(value, tensor.dtype) 3791 converted_inputs.append(value) -> 3792 outputs = self._graph_fn(*converted_inputs) 3793 3794 # EagerTensor.numpy() will often make a copy to ensure memory safety. ~\anaconda3\lib\site-packages\tensorflow\python\eager\function.py in __call__(self, *args, **kwargs) 1603 TypeError: For invalid positional/keyword argument combinations. 1604 """""" -> 1605 return self._call_impl(args, kwargs) 1606 1607 def _call_impl(self, args, kwargs, cancellation_manager=None): ~\anaconda3\lib\site-packages\tensorflow\python\eager\function.py in _call_impl(self, args, kwargs, cancellation_manager) 1643 raise TypeError(""Keyword arguments {} unknown. Expected {}."".format( 1644 list(kwargs.keys()), list(self._arg_keywords))) -> 1645 return self._call_flat(args, self.captured_inputs, cancellation_manager) 1646 1647 def _filtered_call(self, args, kwargs): ~\anaconda3\lib\site-packages\tensorflow\python\eager\function.py in _call_flat(self, args, captured_inputs, cancellation_manager) 1744 # No tape is watching; skip to running the function. 1745 return self._build_call_outputs(self._inference_function.call( -> 1746 ctx, args, cancellation_manager=cancellation_manager)) 1747 forward_backward = self._select_forward_and_backward_functions( 1748 args, ~\anaconda3\lib\site-packages\tensorflow\python\eager\function.py in call(self, ctx, args, cancellation_manager) 596 inputs=args, 597 attrs=attrs, --> 598 ctx=ctx) 599 else: 600 outputs = execute.execute_with_cancellation( ~\anaconda3\lib\site-packages\tensorflow\python\eager\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name) 58 ctx.ensure_initialized() 59 tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name, ---> 60 inputs, attrs, num_outputs) 61 except core._NotOkStatusException as e: 62 if name is not None: InvalidArgumentError: indices[40,19] = 11263 is not in [0, 10000) [[node embedding_1/embedding_lookup (defined at C:\Users\Jorda\anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py:3009) ]] [Op:__inference_keras_scratch_graph_1707] Function call stack: keras_scratch_graph As requested in the comments, the method of tokenisation is as follows: from keras.preprocessing import text, sequence # create a tokenizer token = text.Tokenizer() token.fit_on_texts(tweets) word_index = token.word_index max_tweet_len = 30 # convert text to sequence of tokens and pad them to ensure equal length vectors train_seq_x = sequence.pad_sequences(token.texts_to_sequences(x_train), maxlen=max_tweet_len) test_seq_x = sequence.pad_sequences(token.texts_to_sequences(x_test), maxlen=max_tweet_len) print(train_seq_x) print(test_seq_x)",|python|tensorflow|keras|gpu|,API,4
61781193,"Training loss not changing at all (PyTorch). I am trying to solve a text classification problem. My training data has input as a sequence of 80 numbers in which each represent a word and target value is just a number between 1 and 3. I pass it through this model: class Model(nn.Module): def __init__(self, tokenize_vocab_count): super().__init__() self.embd = nn.Embedding(tokenize_vocab_count+1, 300) self.embd_dropout = nn.Dropout(0.3) self.LSTM = nn.LSTM(input_size=300, hidden_size=100, dropout=0.3, batch_first=True) self.lin1 = nn.Linear(100, 1024) self.lin2 = nn.Linear(1024, 512) self.lin_dropout = nn.Dropout(0.8) self.lin3 = nn.Linear(512, 3) def forward(self, inp): inp = self.embd_dropout(self.embd(inp)) inp, (h_t, h_o) = self.LSTM(inp) h_t = F.relu(self.lin_dropout(self.lin1(h_t))) h_t = F.relu(self.lin_dropout(self.lin2(h_t))) out = F.softmax(self.lin3(h_t)) return out My training loop is as follows: model = Model(tokenizer_obj.count+1).to('cuda') optimizer = optim.AdamW(model.parameters(), lr=1e-2) loss_fn = nn.CrossEntropyLoss() EPOCH = 10 for epoch in range(0, EPOCH): for feature, target in tqdm(author_dataloader): train_loss = loss_fn(model(feature.to('cuda')).view(-1, 3), target.to('cuda')) optimizer.zero_grad() train_loss.backward() optimizer.step() print(f""epoch: {epoch + 1}\tTrain Loss : {train_loss}"") I printed out the feature and target dimension and it is as follows: torch.Size([64, 80]) torch.Size([64]) Here 64 is the batch_size. I am not doing any validation as of now. When I train I am getting a constant loss value and no change /home/koushik/Software/miniconda3/envs/fastai/lib/python3.7/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1 ""num_layers={}"".format(dropout, num_layers)) 0%| | 0/306 [00:00<?, ?it/s]/media/koushik/Backup Plus/Code/Machine Deep Learning/NLP/src/Deep Learning/model.py:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument. out = F.softmax(self.lin3(h_t)) 100%|| 306/306 [00:03<00:00, 89.36it/s] epoch: 1 Train Loss : 1.0986120700836182 100%|| 306/306 [00:03<00:00, 89.97it/s] epoch: 2 Train Loss : 1.0986120700836182 100%|| 306/306 [00:03<00:00, 89.35it/s] epoch: 3 Train Loss : 1.0986120700836182 100%|| 306/306 [00:03<00:00, 89.17it/s] epoch: 4 Train Loss : 1.0986120700836182 100%|| 306/306 [00:03<00:00, 88.72it/s] epoch: 5 Train Loss : 1.0986120700836182 100%|| 306/306 [00:03<00:00, 87.75it/s] epoch: 6 Train Loss : 1.0986120700836182 100%|| 306/306 [00:03<00:00, 85.67it/s] epoch: 7 Train Loss : 1.0986120700836182 100%|| 306/306 [00:03<00:00, 85.40it/s] epoch: 8 Train Loss : 1.0986120700836182 100%|| 306/306 [00:03<00:00, 84.49it/s] epoch: 9 Train Loss : 1.0986120700836182 100%|| 306/306 [00:03<00:00, 84.21it/s] epoch: 10 Train Loss : 1.0986120700836182 Can anyone please help",|deep-learning|neural-network|nlp|pytorch|lstm|,Model,0
61886777,"Keras multi-class classifier probabilities are 0 and 1 valued. I am implementing a classifier to identify 3 different types of images, my last layer has 3 neurons with sigmoid activation from keras.model import Sequential from keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Dropout, Flatten, Dense model = Sequential() model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', input_shape=input_shape)) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(BatchNormalization()) model.add(Dropout(0.2)) # more conv layers model.add(Flatten()) model.add(Dense(512, activation='relu')) model.add(Dense(3, activation='sigmoid')) model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(), metrics=['accuracy']) The training set label uses one hot encoding, and there are abundant training examples for each of the 3 categories. But when I run model.predict(X) on the test set, the first 10 output is [[0. 1. 1.] [1. 1. 1.] [1. 1. 1.] [1. 1. 1.] [0. 1. 1.] [1. 1. 1.] [0. 1. 1.] [1. 1. 1.] [1. 1. 1.] [1. 1. 1.]] model.predict() should output probabilities and each row should sum up to 1, but in the actual results, sometimes every category has a probability of 1. Does anyone know why the probabilities come up this way?",|python|tensorflow|machine-learning|keras|deep-learning|,Model,0
61943896,"Pad torch tensors of different sizes to be equal. I am looking for a way to take an image/target batch for segmentation and return the batch where the image dimensions have been changed to be equal for the whole batch. I have tried this using the code below: def collate_fn_padd(batch): ''' Padds batch of variable length note: it converts things ToTensor manually here since the ToTensor transform assume it takes in images rather than arbitrary tensors. ''' # separate the image and masks image_batch,mask_batch = zip(*batch) # pad the images and masks image_batch = torch.nn.utils.rnn.pad_sequence(image_batch, batch_first=True) mask_batch = torch.nn.utils.rnn.pad_sequence(mask_batch, batch_first=True) # rezip the batch batch = list(zip(image_batch, mask_batch)) return batch However, I get this error: RuntimeError: The expanded size of the tensor (650) must match the existing size (439) at non-singleton dimension 2. Target sizes: [3, 650, 650]. Tensor sizes: [3, 406, 439] How do I efficiently pad the tensors to be of equal dimensions and avoid this issue?",|python|machine-learning|pytorch|padding|tensor|,Tensors&Inputs,1
61997645,"RNN model not learning anything. I am practicing with RNN. I randomly create 5 integers. If the first integer is an odd number, the y value is 1, otherwise y is 0 (So, only the first x counts). Problem is, when I run this model, it does not 'learn': val_loss and val_accuracy does not change over epochs. What would be the cause? from keras.layers import SimpleRNN, LSTM, GRU, Dropout, Dense from keras.models import Sequential import numpy as np data_len = 300 x = [] y = [] for i in range(data_len): a = np.random.randint(1,10,5) if a[0] % 2 == 0: y.append('0') else: y.append('1') a = a.reshape(5, 1) x.append(a) print(x) X = np.array(x) Y = np.array(y) model = Sequential() model.add(GRU(units=24, activation='relu', return_sequences=True, input_shape=[5,1])) model.add(Dropout(rate=0.5)) model.add(GRU(units=12, activation='relu')) model.add(Dropout(rate=0.5)) model.add(Dense(units=1, activation='softmax')) model.compile(optimizer='adam', loss='mse', metrics=['accuracy']) model.summary() history = model.fit(X[:210], Y[:210], epochs=20, validation_split=0.2) Epoch 1/20 168/168 [==============================] - 1s 6ms/step - loss: 0.4345 - accuracy: 0.5655 - val_loss: 0.5000 - val_accuracy: 0.5000 ... Epoch 20/20 168/168 [==============================] - 0s 315us/step - loss: 0.4345 - accuracy: 0.5655 - val_loss: 0.5000 - val_accuracy: 0.5000",|tensorflow|keras|recurrent-neural-network|,Model,0
62055783,"Tf.keras model.predict() returns class probabilities that are higher than 1?. I am trying to call model.predict() in tf.keras on a CNN to predict the class for a single image. For some reason, the class probabilities are coming back higher than 1 which is nonsensical. I am unsure why this is occurring. Below is how I train my CNN: class_names = ['Angry','Disgust','Fear','Happy','Sad','Surprise','Neutral'] model = models.Sequential() model.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape=(48, 48, 1), kernel_regularizer=tf.keras.regularizers.l1(0.01))) model.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu')) model.add(layers.MaxPooling2D((2, 2))) model.add(tf.keras.layers.Dropout(0.5)) model.add(layers.Conv2D(64, (3, 3), activation='relu')) model.add(layers.MaxPooling2D((2, 2))) model.add(tf.keras.layers.Dropout(0.5)) model.add(layers.Conv2D(64, (3, 3), activation='relu')) model.summary() model.add(layers.Flatten()) model.add(layers.Dense(64, activation='relu')) model.add(layers.Dense(7)) #model.summary() model.compile(optimizer='adam',loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),metrics=['accuracy']) lr_reducer = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=3) #monitors the validation loss for signs of a plateau and then alter the learning rate by the specified factor if a plateau is detected early_stopper = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, patience=6, mode='auto') #This will monitor and stop the model training if it is not further converging checkpointer = tf.keras.callbacks.ModelCheckpoint('C:\\Users\\rtlum\\Documents\\DataSci_Projects\\PythonTensorFlowProjects\\Datasets\\FER2013_Model_Weights\\Model\\weights.hd5', monitor='val_loss', verbose=1, save_best_only=True) #This allows checkpoints to be saved each epoch just in case the model stops training epochs = 100 batch_size = 64 learning_rate = 0.001 model.fit( train_data, train_labels, epochs = epochs, batch_size = batch_size, validation_split = 0.2, shuffle = True, callbacks=[lr_reducer, checkpointer, early_stopper] ) Below is how I call model.predict() and pass in a single image to predict: model = tf.keras.models.load_model('Model\\weights.hd5') img = Image.open(test_image).convert('L') img = img.resize([48, 48]) image_data = np.asarray(img, dtype=np.uint8) #image_data = np.resize(img,3072) image_data = image_data / 255 image_data_test = image_data.reshape((1, 48, 48, 1)) class_names = ['Angry','Disgust','Fear','Happy','Sad','Surprise','Neutral'] x = model.predict(image_data_test) app.logger.info(x) image_pred = np.argmax(x) y = round(x[0][np.argmax(x)], 2) confidence = y * 100 print(class_names[image_pred], confidence) And finally, below is the class probabilities I receive from model.predict(): >>> x = model.predict(image_data_test) >>> x array([[ 1.0593076 , -3.5140653 , 0.7505076 , 2.1341033 , 0.02394461, -0.08749148, 0.6640976 ]], dtype=float32)",|python|tensorflow|machine-learning|keras|conv-neural-network|,Model,0
62068109,"CNN model not working well on 4 species but works well with 2 species. I tried CNN model on two classes and got 80% but when i tried the same model with 4 classes i got very bad result. What is the reason pls help. The model of CNN i used is: model= Sequential() model.add(Conv2D(64,(3,3),input_shape=input_shape)) model.add(Activation('relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Conv2D(64,(3,3))) model.add(Activation('relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Conv2D(64,(3,3))) model.add(Activation('relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Flatten()) model.add(Dense(64)) model.add(Activation('relu')) model.add(Dropout(0.5)) model.add(Dense(1)) model.add(Activation('sigmoid')) #opt = SGD( lr=0.01) model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) history = model.fit_generator( train_generator, steps_per_epoch=nb_train_samples//batch_size, epochs=epochs, validation_data = validation_generator, validation_steps = validation_generator.samples // batch_size, ) The result of 2 classes something like this i lost the actual result of it: Epoch 29/35 46/46 [==============================] - 188s 4s/step - loss: 0.6511 - accuracy: 0.5880 - val_loss: 0.7534 - val_accuracy: 0.5175 The result with 4 classes is: 46/46 [==============================] - 367s 8s/step - loss: -10550614391401.7266 - accuracy: 0.2541 - val_loss: -15023441182720.0000 - val_accuracy: 0.2354",|python|tensorflow|keras|,Model,0
62163194,"PyTorch: The number of sizes provided (0) must be greater or equal to the number of dimensions in the tensor (1). I'm trying to convert a CPU model to GPU using Pytorch, but I'm running into issues. I'm running this on Colab and I'm sure that Pytorch detects a GPU. This is a deep Q network (RL). I declare my network as: Q = Q_Network(input_size, hidden_size, output_size).to(device) I ran into an issue when I tried to pass arguments through the network (It expected type cuda but got type cpu) so I add .to(device): batch = np.array(shuffled_memory[i:i+batch_size]) b_pobs = np.array(batch[:, 0].tolist(), dtype=np.float32).reshape(batch_size, -1) b_pact = np.array(batch[:, 1].tolist(), dtype=np.int32) b_reward = np.array(batch[:, 2].tolist(), dtype=np.int32) b_obs = np.array(batch[:, 3].tolist(), dtype=np.float32).reshape(batch_size, -1) b_done = np.array(batch[:, 4].tolist(), dtype=np.bool) q = Q(torch.from_numpy(b_pobs).to(device)) q_ = Q_ast(torch.from_numpy(b_obs).to(device)) maxq = torch.max(q_.data,axis=1) target = copy.deepcopy(q.data) for j in range(batch_size): print(target[j, b_pact[j]].shape) # torch.Size([]) target[j, b_pact[j]] = b_reward[j]+gamma*maxq[j]*(not b_done[j]) #I run into issues here Here is the error: RuntimeError: expand(torch.cuda.FloatTensor{[50]}, size=[]): the number of sizes provided (0) must be greater or equal to the number of dimensions in the tensor (1)",|python|numpy|pytorch|tensor|,Tensors&Inputs,1
62313327,"Network loss stalls where it should fall to zero quickly. I have a neural network with 30 input nodes, 1 hidden node, and 1 output node. I am training it on a dataset where the inputs are 30-dimensional vectors with entries between -1 and 1, and the targets are the 2nd entry of these vectors. I expect the network to train and learn to output the 2nd entry of the input vector quickly, since this is as simple as decreasing the weights in the network which connect the input nodes to the hidden node to zero except the one for the 2nd entry. However, The loss stalls quickly at approximately 0.168. I'ld expect it to quickly go to zero, which is the case if the targets are just 0. The following code showcases the problem with a randomised dataset. import numpy as np from tensorflow.keras import models from tensorflow.keras import layers import tensorflow as tf np.random.seed(123) dataSize = 100000 xdata = np.zeros((dataSize, 30)) ydata = np.zeros((dataSize)) for i in range(dataSize): vec = (np.random.rand(30) * 2) - 1 xdata[i] = vec ydata[i] = vec[1] model = models.Sequential() model.add(layers.Dense(1, activation=""relu"", input_shape=(30, ))) model.add(layers.Dense(1, activation=""sigmoid"")) optimizer = tf.keras.optimizers.Adam(learning_rate=0.01) lossObject = tf.keras.losses.MeanSquaredError() model.compile(optimizer=optimizer, loss=lossObject) model.fit(xdata, ydata, epochs=200, batch_size=32) I have tried multiple different optimizers, loss functions, batch sizes, dataset sizes and learning rates, however the result is always the loss stalling at a relatively high value. Why is this happening? I am not interested in responses asking why I am doing this. I am new to neural networks and I need to understand why this is happening before I can continue with my original task. Thank you in advance.",|python|tensorflow|keras|deep-learning|neural-network|,Model,0
62386631,"Cannot import BertModel from transformers. I am trying to import BertModel from transformers, but it fails. This is code I am using from transformers import BertModel, BertForMaskedLM This is the error I get ImportError: cannot import name 'BertModel' from 'transformers' Can anyone help me fix this?",|python|nlp|pytorch|huggingface-transformers|bert-language-model|,API,4
62486779,"ImportError: cannot import name 'device_spec' from 'tensorflow.python.framework'. When i try to run python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/faster_rcnn_inception_v2_pets.config command this error pops out. (tensorflow1.13) C:\tensorflow1\models\research\object_detection>python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/faster_rcnn_inception_v2_pets.config Traceback (most recent call last): File ""train.py"", line 51, in from object_detection.builders import dataset_builder File ""C:\tensorflow1\models\research\object_detection\builders\dataset_builder.py"", line 33, in from object_detection.builders import decoder_builder File ""C:\tensorflow1\models\research\object_detection\builders\decoder_builder.py"", line 25, in from object_detection.data_decoders import tf_example_decoder File ""C:\tensorflow1\models\research\object_detection\data_decoders\tf_example_decoder.py"", line 28, in from tf_slim import tfexample_decoder as slim_example_decoder File ""C:\Users\user\anaconda3\envs\tensorflow1\lib\site-packages\tf_slim_init_.py"", line 25, in from tf_slim.layers import * File ""C:\Users\user\anaconda3\envs\tensorflow1\lib\site-packages\tf_slim\layers_init_.py"", line 25, in from tf_slim.layers.layers import * File ""C:\Users\user\anaconda3\envs\tensorflow1\lib\site-packages\tf_slim\layers\layers.py"", line 30, in from tf_slim.ops import variables File ""C:\Users\user\anaconda3\envs\tensorflow1\lib\site-packages\tf_slim\ops\variables.py"", line 27, in from tensorflow.python.framework import device_spec as tf_device ImportError: cannot import name 'device_spec'",|python|python-3.x|tensorflow|training-data|faster-rcnn|,API,4
62440336,"Tensorflow model not improving. I'm just starting learning ML/Tensorflow/etc, so I'm pretty novice and still don't really know what the troubleshooting method is like. I'm currently having an issue with my model as it doesn't seem to really ever improve. For instance, the output appears as Epoch 1/10 4/4 [==============================] - 41s 10s/step - loss: 0.8833 - accuracy: 0.4300 Epoch 2/10 4/4 [==============================] - 12s 3s/step - loss: 0.8833 - accuracy: 0.4300 Epoch 3/10 4/4 [==============================] - 10s 3s/step - loss: 0.8833 - accuracy: 0.4300 Epoch 7/1000 4/4 [==============================] - 10s 3s/step - loss: 0.8833 - accuracy: 0.4300 The main aspect that worries me is that it doesn't change at all which makes me think I'm doing something completely wrong. To give some more context and code, I am trying to do some time series classification. Basically, the input is the normalized time series of the song and the net should classify if it's classical music (output of 1 means it is, output of 0 means it isn't). This is the current model I am trying. model = keras.Sequential([ keras.layers.Conv1D(filters=100, kernel_size=10000, strides=5000, input_shape=(1323000, 1), activation='relu'), keras.layers.Conv1D(filters=100, kernel_size=10, strides=3, input_shape=(263, 100), activation='relu'), keras.layers.LSTM(1000), keras.layers.Dense(500, activation='relu'), keras.layers.Dense(250, activation='relu'), keras.layers.Dense(1, activation='softmax') ]) model.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=['accuracy']) This is how I get the training data (x and y are dictionaries with time series from different songs). minute = 1323000 x_train = np.zeros((100, minute, 1)) y_train = np.zeros((100,)) for kk in range(0, 100): num = randint(0, 41) ts = x[num] start = randint(0, len(ts) - minute) x_train[kk, :] = np.array([ts[start:(start + minute)]]).T y_train[kk] = 1 - y[num] and then training: for kk in range(1, 1000): x_train, y_train = create_training_set(x, y) model.fit(x_train, y_train, epochs=1000) I looked at some similar questions asked, however, I was already doing what was suggested or the advice was too specific for the asker. I also tried some relatively different models/activators, so I don't think it's because the model is too complicated and the data is already normalized, so that shouldn't be an issue. But, as I said, I'm knew to all this and could be wrong.",|python|tensorflow|machine-learning|keras|deep-learning|,Model,0
62600335,"Getting OutOfMemory Error in GpuMemory: 0?from small CNN and small data-set. My objective is to train a very simple CNN on MNIST using Tensorflow, convert it to TensorRT, and use it to perform inference on the MNIST test set using TensorRT, all on a Jetson Nano, but I am getting several errors and warnings, including OutOfMemory Error in GpuMemory: 0? To try and reduce memory footprint, I tried also creating a script where I simply load the TensorRT model (that had already been converted and saved in the previous script) and use it to perform inference on a small subset of the MNIST test set (100 floating point values), but I am still getting the same out of memory error. The entire directory containing the TensorRT model is only 488 KB, and the 100 test points cant be taking up very much memory, so I am confused about why GPU memory is running out. What could be the reason for this, and how can I solve it? Another thing which seems suspicious is that some of the Tensorflow logging info messages are being printed multiple times, EG Successfully opened dynamic library libcudart? Successfully opened dynamic library libcublas? ARM64 does not support NUMA - returning NUMA node zero? What could be the reason for this (EG dynamic libraries being opened over and over again), and could this have something to do with why the GPU memory keeps running out? Shown below are the 2 Python scripts; the console output from each one is too long to post on Stack Overflow, but they can be seen attached to this Gist: https://gist.github.com/jakelevi1996/8a86f2c2257001afc939343891ee5de7 """""" Example script which trains a simple CNN for 1 epoch on a subset of MNIST, and converts the model to TensorRT format, for enhanced performance which fully utilises the NVIDIA GPU, and then performs inference. Useful resources: - https://stackoverflow.com/questions/58846828/how-to-convert-tensorflow-2-0-savedmodel-to-tensorrt - https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html#worflow-with-savedmodel - https://www.tensorflow.org/api_docs/python/tf/experimental/tensorrt/Converter - https://github.com/tensorflow/tensorflow/issues/34339 - https://github.com/tensorflow/tensorrt/blob/master/tftrt/examples/image-classification/image_classification.py Tested on the NVIDIA Jetson Nano, Python 3.6.9, tensorflow 2.1.0+nv20.4, numpy 1.16.1 """""" import os from time import perf_counter import numpy as np t0 = perf_counter() import tensorflow as tf from tensorflow.keras import datasets, layers, models, Input from tensorflow.python.compiler.tensorrt import trt_convert as trt from tensorflow.python.saved_model import signature_constants from tensorflow.python.saved_model import tag_constants from tensorflow.python.framework import convert_to_constants tf.compat.v1.enable_eager_execution() # see github issue above # Get training and test data (x_train, y_train), (x_test, y_test) = datasets.mnist.load_data() x_train = np.expand_dims(x_train, -1) / 255.0 x_test = np.expand_dims(x_test, -1) / 255.0 # Create model model = models.Sequential() # model.add(Input(shape=x_train.shape[1:], batch_size=batch_size)) model.add(layers.Conv2D(10, (5, 5), activation='relu', padding=""same"")) model.add(layers.MaxPooling2D((2, 2))) model.add(layers.Flatten()) model.add(layers.Dense(10)) # Compile and train model model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy']) model.fit( x_train[:10000], y_train[:10000], validation_data=(x_test, y_test), batch_size=100, epochs=1, ) # Save model print(""Saving model..."") current_dir = os.path.dirname(os.path.abspath(__file__)) model_dir = os.path.join(current_dir, ""CNN_MNIST"") if not os.path.isdir(model_dir): os.makedirs(model_dir) # model.save(model_dir) tf.saved_model.save(model, model_dir) # Convert to TRT format trt_model_dir = os.path.join(current_dir, ""CNN_MNIST_TRT"") converter = trt.TrtGraphConverterV2(input_saved_model_dir=model_dir) converter.convert() converter.save(trt_model_dir) t1 = perf_counter() print(""Finished TRT conversion; time taken = {:.3f} s"".format(t1 - t0)) # Make predictions using saved model, and print the results (NB using an alias # for tf.saved_model.load, because the normal way of calling this function # throws an error because for some reason it is expecting a sess) saved_model_loaded = tf.compat.v1.saved_model.load_v2( export_dir=trt_model_dir, tags=[tag_constants.SERVING]) graph_func = saved_model_loaded.signatures[ signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY] graph_func = convert_to_constants.convert_variables_to_constants_v2(graph_func) x_test_tensor = tf.convert_to_tensor(x_test, dtype=tf.float32) preds = graph_func(x_test_tensor)[0].numpy() print(preds.shape, y_test.shape) accuracy = list(preds.argmax(axis=1) == y_test).count(True) / y_test.size print(""Accuracy of predictions = {:.2f} %"".format(accuracy * 100)) """""" Example script which trains a simple CNN for 1 epoch on a subset of MNIST, and converts the model to TensorRT format, for enhanced performance which fully utilises the NVIDIA GPU. Useful resources: - https://stackoverflow.com/questions/58846828/how-to-convert-tensorflow-2-0-savedmodel-to-tensorrt - https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html#worflow-with-savedmodel - https://www.tensorflow.org/api_docs/python/tf/experimental/tensorrt/Converter - https://github.com/tensorflow/tensorflow/issues/34339 - https://github.com/tensorflow/tensorrt/blob/master/tftrt/examples/image-classification/image_classification.py Tested on the NVIDIA Jetson Nano, Python 3.6.9, tensorflow 2.1.0+nv20.4, numpy 1.16.1 """""" import os from time import perf_counter import numpy as np t0 = perf_counter() import tensorflow as tf from tensorflow.keras import datasets from tensorflow.python.saved_model import signature_constants from tensorflow.python.saved_model import tag_constants from tensorflow.python.framework import convert_to_constants tf.compat.v1.enable_eager_execution() # see github issue above # Get training and test data (x_train, y_train), (x_test, y_test) = datasets.mnist.load_data() x_train = np.expand_dims(x_train, -1) / 255.0 x_test = np.expand_dims(x_test, -1) / 255.0 # TEMPORARY: just use 100 test points to minimise GPU memory num_points = 100 x_test, y_test = x_test[:num_points], y_test[:num_points] current_dir = os.path.dirname(os.path.abspath(__file__)) trt_model_dir = os.path.join(current_dir, ""CNN_MNIST_TRT"") # Make predictions using saved model, and print the results (NB using an alias # for tf.saved_model.load, because the normal way of calling this function # throws an error because for some reason it is expecting a sess) saved_model_loaded = tf.compat.v1.saved_model.load_v2( export_dir=trt_model_dir, tags=[tag_constants.SERVING]) graph_func = saved_model_loaded.signatures[ signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY] graph_func = convert_to_constants.convert_variables_to_constants_v2(graph_func) x_test_tensor = tf.convert_to_tensor(x_test, dtype=tf.float32) preds = graph_func(x_test_tensor)[0].numpy() print(preds.shape, y_test.shape) accuracy = list(preds.argmax(axis=1) == y_test).count(True) / y_test.size print(""Accuracy of predictions = {:.2f} %"".format(accuracy * 100)) t1 = perf_counter() print(""Finished inference; time taken = {:.3f} s"".format(t1 - t0))",|out-of-memory|gpu|tensorflow2.0|tensorrt|nvidia-jetson-nano|,GPU Usage,3
62612226,"Tensorflow Check failed: status == CUDNN_STATUS_SUCCESS (7 vs. 0)Failed to set cuDNN stream. When I run my tensorflow keras model it will sometimes stop after an epoch and throw the following error: tensorflow/stream_executor/dnn.cc:613] CUDNN_STATUS_EXECUTION_FAILED in tensorflow/stream_executor/cuda/cuda_dnn.cc(1867): 'cudnnRNNForwardTraining( cudnn.handle(), rnn_desc.handle(), model_dims.max_seq_length, input_desc.handles(), input_data.opaque(), input_h_desc.handle(), input_h_data.opaque(), input_c_desc.handle(), input_c_data.opaque(), rnn_desc.params_handle(), params.opaque(), output_desc.handles(), output_data->opaque(), output_h_desc.handle(), output_h_data->opaque(), output_c_desc.handle(), output_c_data->opaque(), workspace.opaque(), workspace.size(), reserve_space.opaque(), reserve_space.size())' 2020-06-27 17:19:45.741256: F tensorflow/stream_executor/cuda/cuda_dnn.cc:189] Check failed: status == CUDNN_STATUS_SUCCESS (7 vs. 0)Failed to set cuDNN stream. Im using the GPU version of tensorflow and it has worked without problems up until about 3 days ago when it began giving me this error. I also have the following callbacks in my model so that it saves checkpoints and epochs locally. Please note I am using Autokeras thus why the nodes have the ak. library prefix. However it works just like keras, it just automatically tries to find the best set of hyperparameters when running the .fit function. input_node = ak.Input() #output_node1 = ak.Normalization()(input_node) output_node1 = ak.RNNBlock(return_sequences=True, layer_type='lstm')(input_node) output_node2 = ak.RNNBlock(return_sequences=True, layer_type='lstm')(output_node1) output_node3 = ak.RNNBlock(return_sequences=True, layer_type='lstm')(output_node2) output_node4 = ak.RNNBlock(layer_type='lstm')(output_node3) output_node5 = ak.DenseBlock()(output_node4) output_node = ak.Merge()( [output_node1, output_node2, output_node3, output_node4, output_node5]) output_nodefinal = ak.ClassificationHead()(output_node) my_callbacks = [tf.keras.callbacks.EarlyStopping(patience=15, restore_best_weights=True, monitor=""val_accuracy"", mode=""auto""), tf.keras.callbacks.ModelCheckpoint(filepath=""D:\AutoKerasProject\TimeseriesCallbackModel\TimeseriesCallbackModel"", save_weights_only=True, monitor='val_accuracy', mode='auto', save_best_only=True)] classifier = ak.AutoModel( inputs=input_node, outputs=output_nodefinal, max_trials=500, directory=""D:\AutoKerasProject\TimeseriesModel"", overwrite=True) My PC and software specs are as follows PC Specs: Windows 8.1 GTX 1060 6GB Intel i7-4770 Software Specs: Nvidia Geforce Experience Driver 451.81 CuDNN 7.6.5 (windows 7 version) Cuda 10.1.243 Conda Python 3.6.10 Tensorflow 2.2.0",|python|pandas|machine-learning|keras|gpu|,GPU Usage,3
62780158,"Expected object of device type cuda but got device type cpu for argument #2 'mat2' in call to _th_mm. I recently have access to a GPU and have tried to run my code but there is an error. I've read about adding .cuda() to the layers could help but I have ran similar code without the ""qml"" part and it worked just fine without changing the layer (with cuda I mean). For the sake of brevity, I only added the class for the neural network and the entire loop of the code. The missing part of the code is for other aspects. But if there is a need for the rest of the code, I would be happy to include everything. Code: class DQN(nn.Module): def __init__(self, img_height, img_width): super().__init__() self.flatten = nn.Flatten() self.fc1 = nn.Linear(in_features=img_height * img_width * 3, out_features=12) self.fc2 = nn.Linear(in_features=12, out_features=8) # self.fc3 = nn.Linear(in_features=10, out_features=8) self.clayer_in = torch.nn.Linear(in_features=8, out_features=wires) self.clayer_out = torch.nn.Linear(wires, out_dim) dev = qml.device('strawberryfields.fock', wires=wires, cutoff_dim=3) self.layer_qnode = qml.QNode(layer, dev) weights = qml.init.cvqnn_layers_all(n_quantum_layers, wires) weight_shapes = {""w{}"".format(i): w.shape for i, w in enumerate(weights)} self.qlayer = qml.qnn.TorchLayer(self.layer_qnode, weight_shapes) def forward(self, t): t = self.flatten(t) t = self.fc1(t) t = self.fc2(t) # t = self.fc3(t) t = self.clayer_in(t) t = self.qlayer(t) t = self.clayer_out(t) t = t.sigmoid() return t device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"") #print(device) em = CartPoleEnvManager(device) strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay) agent = Agent(strategy, em.num_actions_available(), device) memory = ReplayMemory(memory_size) #learning_rate = LearningRate(lr_start,lr_end,lr_decay) #learn = lr(learning_rate) policy_net = DQN(em.get_screen_height(), em.get_screen_width()).to(device) target_net = DQN(em.get_screen_height(), em.get_screen_width()).to(device) target_net.load_state_dict(policy_net.state_dict()) target_net.eval() #tells pytorch that target_net is only used for inference, not training optimizer = optim.Adam(params=policy_net.parameters(), lr=0.01) i = 0 episode_durations = [] for episode in range(num_episodes): #iterate over each episode program_starts = time.time() em.reset() state = em.get_state() for timestep in count(): action = agent.select_action(state, policy_net) reward = em.take_action(action) next_state = em.get_state() memory.push(Experience(state, action, next_state, reward)) state = next_state #i+=1 #print(i) if memory.can_provide_sample(batch_size): scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.9) experiences = memory.sample(batch_size) states, actions, rewards, next_states = extract_tensors(experiences) current_q_values = QValues.get_current(policy_net, states, actions) next_q_values = QValues.get_next(target_net, next_states) #will get the max qvalues of the next state, q values of next state are used via next state target_q_values = (next_q_values * gamma) + rewards loss = F.mse_loss(current_q_values, target_q_values.unsqueeze(1)) optimizer.zero_grad() # sets the gradiesnt of all weights n biases in policy_net to zero loss.backward() #computes gradient of loss with respect to all weights n biases in the policy net optimizer.step() # updates the weights n biases with the gradients that were computed form loss.backwards scheduler.step() if em.done: episode_durations.append(timestep) plot(episode_durations, 100) break if episode % target_update == 0: target_net.load_state_dict(policy_net.state_dict()) now = time.time() print(""Episode hat {0} Sekunden gedauert"".format(now - program_starts)) em.close() And the error: Traceback (most recent call last): File ""qdqn.py"", line 328, in <module> loss.backward() #computes gradient of loss with respect to all weights n biases in the policy net File ""/home/ubuntu/anaconda3/envs/gymm/lib/python3.8/site-packages/torch/tensor.py"", line 198, in backward torch.autograd.backward(self, gradient, retain_graph, create_graph) File ""/home/ubuntu/anaconda3/envs/gymm/lib/python3.8/site-packages/torch/autograd/__init__.py"", line 98, in backward Variable._execution_engine.run_backward( RuntimeError: Expected object of device type cuda but got device type cpu for argument #2 'mat2' in call to _th_mm",|python|pytorch|gpu|,GPU Usage,3
62887574,"Can numpy arrays run in GPUs?. I am using PyTorch. I have the following code: import numpy as np import torch X = np.array([[1, 3, 2, 3], [2, 3, 5, 6], [1, 2, 3, 4]]) X = torch.DoubleTensor(X).cuda() X_split = np.array_split(X.numpy(), indices_or_sections = 2, axis = 0) X_split but I am getting this error: --------------------------------------------------------------------------- TypeError Traceback (most recent call last) <ipython-input-121-870b5d3f67b6> in <module>() ----> 1 X_prime_class_split = np.array_split(X_prime_class.numpy(), 2 indices_or_sections = 2, 3 axis = 0) 4 X_prime_class_split TypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first. The error message is clear and I know how to fix this error by just including .cpu(), ie. X_prime_class.cpu().numpy(). I am just curious to know if this confirms that numpy arrays cannot run in GPUs/Cuda?",|python|numpy|pytorch|gpu|,GPU Usage,3
62957334,"Keras loss gets stuck at one point CNN. I have 12k rgb images with 6 in one group of 100x100. So I used Conv3d with shape (-1,100,100,3,6) As an output, I have boolean (0 or 1) So what I did is, I kept few CNN layers at first, flatten it for DNN layer and at lastly apply sigmoid activation function to get result from 0 to 1. I have first 400 image with bool output 1 and other 1600 images with bool output 0. So I wrote this code import numpy as np from tensorflow.contrib.keras import models,layers,losses,optimizers x=np.load('features.npy') y=np.zeros(2000) y[:400]=1 model = models.Sequential() model.add(layers.Conv3D(32, (3,3,3), input_shape=(100,100,3,6),activation='linear',padding='same')) model.add(layers.MaxPool3D((2,2,2),padding='same')) model.add(layers.Conv3D(32, (3,3,3),activation='linear',padding='same')) model.add(layers.MaxPool3D((2,2,2),padding='same')) model.add(layers.Conv3D(32, (3,3,3),activation='linear',padding='same')) model.add(layers.Flatten()) model.add(layers.Dense(10, activation='linear')) model.add(layers.Dense(units=1,activation='sigmoid')) model.compile(optimizer=optimizers.Nadam(),loss=losses.mean_absolute_percentage_error) model.fit(x,y,epochs=10,shuffle=True,batch_size=20) model.fit(x,y,epochs=100,shuffle=True,batch_size=20) model.save_weights(""model0.h5"") print(""Saved model to disk"") but the problem is, when applied sigmoid or any function which maps to limited space, the model always gives output as 0 or 1 and the loss always stays at 20 when i replace the last activation with linear, the model works great but as the linear function isn't suitable for binary output, it gives loss over a million.",|python|tensorflow|keras|deep-learning|,Model,0
63008865,"Load pickle saved GPU tensor with CPU?. I save the last hidden layer of Bert for my following process using pickle on GPU. # output is the last hidden layer of bert, transformed on GPU with open(filename, 'wb') as f: pk.dump(output, f) Is it possible to load it on my person laptop without GPU? I tried following code, but all failed. # 1st try with open(filename, 'rb') as f: torch.load(f, map_location='cpu') # 2nd torch.load(filename, map_location=torch.device('cpu')) All get the following error RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU. Is it possible to load the file on my laptop?",|pytorch|gpu|cpu|,GPU Usage,3
63073170,"Test data gives ""Found 0 images in 0 classes"" using flow_from_directory. This is my flow_from_directory code train_image_generator = ImageDataGenerator(rescale=1./255) validation_image_generator = ImageDataGenerator(rescale=1./255) test_image_generator = ImageDataGenerator(rescale=1./255) train_data_gen = train_image_generator.flow_from_directory( train_dir, target_size=(150, 150), batch_size=batch_size, class_mode='binary') val_data_gen =validation_image_generator.flow_from_directory( validation_dir, target_size=(150, 150), batch_size=batch_size, class_mode='binary') test_data_gen = test_image_generator.flow_from_directory( test_dir, target_size=(150, 150), batch_size=batch_size, class_mode='binary', shuffle = False,) And it prints: Found 2000 images belonging to 2 classes. Found 1000 images belonging to 2 classes. Found 0 images belonging to 0 classes. Even though the data exists as I used: !ls /root/.keras/datasets/cats_and_dogs/test Which gives the output: 10.jpg 15.jpg 1.jpg 24.jpg 29.jpg 33.jpg 38.jpg 42.jpg 47.jpg 5.jpg 11.jpg 16.jpg 20.jpg 25.jpg 2.jpg 34.jpg 39.jpg 43.jpg 48.jpg 6.jpg 12.jpg 17.jpg 21.jpg 26.jpg 30.jpg 35.jpg 3.jpg 44.jpg 49.jpg 7.jpg 13.jpg 18.jpg 22.jpg 27.jpg 31.jpg 36.jpg 40.jpg 45.jpg 4.jpg 8.jpg 14.jpg 19.jpg 23.jpg 28.jpg 32.jpg 37.jpg 41.jpg 46.jpg 50.jpg 9.jpg what am I doing wrong or what must be Done?",|machine-learning|keras|conv-neural-network|machine-learning-model|,Training,2
63083555,"Binary Crossentropy accuracy of keras model is not changing. I have seen many questions of this problem online, but there are no definitive solutions and my case might be different, as it is with time series data and a LSTM architecture. model = Sequential() model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(n_steps, n_features))) model.add(LSTM(50, activation='relu')) model.add(Dense(1, activation = 'sigmoid')) model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy']) Logs: Train on 290 samples, validate on 190 samples Epoch 1/4000 - 1s - loss: 0.6896 - accuracy: 0.5586 - val_loss: 0.6846 - val_accuracy: 0.6105 Epoch 2/4000 - 0s - loss: 0.6890 - accuracy: 0.5586 - val_loss: 0.6843 - val_accuracy: 0.6105 Epoch 3/4000 - 0s - loss: 0.6889 - accuracy: 0.5586 - val_loss: 0.6829 - val_accuracy: 0.6105 Epoch 4/4000 - 0s - loss: 0.6884 - accuracy: 0.5586 - val_loss: 0.6827 - val_accuracy: 0.6105 Epoch 5/4000 - 0s - loss: 0.6883 - accuracy: 0.5586 - val_loss: 0.6825 - val_accuracy: 0.6105 Epoch 6/4000 - 0s - loss: 0.6882 - accuracy: 0.5586 - val_loss: 0.6822 - val_accuracy: 0.6105 Epoch 7/4000 - 0s - loss: 0.6882 - accuracy: 0.5586 - val_loss: 0.6820 - val_accuracy: 0.6105 Epoch 8/4000 - 0s - loss: 0.6880 - accuracy: 0.5586 - val_loss: 0.6818 - val_accuracy: 0.6105 Epoch 9/4000 - 0s - loss: 0.6880 - accuracy: 0.5586 - val_loss: 0.6806 - val_accuracy: 0.6105 Epoch 10/4000 - 0s - loss: 0.6876 - accuracy: 0.5586 - val_loss: 0.6795 - val_accuracy: 0.6105",|python|machine-learning|keras|loss|,Training,2
63188879,"model.summary() - AttributeError: 'Tensor' object has no attribute 'summary'. This are my imports: import tensorflow as tf import keras from keras.models import Sequential, Model from keras.layers import Conv2D, Flatten, MaxPooling2D, Dense, Input, Reshape, Concatenate, GlobalAveragePooling2D, BatchNormalization, Dropout, Activation, GlobalMaxPooling2D from keras.utils import Sequence I defined this model: def create_ST_layer(input_shape = (64, 128, 3)): input_img = Input(shape=input_shape) model = Conv2D(48, kernel_size=(5, 5), input_shape = input_shape, strides = (1, 1), activation = ""relu"")(input_img) model = MaxPooling2D(pool_size=(2, 2), strides = (2, 2))(model) model = Conv2D(32, kernel_size=(5, 5), strides = (1, 1), activation = ""relu"")(model) model = MaxPooling2D(pool_size=(2, 2), strides = (2, 2))(model) model = Dense(50, activation = ""relu"")(model) model = Dense(6)(model) return model And created the model by: model = create_ST_layer() When I now try to get the summary of the model: model.summary() I get the following error: --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) <ipython-input-7-5f15418b3570> in <module>() ----> 1 model.summary() AttributeError: 'Tensor' object has no attribute 'summary' Is there something wrong with my imports? Thanks a lot!",|python|tensorflow|keras|error-handling|attributeerror|,API,4
63206710,"Multiply output of Keras layer with a scalar. Pardon me if this is a poorly framed question. This happens to be my first question here. Say I have an output layer in Keras, and I want to multiply the last value (result of sigmoid activation) with a scalar (say 5). (I have attached a code snippet here. Assume all necessary libraries / dependencies included) def create_model(): inp = Input(shape=(561,)) x = Dense(units=1024,input_dim=561)(inp) x = LeakyReLU(0.2)(x) x = Dropout(0.3)(x) x = Dense(units=512)(x) x = LeakyReLU(0.2)(x) x = Dropout(0.3)(x) x = Dense(units=256)(x) x = LeakyReLU(0.2)(x) x = Dense(units=1, activation='sigmoid')(x) m = tf.convert_to_tensor(5) #creating a tensor of value = 5 o = Multiply()([x, m]) #trying to multiply x with o. Doesn't work though! model = Model(inputs=[inp], outputs=[o]) model.compile(loss='binary_crossentropy', optimizer = Adam(lr=0.0002, beta_1=0.5)) return model model = create_model() model.summary() I tried this, and I am getting ""tuple index out of range"" error. I would be glad if someone could help me (i.e in multiplication of last layer's output with a scalar)",|keras|keras-layer|tf.keras|,Model,0
63176966,"Adding AdditiveGaussianNoise to a single image - AssertionError: Expected boolean as argument for 'return_batch'. I would like to add AdditiveGaussianNoise (link: https://imgaug.readthedocs.io/en/latest/source/overview/arithmetic.html#additivegaussiannoise) to a single image which I resized before. This is my code: from skimage.io import imread from skimage.transform import resize import imgaug.augmenters as iaa file_name = ""path/to/image.jpg"" resized_img = resize(imread(file_name), (224, 224)) aug = iaa.AdditiveGaussianNoise(scale=(0, 0.2*255)) augmented_image = aug(resized_img) And I get this error message: --------------------------------------------------------------------------- AssertionError Traceback (most recent call last) <ipython-input-20-e4a0b17d4ac4> in <module>() ----> 1 augmented_image =aug(resized_img) 1 frames /usr/local/lib/python3.6/dist-packages/imgaug/augmenters/meta.py in augment(self, return_batch, hooks, **kwargs) 1782 (""Expected boolean as argument for 'return_batch', got type %s. "" 1783 + ""Call augment() only with named arguments, e.g. "" -> 1784 + ""augment(images=<array>)."") % (str(type(return_batch)),) 1785 ) 1786 AssertionError: Expected boolean as argument for 'return_batch', got type <class 'numpy.ndarray'>. Call augment() only with named arguments, e.g. augment(images=<array>). How do I have to amend my code? Thank you very much!",|python|machine-learning|keras|deep-learning|data-augmentation|,API,4
63200848,"Correcting NaN values/loss for ANN in tensorflow. I am running a churn model using tensorflow and running into a NaN loss. Reading around, I found that I probably had some NaN values in my data as was confirmed by print(np.any(np.isnan(X_test))). I tried using def standardize(train, test): mean = np.mean(train, axis=0) std = np.std(train, axis=0)+0.000001 X_train = (train - mean) / std X_test = (test - mean) /std return X_train, X_test But still coming up with NaN values. Here's the full code if it helps: import numpy as np import matplotlib.pyplot as plt import pandas as pd import tensorflow as tf dataset = pd.read_excel('CHURN DATA.xlsx') X = dataset.iloc[:, 2:45].values y = dataset.iloc[:, 45].values from sklearn.preprocessing import LabelEncoder le = LabelEncoder() X[:, 1] = le.fit_transform(X[:,1]) from sklearn.compose import ColumnTransformer from sklearn.preprocessing import OneHotEncoder ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(),[0])], remainder = 'passthrough') X = np.array(ct.fit_transform(X)) from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2) from sklearn.preprocessing import StandardScaler sc = StandardScaler() X_train = sc.fit_transform(X_train) X_test = sc.transform(X_test) ann = tf.keras.models.Sequential() ann.add(tf.keras.layers.Dense(units = 43, activation = 'relu')) ann.add(tf.keras.layers.Dense(units = 43, activation = 'relu')) ann.add(tf.keras.layers.Dense(units = 1, activation = 'sigmoid')) ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy']) ann.fit(X_train, y_train, batch_size = 256, epochs = 50)",|python|tensorflow|machine-learning|nan|,Training,2
63204176,"Pytorch training loss function throws: ""TypeError: 'Tensor' object is not callable"". I use Python 3.x, and pytorch 1.5.0 with a GPU. I am trying to write a simple multinomial logistic regression using mnist data. My issue is the loss() function throws a TypeError: 'Tensor' object is not callable while looping through the training batches. The thing that baffles me is that the error does not show up in the first iteration of the loop, but for the second batch, I get the full error below: Traceback (most recent call last): File ""/snap/pycharm-community/207/plugins/python-ce/helpers/pydev/pydevd.py"", line 1448, in _exec pydev_imports.execfile(file, globals, locals) # execute the script File ""/snap/pycharm-community/207/plugins/python-ce/helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18, in execfile exec(compile(contents+""\n"", file, 'exec'), glob, loc) File ""/home/pytorch_tutorial/Pytorch_feed_fwd_310720.py"", line 78, in <module> loss = loss(preds,ys) TypeError: 'Tensor' object is not callable The loss() function here is simply loss = nn.CrossEntropyLoss(). The full code is below. Any pointers would be very welcome. for epoch in range(5): running_loss = 0.0 for i, data in enumerate(trainloader, 0): xs, ys = data opt.zero_grad() preds = net(xs) loss = loss(preds,ys) loss.backward() opt.step() # print statistics running_loss += loss.item() if i % 1000 == 999: # print every 1000 mini-batches print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000)) running_loss = 0.0 print('epoch {}, loss {}'.format(epoch, loss.item())) a=1",|pytorch|tensor|,API,4
63235060,"import efficientnet.keras as efn - AttributeError: module 'keras.utils' has no attribute 'generic_utils'. I am trying to use EfficientNet from https://github.com/qubvel/segmentation_models. So, I installed this via pip: !pip install git+https://github.com/qubvel/segmentation_models Then I tried to import efficientnet.keras: import efficientnet.keras as efn And got this error: --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) <ipython-input-57-61d736540c72> in <module>() ----> 1 import efficientnet.keras as efn 1 frames /usr/local/lib/python3.6/dist-packages/efficientnet/__init__.py in init_keras_custom_objects() 69 } 70 ---> 71 keras.utils.generic_utils.get_custom_objects().update(custom_objects) 72 73 AttributeError: module 'keras.utils' has no attribute 'generic_utils' This is very strange since it worked yesterday without any problems and today in one notebook as well but in the other ones I got this error. Does anyone know what to do?",|python|keras|error-handling|attributeerror|efficientnet|,API,4
63212385,"Keras CNN model accuracy not improving and decreasing over epoch?. Newbie to machine learning here. I'm currently working on a diagnostic machine learning framework using 3D-CNNs on fMRI imaging. My dataset consists of 636 images right now, and I'm trying to distinguish between control and affected (binary classification). However, when I tried to train my model, after every epoch, my accuracy remains at 48.13%, no matter what I do. Additionally, over the epoch, the accuracy decreases from 56% to 48.13%. So far, I have tried: changing my loss functions (poisson, categorical cross entropy, binary cross entropy, sparse categorical cross entropy, mean squared error, mean absolute error, hinge, hinge squared) changing my optimizer (I've tried Adam and SGD) changing the number of layers using weight regularization changing from ReLU to leaky ReLU (I thought perhaps that could help if this was a case of overfitting) Nothing has worked so far. Any tips? Here's my code: #importing important packages import tensorflow as tf import os import keras from keras.models import Sequential from keras.layers import Dense, Flatten, Conv3D, MaxPooling3D, Dropout, BatchNormalization, LeakyReLU import numpy as np from keras.regularizers import l2 from sklearn.utils import compute_class_weight from keras.optimizers import SGD BATCH_SIZE = 64 input_shape=(64, 64, 40, 20) # Create the model model = Sequential() model.add(Conv3D(64, kernel_size=(3,3,3), activation='relu', input_shape=input_shape, kernel_regularizer=l2(0.005), bias_regularizer=l2(0.005), data_format = 'channels_first', padding='same')) model.add(MaxPooling3D(pool_size=(2, 2, 2))) model.add(Conv3D(64, kernel_size=(3,3,3), activation='relu', input_shape=input_shape, kernel_regularizer=l2(0.005), bias_regularizer=l2(0.005), data_format = 'channels_first', padding='same')) model.add(MaxPooling3D(pool_size=(2, 2, 2))) model.add(BatchNormalization(center=True, scale=True)) model.add(Conv3D(64, kernel_size=(3,3,3), activation='relu', input_shape=input_shape, kernel_regularizer=l2(0.005), bias_regularizer=l2(0.005), data_format = 'channels_first', padding='same')) model.add(MaxPooling3D(pool_size=(2, 2, 2))) model.add(Conv3D(64, kernel_size=(3,3,3), activation='relu', input_shape=input_shape, kernel_regularizer=l2(0.005), bias_regularizer=l2(0.005), data_format = 'channels_first', padding='same')) model.add(MaxPooling3D(pool_size=(2, 2, 2))) model.add(BatchNormalization(center=True, scale=True)) model.add(Flatten()) model.add(BatchNormalization(center=True, scale=True)) model.add(Dense(128, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01))) model.add(Dropout(0.5)) model.add(Dense(128, activation='sigmoid', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01))) model.add(Dense(1, activation='softmax', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01))) # Compile the model model.compile(optimizer = keras.optimizers.sgd(lr=0.000001), loss='poisson', metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]) # Model Testing history = model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=50, verbose=1, shuffle=True)",|python|tensorflow|machine-learning|keras|deep-learning|,Model,0
63421764,"Tensorflow-gpu not detecting GPU?. I have tensorflow-gpu version 2.2.0 installed with Anaconda in python 3.7.4, but my code always runs on CPU and It's not able to detect my GPU. physical_devices= tf.config.experimental.list_physical_devices('GPU') print(len(physical_devices)) >>> 0 When I run this: from tensorflow.python.client import device_lib print(device_lib.list_local_devices()) I get: [name: ""/device:CPU:0"" device_type: ""CPU"" memory_limit: 268435456 locality { } incarnation: 6159543114609950707 , name: ""/device:XLA_CPU:0"" device_type: ""XLA_CPU"" memory_limit: 17179869184 locality { } incarnation: 4043157374881641271 physical_device_desc: ""device: XLA_CPU device"" , name: ""/device:XLA_GPU:0"" device_type: ""XLA_GPU"" memory_limit: 17179869184 locality { } incarnation: 1519736160969870434 physical_device_desc: ""device: XLA_GPU device"" ] The XLA_GPU:0 corresponds to the integrated graphics that runs on CPU, because I have tested running my code under with tf.device('/GPU:0'): and Task Manager shows only CPU usage. I checked in my NVIDIA Control Panel -> System Information -> Components and under 3D Settings I have NVCUDA64.dll described as NVIDIA CUDA 11.0.208 Driver. Up to this moment I haven't tried reinstalled tensorflow-gpu, expecting there could be something else that has to be done before.",|python-3.x|tensorflow|gpu|,GPU Usage,3
63515767,"Tensorflow 2.3.0 does not detect GPU. The tensorflow does not detect the GPU card. I have following the procedures suggest at Nvidia website and tensorflow/install/gpu. How can I fix it? I am using the following packages and drives: NVIDIA [nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2019 NVIDIA Corporation Built on Sun_Jul_28_19:12:52_Pacific_Daylight_Time_2019 Cuda compilation tools, release 10.1, V10.1.243][1] Cudnn Version 8.0.2 Tensor Flow Name Version Build Channel tensorflow 2.3.0 pypi_0 pypi tensorflow-addons 0.11.1 pypi_0 pypi tensorflow-estimator 2.3.0 pypi_0 pypi I use the following code to check it; Python 3.7.7 (default, May 6 2020, 11:45:54) [MSC v.1916 64 bit (AMD64)] Type ""copyright"", ""credits"" or ""license"" for more information. IPython 7.17.0 -- An enhanced Interactive Python. from tensorflow.python.client import device_lib device_lib.list_local_devices() Result 2020-08-20 22:58:38.419555: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll Out[1]: [name: ""/device:CPU:0"" device_type: ""CPU"" memory_limit: 268435456 locality { } incarnation: 12639439165040732604, name: ""/device:XLA_CPU:0"" device_type: ""XLA_CPU"" memory_limit: 17179869184 locality { } incarnation: 2249215130251849864 physical_device_desc: ""device: XLA_CPU device"", name: ""/device:XLA_GPU:0"" device_type: ""XLA_GPU"" memory_limit: 17179869184 locality { } incarnation: 7640064762024919839 physical_device_desc: ""device: XLA_GPU device""] 2020-08-20 22:58:38.419555: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll 2020-08-20 22:58:40.332579: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2020-08-20 22:58:40.340307: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x22481a47710 initialized for platform Host (this does not guarantee that XLA will be used). Devices: 2020-08-20 22:58:40.341741: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version 2020-08-20 22:58:40.342711: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll 2020-08-20 22:58:40.362324: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: pciBusID: 0000:01:00.0 name: GeForce GTX 1050 computeCapability: 6.1 coreClock: 1.493GHz coreCount: 5 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.43GiB/s 2020-08-20 22:58:40.362354: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll 2020-08-20 22:58:40.366447: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll 2020-08-20 22:58:40.369790: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll 2020-08-20 22:58:40.370968: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll 2020-08-20 22:58:40.374957: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll 2020-08-20 22:58:40.377382: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll 2020-08-20 22:58:40.378955: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudnn64_7.dll'; dlerror: cudnn64_7.dll not found 2020-08-20 22:58:40.378977: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform. Skipping registering GPU devices... 2020-08-20 22:58:40.455688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix: 2020-08-20 22:58:40.455717: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263] 0 2020-08-20 22:58:40.455728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0: N 2020-08-20 22:58:40.458391: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x22490b5c830 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices: 2020-08-20 22:58:40.458412: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): GeForce GTX 1050, Compute Capability 6.1",|python|tensorflow|gpu|,GPU Usage,3
63566232,"RuntimeError: The size of tensor a (1024) must match the size of tensor b (512) at non-singleton dimension 3. I am doing the following operation, energy.masked_fill(mask == 0, float(""-1e20"")) my python traces are below, File ""/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py"", line 722, in _call_impl result = self.forward(*input, **kwargs) File ""seq_sum.py"", line 418, in forward enc_src = self.encoder(src, src_mask) File ""/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py"", line 722, in _call_impl result = self.forward(*input, **kwargs) File ""seq_sum.py"", line 71, in forward src = layer(src, src_mask) File ""/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py"", line 722, in _call_impl result = self.forward(*input, **kwargs) File ""seq_sum.py"", line 110, in forward _src, _ = self.self_attention(src, src, src, src_mask) File ""/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py"", line 722, in _call_impl result = self.forward(*input, **kwargs) File ""seq_sum.py"", line 191, in forward energy = energy.masked_fill(mask == 0, float(""-1e20"")) RuntimeError: The size of tensor a (1024) must match the size of tensor b (512) at non-singleton dimension 3 These are my attention layers code, Q = self.fc_q(query) K = self.fc_k(key) V = self.fc_v(value) #Q = [batch size, query len, hid dim] #K = [batch size, key len, hid dim] #V = [batch size, value len, hid dim] # Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3) # K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3) # V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3) Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).view(-1, 1024) K = K.view(batch_size, -1, self.n_heads, self.head_dim).view(-1, 1024) V = V.view(batch_size, -1, self.n_heads, self.head_dim).view(-1, 1024) energy = torch.matmul(Q, K.transpose(1,0)) / self.scale I am following below github code to do my seq to seq operation,seq2seq pytorch actual testing code is available on the below location, code to test a seq of 1024 to 1024 output 2nd example tried here I have commented out pos_embedding due CUDA error with large index (RuntimeError: cuda runtime error (59)",|python|pytorch|transformer-model|seq2seq|,Tensors&Inputs,1
63871643,"Runtimeerror: Cuda out of memory - problem in code or gpu?. I am currently working on a computer vision project. I keep getting a runtime error that says ""CUDA out of memory"". I have tried all possible ways like reducing batch size and image resolution, clearing the cache, deleting variables after training starts, reducing image data and so on... Unfortunately, this error doesn't stop. I have a Nvidia Geforce 940MX graphics card on my HP Pavilion laptop. I have installed cuda 10.2 and cudNN from the pytorch installation page. My aim was to create a flask website out of this model but I am stuck with this issue. Any suggestions to this problem will be helpful. This is my code import pandas as pd import numpy as np import torch import torch.nn as nn import os import cv2 import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split import albumentations as A from torch.utils.data import TensorDataset, DataLoader,Dataset from torchvision import models from collections import defaultdict from torch.utils.data.sampler import RandomSampler import torch.optim as optim from torch.optim import lr_scheduler from sklearn import model_selection from tqdm import tqdm import gc # generate data from csv file class Build_dataset(Dataset): def __init__(self, csv, split, mode, transform=None): self.csv = csv.reset_index(drop=True) self.split = split self.mode = mode self.transform = transform def __len__(self): return self.csv.shape[0] def __getitem__(self, index): row = self.csv.iloc[index] image = cv2.imread(row.filepath) image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) if self.transform is not None: res = self.transform(image=image) image = res['image'].astype(np.float32) else: image = image.astype(np.float32) image = image.transpose(2, 0, 1) data = torch.tensor(image).float() if self.mode == 'test': return data else: return data, torch.tensor(self.csv.iloc[index].target).long() # training epoch def train_epoch(model, loader, optimizer,loss_fn,device, scheduler,n_examples): model = model.train() losses = [] correct_predictions = 0 for inputs, labels in tqdm(loader): inputs = inputs.to(device) labels = labels.to(device) outputs = model(inputs) _, preds = torch.max(outputs, dim=1) loss = loss_fn(outputs, labels) correct_predictions += torch.sum(preds == labels) losses.append(loss.item()) loss.backward() optimizer.step() optimizer.zero_grad() # here you delete inputs and labels and then use gc.collect del inputs, labels gc.collect() return correct_predictions.double() / n_examples, np.mean(losses) # validation epoch def val_epoch(model, loader,loss_fn, device,n_examples): model = model.eval() losses = [] correct_predictions = 0 with torch.no_grad(): for inputs, labels in tqdm(loader): inputs = inputs.to(device) labels = labels.to(device) outputs = model(inputs) _, preds = torch.max(outputs, dim=1) loss = loss_fn(outputs, labels) correct_predictions += torch.sum(preds == labels) losses.append(loss.item()) # here you delete inputs and labels and then use gc.collect del inputs, labels gc.collect() return correct_predictions.double() / n_examples, np.mean(losses) def train(model,device, num_epochs): # generate data dataset_train = Build_dataset(df_train, 'train', 'train', transform=transforms_train) dataset_valid = Build_dataset(df_valid, 'train', 'val', transform=transforms_val) #load data train_loader = DataLoader(dataset_train, batch_size = 16,sampler=RandomSampler(dataset_train), num_workers=4) valid_loader = DataLoader(dataset_valid, batch_size = 16,shuffle = True, num_workers= 4 ) dataset_train_size = len(dataset_train) dataset_valid_size = len(dataset_valid) optimizer = optim.Adam(model.parameters(), lr = 3e-5) model = model.to(device) scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, patience = 3,threshold = 0.001, mode = 'max') loss_fn = nn.CrossEntropyLoss().to(device) history = defaultdict(list) best_accuracy = 0.0 for epoch in range(num_epochs): print(f'Epoch {epoch+1} / {num_epochs}') print ('-'*30) train_acc, train_loss = train_epoch(model, train_loader, optimizer, loss_fn, device, scheduler, dataset_train_size) print(f'Train loss {train_loss} accuracy {train_acc}') valid_acc, valid_loss = val_epoch(model, valid_loader, loss_fn, device,dataset_valid_size) print(f'Val loss {valid_loss} accuracy {valid_acc}') print() history['train_acc'].append(train_acc) history['train_loss'].append(train_loss) history['val_acc'].append(valid_acc) history['val_loss'].append(valid_loss) if valid_acc > best_accuracy: torch.save(model.state_dict(), 'best_model_state.bin') best_accuracy = valid_acc print('Best Accuracy: {best_accuracy}') model.load_state_dict(torch.load('best_model_state.bin')) return model, history if __name__ == '__main__': #competition data -2020 data_dir = ""C:\\Users\\Aniruddh\\Documents\\kaggle\\jpeg_melanoma_2020"" #competition data - 2019 data_dir2 = ""C:\\Users\\Aniruddh\\Documents\\kaggle\\jpeg_melanoma_2019"" # device device = torch.device(""cuda"") # augmenting images image_size = 384 transforms_train = A.Compose([ A.Transpose(p=0.5), A.VerticalFlip(p=0.5), A.HorizontalFlip(p=0.5), A.RandomBrightness(limit=0.2, p=0.75), A.RandomContrast(limit=0.2, p=0.75), A.OneOf([ A.MedianBlur(blur_limit=5), A.GaussianBlur(blur_limit=5), A.GaussNoise(var_limit=(5.0, 30.0)), ], p=0.7), A.OneOf([ A.OpticalDistortion(distort_limit=1.0), A.GridDistortion(num_steps=5, distort_limit=1.), A.ElasticTransform(alpha=3), ], p=0.7), A.CLAHE(clip_limit=4.0, p=0.7), A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10, p=0.5), A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, border_mode=0, p=0.85), A.Resize(image_size, image_size), A.Cutout(max_h_size=int(image_size * 0.375), max_w_size=int(image_size * 0.375), num_holes=1, p=0.7), A.Normalize() ]) transforms_val = A.Compose([ A.Resize(image_size, image_size), A.Normalize() ]) # create data df_train = pd.read_csv(""C:\\Users\\Aniruddh\\Documents\\kaggle\\jpeg_melanoma_2020\\train.csv"") #/kaggle/input/siim-isic-melanoma-classification/train.csv df_train.head() df_train['is_ext'] = 0 df_train['filepath'] = df_train['image_name'].apply(lambda x: os.path.join(data_dir, 'train', f'{x}.jpg')) # dataset from 2020 data df_train['diagnosis'] = df_train['diagnosis'].apply(lambda x: x.replace('seborrheic keratosis', 'BKL')) df_train['diagnosis'] = df_train['diagnosis'].apply(lambda x: x.replace('lichenoid keratosis', 'BKL')) df_train['diagnosis'] = df_train['diagnosis'].apply(lambda x: x.replace('solar lentigo', 'BKL')) df_train['diagnosis'] = df_train['diagnosis'].apply(lambda x: x.replace('lentigo NOS', 'BKL')) df_train['diagnosis'] = df_train['diagnosis'].apply(lambda x: x.replace('cafe-au-lait macule', 'unknown')) df_train['diagnosis'] = df_train['diagnosis'].apply(lambda x: x.replace('atypical melanocytic proliferation', 'unknown')) # dataset from 2019 data df_train2 = pd.read_csv('/content/drive/My Drive/siim_melanoma images/train_2019.csv') df_train2 = df_train2[df_train2['tfrecord'] >= 0].reset_index(drop=True) #df_train2['fold'] = df_train2['tfrecord'] % 5 df_train2['is_ext'] = 1 df_train2['filepath'] = df_train2['image_name'].apply(lambda x: os.path.join(data_dir2, 'train', f'{x}.jpg')) df_train2['diagnosis'] = df_train2['diagnosis'].apply(lambda x: x.replace('NV', 'nevus')) df_train2['diagnosis'] = df_train2['diagnosis'].apply(lambda x: x.replace('MEL', 'melanoma')) #concat both 2019 and 2020 data df_train = pd.concat([df_train, df_train2]).reset_index(drop=True) # shuffle data df = df_train.sample(frac=1).reset_index(drop=True) # creating 8 different target values new_target = {d: idx for idx, d in enumerate(sorted(df.diagnosis.unique()))} df['target'] = df['diagnosis'].map(new_target) mel_idx = new_target['melanoma'] df = df[['filepath','diagnosis', 'target', 'is_ext']] class_names = list(df['diagnosis'].unique()) # splitting train and validation data by 20% df_valid = df[:11471] df_train = df[11472:].reset_index() df_train = df_train.drop(columns = ['index']) # create model def create_model(n_classes): model = models.resnet50(pretrained=True) n_features = model.fc.in_features model.fc = nn.Linear(n_features, n_classes) return model.to(device) # model base_model = create_model(len(class_names)) # train model base_model, history = train(base_model, device, num_epochs = 15) Code Objective The purpose of the project is to classify skin cancer images by creating 8 different target variables from the given datasets (i.e the competition was about classifying benign and malignant images but I used the diagnosis column on the dataset as my target variable as the data was really skewed). The model used is Resnet-50 from torchvision models. These were the data used skin images (this year competition): https://www.kaggle.com/cdeotte/jpeg-melanoma-384x384 skin images (last year competition): https://www.kaggle.com/cdeotte/jpeg-isic2019-384x384 I decided to create a Flask application out of this but, the CUDA memory was always causing a runtime error RuntimeError: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 2.00 GiB total capacity; 1.21 GiB already allocated; 43.55 MiB free; 1.23 GiB reserved in total by PyTorch) These are the details about my Nvidia GPU Sun Sep 13 19:09:34 2020 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 451.67 Driver Version: 451.67 CUDA Version: 11.0 | |-------------------------------+----------------------+----------------------+ | GPU Name TCC/WDDM | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce 940MX WDDM | 00000000:01:00.0 Off | N/A | | N/A 63C P8 N/A / N/A | 37MiB / 2048MiB | 0% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ # more information about my GPU ==============NVSMI LOG============== Timestamp : Sun Sep 13 19:11:22 2020 Driver Version : 451.67 CUDA Version : 11.0 Attached GPUs : 1 GPU 00000000:01:00.0 Product Name : GeForce 940MX Product Brand : GeForce Display Mode : Disabled Display Active : Disabled Persistence Mode : N/A Accounting Mode : Disabled Accounting Mode Buffer Size : 4000 Driver Model Current : WDDM Pending : WDDM Serial Number : N/A GPU UUID : GPU-9a8c69df-26f2-2a98-3712-ea22f6add038 Minor Number : N/A VBIOS Version : 82.08.6D.00.8C MultiGPU Board : No Board ID : 0x100 GPU Part Number : N/A Inforom Version Image Version : N/A OEM Object : N/A ECC Object : N/A Power Management Object : N/A GPU Operation Mode Current : N/A Pending : N/A GPU Virtualization Mode Virtualization Mode : None Host VGPU Mode : N/A IBMNPU Relaxed Ordering Mode : N/A PCI Bus : 0x01 Device : 0x00 Domain : 0x0000 Device Id : 0x134D10DE Bus Id : 00000000:01:00.0 Sub System Id : 0x83F9103C GPU Link Info PCIe Generation Max : 3 Current : 1 Link Width Max : 4x Current : 4x Bridge Chip Type : N/A Firmware : N/A Replays Since Reset : 0 Replay Number Rollovers : 0 Tx Throughput : 0 KB/s Rx Throughput : 0 KB/s Fan Speed : N/A Performance State : P8 Clocks Throttle Reasons Idle : Not Active Applications Clocks Setting : Not Active SW Power Cap : Not Active HW Slowdown : Not Active HW Thermal Slowdown : N/A HW Power Brake Slowdown : N/A Sync Boost : Not Active SW Thermal Slowdown : Not Active Display Clock Setting : Not Active FB Memory Usage Total : 2048 MiB Used : 37 MiB Free : 2011 MiB BAR1 Memory Usage Total : 256 MiB Used : 225 MiB Free : 31 MiB Compute Mode : Default Utilization Gpu : 0 % Memory : 0 % Encoder : N/A Decoder : N/A Encoder Stats Active Sessions : 0 Average FPS : 0 Average Latency : 0 FBC Stats Active Sessions : 0 Average FPS : 0 Average Latency : 0 Ecc Mode Current : N/A Pending : N/A ECC Errors Volatile Single Bit Device Memory : N/A Register File : N/A L1 Cache : N/A L2 Cache : N/A Texture Memory : N/A Texture Shared : N/A CBU : N/A Total : N/A Double Bit Device Memory : N/A Register File : N/A L1 Cache : N/A L2 Cache : N/A Texture Memory : N/A Texture Shared : N/A CBU : N/A Total : N/A Aggregate Single Bit Device Memory : N/A Register File : N/A L1 Cache : N/A L2 Cache : N/A Texture Memory : N/A Texture Shared : N/A CBU : N/A Total : N/A Double Bit Device Memory : N/A Register File : N/A L1 Cache : N/A L2 Cache : N/A Texture Memory : N/A Texture Shared : N/A CBU : N/A Total : N/A Retired Pages Single Bit ECC : N/A Double Bit ECC : N/A Pending Page Blacklist : N/A Remapped Rows : N/A Temperature GPU Current Temp : 60 C GPU Shutdown Temp : 99 C GPU Slowdown Temp : 94 C GPU Max Operating Temp : 90 C Memory Current Temp : N/A Memory Max Operating Temp : N/A Power Readings Power Management : N/A Power Draw : N/A Power Limit : N/A Default Power Limit : N/A Enforced Power Limit : N/A Min Power Limit : N/A Max Power Limit : N/A Clocks Graphics : 405 MHz SM : 405 MHz Memory : 405 MHz Video : 396 MHz Applications Clocks Graphics : 1006 MHz Memory : 1001 MHz Default Applications Clocks Graphics : 1004 MHz Memory : 1001 MHz Max Clocks Graphics : 1241 MHz SM : 1241 MHz Memory : 1001 MHz Video : 1216 MHz Max Customer Boost Clocks Graphics : N/A Clock Policy Auto Boost : N/A Auto Boost Default : N/A Processes : None if I try running this on the CPU, the whole system freezes to the point where I have to manually restart the computer. Also if I try running the code with lower image resolution, lower batch sizes etc, each epoch takes around 12 hours to complete on a CPU which is definitely impractical.",|deep-learning|pytorch|cuda|gpu|,GPU Usage,3
64193633,"Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory;. When I try to run a python script , which uses tensorflow, it shows following error ... 2020-10-04 16:01:44.994797: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1 2020-10-04 16:01:46.780656: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1 2020-10-04 16:01:46.795642: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: pciBusID: 0000:03:00.0 name: TITAN X (Pascal) computeCapability: 6.1 coreClock: 1.531GHz coreCount: 28 deviceMemorySize: 11.91GiB deviceMemoryBandwidth: 447.48GiB/s 2020-10-04 16:01:46.795699: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1 2020-10-04 16:01:46.795808: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64/:/usr/local/cuda-10.0/lib64 2020-10-04 16:01:46.797391: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10 2020-10-04 16:01:46.797707: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10 2020-10-04 16:01:46.799529: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10 2020-10-04 16:01:46.800524: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10 2020-10-04 16:01:46.804150: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7 2020-10-04 16:01:46.804169: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform. Skipping registering GPU devices... Output of nvidia-smi +-----------------------------------------------------------------------------+ | NVIDIA-SMI 455.23.05 Driver Version: 455.23.05 CUDA Version: 11.1 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 TITAN X (Pascal) On | 00000000:03:00.0 Off | N/A | | 23% 28C P8 9W / 250W | 18MiB / 12194MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | 0 N/A N/A 1825 G /usr/lib/xorg/Xorg 9MiB | | 0 N/A N/A 1957 G /usr/bin/gnome-shell 6MiB | +-----------------------------------------------------------------------------+ Tensorflow version 2.3.1, Ubuntu - 18.04 I tried to completely remove cuda toolkit and install from scratch but the error remains. Anybody could help me to identify the source of problem??",|tensorflow|gpu|ubuntu-18.04|nvidia|,GPU Usage,3
64237179,"Why is my accuracy in a keras 3d CNN always 0?. I'm fairly new to keras, cnn and deep learning in general, so i'm super sorry, if the solution is quite simple. I'm really at a dead end here. So - the thing is, I have a 3D CNN with cubes of 50x50x50 as input data. I want to classify them into either 0 or 1, so it is a binary classification problem. Before fitting it into the NN, I of course prepared the data, refactored, resized and normalized it. So the images are comparable (1 voxel is 2 mm), normalized to a 0 to 1 range and all the same size. So, when i tried to fit the data into my model, the results are not that encouraging. The accuracy is always displayed as 0, with the highest accuracy i've had in a single epoch is accuracy: 0.0159, the loss always in between 3.2 and 3.5 I variated the number of epochs as well, but it does not matter if i have 5 or 50. the result is always the same. This is the code of my CNN-architecture model = Sequential() model.add(Conv3D(64, kernel_size=(5, 5, 5), activation='linear', kernel_initializer='glorot_uniform', input_shape=shape)) model.add(BatchNormalization(center=True, scale=True)) model.add(LeakyReLU(.1)) model.add(Dropout(.25)) model.add(Conv3D(128, kernel_size=(3, 3, 3), activation='linear', kernel_initializer='glorot_uniform')) model.add(BatchNormalization(center=True, scale=True)) model.add(LeakyReLU(.1)) model.add(MaxPooling3D(pool_size=(3, 3, 3))) model.add(Dropout(.25)) model.add(Conv3D(256, kernel_size=(3, 3, 3), activation='linear', kernel_initializer='glorot_uniform')) model.add(BatchNormalization(center=True, scale=True)) model.add(LeakyReLU(.1)) model.add(Dropout(.25)) model.add(Conv3D(512, kernel_size=(3, 3, 3), activation='linear', kernel_initializer='glorot_uniform')) model.add(BatchNormalization(center=True, scale=True)) model.add(LeakyReLU(.1)) model.add(MaxPooling3D(pool_size=(3, 3, 3))) model.add(Dropout(.25)) model.add(Flatten()) model.add(Dense(256)) model.add(BatchNormalization(center=True, scale=True)) model.add(LeakyReLU(.1)) model.add(Dropout(.5)) model.add(Dense(512)) model.add(BatchNormalization(center=True, scale=True)) model.add(LeakyReLU(.1)) model.add(Dropout(.5)) model.add(Dense(256)) model.add(BatchNormalization(center=True, scale=True)) model.add(Activation('softmax')) model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) print(training_patients[0]) model.summary() model.fit(training_patients, training_labels, epochs=50, batch_size=50) So I wanted to ask, if there is anything wrong with my architecture? Or do you think the problem is more on the data-side? I have only 420 images; 3/4 of them I use for training, 1/4 is for testing. Could this be the problem? I wanted to augment the images later on, when the base model is running stable. Or do I have to do this first? Thank you very much in advance!",|python|tensorflow|keras|deep-learning|conv-neural-network|,Model,0
64395424,"Basic Tensorflow Model shows random outcomes. I am working on building some new models, and wanted to get back to some basics. So I decided to write a classifier that classifies [1, 1] as a 1 and all other combos as a 0. I have written several different variations on this and keep getting mixed results. from tensorflow.keras import layers, models from tensorflow import keras data = [[1., 1.], [1., 0.], [0., 1.], [0., 0.]] results = [[1.], [0.], [0.], [0.]] def build_model(): model = models.Sequential() model.add(layers.Dense(len(data[0]), activation='relu')) model.add(layers.Dense(128, activation='relu')) model.add(layers.Dense(1)) model.compile(loss=keras.losses.BinaryCrossentropy(), metrics=[keras.metrics.Accuracy()], optimizer='adam') return model model = build_model() model.fit(data, results, epochs=1000) model.summary() print(model.predict([data[0]])) print(model.predict([data[1]])) print(model.predict([data[2]])) print(model.predict([data[3]])) Sometimes the output is completely wrong: [[0.]] [[0.]] [[0.]] [[0.]] and the model never gets accurate. Epoch 1000/1000 1/1 [==============================] - 0s 910us/step - loss: 3.8562 - accuracy: 0.7500 Sometimes it shows lower accuracy and produces bad results: Epoch 1000/1000 1/1 [==============================] - 0s 918us/step - loss: 3.8562 - accuracy: 0.2500 [[-0.1101699]] [[-0.13835455]] [[-0.03829439]] [[0.]] Other times it ""kinda"" works: Epoch 1000/1000 1/1 [==============================] - 0s 898us/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 (although I would expect accuracy to be 1) [[1.1292353]] [[-0.167045]] [[-0.03134967]] [[-0.3522459]] Could someone help me understand the variance in my outcomes. I have created several version of this model with more layers, less layers, various sized Dense() layers. I have tried several loss and metrics, however, at this point I am just randomly doing things.",|python|tensorflow|machine-learning|keras|tensorflow2.0|,Model,0
64387251,"Train accuracy decreases with train loss. I wrote this very simple code model = keras.models.Sequential() model.add(layers.Dense(13000, input_dim=X_train.shape[1], activation='relu', trainable=False)) model.add(layers.Dense(1, input_dim=13000, activation='linear')) model.compile(loss=""binary_crossentropy"", optimizer='adam', metrics=[""accuracy""]) model.fit(X_train, y_train, batch_size=X_train.shape[0], epochs=1000000, verbose=1) The data is MNIST but only for digits '0' and '1'. I have a very strange issue, where the loss is monotonically decreasing to zero, as expected, yet the accuracy instead of increasing, is also decreasing. Here is a sample output 12665/12665 [==============================] - 0s 11us/step - loss: 0.0107 - accuracy: 0.2355 Epoch 181/1000000 12665/12665 [==============================] - 0s 11us/step - loss: 0.0114 - accuracy: 0.2568 Epoch 182/1000000 12665/12665 [==============================] - 0s 11us/step - loss: 0.0128 - accuracy: 0.2726 Epoch 183/1000000 12665/12665 [==============================] - 0s 11us/step - loss: 0.0133 - accuracy: 0.2839 Epoch 184/1000000 12665/12665 [==============================] - 0s 11us/step - loss: 0.0134 - accuracy: 0.2887 Epoch 185/1000000 12665/12665 [==============================] - 0s 11us/step - loss: 0.0110 - accuracy: 0.2842 Epoch 186/1000000 12665/12665 [==============================] - 0s 11us/step - loss: 0.0101 - accuracy: 0.2722 Epoch 187/1000000 12665/12665 [==============================] - 0s 11us/step - loss: 0.0094 - accuracy: 0.2583 Since we only have two classes, the benchmark for lowest possible accuracy should be 0.5, and furthermore we are monitoring accuracy on the training set, so it should very going up to 100%, I expect overfitting and I am overfitting according to the loss function. At the final epoch, this is the situation 12665/12665 [==============================] - 0s 11us/step - loss: 9.9710e-06 - accuracy: 0.0758 a 7% accuracy when the worst theoretical possibility if you guess randomly is 50%. This is no accident. Something is going on here. Can anyone see the problem? Entire code from tensorflow import keras import numpy as np from matplotlib import pyplot as plt import keras from keras.callbacks import Callback from keras import layers import warnings class EarlyStoppingByLossVal(Callback): def __init__(self, monitor='val_loss', value=0.00001, verbose=0): super(Callback, self).__init__() self.monitor = monitor self.value = value self.verbose = verbose def on_epoch_end(self, epoch, logs={}): current = logs.get(self.monitor) if current is None: warnings.warn(""Early stopping requires %s available!"" % self.monitor, RuntimeWarning) if current < self.value: if self.verbose > 0: print(""Epoch %05d: early stopping THR"" % epoch) self.model.stop_training = True def load_mnist(): mnist = keras.datasets.mnist (train_images, train_labels), (test_images, test_labels) = mnist.load_data() train_images = np.reshape(train_images, (train_images.shape[0], train_images.shape[1] * train_images.shape[2])) test_images = np.reshape(test_images, (test_images.shape[0], test_images.shape[1] * test_images.shape[2])) train_labels = np.reshape(train_labels, (train_labels.shape[0],)) test_labels = np.reshape(test_labels, (test_labels.shape[0],)) train_images = train_images[(train_labels == 0) | (train_labels == 1)] test_images = test_images[(test_labels == 0) | (test_labels == 1)] train_labels = train_labels[(train_labels == 0) | (train_labels == 1)] test_labels = test_labels[(test_labels == 0) | (test_labels == 1)] train_images, test_images = train_images / 255, test_images / 255 return train_images, train_labels, test_images, test_labels X_train, y_train, X_test, y_test = load_mnist() train_acc = [] train_errors = [] test_acc = [] test_errors = [] width_list = [13000] for width in width_list: print(width) model = keras.models.Sequential() model.add(layers.Dense(width, input_dim=X_train.shape[1], activation='relu', trainable=False)) model.add(layers.Dense(1, input_dim=width, activation='linear')) model.compile(loss=""binary_crossentropy"", optimizer='adam', metrics=[""accuracy""]) callbacks = [EarlyStoppingByLossVal(monitor='loss', value=0.00001, verbose=1)] model.fit(X_train, y_train, batch_size=X_train.shape[0], epochs=1000000, verbose=1, callbacks=callbacks) train_errors.append(model.evaluate(X_train, y_train)[0]) test_errors.append(model.evaluate(X_test, y_test)[0]) train_acc.append(model.evaluate(X_train, y_train)[1]) test_acc.append(model.evaluate(X_test, y_test)[1]) plt.plot(width_list, train_errors, marker='D') plt.xlabel(""width"") plt.ylabel(""train loss"") plt.show() plt.plot(width_list, test_errors, marker='D') plt.xlabel(""width"") plt.ylabel(""test loss"") plt.show() plt.plot(width_list, train_acc, marker='D') plt.xlabel(""width"") plt.ylabel(""train acc"") plt.show() plt.plot(width_list, test_acc, marker='D') plt.xlabel(""width"") plt.ylabel(""test acc"") plt.show()",|python|tensorflow|machine-learning|keras|neural-network|,Model,0
64413907,"Why masking input produces the same loss as unmasked input on Keras?. I am experimenting with LSTM using variable-length input due to this reason. I wanted to be sure that loss is calculated correctly under masking. So, I trained the below model that uses Masking layer on padded sequences. from tensorflow.keras.layers import LSTM, Masking, Dense from tensorflow.keras.utils import to_categorical from tensorflow.keras import models, losses import tensorflow as tf import numpy as np import os """""" For generating reproducible results, set seed. """""" def set_seed(seed): os.environ['PYTHONHASHSEED'] = str(seed) np.random.seed(seed) tf.random.set_seed(seed) """""" Set some right most indices to mask value like padding """""" def create_padded_seq(num_samples, timesteps, num_feats, mask_value): feats = np.random.random([num_samples, timesteps, num_feats]).astype(np.float32) # Generate samples for i in range(0, num_samples): rand_index = np.random.randint(low=2, high=timesteps, size=1)[0] # Apply padding feats[i, rand_index:, 0] = mask_value return feats set_seed(42) num_samples = 100 timesteps = 6 num_feats = 1 num_classes = 3 num_lstm_cells = 1 mask_value = -100 num_epochs = 5 X_train = create_padded_seq(num_samples, timesteps, num_feats, mask_value) y_train = np.random.randint(num_classes, size=num_samples) cat_y_train = to_categorical(y_train, num_classes) masked_model = models.Sequential(name='masked') masked_model.add(Masking(mask_value=mask_value, input_shape=(timesteps, num_feats))) masked_model.add(LSTM(num_lstm_cells, return_sequences=False)) masked_model.add(Dense(num_classes, activation='relu')) masked_model.compile(loss=losses.categorical_crossentropy, optimizer='adam', metrics=[""accuracy""]) print(masked_model.summary()) masked_model.fit(X_train, cat_y_train, batch_size=1, epochs=5, verbose=True) This is the verbose output, Model: ""masked"" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= masking (Masking) (None, 6, 1) 0 _________________________________________________________________ lstm (LSTM) (None, 1) 12 _________________________________________________________________ dense (Dense) (None, 3) 6 ================================================================= Total params: 18 Trainable params: 18 Non-trainable params: 0 _________________________________________________________________ None Epoch 1/5 100/100 [==============================] - 0s 2ms/step - loss: 10.6379 - accuracy: 0.3400 Epoch 2/5 100/100 [==============================] - 0s 2ms/step - loss: 10.6379 - accuracy: 0.3400 Epoch 3/5 100/100 [==============================] - 0s 2ms/step - loss: 10.6379 - accuracy: 0.3400 Epoch 4/5 100/100 [==============================] - 0s 2ms/step - loss: 10.6379 - accuracy: 0.3400 Epoch 5/5 100/100 [==============================] - 0s 2ms/step - loss: 10.6379 - accuracy: 0.3400 I also removed Masking layer and trained another model on the same data to see the effect of masking, this is the model, unmasked_model = models.Sequential(name='unmasked') unmasked_model.add(LSTM(num_lstm_cells, return_sequences=False, input_shape=(timesteps, num_feats))) unmasked_model.add(Dense(num_classes, activation='relu')) unmasked_model.compile(loss=losses.categorical_crossentropy, optimizer='adam', metrics=[""accuracy""]) print(unmasked_model.summary()) unmasked_model.fit(X_train, cat_y_train, batch_size=1, epochs=5, verbose=True) And this is the verbose output, Model: ""unmasked"" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= lstm (LSTM) (None, 1) 12 _________________________________________________________________ dense (Dense) (None, 3) 6 ================================================================= Total params: 18 Trainable params: 18 Non-trainable params: 0 _________________________________________________________________ None Epoch 1/5 100/100 [==============================] - 0s 1ms/step - loss: 10.6379 - accuracy: 0.3400 Epoch 2/5 100/100 [==============================] - 0s 2ms/step - loss: 10.6379 - accuracy: 0.3400 Epoch 3/5 100/100 [==============================] - 0s 1ms/step - loss: 10.6379 - accuracy: 0.3400 Epoch 4/5 100/100 [==============================] - 0s 1ms/step - loss: 10.6379 - accuracy: 0.3400 Epoch 5/5 100/100 [==============================] - 0s 1ms/step - loss: 10.6379 - accuracy: 0.3400 Losses are the same in both outputs, what is the reason for that ? It seems like Masking layer has no effect on loss, is that correct ? If not, then how can I observe the effect of Masking layer ?",|tensorflow|keras|deep-learning|lstm|,Model,0
64576751,"Neural Network TypeError: unsupported operand type(s) for +=: 'Dense' and 'str'. I am trying to use a neural network to predict the price of houses. Here is what the top of the dataset looks like: Price Beds SqFt Built Garage FullBaths HalfBaths LotSqFt 485000 3 2336 2004 2 2.0 1.0 2178.0 430000 4 2106 2005 2 2.0 1.0 2178.0 445000 3 1410 1999 1 2.0 0.0 3049.0 ... I am using the ReLU activation function. When I try to evaluate my model on my test data, I get this TypeError: unsupported operand type(s) for +=: 'Dense' and 'str'. I looked at the types of the columns from my original dataframe, and everything looks fine. print(df.dtypes) ## Output #Price int64 #Beds int64 #SqFt int64 #Built int64 #Garage int64 #FullBaths float64 #HalfBaths float64 #LotSqFt float64 #dtype: object I'm not sure if I am messing something up in my neural network to cause this error. Any help is appreciated! Here is my code for reference. Prepare Data for Network dataset = df.values X = dataset[:, 1:8] Y = dataset[:,0] ## Normalize X-Values from sklearn import preprocessing min_max_scaler = preprocessing.MinMaxScaler() X_scale = min_max_scaler.fit_transform(X) X_scale ##Partition Data from sklearn.model_selection import train_test_split X_train, X_val_and_test, Y_train, Y_val_and_test = train_test_split(X_scale, Y, test_size=0.3) X_val, X_test, Y_val, Y_test = train_test_split(X_val_and_test, Y_val_and_test, test_size=0.5) print(X_train.shape, X_val.shape, X_test.shape, Y_train.shape, Y_val.shape, Y_test.shape) Begin Model Building from keras.models import Sequential from keras.layers import Dense model = Sequential( Dense(32, activation='relu', input_shape=(7,)), Dense(1, activation='linear')) model.compile(optimizer='sgd', loss='mse', metrics=['mean_squared_error']) model.evaluate(X_test, Y_test)[1] ##Type Error is here!",|python|keras|neural-network|typeerror|relu|,API,4
64601301,"Pytorch input tensor size with wrong dimension Conv1D. def train(epoch): model.train() train_loss = 0 for batch_idx, (data, _) in enumerate(train_loader): data = data[None, :, :] print(data.size()) # something seems to change between here data = data.to(device) optimizer.zero_grad() recon_batch, mu, logvar = model(data) # and here??? loss = loss_function(recon_batch, data, mu, logvar) loss.backward() train_loss += loss.item() optimizer.step() if batch_idx % 1000 == 0: print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format( epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item() / len(data))) print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset))) for epoch in range(1, 4): train(epoch) This is very strange looking at the training loop it does recognize that the size is [1,1,1998] but then something changes after it is sent to the device? torch.Size([1, 1, 1998]) --------------------------------------------------------------------------- RuntimeError Traceback (most recent call last) <ipython-input-138-70cca679f91a> in <module>() 27 28 for epoch in range(1, 4): ---> 29 train(epoch) 5 frames /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py in forward(self, input) 255 _single(0), self.dilation, self.groups) 256 return F.conv1d(input, self.weight, self.bias, self.stride, --> 257 self.padding, self.dilation, self.groups) 258 259 RuntimeError: Expected 3-dimensional input for 3-dimensional weight [12, 1, 1], but got 2-dimensional input of size [1, 1998] instead Also here is my model (I recognize there is likely a couple of other issues here but I am asking about the tensor size not registering) class VAE(nn.Module): def __init__(self): super(VAE, self).__init__() self.conv1 = nn.Conv1d( 1,12, kernel_size=1,stride=5,padding=0) self.conv1_drop = nn.Dropout2d() self.pool1 = nn.MaxPool1d(kernel_size=3, stride=2) self.fc21 = nn.Linear(198, 1) self.fc22 = nn.Linear(198, 1) self.fc3 = nn.Linear(1, 198) self.fc4 = nn.Linear(198, 1998) def encode(self, x): h1 = self.conv1(x) h1 = self.conv1_drop(h1) h1 = self.pool1(h1) h1 = F.relu(h1) h1 = h1.view(1, -1) # 1 is the batch size return self.fc21(h1), self.fc22(h1) def reparameterize(self, mu, logvar): std = torch.exp(0.5*logvar) eps = torch.rand_like(std) return mu + eps*std def decode(self, z): h3 = F.relu(self.fc3(z)) return torch.sigmoid(self.fc4(h3)) def forward(self, x): mu, logvar = self.encode(x.view(-1, 1998)) z = self.reparameterize(mu, logvar) return self.decode(z), mu, logvar So why doesn't Pytorch keep the dimensions after reshaping and would that be the correct tensor size if it did?",|python|pytorch|tensor|,Tensors&Inputs,1
64777195,"ValueError: Expected tensor to be a tensor image of size (C, H, W). Got tensor.size() = torch.Size([1800, 800]). Here's my evaluation cell: start_time = time.time() with torch.no_grad(): best_network = Network() best_network.cuda() best_network.load_state_dict(torch.load('../moth_landmarks.pth')) best_network.eval() batch = next(iter(train_loader)) images, landmarks = batch['image'], batch['landmarks'] #images = images.unsqueeze_(1) images = torch.cat((images,images,images),1) images = images.cuda() norm_image = transforms.Normalize(0.3812, 0.1123) for image in images: image = image.float() ##image = to_tensor(image) #TypeError: pic should be PIL Image or ndarray. Got <class 'torch.Tensor'> image = norm_image(image) landmarks = (landmarks + 0.5) * 224 ##[8, 600, 800] --> [8,3,600,800] images = images.unsqueeze(1) images = torch.cat((images, images, images), 1) predictions = (best_network(images).cpu() + 0.5) * 224 predictions = predictions.view(-1,4,2) plt.figure(figsize=(10,40)) for img_num in range(8): plt.subplot(8,1,img_num+1) plt.imshow(images[img_num].cpu().numpy().transpose(1,2,0).squeeze(), cmap='gray') plt.scatter(predictions[img_num,:,0], predictions[img_num,:,1], c = 'r') plt.scatter(landmarks[img_num,:,0], landmarks[img_num,:,1], c = 'g') print('Total number of test images: {}'.format(len(test_dataset))) end_time = time.time() print(""Elapsed Time : {}"".format(end_time - start_time)) How should I fix the following error? --------------------------------------------------------------------------- ValueError Traceback (most recent call last) <ipython-input-59-e4aa0ace8c75> in <module> 19 image = image.float() 20 ##image = to_tensor(image) #TypeError: pic should be PIL Image or ndarray. Got <class 'torch.Tensor'> ---> 21 image = norm_image(image) 22 landmarks = (landmarks + 0.5) * 224 23 ~/anaconda3/lib/python3.7/site-packages/torchvision/transforms/transforms.py in __call__(self, tensor) 210 Tensor: Normalized Tensor image. 211 """""" --> 212 return F.normalize(tensor, self.mean, self.std, self.inplace) 213 214 def __repr__(self): ~/anaconda3/lib/python3.7/site-packages/torchvision/transforms/functional.py in normalize(tensor, mean, std, inplace) 282 if tensor.ndimension() != 3: 283 raise ValueError('Expected tensor to be a tensor image of size (C, H, W). Got tensor.size() = ' --> 284 '{}.'.format(tensor.size())) 285 286 if not inplace: ValueError: Expected tensor to be a tensor image of size (C, H, W). Got tensor.size() = torch.Size([1800, 800]). If I remove the following line: images = torch.cat((images,images,images),1) I will get this new error: --------------------------------------------------------------------------- ValueError Traceback (most recent call last) <ipython-input-61-3e892b69015c> in <module> 19 image = image.float() 20 ##image = to_tensor(image) #TypeError: pic should be PIL Image or ndarray. Got <class 'torch.Tensor'> ---> 21 image = norm_image(image) 22 landmarks = (landmarks + 0.5) * 224 23 ~/anaconda3/lib/python3.7/site-packages/torchvision/transforms/transforms.py in __call__(self, tensor) 210 Tensor: Normalized Tensor image. 211 """""" --> 212 return F.normalize(tensor, self.mean, self.std, self.inplace) 213 214 def __repr__(self): ~/anaconda3/lib/python3.7/site-packages/torchvision/transforms/functional.py in normalize(tensor, mean, std, inplace) 282 if tensor.ndimension() != 3: 283 raise ValueError('Expected tensor to be a tensor image of size (C, H, W). Got tensor.size() = ' --> 284 '{}.'.format(tensor.size())) 285 286 if not inplace: ValueError: Expected tensor to be a tensor image of size (C, H, W). Got tensor.size() = torch.Size([600, 800]).",|python|deep-learning|computer-vision|pytorch|tensor|,Tensors&Inputs,1
64809589,"Running out of GPU memory with PyTorch. I am running my own custom deep belief network code using PyTorch and using the LBFGS optimizer. After optimization starts, my GPU starts to run out of memory, fully running out after a couple of batches, but I'm not sure why. Should I be purging memory after each batch is run through the optimizer? My code is as follows (with the portion of code that causes the problem marked): def fine_tuning(self, data, labels, num_epochs=10, max_iter=3): ''' Parameters ---------- data : TYPE torch.Tensor N x D tensor with N = num samples, D = num dimensions labels : TYPE torch.Tensor N x 1 vector of labels for each sample num_epochs : TYPE, optional DESCRIPTION. The default is 10. max_iter : TYPE, optional DESCRIPTION. The default is 3. Returns ------- None. ''' N = data.shape[0] #need to unroll the weights into a typical autoencoder structure #encode - code - decode for ii in range(len(self.rbm_layers)-1, -1, -1): self.rbm_layers.append(self.rbm_layers[ii]) L = len(self.rbm_layers) optimizer = torch.optim.LBFGS(params=list(itertools.chain(*[list(self.rbm_layers[ii].parameters()) for ii in range(L)] )), max_iter=max_iter, line_search_fn='strong_wolfe') dataset = torch.utils.data.TensorDataset(data, labels) dataloader = torch.utils.data.DataLoader(dataset, batch_size=self.batch_size*10, shuffle=True) #fine tune weights for num_epochs for epoch in range(1,num_epochs+1): with torch.no_grad(): #get squared error before optimization v = self.pass_through_full(data) err = (1/N) * torch.sum(torch.pow(data-v.to(""cpu""), 2)) print(""\nBefore epoch {}, train squared error: {:.4f}\n"".format(epoch, err)) #*******THIS IS THE PROBLEM SECTION*******# for ii,(batch,_) in tqdm(enumerate(dataloader), ascii=True, desc=""DBN fine-tuning"", file=sys.stdout): print(""Fine-tuning epoch {}, batch {}"".format(epoch, ii)) with torch.no_grad(): batch = batch.view(len(batch) , self.rbm_layers[0].visible_units) if self.use_gpu: #are we using a GPU? batch = batch.to(self.device) #if so, send batch to GPU B = batch.shape[0] def closure(): optimizer.zero_grad() output = self.pass_through_full(batch) loss = nn.BCELoss(reduction='sum')(output, batch)/B print(""Batch {}, loss: {}\r"".format(ii, loss)) loss.backward() return loss optimizer.step(closure) The error I get is: DBN fine-tuning: 0it [00:00, ?it/s]Fine-tuning epoch 1, batch 0 Batch 0, loss: 4021.35400390625 Batch 0, loss: 4017.994873046875 DBN fine-tuning: 0it [00:00, ?it/s] Traceback (most recent call last): File ""<stdin>"", line 1, in <module> File ""/home/deep_autoencoder/deep_autoencoder.py"", line 260, in fine_tuning optimizer.step(closure) File ""/home/anaconda3/envs/torch_env/lib/python3.8/site-packages/torch/autograd /grad_mode.py"", line 15, in decorate_context return func(*args, **kwargs) File ""/home/anaconda3/envs/torch_env/lib/python3.8/site-packages/torch/optim/lb fgs.py"", line 425, in step loss, flat_grad, t, ls_func_evals = _strong_wolfe( File ""/home/anaconda3/envs/torch_env/lib/python3.8/site-packages/torch/optim/lb fgs.py"", line 96, in _strong_wolfe g_prev = g_new.clone(memory_format=torch.contiguous_format) RuntimeError: CUDA out of memory. Tried to allocate 1.57 GiB (GPU 0; 24.00 GiB total capac ity; 13.24 GiB already allocated; 1.41 GiB free; 20.07 GiB reserved in total by PyTorch) This also racks up memory if I use CPU, so I'm not sure what the solution is here...",|python|pytorch|gpu|,GPU Usage,3
64958934,"error when trying to import tensorflow GPU. here's the code that i use to check if tf.gpu is working or not import tensorflow as tf if tf.test.gpu_device_name(): print('Default GPU Device:{}'.format(tf.test.gpu_device_name())) else: print(""Please install GPU version of TF"") and here's the error Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors for some common reasons and solutions. Include the entire stack trace above this error message when asking for help. 2020-11-22 21:53:40.971514: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found 2020-11-22 21:53:40.971756: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.",|tensorflow|gpu|,GPU Usage,3
65204482,"Training loss decrease but accuracy is always 0?. I try to train a model, input is (3000,1) vector that is consist of negative numbers mostly, inormalize input. Output is binary image which is represented as vector (2500,1). My model is like this: model = Sequential() model.add(Dense(3000, input_shape=(x_train.shape[1:]), activation='linear')) model.add(Dense(2500, activation='relu')) model.add(Dense(2500, activation='relu')) model.add(Dense(2500, activation='relu')) model.add(Dense(2500, activation='relu')) model.add(Dense(y_train.shape[1], activation='sigmoid')) model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy']) Result is like this: Epoch 1/300 1/1 - 0s - loss: 0.6999 - accuracy: 0.0000e+00 - val_loss: 0.6930 - val_accuracy: 0.0000e+00 Epoch 2/300 1/1 - 0s - loss: 0.6843 - accuracy: 0.0000e+00 - val_loss: 0.6911 - val_accuracy: 0.0000e+00 Epoch 3/300 1/1 - 0s - loss: 0.6700 - accuracy: 0.0000e+00 - val_loss: 0.6944 - val_accuracy: 0.0000e+00 Epoch 4/300 1/1 - 0s - loss: 0.6515 - accuracy: 0.0000e+00 - val_loss: 0.7081 - val_accuracy: 0.0000e+00 Epoch 5/300 1/1 - 0s - loss: 0.6314 - accuracy: 0.0000e+00 - val_loss: 0.7349 - val_accuracy: 0.0000e+00 Epoch 6/300 1/1 - 0s - loss: 0.6147 - accuracy: 0.0000e+00 - val_loss: 0.7568 - val_accuracy: 0.0000e+00 Epoch 7/300 1/1 - 0s - loss: 0.6006 - accuracy: 0.0000e+00 - val_loss: 0.7615 - val_accuracy: 0.0000e+00 Epoch 8/300 1/1 - 0s - loss: 0.5865 - accuracy: 0.0000e+00 - val_loss: 0.7560 - val_accuracy: 0.0000e+00 Epoch 9/300 1/1 - 0s - loss: 0.5738 - accuracy: 0.0000e+00 - val_loss: 0.7515 - val_accuracy: 0.0000e+00 Epoch 10/300 1/1 - 0s - loss: 0.5637 - accuracy: 0.0000e+00 - val_loss: 0.7533 - val_accuracy: 0.0000e+00 Epoch 11/300 1/1 - 0s - loss: 0.5555 - accuracy: 0.0000e+00 - val_loss: 0.7629 - val_accuracy: 0.0000e+00 Epoch 12/300 1/1 - 0s - loss: 0.5490 - accuracy: 0.0000e+00 - val_loss: 0.7766 - val_accuracy: 0.0000e+00 Epoch 13/300 1/1 - 0s - loss: 0.5441 - accuracy: 0.0000e+00 - val_loss: 0.7877 - val_accuracy: 0.0000e+00 Epoch 14/300 1/1 - 0s - loss: 0.5402 - accuracy: 0.0000e+00 - val_loss: 0.7937 - val_accuracy: 0.0000e+00 Epoch 15/300 1/1 - 0s - loss: 0.5370 - accuracy: 0.0000e+00 - val_loss: 0.7966 - val_accuracy: 0.0000e+00 Epoch 16/300 1/1 - 0s - loss: 0.5346 - accuracy: 0.0000e+00 - val_loss: 0.8001 - val_accuracy: 0.0000e+00 Epoch 17/300 1/1 - 0s - loss: 0.5329 - accuracy: 0.0000e+00 - val_loss: 0.8065 - val_accuracy: 0.0000e+00 Epoch 18/300 1/1 - 0s - loss: 0.5315 - accuracy: 0.0000e+00 - val_loss: 0.8152 - val_accuracy: 0.0000e+00 Epoch 19/300 1/1 - 0s - loss: 0.5305 - accuracy: 0.0000e+00 - val_loss: 0.8253 - val_accuracy: 0.0000e+00 Epoch 20/300 1/1 - 0s - loss: 0.5294 - accuracy: 0.0000e+00 - val_loss: 0.8337 - val_accuracy: 0.0000e+00 Epoch 21/300 1/1 - 0s - loss: 0.5283 - accuracy: 0.0000e+00 - val_loss: 0.8408 - val_accuracy: 0.0000e+00 Epoch 22/300 1/1 - 0s - loss: 0.5271 - accuracy: 0.0000e+00 - val_loss: 0.8476 - val_accuracy: 0.0000e+00 Epoch 23/300 1/1 - 0s - loss: 0.5259 - accuracy: 0.0000e+00 - val_loss: 0.8550 - val_accuracy: 0.0000e+00 Epoch 24/300 1/1 - 0s - loss: 0.5247 - accuracy: 0.0000e+00 - val_loss: 0.8625 - val_accuracy: 0.0000e+00 Epoch 25/300 1/1 - 0s - loss: 0.5235 - accuracy: 0.0000e+00 - val_loss: 0.8705 - val_accuracy: 0.0000e+00 Epoch 26/300 1/1 - 0s - loss: 0.5223 - accuracy: 0.0000e+00 - val_loss: 0.8794 - val_accuracy: 0.0000e+00 Epoch 27/300 1/1 - 0s - loss: 0.5211 - accuracy: 0.0000e+00 - val_loss: 0.8872 - val_accuracy: 0.0000e+00 Epoch 28/300 1/1 - 0s - loss: 0.5200 - accuracy: 0.0000e+00 - val_loss: 0.8940 - val_accuracy: 0.0000e+00 Epoch 29/300 1/1 - 0s - loss: 0.5188 - accuracy: 0.0000e+00 - val_loss: 0.8982 - val_accuracy: 0.0000e+00 Accuracy and validation did not increase. Validation loss started to increase after some point. Even when i try this network really small dataset(17 daatset), it does not converge smoothly. Then i try decision tree regressor, score of the decision tree was negative number. I check the dataset, but i could not find something wrong. what could be wrong, can you please help me?",|python|keras|training-data|,Training,2
65228352,"Matrix inverse approximation with keras dense model. I am training a neural network to calculate the inverse of a 3x3 matrix. I am using a Keras dense model with 1 layer and 9 neurons. The activation function on the first layer is 'relu' and linear on the output layer. I am using 10000 matrices of determinant 1. The results I am getting are not very good (RMSE is in the hundreds). I have been trying more layers, more neurons, and other activation functions, but the gain is very small. Here is the code: import numpy as np from tensorflow.keras import Sequential from tensorflow.keras.layers import Dense def generator(nb_samples, matrix_size = 2, entries_range = (0,1), determinant = None): ''' Generate nb_samples random matrices of size matrix_size with float entries in interval entries_range and of determinant determinant ''' matrices = [] if determinant: inverses = [] for i in range(nb_samples): matrix = np.random.uniform(entries_range[0], entries_range[1], (matrix_size,matrix_size)) matrix[0] *= determinant/np.linalg.det(matrix) matrices.append(matrix.reshape(matrix_size**2,)) inverses.append(np.array(np.linalg.inv(matrix)).reshape(matrix_size**2,)) return np.array(matrices), np.array(inverses) else: determinants = [] for i in range(nb_samples): matrix = np.random.uniform(entries_range[0], entries_range[1], (matrix_size,matrix_size)) determinants.append(np.array(np.linalg.det(matrix)).reshape(1,)) matrices.append(matrix.reshape(matrix_size**2,)) return np.array(matrices), np.array(determinants) ### Select number of samples, matrix size and range of entries in matrices nb_samples = 10000 matrix_size = 3 entries_range = (0, 100) determinant = 1 ### Generate random matrices and determinants matrices, inverses = generator(nb_samples, matrix_size = matrix_size, entries_range = entries_range, determinant = determinant) ### Select number of layers and neurons nb_hidden_layers = 1 nb_neurons = matrix_size**2 activation = 'relu' ### Create dense neural network with nb_hidden_layers hidden layers having nb_neurons neurons each model = Sequential() model.add(Dense(nb_neurons, input_dim = matrix_size**2, activation = activation)) for i in range(nb_hidden_layers): model.add(Dense(nb_neurons, activation = activation)) model.add(Dense(matrix_size**2)) model.compile(loss='mse', optimizer='adam') ### Train and save model using train size of 0.66 history = model.fit(matrices, inverses, epochs = 400, batch_size = 100, verbose = 0, validation_split = 0.33) ### Get validation loss from object 'history' rmse = np.sqrt(history.history['val_loss'][-1]) ### Print RMSE and parameter values print(''' Validation RMSE: {} Number of hidden layers: {} Number of neurons: {} Number of samples: {} Matrices size: {} Range of entries: {} Determinant: {} '''.format(rmse,nb_hidden_layers,nb_neurons,nb_samples,matrix_size,entries_range,determinant)) I have checked online and there seem to be papers dealing with the problem of inverse matrix approximation. However, before changing the model I would like to know if there would be other parameters I could change that could have a bigger impact on the error. I hope someone can provide some insight. Thank you.",|python|matrix|keras|neural-network|multi-layer|,Model,0
65225612,"Why does adding convolution/pool layer crash Keras/Tensorflow model while running on RTX 3070/cudnn8/CUDA11.1?. System Info OS: Windows 10, cudnn: 8.0, CUDA toolkit: 11.1 installed overtop of 10.2, GPU: Nvidia RTX 3070, CPU: Intel I7 10700f, Tensorflow: tf.__version__==2.4.0rc-0 (have also tried with tf-nightly-gpu as late as Dec 7, 2020) CUDA, cudnn compiled manually from source Test Code The below code successfully compiles a model but crashes when model.fit(...) is called. from tensorflow.keras import datasets, layers, models import matplotlib.pyplot as plt (train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data() train_images, test_images = train_images / 255.0, test_images / 255.0 model = models.Sequential() model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3))) model.add(layers.MaxPooling2D((2, 2))) model.add(layers.Conv2D(64, (3, 3), activation='relu')) model.add(layers.MaxPooling2D((2, 2))) model.add(layers.Conv2D(64, (3, 3), activation='relu')) model.add(layers.Flatten()) model.add(layers.Dense(64, activation='relu')) model.add(layers.Dense(10)) model.compile(optimizer='Adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)) history = model.fit(train_images, train_labels, batch_size=10, epochs=100) By removing the convolutional and maxpooling layers and just flattening the tensors after input the model is able to train fine (obviously the output of this model is useless but it is still able to train). The error code when program crashes is >Process finished with exit code -1073740791 (0xC0000409) Additionally tensorflow is able to open library, find the GPU, and logs GPU as available when tf.config.list_physical_devices('GPU') is called UPDATE I opened an issue on the tensorflow github page which you can find here",|python|tensorflow|keras|gpu|,GPU Usage,3
65434193,"Multi-GPU TFF simulation errors ""Detected dataset reduce op in multi-GPU TFF simulation"". I ran my code for an emotion detection model using Tensorflow Federated simulation. My code work perfectly fine using CPUs only. However, I received this error when trying to run TFF with GPU. ValueError: Detected dataset reduce op in multi-GPU TFF simulation: `use_experimental_simulation_loop=True` for `tff.learning`; or use `for ... in iter(dataset)` for your own dataset iteration.Reduce op will be functional after b/159180073. What is this error about and how can I fix it? I tried to search many places but found no answer. Here is the call stack if it help. It is very long so I pasted into this link: https://pastebin.com/b1R93gf1 EDIT: Here is the code containing iterative_process def startTraining(output_file): iterative_process = tff.learning.build_federated_averaging_process( model_fn, client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.01), server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0), use_experimental_simulation_loop=True ) flstate = iterative_process.initialize() evaluation = tff.learning.build_federated_evaluation(model_fn) output_file.write( 'round,available_users,loss,sparse_categorical_accuracy,val_loss,val_sparse_categorical_accuracy,test_loss,test_sparse_categorical_accuracy\n') curr_round_result = [0,0,100,0,100,0] min_val_loss = 100 for round in range(1,ROUND_COUNT + 1): available_users = fetch_available_users_and_increase_time(ROUND_DURATION_AVERAGE + random.randint(-ROUND_DURATION_VARIATION, ROUND_DURATION_VARIATION + 1)) if(len(available_users) == 0): write_to_file(curr_round_result) continue train_data = make_federated_data(available_users, 'train') flstate, metrics = iterative_process.next(flstate, train_data) val_data = make_federated_data(available_users, 'val') val_metrics = evaluation(flstate.model, val_data) curr_round_result[0] = round curr_round_result[1] = len(available_users) curr_round_result[2] = metrics['train']['loss'] curr_round_result[3] = metrics['train']['sparse_categorical_accuracy'] curr_round_result[4] = val_metrics['loss'] curr_round_result[5] = val_metrics['sparse_categorical_accuracy'] write_to_file(curr_round_result) Here is the code for make_federated_data def make_federated_data(users, dataset_type): offset = 0 if(dataset_type == 'val'): offset = train_size elif(dataset_type == 'test'): offset = train_size + val_size global LOADED_USER for id in users: if(id + offset not in LOADED_USER): LOADED_USER[id + offset] = getDatasetFromFilePath(filepaths[id + offset]) return [ LOADED_USER[id + offset] for id in users ]",|tensorflow|gpu|tensorflow-federated|,GPU Usage,3
65522548,"Why am I getting a low error before I did any optimization?. I am using a model training program I have built for a toy example and trying to use it on another example. The only difference is this model was used for regression, hence I was using MSE as the error criterion, and now it is used for binary classification, hence I am using BCEWithLogitsLoss. The model is very simple: class Model(nn.Module): def __init__(self, input_size, output_size): super(Model, self).__init__() self.fc1 = nn.Sequential( nn.Linear(input_size, 8*input_size), nn.PReLU() #parametric relu - same as leaky relu except the slope is learned ) self.fc2 = nn.Sequential( nn.Linear(8*input_size, 80*input_size), nn.PReLU() ) self.fc3 = nn.Sequential( nn.Linear(80*input_size, 32*input_size), nn.PReLU() ) self.fc4 = nn.Sequential( nn.Linear(32*input_size, 4*input_size), nn.PReLU() ) self.fc = nn.Sequential( nn.Linear(4*input_size, output_size), nn.PReLU() ) def forward(self, x, dropout=dropout, batchnorm=batchnorm): x = self.fc1(x) x = self.fc2(x) x = self.fc3(x) x = self.fc4(x) x = self.fc(x) return x And this is where I run it: model = Model(input_size, output_size) if (loss == 'MSE'): criterion = nn.MSELoss() if (loss == 'BCELoss'): criterion = nn.BCEWithLogitsLoss() optimizer = torch.optim.SGD(model.parameters(), lr = lr) model.train() for epoch in range(num_epochs): # Forward pass and loss train_predictions = model(train_features) print(train_predictions) print(train_targets) loss = criterion(train_predictions, train_targets) # Backward pass and update loss.backward() optimizer.step() # zero grad before new step optimizer.zero_grad() train_size = len(train_features) train_loss = criterion(train_predictions, train_targets).item() pred = train_predictions.max(1, keepdim=True)[1] correct = pred.eq(train_targets.view_as(pred)).sum().item() #train_loss /= train_size accuracy = correct / train_size print('\nTrain set: Loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format( train_loss, correct, train_size, 100. * accuracy)) However, when I print the loss, for some reason the loss already starts very low (around 0.6) before I have done any backwards pass! It remains this low all subsequent epochs. The prediction vector, however, looks like random garbage... tensor([[-0.0447], [-0.0640], [-0.0564], ..., [-0.0924], [-0.0113], [-0.0774]], grad_fn=<PreluBackward>) tensor([[0.], [0.], [0.], ..., [0.], [0.], [1.]]) epoch: 1, loss = 0.6842 I have no clue why is it doing that, and would appriciate any help. Thanks! EDIT: I added the params if they can help anyone figuring this out: if (dataset == 'adult_train.csv'): input_size=9 print_every = 1 output_size = 1 lr = 0.001 num_epochs = 10 loss='BCELoss' EDIT2: Added accuracy calculation in the middle block",|python|optimization|neural-network|pytorch|cross-entropy|,Training,2
65503731,"Tensorflow on gpu: tf cant find gpu. I tried everything but tensorflow cant see my gpu.  gonna show all versions to i have, can anyone has idea about this? 1- my nvidia 2 Cuda :version CuDNN version : cuDNN (7.6.5) my tf version I follow all this steps from there : https://medium.com/analytics-vidhya/installing-tensorflow-with-cuda-cudnn-gpu-support-on-ubuntu-20-04-f6f67745750a After this steps i controlled tf ; >>> tf.config.list_physical_devices('GPU') 2020-12-30 10:41:50.035846: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set 2020-12-30 10:41:50.047043: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1 2020-12-30 10:41:50.080921: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-12-30 10:41:50.081141: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: pciBusID: 0000:01:00.0 name: GeForce RTX 2060 SUPER computeCapability: 7.5 coreClock: 1.665GHz coreCount: 34 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s 2020-12-30 10:41:50.081155: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0 2020-12-30 10:41:50.107337: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11 2020-12-30 10:41:50.107387: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11 2020-12-30 10:41:50.126300: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10 2020-12-30 10:41:50.132954: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10 2020-12-30 10:41:50.204340: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10 2020-12-30 10:41:50.212418: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11 2020-12-30 10:41:50.212534: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/cuda/include:/usr/lib/cuda/lib64: 2020-12-30 10:41:50.212543: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform. Skipping registering GPU devices... [] >>> tf.config.list_physical_devices('GPU') `[]` >>> tf.test.is_built_with_cuda() true >>> tf.test.is_gpu_available(cuda_only=False, min_cuda_compute_capability=None) WARNING:tensorflow:From <stdin>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version. Instructions for updating: Use `tf.config.list_physical_devices('GPU')` instead. 2020-12-30 10:42:47.612041: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set 2020-12-30 10:42:47.612151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-12-30 10:42:47.612381: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: pciBusID: 0000:01:00.0 name: GeForce RTX 2060 SUPER computeCapability: 7.5 coreClock: 1.665GHz coreCount: 34 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s 2020-12-30 10:42:47.612400: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0 2020-12-30 10:42:47.612421: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11 2020-12-30 10:42:47.612431: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11 2020-12-30 10:42:47.612441: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10 2020-12-30 10:42:47.612450: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10 2020-12-30 10:42:47.612459: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10 2020-12-30 10:42:47.612467: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11 2020-12-30 10:42:47.615079: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/cuda/include:/usr/lib/cuda/lib64: 2020-12-30 10:42:47.615090: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform. Skipping registering GPU devices... 2020-12-30 10:42:47.725208: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix: 2020-12-30 10:42:47.725230: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267] 0 2020-12-30 10:42:47.725235: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0: N False",|python|tensorflow|gpu|tensorflow2.0|,GPU Usage,3
65669308,"Why does my Tensorflow Keras model output weird loss and accuracy values while training?. I have trained a custom text classifier in Tensorflow with python for classifying sentences into questions/sentences containing information using this code: import tensorflow as tf from tensorflow import keras from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.preprocessing.sequence import pad_sequences text = """" with open(""/content/train_new.txt"") as source: for line in source.readlines(): text = text + line print(""text: "" + text) sentences = [] labels = [] for item in text.split(""<n>""): parts = item.split(""<t>"") print(parts) sentences.append(parts[0]) labels.append(parts[1]) print(sentences) print(labels) print(""----"") train_test_split_percentage = 80 training_size = round((len(sentences)/100)*train_test_split_percentage) print(""training size: "" + str(training_size) + "" of "" + str(len(labels))) training_sentences = sentences[0:training_size] testing_sentences = sentences[training_size:] training_labels = labels[0:training_size] testing_labels = labels[training_size:] vocab_size = 100 max_length = 10 tokenizer = Tokenizer(num_words = vocab_size, oov_token=""<OOV>"") tokenizer.fit_on_texts(sentences) word_index = tokenizer.word_index training_sequences = tokenizer.texts_to_sequences(training_sentences) training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=""post"", truncating=""post"") testing_sequences = tokenizer.texts_to_sequences(testing_sentences) testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=""post"", truncating=""post"") # convert training & testing data into numpy array # Need this block to get it to work with TensorFlow 2.x import numpy as np training_padded = np.array(training_padded) training_labels = np.asarray(training_labels).astype('float32').reshape((-1,1)) testing_padded = np.array(testing_padded) testing_labels = np.asarray(testing_labels).astype('float32').reshape((-1,1)) # defining the model model = tf.keras.Sequential([ tf.keras.layers.Embedding(vocab_size, 24, input_length=max_length), tf.keras.layers.GlobalAveragePooling1D(), tf.keras.layers.Dense(24, activation='relu'), tf.keras.layers.Dense(1, activation='softmax') ]) model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy']) # training the model num_epochs = 1000 history = model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(testing_padded, testing_labels), verbose=2) However, while training, it prints weird accuracy and loss values like this: Epoch 972/1000 9/9 - 0s - loss: -8.2316e+03 - accuracy: 0.7345 - val_loss: -2.7299e+04 - val_accuracy: 0.0000e+00 Epoch 973/1000 9/9 - 0s - loss: -8.2452e+03 - accuracy: 0.7345 - val_loss: -2.7351e+04 - val_accuracy: 0.0000e+00 Epoch 974/1000 9/9 - 0s - loss: -8.2571e+03 - accuracy: 0.7345 - val_loss: -2.7363e+04 - val_accuracy: 0.0000e+00 Epoch 975/1000 9/9 - 0s - loss: -8.2703e+03 - accuracy: 0.7345 - val_loss: -2.7416e+04 - val_accuracy: 0.0000e+00 The train_new.txt file contains data in the form of text<t>class_num<n> When trying to predict using the model.predict() function, it always outputs [[1.]] What's the issue with my code?",|python|tensorflow|machine-learning|keras|,Model,0
65679823,"TypeError: expected CPU (got CUDA). import torch torch.cuda.is_available() torch.cuda.current_device() torch.cuda.get_device_name(0) torch.cuda.memory_reserved() torch.cuda.memory_allocated() torch.cuda.memory_allocated() var1=torch.FloatTensor([1.0,2.0,3.0]).cuda() var1 var1.device import pandas as pd df=pd.read_csv('diabetes.csv') df.head() df.isnull().sum() import seaborn as sns import numpy as np df['Outcome']=np.where(df['Outcome']==1,""Diabetic"",""No Diabetic"") df.head() sns.pairplot(df,hue=""Outcome"") X=df.drop('Outcome',axis=1).values### independent features y=df['Outcome'].values###dependent features from sklearn.model_selection import train_test_split X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0) y_train import torch import torch.nn as nn import torch.nn.functional as F X_train=torch.FloatTensor(X_train).cuda() X_test=torch.FloatTensor(X_test).cuda() y_train=torch.LongTensor(y_train).cuda() y_test=torch.LongTensor(y_test).cuda() when I Run this code I got this error: Traceback (most recent call last): File ""<stdin>"", line 24, in <module> TypeError: expected CPU (got CUDA) How to can I solve this error?",|python|pytorch|gpu|,GPU Usage,3
65777704,"Getting a very low accuracy on implementing Neural Network in Keras. I am trying to implement ANN on a Cifar-10 dataset using keras but for some reason I dont know I am getting only 10% accuracy ? I have used 5 hidden layers iwth 8,16,32,64,128 neurons respectively. This is the link to the jupyter notebook model = Sequential() model.add(Dense(units = 8,activation = 'sigmoid' , input_dim = X.shape[1])) model.add(Dense(units = 16 , activation = 'sigmoid')) model.add(Dense(units = 32 , activation = 'sigmoid')) model.add(Dense(units = 64 , activation = 'sigmoid')) model.add(Dense(units = 128 , activation = 'sigmoid')) model.add(Dense(units = 10 , activation = 'softmax')) model.compile(loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = ['accuracy']) model.fit(x_train,y_train,epochs = 1000, batch_size = 500 )",|python|tensorflow|keras|neural-network|,Model,0
65801034,"PyTorch: ""ValueError: can't optimize a non-leaf Tensor"" after changing pretrained model from 3 RGB Channels to 4 Channels. I have been trying to change the pretrained PyTorch Densenet's first conv layer from 3 channels to 4 channels while maintaining its original RGB channel's pretrained weights. I have done the following codes, but the optimizer part throws me this error: ""ValueError: can't optimize a non-leaf Tensor"" . import torchvision.models as models import torch.nn as nn backbone = models.__dict__['densenet169'](pretrained=True) weight1 = backbone.features.conv0.weight.data.clone() new_first_layer = nn.Conv2d(4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) with torch.no_grad(): new_first_layer.weight[:,:3] = weight1 backbone.features.conv0 = new_first_layer optimizer = torch.optim.SGD(backbone.parameters(), 0.001, weight_decay=0.1) # Changing this optimizer from SGD to ADAM I have also tried to remove the argument with torch.no_grad(): but this issue still remains: ValueError Traceback (most recent call last) <ipython-input-343-5fc87352da04> in <module>() 11 backbone.features.conv0 = new_first_layer 12 optimizer = torch.optim.SGD(res.parameters(), 0.001, ---> 13 weight_decay=0.1) # Changing this optimizer from SGD to ADAM ~/anaconda3/envs/detectron2/lib/python3.6/site-packages/torch/optim/sgd.py in __init__(self, params, lr, momentum, dampening, weight_decay, nesterov) 66 if nesterov and (momentum <= 0 or dampening != 0): 67 raise ValueError(""Nesterov momentum requires a momentum and zero dampening"") ---> 68 super(SGD, self).__init__(params, defaults) 69 70 def __setstate__(self, state): ~/anaconda3/envs/detectron2/lib/python3.6/site-packages/torch/optim/optimizer.py in __init__(self, params, defaults) 50 51 for param_group in param_groups: ---> 52 self.add_param_group(param_group) 53 54 def __getstate__(self): ~/anaconda3/envs/detectron2/lib/python3.6/site-packages/torch/optim/optimizer.py in add_param_group(self, param_group) 231 ""but one of the params is "" + torch.typename(param)) 232 if not param.is_leaf: --> 233 raise ValueError(""can't optimize a non-leaf Tensor"") 234 235 for name, default in self.defaults.items(): ValueError: can't optimize a non-leaf Tensor My PyTorch version is: 1.7.0. Could you guys please help? Thanks alot! Regards.",|python|pytorch|pre-trained-model|densenet|,API,4
65889068,"ValueError while running keras model in Python. I am trying to run the Keras tutorial mentioned below in python: #Import Libraries from keras.models import Sequential from keras.layers import Dense, Conv2D, MaxPool2D , Flatten from keras.optimizers import SGD #model details vgg19 = Sequential() vgg19.add(Conv2D(input_shape=(224,224,3),filters=64,kernel_size=(3,3),padding=""same"", activation=""relu"")) vgg19.add(Conv2D(filters=64,kernel_size=(3,3),padding=""same"", activation=""relu"")) vgg19.add(MaxPool2D(pool_size=(2,2),strides=(2,2))) vgg19.add(Conv2D(filters=128, kernel_size=(3,3), padding=""same"", activation=""relu"")) vgg19.add(Conv2D(filters=128, kernel_size=(3,3), padding=""same"", activation=""relu"")) vgg19.add(MaxPool2D(pool_size=(2,2),strides=(2,2))) vgg19.add(Conv2D(filters=256, kernel_size=(3,3), padding=""same"", activation=""relu"")) vgg19.add(Conv2D(filters=256, kernel_size=(3,3), padding=""same"", activation=""relu"")) vgg19.add(Conv2D(filters=256, kernel_size=(3,3), padding=""same"", activation=""relu"")) vgg19.add(MaxPool2D(pool_size=(2,2),strides=(2,2))) vgg19.add(Conv2D(filters=512, kernel_size=(3,3), padding=""same"", activation=""relu"")) vgg19.add(Conv2D(filters=512, kernel_size=(3,3), padding=""same"", activation=""relu"")) vgg19.add(Conv2D(filters=512, kernel_size=(3,3), padding=""same"", activation=""relu"")) vgg19.add(MaxPool2D(pool_size=(2,2),strides=(2,2))) vgg19.add(Conv2D(filters=512, kernel_size=(3,3), padding=""same"", activation=""relu"")) vgg19.add(Conv2D(filters=512, kernel_size=(3,3), padding=""same"", activation=""relu"")) vgg19.add(Conv2D(filters=512, kernel_size=(3,3), padding=""same"", activation=""relu"")) vgg19.add(MaxPool2D(pool_size=(2,2),strides=(2,2))) vgg19.add(Flatten()) vgg19.add(Dense(units=4096,activation=""relu"")) vgg19.add(Dense(units=4096,activation=""relu"")) vgg19.add(Dense(units=10, activation=""softmax"")) #Preparing Dataset from keras.datasets import cifar10 from keras.utils import to_categorical (X, Y), (tsX, tsY) = cifar10.load_data() # Use a one-hot-encoding Y = to_categorical(Y) tsY = to_categorical(tsY) # Change datatype to float X = X.astype('float32') tsX = tsX.astype('float32') # Scale X and tsX so each entry is between 0 and 1 X = X / 255.0 tsX = tsX / 255.0 #training optimizer = SGD(lr=0.001, momentum=0.9) vgg19.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy']) history = vgg19.fit(X, Y, epochs=100, batch_size=64, validation_data=(tsX, tsY), verbose=0) Upon training the model, I am getting the below mentioned value error: ValueError: Input 0 of layer dense_9 is incompatible with the layer: expected axis -1 of input shape to have value 25088 but received input with shape (None, 512) Please suggest, how to fix the input shape and would be better if someone can provide a brief explanation of the issue. Thanks in advance!",|python|keras|valueerror|,Model,0
65993928,"IndexError: Dimension out of range - PyTorch dimension expected to be in range of [-1, 0], but got 1. Despite already numerous answers on this very topic, failing to see in the example below (extract from https://gist.github.com/lirnli/c16ef186c75588e705d9864fb816a13c on Variational Recurrent Networks) which input and output dimensions trigger the error. Having tried to change dimensions in torch.cat and also suppress the call to squeeze(), the error persists, <ipython-input-51-cdc928891ad7> in generate(self, hidden, temperature) 56 x_sample = x = x_out.div(temperature).exp().multinomial(1).squeeze() 57 x = self.phi_x(x) ---> 58 tc = torch.cat([x,z], dim=1) 59 60 hidden_next = self.rnn(tc,hidden) IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1) Thus how to shape the dimensions in x and z in tc = torch.cat([x,z], dim=1)? Note the code as follows, import torch from torch import nn, optim from torch.autograd import Variable class VRNNCell(nn.Module): def __init__(self): super(VRNNCell,self).__init__() self.phi_x = nn.Sequential(nn.Embedding(128,64), nn.Linear(64,64), nn.ELU()) self.encoder = nn.Linear(128,64*2) # output hyperparameters self.phi_z = nn.Sequential(nn.Linear(64,64), nn.ELU()) self.decoder = nn.Linear(128,128) # logits self.prior = nn.Linear(64,64*2) # output hyperparameters self.rnn = nn.GRUCell(128,64) def forward(self, x, hidden): x = self.phi_x(x) # 1. h => z z_prior = self.prior(hidden) # 2. x + h => z z_infer = self.encoder(torch.cat([x,hidden], dim=1)) # sampling z = Variable(torch.randn(x.size(0),64))*z_infer[:,64:].exp()+z_infer[:,:64] z = self.phi_z(z) # 3. h + z => x x_out = self.decoder(torch.cat([hidden, z], dim=1)) # 4. x + z => h hidden_next = self.rnn(torch.cat([x,z], dim=1),hidden) return x_out, hidden_next, z_prior, z_infer def calculate_loss(self, x, hidden): x_out, hidden_next, z_prior, z_infer = self.forward(x, hidden) # 1. logistic regression loss loss1 = nn.functional.cross_entropy(x_out, x) # 2. KL Divergence between Multivariate Gaussian mu_infer, log_sigma_infer = z_infer[:,:64], z_infer[:,64:] mu_prior, log_sigma_prior = z_prior[:,:64], z_prior[:,64:] loss2 = (2*(log_sigma_infer-log_sigma_prior)).exp() \ + ((mu_infer-mu_prior)/log_sigma_prior.exp())**2 \ - 2*(log_sigma_infer-log_sigma_prior) - 1 loss2 = 0.5*loss2.sum(dim=1).mean() return loss1, loss2, hidden_next def generate(self, hidden=None, temperature=None): if hidden is None: hidden=Variable(torch.zeros(1,64)) if temperature is None: temperature = 0.8 # 1. h => z z_prior = self.prior(hidden) # sampling z = Variable(torch.randn(z_prior.size(0),64))*z_prior[:,64:].exp()+z_prior[:,:64] z = self.phi_z(z) # 2. h + z => x x_out = self.decoder(torch.cat([hidden, z], dim=1)) # sampling x_sample = x = x_out.div(temperature).exp().multinomial(1).squeeze() x = self.phi_x(x) # 3. x + z => h # hidden_next = self.rnn(torch.cat([x,z], dim=1),hidden) tc = torch.cat([x,z], dim=1) hidden_next = self.rnn(tc,hidden) return x_sample, hidden_next def generate_text(self, hidden=None,temperature=None, n=100): res = [] hidden = None for _ in range(n): x_sample, hidden = self.generate(hidden,temperature) res.append(chr(x_sample.data[0])) return """".join(res) # Test net = VRNNCell() x = Variable(torch.LongTensor([12,13,14])) hidden = Variable(torch.rand(3,64)) output, hidden_next, z_infer, z_prior = net(x, hidden) loss1, loss2, _ = net.calculate_loss(x, hidden) loss1, loss2 hidden = Variable(torch.zeros(1,64)) net.generate_text()",|python|pytorch|tensor|dimensions|index-error|,Tensors&Inputs,1
66015472,"problem with nn.ModuleDict ('method' object is not subscriptable). I am trying to use nn.ModuleDict following this documentation page: I have this PyTorch network: class Net(nn.Module): def __init__(self, kernel_size): super(Net, self).__init__() modules = {} modules[""layer1""] = nn.Conv2d(3, 16, kernel_size=kernel_size, stride=1, padding=2) self.modules = nn.ModuleDict(modules) def forward(self, x): x = self.modules[""layer1""](x) when I use the forward method, I get the following error: 'method' object is not subscriptable when I change the forward method to: def forward(self, x): x = self.modules()[""layer1""](x) I get the following error: TypeError: 'generator' object is not subscriptable",|python|oop|pytorch|typeerror|,API,4
66059474,"why torch.Tensor subtract works well when tensor size is different?. This example will make it easier to understand. The following fails: A = tensor.torch([[1, 2, 3], [4, 5, 6]]) # shape : (2, 3) B = tensor.torch([[1, 2], [3, 4], [5, 6]]) # shape : (3, 2) print((A - B).shape) # RuntimeError: The size of tensor A (3) must match the size of tensor B (2) at non-singleton dimension 1 # ================================================================== A = tensor.torch([[1, 2], [3, 4], [5, 6]]) # shape : (3, 2) B = tensor.torch([[1, 2], [3, 4],]) # shape : (2, 2) print((A - B).shape) # RuntimeError: The size of tensor A (3) must match the size of tensor B (2) at non-singleton dimension 0 But the following works well: a = torch.ones(8).unsqueeze(0).unsqueeze(-1).expand(4, 8, 7) a_temp = a.unsqueeze(2) # shape : ( 4, 8, 1, 7 ) b_temp = torch.transpose(a_temp, 1, 2) # shape : ( 4, 1, 8, 7 ) print(a_temp-b_temp) # shape : ( 4, 8, 8, 7 ) Why does the latter work, but not the former? How/why has the result shape been expanded?",|pytorch|tensor|torch|,Tensors&Inputs,1
66087653,"My image classification model written in tensorflow don't learn. im trying to build image classification model that will predict if you are wearing a mask.This is a first time i make my own model and when im training it the accuracy jump arround 50% and if i predict it always says ""no mask"" i tryied changing number of epoches, batch size, number of training data changing model code and nothing works. This is my code: import os import cv2 import random import numpy as np from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, Flatten,Dropout from tensorflow.keras.optimizers import SGD def preproccesImage(img): img = cv2.resize(img,dsize=(150,150 ), interpolation = cv2.INTER_CUBIC) return img def getData(): training = [] for image in os.listdir(""src/data/with_mask""): img = cv2.imread(f""src/data/with_mask/{image}"",cv2.IMREAD_GRAYSCALE) proccesed = preproccesImage(img) training.append([proccesed.tolist(),1]) for image in os.listdir(""src/data/without_mask""): img = cv2.imread(f""src/data/without_mask/{image}"",cv2.IMREAD_GRAYSCALE) proccesed = preproccesImage(img) training.append([proccesed.tolist(),0]) random.shuffle(training) train_x = np.array([x[0] for x in training],dtype=np.float32) train_y = np.array([x[1] for x in training],dtype=np.float32) print(train_x) print(train_y) return (train_x ,train_y) train_x , train_y = getData() model = Sequential() model.add(Dense(32,input_shape=(len(train_x[0]),150),activation=""relu"")) model.add(Flatten()) model.add(Dense(128,activation=""relu"")) model.add(Dropout(0.2)) model.add(Dense(128,activation=""relu"")) model.add(Flatten()) model.add(Dense(1,activation=""softmax"")) model.compile(loss=""categorical_crossentropy"", optimizer=""adam"", metrics=[""accuracy""]) hist = model.fit( train_x,train_y, epochs=200, batch_size=2, verbose=1) model.save(""model.h5"", hist) img = cv2.imread(""src/me.png"",cv2.IMREAD_GRAYSCALE) resized = cv2.resize(img,dsize=(150,150 ), interpolation = cv2.INTER_CUBIC) def predict(): res = model.predict([resized.tolist()])[0] resoult = [[i, r] for i, r in enumerate(res)] predicted = [] for r in resoult: predicted.append({""intent"": 1 if r[0] == 1 else 0, ""probability"": r[1]}) if predicted[0][""intent""] == 1: print(""mask on"") else: print('no mask') predict() I will be pleased if someone can help",|python|tensorflow|machine-learning|keras|,Model,0
66235657,"Conv2d Tensorflow results wrong - accuracy = 0.0000e+00. I am using tensorflow and keras to classify build a classification model. When running the code below it seems that the output does not seem to converge after each epoch, with the loss steadily increasing and the accuracy contantly set to 0.0000e+00. I am new to machine learning and am not too sure why this is happening. from sklearn.model_selection import train_test_split from sklearn.preprocessing import LabelEncoder from tensorflow.keras.models import Sequential from tensorflow.keras import Sequential from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten from tensorflow.keras.layers import Conv2D, MaxPooling2D import numpy as np import time import tensorflow as tf from google.colab import drive drive.mount('/content/drive') import pandas as pd data = pd.read_csv(""hmnist_28_28_RGB.csv"") X = data.iloc[:, 0:-1] y = data.iloc[:, -1] X = X / 255.0 X = X.values.reshape(-1,28,28,3) print(X.shape) model = Sequential() model.add(Conv2D(256, (3, 3), input_shape=X.shape[1:])) model.add(Activation('relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Conv2D(256, (3, 3))) model.add(Activation('relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Flatten()) # this converts our 3D feature maps to 1D feature vectors model.add(Dense(64)) model.add(Dense(1)) model.add(Activation('sigmoid')) model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) model.fit(X, y, batch_size=32, epochs=10, validation_split=0.3) Output (378, 28, 28, 3) Epoch 1/10 9/9 [==============================] - 4s 429ms/step - loss: -34.6735 - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00 Epoch 2/10 9/9 [==============================] - 4s 400ms/step - loss: -1074.2162 - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00 Epoch 3/10 9/9 [==============================] - 4s 399ms/step - loss: -7446.1872 - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00 Epoch 4/10 9/9 [==============================] - 4s 396ms/step - loss: -30012.9553 - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00 Epoch 5/10 9/9 [==============================] - 4s 406ms/step - loss: -89006.4180 - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00 Epoch 6/10 9/9 [==============================] - 4s 400ms/step - loss: -221087.9078 - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00 Epoch 7/10 9/9 [==============================] - 4s 399ms/step - loss: -480032.9313 - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00 Epoch 8/10 9/9 [==============================] - 4s 403ms/step - loss: -956052.3375 - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00 Epoch 9/10 9/9 [==============================] - 4s 396ms/step - loss: -1733128.9000 - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00 Epoch 10/10 9/9 [==============================] - 4s 401ms/step - loss: -2953626.5750 - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00",|python|keras|tensorflow2.0|,Model,0
66260320,"Casting Pytorch's tensor elements the type ""float"" instead of ""double"". I had a matrix saved as a numpy type, call it ""X_before"" (for example, its shape is 100*30). Since I want to feed it to an AutoEncoder using Pytorch library, I converted it to torch.tensor like this: X_tensor = torch.from_numpy(X_before, dtype=torch) Then, I got the following error: expected scalar type Float but found Double Next, I tried to make elements as ""float"" and then convert them torch.tensor: X_before = X_before.astype(float) X_tensor = torch.from_numpy(X_before) Again, the same error happens. How should I solve this issue? How can I convert the type of elements in a torch.tensor object to another type? Thanks in advance",|arrays|casting|pytorch|tensor|autoencoder|,API,4
66370996,"Input tensor incompatible with Python signature in Tensorflow object detection. I recently trained a object detection model in Tensorflow but for some reason some of the images have input tensors that are incompatible with the python signature. This is the code I'm running in google colab for inference: import numpy as np from PIL import Image import matplotlib.pyplot as plt import warnings warnings.filterwarnings('ignore') # Suppress Matplotlib warnings def load_image_into_numpy_array(path): """"""Load an image from file into a numpy array. Puts image into numpy array to feed into tensorflow graph. Note that by convention we put it into a numpy array with shape (height, width, channels), where channels=3 for RGB. Args: path: the file path to the image Returns: uint8 numpy array with shape (img_height, img_width, 3) """""" return np.array(Image.open(path)) for image_path in img: print('Running inference for {}... '.format(image_path), end='') image_np=load_image_into_numpy_array(image_path) # Things to try: # Flip horizontally # image_np = np.fliplr(image_np).copy() # Convert image to grayscale # image_np = np.tile( # np.mean(image_np, 2, keepdims=True), (1, 1, 3)).astype(np.uint8) # The input needs to be a tensor, convert it using `tf.convert_to_tensor`. input_tensor=tf.convert_to_tensor(image_np) # The model expects a batch of images, so add an axis with `tf.newaxis`. input_tensor=input_tensor[tf.newaxis, ...] # input_tensor = np.expand_dims(image_np, 0) detections=detect_fn(input_tensor) # All outputs are batches tensors. # Convert to numpy arrays, and take index [0] to remove the batch dimension. # We're only interested in the first num_detections. num_detections=int(detections.pop('num_detections')) detections={key:value[0,:num_detections].numpy() for key,value in detections.items()} detections['num_detections']=num_detections # detection_classes should be ints. detections['detection_classes']=detections['detection_classes'].astype(np.int64) image_np_with_detections=image_np.copy() viz_utils.visualize_boxes_and_labels_on_image_array( image_np_with_detections, detections['detection_boxes'], detections['detection_classes'], detections['detection_scores'], category_index, use_normalized_coordinates=True, max_boxes_to_draw=100, #max number of bounding boxes in the image min_score_thresh=.25, #min prediction threshold agnostic_mode=False) %matplotlib inline plt.figure() plt.imshow(image_np_with_detections) print('Done') plt.show() And this is the error message I get when running inference: Running inference for /content/gdrive/MyDrive/TensorFlow/workspace/training_demo/images/test/image_part_002.png... --------------------------------------------------------------------------- ValueError Traceback (most recent call last) <ipython-input-23-5b465e5474df> in <module>() 40 41 # input_tensor = np.expand_dims(image_np, 0) ---> 42 detections=detect_fn(input_tensor) 43 44 # All outputs are batches tensors. 6 frames /usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py in _convert_inputs_to_signature(inputs, input_signature, flat_input_signature) 2804 flatten_inputs)): 2805 raise ValueError(""Python inputs incompatible with input_signature:\n%s"" % -> 2806 format_error_message(inputs, input_signature)) 2807 2808 if need_packing: ValueError: Python inputs incompatible with input_signature: inputs: ( tf.Tensor( [[[[ 0 0 0 255] [ 0 0 0 255] [ 0 0 0 255] ... [ 0 0 0 255] [ 0 0 0 255] [ 0 0 0 255]] [[ 0 0 0 255] [ 0 0 0 255] [ 0 0 0 255] ... [ 0 0 0 255] [ 0 0 0 255] [ 0 0 0 255]] [[ 0 0 0 255] [ 0 0 0 255] [ 0 0 0 255] ... [ 0 0 0 255] [ 0 0 0 255] [ 0 0 0 255]] ... [[ 34 32 34 255] [ 35 33 35 255] [ 35 33 35 255] ... [ 41 38 38 255] [ 40 37 37 255] [ 40 37 37 255]] [[ 36 34 36 255] [ 35 33 35 255] [ 36 34 36 255] ... [ 41 38 38 255] [ 41 38 38 255] [ 43 40 40 255]] [[ 36 34 36 255] [ 36 34 36 255] [ 37 35 37 255] ... [ 41 38 38 255] [ 40 37 37 255] [ 39 36 36 255]]]], shape=(1, 1219, 1920, 4), dtype=uint8)) input_signature: ( TensorSpec(shape=(1, None, None, 3), dtype=tf.uint8, name='input_tensor')) Does anyone know a way I could convert the input tensors of my images so I can run inference on them? I know for example one image where the inference works gas resolution 400x291 and the image where inference doesn't work has resolution 1920x1219. I used the SSD MobileNet V1 FPN 640x640 Model for my training.",|python|tensorflow|tensorflow2.0|object-detection|tensor|,Tensors&Inputs,1
66407123,"Keras accuracy returning 0. So basically, I am working on this bullet optimization program. I wish to study how different ballistics parameters such as weight, length, and mass affect a ballistics coefficient. However, my training accuracy is 0, although there is loss and val_loss. I've read similar Stackoverflow posts regarding this, but none have helped me so far. Perhaps I just didn't do them right; I am referencing https://stackoverflow.com/a/63513872/12349188 import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from sklearn.utils import shuffle df = pd.read_csv('Bullet Optimization\ShootForum Bullet DB_2.csv') from sklearn.model_selection import train_test_split from sklearn import preprocessing dataset = df.values X = dataset[:,0:12] X = np.asarray(X).astype(np.float32) y = dataset[:,13] y = np.asarray(y).astype(np.float32) X_train, X_val_and_test, y_train, y_val_and_test = train_test_split(X, y, test_size=0.3, shuffle=True) X_val, X_test, y_val, y_test = train_test_split(X_val_and_test, y_val_and_test, test_size=0.5) from keras.models import Sequential from keras.layers import Dense, BatchNormalization model = Sequential( [ #2430 is the shape of X_train #BatchNormalization(axis=-1, momentum = 0.1), Dense(32, activation='relu'), Dense(32, activation='relu'), Dense(1,activation='softmax'), ] ) model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy']) history = model.fit(X_train, y_train, batch_size=64, epochs=100, validation_data=(X_val, y_val)) Did I do something wrong in my code? I know some python but I just kind of built upon the tutorials for my own purposes.",|python|pandas|numpy|keras|sequential|,Model,0
66493943,"Tensor for argument #2 'mat1' is on CPU, but expected it to be on GPU. Following my previous question , I have written this code to train an autoencoder and then extract the features. (There might be some changes in the variable names) # Autoencoder class #https://medium.com/pytorch/implementing-an-autoencoder-in-pytorch-19baa22647d1 class AE_class(nn.Module): def __init__(self, **kwargs): super().__init__() self.encoder_hidden_layer = nn.Linear( in_features=kwargs[""input_shape""], out_features=128 ) self.encoder_output_layer = nn.Linear( in_features=128, out_features=128 ) self.decoder_hidden_layer = nn.Linear( in_features=128, out_features=128 ) self.decoder_output_layer = nn.Linear( in_features=128, out_features=kwargs[""input_shape""] ) def forward(self, features): #print(""in forward"") #print(type(features)) activation = self.encoder_hidden_layer(features) activation = torch.relu(activation) code = self.encoder_output_layer(activation) code = torch.relu(code) activation = self.decoder_hidden_layer(code) activation = torch.relu(activation) activation = self.decoder_output_layer(activation) reconstructed = torch.relu(activation) return reconstructed def encode(self, features_h): activation_h = self.encoder_hidden_layer(features_h) activation_h = torch.relu(activation_h) code_h = self.encoder_output_layer(activation_h) code_h = torch.relu(code_h) return code_h And then, for training: def retrieve_AE_features(X_before, n_voxel_region): # use gpu if available #https://discuss.pytorch.org/t/runtimeerror-tensor-for-out-is-on-cpu-tensor-for-argument-1-self-is-on-cpu-but-expected-them-to-be-on-gpu-while-checking-arguments-for-addmm/105453 device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"") # create a model from `AE` autoencoder class # load it to the specified device, either gpu or cpu model_AE = AE_class(input_shape=n_voxel_region).to(device) # create an optimizer object # Adam optimizer with learning rate 1e-3 optimizer = optim.Adam(model_AE.parameters(), lr=1e-3) # mean-squared error loss criterion = nn.MSELoss() X_tensor = torch.tensor(X_before, dtype=torch.float32) print(type(X_tensor)) train_loader = torch.utils.data.DataLoader( X_tensor, batch_size=64, shuffle=True, num_workers=2, pin_memory=True ) test_loader = torch.utils.data.DataLoader( X_tensor, batch_size=32, shuffle=False, num_workers=2 ) print(type(train_loader)) for epoch in range(epochs_AE): loss = 0 for batch_features in train_loader: # reshape mini-batch data to [N, 784] matrix # load it to the active device #batch_features = batch_features.view(-1, 784).to(device) #print(batch_features.shape) # reset the gradients back to zero # PyTorch accumulates gradients on subsequent backward passes optimizer.zero_grad() # compute reconstructions outputs = model_AE(batch_features) # compute training reconstruction loss train_loss = criterion(outputs, batch_features) # compute accumulated gradients train_loss.backward() # perform parameter update based on current gradients optimizer.step() # add the mini-batch training loss to epoch loss loss += train_loss.item() # compute the epoch training loss loss = loss / len(train_loader) # display the epoch training loss print(""AE, epoch : {}/{}, loss = {:.6f}"".format(epoch + 1, epochs_AE, loss)) #After training hidden_features = model_AE.encode(X_before) return hidden_features However, I received the following error: Tensor for argument #2 'mat1' is on CPU, but expected it to be on GPU (while checking arguments for addmm) It seems some of my variables should be defined in another way to be able to be executed on GPU. My questions: How can I understand which variables will be executed on GPU and which ones on CPU? How to fix it? In other words, how to define a variable executable on GPU? Thanks in advance",|python|debugging|pytorch|gpu|autoencoder|,GPU Usage,3
66472843,"What does mean Python inputs incompatible with input_signature. I am getting the issue ValueError: Python inputs incompatible with input_signature: When I do : image_np = np.asarray(np.array(Image.open(image_path))) input_tensor = tf.convert_to_tensor(image_np) input_tensor = input_tensor[tf.newaxis, ...] detections = detect_fn(input_tensor) the issue happen precisely on this line : detections = detect_fn(input_tensor) What am I doing wrong ? What does this error mean ? Console Log ValueError: Python inputs incompatible with input_signature: inputs: ( tf.Tensor( [[[[255 255 255 255] [255 255 255 255] [255 255 255 255] ... [255 255 255 255] [255 255 255 255] [255 255 255 255]] [[254 254 254 255] [255 255 255 255] [255 255 255 255] ... [255 255 255 255] [255 255 255 255] [255 255 255 255]] [[254 254 254 255] [254 254 255 255] [255 255 255 255] ... [255 255 255 255] [255 255 255 255] [255 255 255 255]] ... [[ 37 37 37 255] [ 37 37 37 255] [ 39 39 39 255] ... [ 32 32 32 255] [ 33 33 33 255] [ 31 31 31 255]] [[ 37 37 37 255] [ 38 38 38 255] [ 36 36 36 255] ... [ 33 33 33 255] [ 31 31 31 255] [ 32 32 32 255]] [[ 38 38 38 255] [ 37 37 37 255] [ 38 38 38 255] ... [ 32 32 32 255] [ 31 31 31 255] [ 32 32 32 255]]]], shape=(1, 1080, 1915, 4), dtype=uint8)) input_signature: ( TensorSpec(shape=(1, None, None, 3), dtype=tf.uint8, name='input_tensor'))",|python|tensorflow2.0|tensor|,Tensors&Inputs,1
66489112,"TypeError: __init__() got an unexpected keyword argument 'filepath'. I don't know how to solve this error but I hope some of you guys know how to solve this issue. Error:TypeError: __init__() got an unexpected keyword argument 'filepath' Full error message: File ""train.py"", line 167, in <module> main(args) File ""train.py"", line 113, in main checkpoint_callback=checkpoint_callback(), File ""train.py"", line 86, in checkpoint_callback return ModelCheckpoint( TypeError: __init__() got an unexpected keyword argument 'filepath' from pytorch_lightning.callbacks import ModelCheckpoint save_model_path = path/to/your/dir def checkpoint_callback(): return ModelCheckpoint( filepath= save_model_path, save_top_k=True, verbose=True, monitor='val_loss', mode='min', prefix='' )",|python|python-3.x|pytorch|typeerror|,API,4
66524542,"AttributeError: 'str' object has no attribute 'shape' while encoding tensor using BertModel with PyTorch (Hugging Face). AttributeError: 'str' object has no attribute 'shape' while encoding tensor using BertModel with PyTorch (Hugging Face). Below is the code bert_model = BertModel.from_pretrained(r'downloads\bert-pretrained-model') input_ids Output is: tensor([[ 101, 156, 13329, ..., 0, 0, 0], [ 101, 156, 13329, ..., 0, 0, 0], [ 101, 1302, 1251, ..., 0, 0, 0], ..., [ 101, 25456, 1200, ..., 0, 0, 0], [ 101, 143, 9664, ..., 0, 0, 0], [ 101, 2586, 7340, ..., 0, 0, 0]]) Followed by code below last_hidden_state, pooled_output = bert_model( input_ids=encoding['input_ids'], attention_mask=encoding['attention_mask'] ) Followed by code below last_hidden_state.shape Output is AttributeError Traceback (most recent call last) <ipython-input-70-9628339f425d> in <module> ----> 1 last_hidden_state.shape AttributeError: 'str' object has no attribute 'shape' Complete Code link is 'https://colab.research.google.com/drive/1FY4WtqCi2CQ9RjHj4slZwtdMhwaWv2-2?usp=sharing'",|python|string|pytorch|attributeerror|huggingface-transformers|,API,4
66600362,"RuntimeError: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle)` with GPU only. I'm working on the CNN with one-dimensional signal. It works totally fine with CPU device. However, when I training model in GPU, CUDA error occurred. I set os.environ['CUDA_LAUNCH_BLOCKING'] = ""1"" command after I got RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling cublasCreate(handle). With doing this, a cublasSgemm error occurred instead of cublasCreate error. Though the nvidia document doubt the hardware problem, I can training other CNN with images without any error. Below is my code for the data loading and set data in training model. idx = np.arange(len(dataset)) # dataset & label shuffle in once np.random.shuffle(idx) dataset = dataset[idx] sdnn = np.array(sdnn)[idx.astype(int)] train_data, val_data = dataset[:int(0.8 * len(dataset))], dataset[int(0.8 * len(dataset)):] train_label, val_label = sdnn[:int(0.8 * len(sdnn))], sdnn[int(0.8 * len(sdnn)):] train_set = DataLoader(dataset=train_data, batch_size=opt.batch_size, num_workers=opt.workers) for i, data in enumerate(train_set, 0): # data.shape = [batch_size, 3000(len(signal)), 1(channel)] tensor x = data.transpose(1, 2) label = torch.Tensor(train_label[i * opt.batch_size:i * opt.batch_size + opt.batch_size]) x = x.to(device, non_blocking=True) label = label.to(device, non_blocking=True) # [batch size] label = label.view([len(label), 1]) optim.zero_grad() # Feature of signal extract y_predict = model(x) # [batch size, fc3 output] # Error occurred HERE loss = mse(y_predict, label) Below is the error message from this code. File C:/Users/Me/Desktop/Me/Study/Project/Analysis/Regression/main.py"", line 217, in Processing y_predict = model(x) # [batch size, fc3 output] File ""C:\Anaconda\envs\torch\lib\site-packages\torch\nn\modules\module.py"", line 722, in _call_impl result = self.forward(*input, **kwargs) File ""C:\Users\ME\Desktop\ME\Study\Project\Analysis\Regression\cnn.py"", line 104, in forward x = self.fc1(x) File ""C:\Anaconda\envs\torch\lib\site-packages\torch\nn\modules\module.py"", line 722, in _call_impl result = self.forward(*input, **kwargs) File ""C:\Anaconda\envs\torch\lib\site-packages\torch\nn\modules\linear.py"", line 91, in forward return F.linear(input, self.weight, self.bias) File ""C:\Anaconda\envs\torch\lib\site-packages\torch\nn\functional.py"", line 1674, in linear ret = torch.addmm(bias, input, weight.t()) RuntimeError: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)` I've tried to solve this error for weeks but can't find the solution. If you can see anything wrong here, please let me know.",|python|pytorch|gpu|conv-neural-network|,GPU Usage,3
66625138,"GPU Issue Tensorflow 2.4.1. I would like to train a Tensorflow model by using my GPU I'm using : tensorboard 2.4.1 tensorboard-plugin-wit 1.8.0 tensorflow-estimator 2.4.0 tensorflow-gpu 2.4.1 cuda 11.0 cdnn 8.0.4 gpu RTX 3060 Laptop 6Gb Nvidia FrameView SDK 1.1.4923.29548709 Nvidia Graphics Drivers 461.72 Nvidia PhysX 9.19.0218 Python 3.8.5 IDE Spyder 4.2.1 OS Windows 10 LTSC-2019 (modified) What did I do before posting this help ? 1/ I've installed Nvidia Graphics Drivers 2/ I've followed this Tensorflow tutorial : https://www.tensorflow.org/install/gpu So I've copied cuda folder from cdnn download archive in C:\tools\ I've also added all variables required to Path 3/ Tried to train my model (all works if I'm using CPU instead) : with tf.device(""/GPU:0""): history = model.fit(images, imagesID, epochs=50, validation_split=0.2) Error : 2021-03-14 15:07:16.145096: E tensorflow/stream_executor/cuda/cuda_dnn.cc:336] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED 2021-03-14 15:07:16.145335: E tensorflow/stream_executor/cuda/cuda_dnn.cc:340] Error retrieving driver version: Unimplemented: kernel reported driver version not implemented on Windows 2021-03-14 15:07:16.146411: E tensorflow/stream_executor/cuda/cuda_dnn.cc:336] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED 2021-03-14 15:07:16.146595: E tensorflow/stream_executor/cuda/cuda_dnn.cc:340] Error retrieving driver version: Unimplemented: kernel reported driver version not implemented on Windows 2021-03-14 15:07:16.146845: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at conv_ops_fused_impl.h:697 : Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above. So I've found on the Internet this : https://github.com/tensorflow/tensorflow/issues/45779 Thus, I've implemented this code at the top to limit GPU memory : gpus = tf.config.experimental.list_physical_devices('GPU') if gpus: try: for gpu in gpus: tf.config.experimental.set_memory_growth(gpu, True) logical_gpus = tf.config.experimental.list_logical_devices('GPU') print(len(gpus), ""Physical GPUs,"", len(logical_gpus), ""Logical GPUs"") except RuntimeError as e: print(e) Error : Physical devices cannot be modified after being initialized So I've found this : https://github.com/tensorflow/tensorflow/issues/25138 from tensorflow.compat.v1 import ConfigProto from tensorflow.compat.v1 import InteractiveSession config = ConfigProto() config.gpu_options.per_process_gpu_memory_fraction = 0.2 config.gpu_options.allow_growth = True session = InteractiveSession(config=config) But I still have the same error : 2021-03-14 15:07:16.145096: E tensorflow/stream_executor/cuda/cuda_dnn.cc:336] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED ... I'm completely lost because I have a lack of knowledges about Tensorflow-GPU errors... Detail of all logs is here : https://pastebin.com/Xtsv3mLe I'm not very good at writing posts, I hope I was clear enough. Thank you in advance !!",|python|tensorflow|gpu|,GPU Usage,3
66623226,"New Tensorflow 2.4 GPU issues. Question Tensorflow 2.4.1 doesn't recognize my GPU, even though I followed the official instructions from Tensorflow as well as the ones from NVIDIA for CUDA and NVIDIA for cuDNN to install it in my computer. I also installed it in conda (which I'm not sure if it is needed?). When I try the official way to check if TF is using GPUs, I get 0: import tensorflow as tf print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU'))) Num GPUs Available: 0 Specifications Hardware: My NVIDIA fulfills the requirements specified by Tensorflow. Software I installed CUDA (with CUPTI) and cuDNN as mentioned above, so I got: ubuntu 20.04 LTS NVIDIA driver = 460.39 CUDA (+CUPTI) = 11.2 cuDNN = 8.1.1 In a conda environment I have: python = 3.8 tensorflow = 2.4.1 (which I understand is the new way of having the GPU support) and I installed extra the cudatoolkit==11.0 and cudnn==8.0 for conda as mentioned here. Procedure followed: It did not work when I didn't have the conda extra packages, and it still doesn't work even though I installed those extra packages.",|python|tensorflow|gpu|ubuntu-20.04|,GPU Usage,3
66602662,"Keras multi classifier is always giving 0 output. I built this keras model for multi classification model = tf.keras.Sequential([ tf.keras.layers.Dense(32, activation='relu'), tf.keras.layers.Dense(16, activation='relu'), tf.keras.layers.Dropout(0.2), tf.keras.layers.Dense(8, activation='relu'), tf.keras.layers.Dropout(0.2), tf.keras.layers.Dense(1, activation='sigmoid') ]) model.compile(optimizer='adam', loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True)) This is my data, normalized And these are my target values from 0 to 5 I used this code to prepare dataset train_dataset = tf.data.Dataset.from_tensor_slices((dff[:92000], target[:92000])) test_dataset = tf.data.Dataset.from_tensor_slices((dff[500:520])) # random slice of test dataset train_dataset = train_dataset.batch(100) test_dataset = test_dataset.batch(1) Then I trained and tested my model using this model.fit(train_dataset, epochs=20) predictions = model.predict(test_dataset) classes = np.argmax(predictions, axis = 1) classes output is always 0: [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0] I already tried without normalizing my data, but output was still same.",|python|tensorflow|keras|text-classification|multilabel-classification|,Model,0
66688358,"Not able to use Embedding Layer with tf.distribute.MirroredStrategy. I am trying to parallelize a model with embedding layer, on tensorflow version 2.4.1 . But it is throwing me the following error : InvalidArgumentError: Cannot assign a device for operation sequential/emb_layer/embedding_lookup/ReadVariableOp: Could not satisfy explicit device specification '' because the node {{colocation_node sequential/emb_layer/embedding_lookup/ReadVariableOp}} was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/device:GPU:0'. All available devices [/job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:XLA_CPU:0, /job:localhost/replica:0/task:0/device:XLA_GPU:0, /job:localhost/replica:0/task:0/device:GPU:0]. Colocation Debug Info: Colocation group had the following types and supported devices: Root Member(assigned_device_name_index_=2 requested_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' assigned_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' resource_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[] GatherV2: GPU CPU XLA_CPU XLA_GPU Cast: GPU CPU XLA_CPU XLA_GPU Const: GPU CPU XLA_CPU XLA_GPU ResourceSparseApplyAdagradV2: CPU _Arg: GPU CPU XLA_CPU XLA_GPU ReadVariableOp: GPU CPU XLA_CPU XLA_GPU Colocation members, user-requested devices, and framework assigned devices, if any: sequential_emb_layer_embedding_lookup_readvariableop_resource (_Arg) framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0 adagrad_adagrad_update_update_0_resourcesparseapplyadagradv2_accum (_Arg) framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0 sequential/emb_layer/embedding_lookup/ReadVariableOp (ReadVariableOp) sequential/emb_layer/embedding_lookup/axis (Const) sequential/emb_layer/embedding_lookup (GatherV2) gradient_tape/sequential/emb_layer/embedding_lookup/Shape (Const) gradient_tape/sequential/emb_layer/embedding_lookup/Cast (Cast) Adagrad/Adagrad/update/update_0/ResourceSparseApplyAdagradV2 (ResourceSparseApplyAdagradV2) /job:localhost/replica:0/task:0/device:GPU:0 [[{{node sequential/emb_layer/embedding_lookup/ReadVariableOp}}]] [Op:__inference_train_function_631] Simplified the model to a basic model to make it reproducible : import tensorflow as tf central_storage_strategy = tf.distribute.MirroredStrategy() with central_storage_strategy.scope(): user_model = tf.keras.Sequential([ tf.keras.layers.Embedding(10, 2, name = ""emb_layer"") ]) user_model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1), loss=""mse"") user_model.fit([1],[[1,2]], epochs=3) Any help will be highly appreciated. Thanks !",|python|tensorflow|tensorflow2.0|multi-gpu|,GPU Usage,3
66720543,"Pytorch 1D tensors expected but got 2D tensors. I have been working on making neural network from scratch in python. The input tensor is of shape [400,3] and target_tensor has the shape [400]. I am getting error during taking derivative of weights. Below are functions def sigmoid(z): return 1 / (1 + torch.exp(-z)) def nueral_net(data,weights,bias): return sigmoid( ( data @ weights ) + bias ) def loss_function(prediction,actual,m): return (-1/m) * (torch.sum(actual * torch.log(prediction) + (1-actual) * torch.log(1- prediction))) w = torch.randn(input_tensor.shape[1],1) b = torch.randn(1,1) predictions = nueral_net(input_tensor.float() , w, b) #Applying model loss = loss_function(predictions,target_tensor.unsqueeze(1),400) dw = (1/400) * torch.dot(input_tensor,(predictions - target_tensor).T) Running this throws an error. RuntimeError Traceback (most recent call last) <ipython-input-26-632338d8fd16> in <module> 1 predictions = nueral_net(input_tensor.float() , w, b) #Applying model 2 loss = loss_function(predictions,target_tensor.unsqueeze(1),400) ----> 3 dw = (1/400) * torch.dot(input_tensor,(predictions - target_tensor).T) 4 db = (1/400) * torch.sum(predictions - target_tensor) 5 #m = input_tensor.shape[0] RuntimeError: 1D tensors expected, but got 2D and 2D tensor",|python|machine-learning|neural-network|pytorch|tensor|,Tensors&Inputs,1
66801280,"GPU memory increasing at each batch (PyTorch). I am trying to build a convolutionnal network using ConvLSTM layer (LSTM cell but with convolutions instead of matrix multiplications), but the problem is that my GPU memory increases at each batch, even if I'm deleting variables, and getting the true value for the loss (and not the graph) for each iteration. I may be doing something wrong but that exact same script ran without issues with another model (with more parameters and also using ConvLSTM layer). Each batch is composed of num_batch x 3 images (grayscale) and I'm trying to predict the difference |Im(t+1)-Im(t)| with the input Im(t) def main(): config = Config() train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=config.batch_size, num_workers=0, shuffle=True, drop_last=True) nb_img = len(train_dataset) util.clear_progress_dir() step_tensorboard = 0 ################################### # Model Setup # ################################### model = fully_convLSTM() if torch.cuda.is_available(): model = model.float().cuda() lr = 0.001 optimizer = torch.optim.Adam(model.parameters(),lr=lr) util.enumerate_params([model]) ################################### # Training Loop # ################################### model.train() #Put model in training mode train_loss_recon = [] train_loss_recon2 = [] for epoch in tqdm(range(config.num_epochs)): running_loss1 = 0.0 running_loss2 = 0.0 for i, (inputs, outputs) in enumerate(train_dataloader, 0): print(i) torch.cuda.empty_cache() gc.collect() # if torch.cuda.is_available(): inputs = autograd.Variable(inputs.float()).cuda() outputs = autograd.Variable(outputs.float()).cuda() im1 = inputs[:,0,:,:,:] im2 = inputs[:,1,:,:,:] im3 = inputs[:,2,:,:,:] diff1 = torch.abs(im2 - im1).cuda().float() diff2 = torch.abs(im3 - im2).cuda().float() model.initialize_hidden() optimizer.zero_grad() pred1 = model.forward(im1) loss = reconstruction_loss(diff1, pred1) loss.backward() # optimizer.step() model.update_hidden() optimizer.zero_grad() pred2 = model.forward(im2) loss2 = reconstruction_loss(diff2, pred2) loss2.backward() optimizer.step() model.update_hidden() ## print statistics running_loss1 += loss.detach().data running_loss2 += loss2.detach().data if i==0: with torch.no_grad(): img_grid_diff_true = (diff2).cpu() img_grid_diff_pred = (pred2).cpu() f, axes = plt.subplots(2, 4, figsize=(48,48)) for l in range(4): axes[0, l].imshow(img_grid_diff_true[l].squeeze(0).squeeze(0), cmap='gray') axes[1, l].imshow(img_grid_diff_pred[l].squeeze(0).squeeze(0), cmap='gray') plt.show() plt.close() writer_recon_loss.add_scalar('Reconstruction loss', running_loss1, step_tensorboard) writer_recon_loss2.add_scalar('Reconstruction loss2', running_loss2, step_tensorboard) step_tensorboard += 1 del pred1 del pred2 del im1 del im2 del im3 del diff1 del diff2#, im1_noised, im2_noised del inputs del outputs del loss del loss2 for obj in gc.get_objects(): if torch.is_tensor(obj) : del obj torch.cuda.empty_cache() gc.collect() epoch_loss = running_loss1 / len(train_dataloader.dataset) epoch_loss2 = running_loss2/ len(train_dataloader.dataset) print(f""Epoch {epoch} loss reconstruction1: {epoch_loss:.6f}"") print(f""Epoch {epoch} loss reconstruction2: {epoch_loss2:.6f}"") train_loss_recon.append(epoch_loss) train_loss_recon2.append(epoch_loss2) del running_loss1, running_loss2, epoch_loss, epoch_loss2 Here is the model used : class ConvLSTMCell(nn.Module): def __init__(self, input_channels, hidden_channels, kernel_size): super(ConvLSTMCell, self).__init__() # assert hidden_channels % 2 == 0 self.input_channels = input_channels self.hidden_channels = hidden_channels self.kernel_size = kernel_size # self.num_features = 4 self.padding = 1 self.Wxi = nn.Conv2d(self.input_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=True) self.Whi = nn.Conv2d(self.hidden_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=False) self.Wxf = nn.Conv2d(self.input_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=True) self.Whf = nn.Conv2d(self.hidden_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=False) self.Wxc = nn.Conv2d(self.input_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=True) self.Whc = nn.Conv2d(self.hidden_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=False) self.Wxo = nn.Conv2d(self.input_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=True) self.Who = nn.Conv2d(self.hidden_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=False) self.Wci = None self.Wcf = None self.Wco = None def forward(self, x, h, c): ## Equation (3) dans Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting ci = torch.sigmoid(self.Wxi(x) + self.Whi(h) + c * self.Wci) cf = torch.sigmoid(self.Wxf(x) + self.Whf(h) + c * self.Wcf) cc = cf * c + ci * torch.tanh(self.Wxc(x) + self.Whc(h)) ###gt= tanh(cc) co = torch.sigmoid(self.Wxo(x) + self.Who(h) + cc * self.Wco) ##channel out = hidden channel ch = co * torch.tanh(cc) return ch, cc #short memory, long memory def init_hidden(self, batch_size, hidden, shape): if self.Wci is None: self.Wci = nn.Parameter(torch.zeros(1, hidden, shape[0], shape[1])).cuda() self.Wcf = nn.Parameter(torch.zeros(1, hidden, shape[0], shape[1])).cuda() self.Wco = nn.Parameter(torch.zeros(1, hidden, shape[0], shape[1])).cuda() else: assert shape[0] == self.Wci.size()[2], 'Input Height Mismatched!' assert shape[1] == self.Wci.size()[3], 'Input Width Mismatched!' return (autograd.Variable(torch.zeros(batch_size, hidden, shape[0], shape[1])).cuda(), autograd.Variable(torch.zeros(batch_size, hidden, shape[0], shape[1])).cuda()) class fully_convLSTM(nn.Module): def __init__(self): super(fully_convLSTM, self).__init__() layers = [] self.hidden_list = [1,32,32,1]#,32,64,32, for k in range(len(self.hidden_list)-1): # Define blocks of [ConvLSTM,BatchNorm,Relu] name_conv = ""self.convLSTM"" +str(k) cell_conv = ConvLSTMCell(self.hidden_list[k],self.hidden_list[k+1],3) setattr(self, name_conv, cell_conv) name_batchnorm = ""self.batchnorm""+str(k) batchnorm=nn.BatchNorm2d(self.hidden_list[k+1]) setattr(self, name_batchnorm, batchnorm) name_relu ="" self.relu""+str(k) relu=nn.ReLU() setattr(self, name_relu, relu) self.sigmoid = nn.Sigmoid() self.internal_state=[] def initialize_hidden(self): for k in range(len(self.hidden_list)-1): name_conv = ""self.convLSTM"" +str(k) (h,c) = getattr(self,name_conv).init_hidden(config.batch_size, self.hidden_list[k+1],(256,256)) self.internal_state.append((h,c)) self.internal_state_new=[] def update_hidden(self): for i, hidden in enumerate(self.internal_state_new): self.internal_state[i] = (hidden[0].detach(), hidden[1].detach()) self.internal_state_new = [] def forward(self, input): x = input for k in range(len(self.hidden_list)-1): name_conv = ""self.convLSTM"" +str(k) name_batchnorm = ""self.batchnorm""+str(k) name_relu ="" self.relu""+str(k) x, c = getattr(self,name_conv)(x, self.internal_state[k][1], self.internal_state[k][0]) self.internal_state_new.append((x.detach(),c.detach())) x = getattr(self,name_batchnorm)(x) if k!= len(self.hidden_list)-2: x = getattr(self,name_relu)(x) else : x = self.sigmoid(x) return x So my question is, what in my code is causing memory to accumulate during the training phase?",|pytorch|out-of-memory|gpu|conv-neural-network|lstm|,GPU Usage,3
66840108,"Keras producing model with no accuracy. I have the following code for training a model based off of some numbers: from numpy import loadtxt import numpy as np from keras.models import Sequential from keras.layers import Dense from time import sleep dataset = loadtxt(""data.csv"", delimiter="","") X = dataset[:,0:2] y = dataset[:,2] model = Sequential() model.add(Dense(196, input_dim=2, activation='relu')) model.add(Dense(128, activation='relu')) model.add(Dense(1, activation='sigmoid')) model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) model.fit(X, y, epochs=600, batch_size=10) _, accuracy = model.evaluate(X, y) print('Accuracy: %.2f' % (accuracy*100)) For reference, here is some of the data it is being presented with: 433,866,1299,1732 421,842,1263,1684 443,886,1329,1772 142,284,426,568 437,874,1311,1748 455,910,1365,1820 172,344,516,688 219,438,657,876 101,202,303,404 289,578,867,1156 110,220,330,440 421,842,1263,1684 472,944,1416,1888 121,242,363,484 215,430,645,860 134,268,402,536 488,976,1464,1952 467,934,1401,1868 418,836,1254,1672 134,268,402,536 241,482,723,964 116,232,348,464 395,790,1185,1580 438,876,1314,1752 396,792,1188,1584 57,114,171,228 218,436,654,872 372,744,1116,1488 305,610,915,1220 462,924,1386,1848 455,910,1365,1820 42,84,126,168 347,694,1041,1388 394,788,1182,1576 184,368,552,736 302,604,906,1208 326,652,978,1304 333,666,999,1332 335,670,1005,1340 176,352,528,704 168,336,504,672 62,124,186,248 26,52,78,104 335,670,1005,1340 (The first three numbers should be inputs, and the last one an output) The Keras program keeps training but only warrants an accuracy of 0. What am I doing wrong?",|python|machine-learning|keras|,Model,0
66900639,"Tensorflow stuck on first epoch. My execution is stuck on the following step I got GTX 3090 on Python 3.7 on windows 10. I installed cuda, cudnn and tensorflow-gpu using conda as instructed from here conda install tensorflow-gpu=2.3 tensorflow=2.3=mkl_py37h936c3e2_0 Can someone please tell me why is it stuck at this step? 2021-04-01 10:09:38.147949: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll Warning, `split` argument is replaced by `split_val`, please condider to change your source code.The `split` argument will be removed in future releases. class 0, validation count: 68, train count: 159 class 5, validation count: 83, train count: 195 Total data: 2 classes for 354 files for train Total data: 2 classes for 151 files for validation 2021-04-01 10:09:40.046207: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll 2021-04-01 10:09:40.763173: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: pciBusID: 0000:44:00.0 name: NVIDIA GeForce RTX 3090 computeCapability: 8.6 coreClock: 1.755GHz coreCount: 82 deviceMemorySize: 24.00GiB deviceMemoryBandwidth: 871.81GiB/s 2021-04-01 10:09:40.763755: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties: pciBusID: 0000:01:00.0 name: NVIDIA Quadro P1000 computeCapability: 6.1 coreClock: 1.5185GHz coreCount: 4 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 89.53GiB/s 2021-04-01 10:09:40.764222: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll 2021-04-01 10:09:40.768538: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll 2021-04-01 10:09:40.772484: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll 2021-04-01 10:09:40.774033: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll 2021-04-01 10:09:40.778690: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll 2021-04-01 10:09:40.781972: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll 2021-04-01 10:09:40.793304: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll 2021-04-01 10:09:40.793720: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1843] Ignoring visible gpu device (device: 1, name: NVIDIA Quadro P1000, pci bus id: 0000:01:00.0, compute capability: 6.1) with core count: 4. The minimum required count is 8. You can adjust this requirement with the env var TF_MIN_GPU_MULTIPROCESSOR_COUNT. 2021-04-01 10:09:40.794339: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0 2021-04-01 10:09:40.795239: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX AVX2 To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-04-01 10:09:40.808243: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2af71215370 initialized for platform Host (this does not guarantee that XLA will be used). Devices: 2021-04-01 10:09:40.808648: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version 2021-04-01 10:09:40.809155: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: pciBusID: 0000:44:00.0 name: NVIDIA GeForce RTX 3090 computeCapability: 8.6 coreClock: 1.755GHz coreCount: 82 deviceMemorySize: 24.00GiB deviceMemoryBandwidth: 871.81GiB/s 2021-04-01 10:09:40.809789: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll 2021-04-01 10:09:40.810112: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll 2021-04-01 10:09:40.810416: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll 2021-04-01 10:09:40.810728: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll 2021-04-01 10:09:40.811035: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll 2021-04-01 10:09:40.811338: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll 2021-04-01 10:09:40.811624: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll 2021-04-01 10:09:40.811974: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0 2021-04-01 10:09:42.137546: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix: 2021-04-01 10:09:42.137800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263] 0 2021-04-01 10:09:42.137935: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0: N 2021-04-01 10:09:42.138228: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 19112 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:44:00.0, compute capability: 8.6) 2021-04-01 10:09:42.141792: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2af27c8f640 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices: 2021-04-01 10:09:42.142075: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6 Epoch 1/1000 2021-04-01 10:09:46.249137: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll 2021-04-01 10:09:47.132659: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll And here my dependencies # Name Version Build Channel _tflow_select 2.3.0 gpu absl-py 0.12.0 pyhd8ed1ab_0 conda-forge aiohttp 3.7.4 py37hcc03f2d_0 conda-forge astor 0.8.1 pyh9f0ad1d_0 conda-forge astunparse 1.6.3 pyhd8ed1ab_0 conda-forge async-timeout 3.0.1 py_1000 conda-forge attrs 20.3.0 pyhd3deb0d_0 conda-forge blinker 1.4 py_1 conda-forge brotlipy 0.7.0 py37hcc03f2d_1001 conda-forge ca-certificates 2020.12.5 h5b45459_0 conda-forge cachetools 4.2.1 pyhd8ed1ab_0 conda-forge certifi 2020.12.5 py37h03978a9_1 conda-forge cffi 1.14.5 py37hd8e9650_0 conda-forge chardet 4.0.0 py37h03978a9_1 conda-forge click 7.1.2 pyh9f0ad1d_0 conda-forge cryptography 3.4.7 py37h20c650d_0 conda-forge cudatoolkit 10.1.243 h3826478_8 conda-forge cudnn 7.6.5.32 h36d860d_1 conda-forge cycler 0.10.0 pypi_0 pypi gast 0.3.3 py_0 conda-forge google-auth 1.28.0 pyh44b312d_0 conda-forge google-auth-oauthlib 0.4.1 py_2 conda-forge google-pasta 0.2.0 pyh8c360ce_0 conda-forge grpcio 1.36.1 py37h04d2302_0 conda-forge h5py 2.10.0 nompi_py37h23cfb99_105 conda-forge hdf5 1.10.6 nompi_h5268f04_1114 conda-forge idna 2.10 pyh9f0ad1d_0 conda-forge importlib-metadata 3.10.0 py37h03978a9_0 conda-forge intel-openmp 2020.3 h57928b3_311 conda-forge keras 2.4.3 py_0 conda-forge keras-applications 1.0.8 py_1 conda-forge keras-preprocessing 1.1.2 pyhd8ed1ab_0 conda-forge keras-video-generators 1.0.14 pypi_0 pypi kiwisolver 1.3.1 pypi_0 pypi krb5 1.17.2 hbae68bd_0 conda-forge libblas 3.9.0 8_mkl conda-forge libcblas 3.9.0 8_mkl conda-forge libcurl 7.76.0 hf1763fc_0 conda-forge liblapack 3.9.0 8_mkl conda-forge libprotobuf 3.15.6 h7755175_0 conda-forge libssh2 1.9.0 h680486a_6 conda-forge m2w64-gcc-libgfortran 5.3.0 6 conda-forge m2w64-gcc-libs 5.3.0 7 conda-forge m2w64-gcc-libs-core 5.3.0 7 conda-forge m2w64-gmp 6.1.0 2 conda-forge m2w64-libwinpthread-git 5.0.0.4634.697f757 2 conda-forge markdown 3.3.4 pyhd8ed1ab_0 conda-forge matplotlib 3.4.1 pypi_0 pypi mkl 2020.4 hb70f87d_311 conda-forge msys2-conda-epoch 20160418 1 conda-forge multidict 5.1.0 py37hcc03f2d_1 conda-forge numpy 1.19.5 py37hd20adf4_1 conda-forge oauthlib 3.0.1 py_0 conda-forge opencv-python 4.5.1.48 pypi_0 pypi openssl 1.1.1k h8ffe710_0 conda-forge opt_einsum 3.3.0 py_0 conda-forge pillow 8.1.2 pypi_0 pypi pip 21.0.1 pyhd8ed1ab_0 conda-forge protobuf 3.15.6 py37hf2a7229_0 conda-forge pyasn1 0.4.8 py_0 conda-forge pyasn1-modules 0.2.7 py_0 conda-forge pycparser 2.20 pyh9f0ad1d_2 conda-forge pyjwt 2.0.1 pyhd8ed1ab_1 conda-forge pyopenssl 20.0.1 pyhd8ed1ab_0 conda-forge pyparsing 2.4.7 pypi_0 pypi pyreadline 2.1 py37h03978a9_1003 conda-forge pysocks 1.7.1 py37h03978a9_3 conda-forge python 3.7.10 h7840368_100_cpython conda-forge python-dateutil 2.8.1 pypi_0 pypi python_abi 3.7 1_cp37m conda-forge pyyaml 5.4.1 pypi_0 pypi requests 2.25.1 pyhd3deb0d_0 conda-forge requests-oauthlib 1.3.0 pyh9f0ad1d_0 conda-forge rsa 4.7.2 pyh44b312d_0 conda-forge scipy 1.6.2 py37h924764e_0 conda-forge setuptools 49.6.0 py37h03978a9_3 conda-forge six 1.15.0 pyh9f0ad1d_0 conda-forge sqlite 3.35.3 h8ffe710_0 conda-forge tensorboard 2.4.1 pyhd8ed1ab_0 conda-forge tensorboard-plugin-wit 1.8.0 pyh44b312d_0 conda-forge tensorflow 2.3.0 mkl_py37h936c3e2_0 tensorflow-base 2.3.0 gpu_py37h18d21e4_0 tensorflow-estimator 2.4.0 pyh9656e83_0 conda-forge tensorflow-gpu 2.3.0 he13fc11_0 termcolor 1.1.0 py_2 conda-forge tk 8.6.10 h8ffe710_1 conda-forge typing-extensions 3.7.4.3 0 conda-forge typing_extensions 3.7.4.3 py_0 conda-forge urllib3 1.26.4 pyhd8ed1ab_0 conda-forge vc 14.2 hb210afc_4 conda-forge vs2015_runtime 14.28.29325 h5e1d092_4 conda-forge werkzeug 1.0.1 pyh9f0ad1d_0 conda-forge wheel 0.36.2 pyhd3deb0d_0 conda-forge win_inet_pton 1.1.0 py37h03978a9_2 conda-forge wincertstore 0.2 py37h03978a9_1006 conda-forge wrapt 1.12.1 py37hcc03f2d_3 conda-forge yaml 0.2.5 he774522_0 conda-forge yarl 1.6.3 py37hcc03f2d_1 conda-forge zipp 3.4.1 pyhd8ed1ab_0 conda-forge zlib 1.2.11 h62dcd97_1010 conda-forge UPDATE: The same exact code worked on the 2080TI, I switched it with the 3090 which I just got and did a fresh environment and uninstalled all 2080TI related software. Here is model Total params: 5,502,338 Trainable params: 5,500,418 Non-trainable params: 1,920 I also added CUDA_CACHE_MAXSIZE=2147483648 to my enviornment variables.",|python|tensorflow|anaconda|gpu|conda|,GPU Usage,3
67045622,"tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. I am trying to use GPU with Tensorflow. My Tensorflow version is 2.4.1 and I am using Cuda version 11.2. Here is the output of nvidia-smi. +-----------------------------------------------------------------------------+ | NVIDIA-SMI 460.39 Driver Version: 460.39 CUDA Version: 11.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 GeForce MX110 Off | 00000000:01:00.0 Off | N/A | | N/A 52C P0 N/A / N/A | 254MiB / 2004MiB | 8% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | 0 N/A N/A 1151 G /usr/lib/xorg/Xorg 37MiB | | 0 N/A N/A 1654 G /usr/lib/xorg/Xorg 136MiB | | 0 N/A N/A 1830 G /usr/bin/gnome-shell 68MiB | | 0 N/A N/A 5443 G /usr/lib/firefox/firefox 0MiB | | 0 N/A N/A 5659 G /usr/lib/firefox/firefox 0MiB | +-----------------------------------------------------------------------------+ I am facing a strange issue. Previously when I was trying to list all the physical devices using tf.config.list_physical_devices() it was identifying one cpu and one gpu. AFter that I tried to do a simple matrix multiplication on the GPU. It failed with this error : failed to synchronize cuda stream CUDA_LAUNCH_ERROR (the error code was something like that, I forgot to note it). But after that when I again tried the same thing from another terminal, it failed to recognise any GPU. This time, listing physical devices produce this: >>> tf.config.list_physical_devices() 2021-04-11 18:56:47.504776: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set 2021-04-11 18:56:47.507646: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1 2021-04-11 18:56:47.534189: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error 2021-04-11 18:56:47.534233: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: debadri-HP-Laptop-15g-dr0xxx 2021-04-11 18:56:47.534244: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: debadri-HP-Laptop-15g-dr0xxx 2021-04-11 18:56:47.534356: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 460.39.0 2021-04-11 18:56:47.534393: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 460.39.0 2021-04-11 18:56:47.534404: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 460.39.0 [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')] My OS is Ubuntu 20.04, Python version 3.8.5 and Tensorflow , as mentioned before 2.4.1 with Cuda version 11.2. I installed cuda from these instructions. One additional piece of information; when I import tensorflow , it shows the following output: import tensorflow as tf 2021-04-11 18:56:07.716683: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0 What am I missing? Why is it failing to recognise the GPU even though it was recognising previously?",|python-3.x|gpu|tensorflow2.0|nvidia|,GPU Usage,3
67519746,"Pytorch transfer learning error: The size of tensor a (16) must match the size of tensor b (128) at non-singleton dimension 2. Currently, I'm working on an image motion deblurring problem with PyTorch. I have two kinds of images: Blurry images (variable = blur_image) that are the input image and the sharp version of the same images (variable = shar_image), which should be the output. Now I wanted to try out transfer learning, but I can't get it to work. Here is the code for my dataloaders: train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle = True) validation_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle = False) test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle = False) Their shape: Trainloader - Shape of blur_image [N, C, H, W]: torch.Size([16, 3, 128, 128]) Trainloader - Shape of sharp_image [N, C, H, W]: torch.Size([16, 3, 128, 128]) torch.float32 Validationloader - Shape of blur_image [N, C, H, W]: torch.Size([16, 3, 128, 128]) Validationloader - Shape of sharp_image [N, C, H, W]: torch.Size([16, 3, 128, 128]) torch.float32 Testloader- Shape of blur_image [N, C, H, W]: torch.Size([16, 3, 128, 128]) Testloader- Shape of sharp_image [N, C, H, W]: torch.Size([16, 3, 128, 128]) torch.float32 The way I use transfer learning (I thought that for the 'in_features' I have to put in the amount of pixels): model = models.alexnet(pretrained=True) model.classifier[6] = torch.nn.Linear(model.classifier[6].in_features, 128) device_string = ""cuda"" if torch.cuda.is_available() else ""cpu"" device = torch.device(device_string) model = model.to(device) The way I define my training process: # Define the loss function (MSE was chosen due to the comparsion of pixels # between blurred and sharp images criterion = nn.MSELoss() # Define the optimizer and learning rate optimizer = optim.Adam(model.parameters(), lr=0.001) # Learning rate schedule - If the loss value does not improve after 5 epochs # back-to-back then the new learning rate will be: previous_rate*0.5 #scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1) scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau( optimizer, mode='min', patience=5, factor=0.5, verbose=True ) def training(model, trainDataloader, epoch): """""" Function to define the model training Args: model (Model object): The model that is going to be trained. trainDataloader (Dataloader object): Dataloader object of the trainset. epoch (Integer): Number of training epochs. """""" # Changing model into trainings mode model.train() # Supporting variable to display the loss for each epoch running_loss = 0.0 running_psnr = 0.0 for i, data in tqdm(enumerate(trainDataloader), total=int(len(train_dataset)/trainDataloader.batch_size)): blur_image = data[0] sharp_image = data[1] # Transfer the blurred and sharp image instance to the device blur_image = blur_image.to(device) sharp_image = sharp_image.to(device) # Sets the gradient of tensors to zero optimizer.zero_grad() outputs = model(blur_image) loss = criterion(outputs, sharp_image) # Perform backpropagation loss.backward() # Update the weights optimizer.step() # Add the loss that was calculated during the trainigs run running_loss += loss.item() # calculate batch psnr (once every `batch_size` iterations) batch_psnr = psnr(sharp_image, blur_image) running_psnr += batch_psnr # Display trainings loss trainings_loss = running_loss/len(trainDataloader.dataset) final_psnr = running_psnr/int(len(train_dataset)/trainDataloader.batch_size) final_ssim = ssim(sharp_image, blur_image, data_range=1, size_average=True) print(f""Trainings loss: {trainings_loss:.5f}"") print(f""Train PSNR: {final_psnr:.5f}"") print(f""Train SSIM: {final_ssim:.5f}"") return trainings_loss, final_psnr, final_ssim And here is my way to start the training: train_loss = [] val_loss = [] train_PSNR_score = [] train_SSIM_score = [] val_PSNR_score = [] val_SSIM_score = [] start = time.time() for epoch in range(nb_epochs): print(f""Epoch {epoch+1}\n-------------------------------"") train_epoch_loss = training(model, train_loader, nb_epochs) val_epoch_loss = validation(model, validation_loader, nb_epochs) train_loss.append(train_epoch_loss[0]) val_loss.append(val_epoch_loss[0]) train_PSNR_score.append(train_epoch_loss[1]) train_SSIM_score.append(train_epoch_loss[2]) val_PSNR_score.append(val_epoch_loss[1]) val_SSIM_score.append(val_epoch_loss[2]) scheduler.step(train_epoch_loss[0]) scheduler.step(val_epoch_loss[0]) end = time.time() print(f""Took {((end-start)/60):.3f} minutes to train"") But every time when I want to perform the training I receive the following error: 0%| | 0/249 [00:00<?, ?it/s]Epoch 1 ------------------------------- /usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([16, 3, 128, 128])) that is different to the input size (torch.Size([16, 128])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size. return F.mse_loss(input, target, reduction=self.reduction) --------------------------------------------------------------------------- RuntimeError Traceback (most recent call last) <ipython-input-195-ff0214e227cd> in <module>() 9 for epoch in range(nb_epochs): 10 print(f""Epoch {epoch+1}\n-------------------------------"") ---> 11 train_epoch_loss = training(model, train_loader, nb_epochs) 12 val_epoch_loss = validation(model, validation_loader, nb_epochs) 13 train_loss.append(train_epoch_loss[0]) <ipython-input-170-dfa2c212ad23> in training(model, trainDataloader, epoch) 25 optimizer.zero_grad() 26 outputs = model(blur_image) ---> 27 loss = criterion(outputs, sharp_image) 28 29 # Perform backpropagation /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs) 887 result = self._slow_forward(*input, **kwargs) 888 else: --> 889 result = self.forward(*input, **kwargs) 890 for hook in itertools.chain( 891 _global_forward_hooks.values(), /usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py in forward(self, input, target) 526 527 def forward(self, input: Tensor, target: Tensor) -> Tensor: --> 528 return F.mse_loss(input, target, reduction=self.reduction) 529 530 /usr/local/lib/python3.7/dist-packages/torch/nn/functional.py in mse_loss(input, target, size_average, reduce, reduction) 2926 reduction = _Reduction.legacy_get_string(size_average, reduce) 2927 -> 2928 expanded_input, expanded_target = torch.broadcast_tensors(input, target) 2929 return torch._C._nn.mse_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction)) 2930 /usr/local/lib/python3.7/dist-packages/torch/functional.py in broadcast_tensors(*tensors) 72 if has_torch_function(tensors): 73 return handle_torch_function(broadcast_tensors, tensors, *tensors) ---> 74 return _VF.broadcast_tensors(tensors) # type: ignore 75 76 RuntimeError: The size of tensor a (16) must match the size of tensor b (128) at non-singleton dimension 2 model structure: AlexNet( (features): Sequential( (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2)) (1): ReLU(inplace=True) (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)) (4): ReLU(inplace=True) (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (7): ReLU(inplace=True) (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (9): ReLU(inplace=True) (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (11): ReLU(inplace=True) (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) ) (avgpool): AdaptiveAvgPool2d(output_size=(6, 6)) (classifier): Sequential( (0): Dropout(p=0.5, inplace=False) (1): Linear(in_features=9216, out_features=4096, bias=True) (2): ReLU(inplace=True) (3): Dropout(p=0.5, inplace=False) (4): Linear(in_features=4096, out_features=4096, bias=True) (5): ReLU(inplace=True) (6): Linear(in_features=4096, out_features=128, bias=True) ) ) I'm a newbie in terms of using Pytorch (and image deblurring in general) and so I rather confused about the meaning of the error message and how to fix it. I tried to change my parameters and nothing worked. Does anyone have any advice for me on how to solve this problem? I would appreciate every input :)",|python|image-processing|pytorch|tensor|motion-blur|,Tensors&Inputs,1
67529687,"Accuracy and loss do not improve CNN model. I am working on diabetic retinopathy , it's my first project machine learning deep learning . I am using this dataset: https://www.kaggle.com/sovitrath/diabetic-retinopathy-2015-data-colored-resized. To start i want to classify the DR to 2 classes : YES_DR , NO_DR to balance data i augmented the 2 classes. NO_DR from 25k to 50k all lvl of YES_DR from 9k to 50k. I tried to use small cnn-architecture and large one its always the same results the accuracy-val_acc and loss-val_loss fun not changing graph plot : Results : WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen. WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen. Epoch 1/100 /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators. warnings.warn('`Model.fit_generator` is deprecated and ' 100/100 [==============================] - 37s 368ms/step - loss: 2.0042 - precision: 0.4906 - auc: 0.4806 - accuracy: 0.4947 - false_negatives: 1742.0000 - false_positives: 1514.0000 - true_negatives: 1686.0000 - true_positives: 1458.0000 - val_loss: 0.6931 - val_precision: 0.5069 - val_auc: 0.5000 - val_accuracy: 0.5069 - val_false_negatives: 1578.0000 - val_false_positives: 1578.0000 - val_true_negatives: 1622.0000 - val_true_positives: 1622.0000 Epoch 00001: val_accuracy improved from -inf to 0.50687, saving model to mymodel.h5 Epoch 2/100 100/100 [==============================] - 34s 343ms/step - loss: 0.6923 - precision: 0.5256 - auc: 0.5279 - accuracy: 0.5256 - false_negatives: 1518.0000 - false_positives: 1518.0000 - true_negatives: 1682.0000 - true_positives: 1682.0000 - val_loss: 0.6928 - val_precision: 0.5150 - val_auc: 0.5150 - val_accuracy: 0.5150 - val_false_negatives: 1552.0000 - val_false_positives: 1552.0000 - val_true_negatives: 1648.0000 - val_true_positives: 1648.0000 Epoch 00002: val_accuracy improved from 0.50687 to 0.51500, saving model to mymodel.h5 Epoch 3/100 100/100 [==============================] - 34s 337ms/step - loss: 0.6926 - precision: 0.5213 - auc: 0.5151 - accuracy: 0.5213 - false_negatives: 1532.0000 - false_positives: 1532.0000 - true_negatives: 1668.0000 - true_positives: 1668.0000 - val_loss: 0.6934 - val_precision: 0.5125 - val_auc: 0.5125 - val_accuracy: 0.5125 - val_false_negatives: 1560.0000 - val_false_positives: 1560.0000 - val_true_negatives: 1640.0000 - val_true_positives: 1640.0000 Epoch 00003: val_accuracy did not improve from 0.51500 Epoch 4/100 100/100 [==============================] - 34s 338ms/step - loss: 0.6934 - precision: 0.5131 - auc: 0.5077 - accuracy: 0.5131 - false_negatives: 1558.0000 - false_positives: 1558.0000 - true_negatives: 1642.0000 - true_positives: 1642.0000 - val_loss: 0.6919 - val_precision: 0.5284 - val_auc: 0.5411 - val_accuracy: 0.5284 - val_false_negatives: 1509.0000 - val_false_positives: 1509.0000 - val_true_negatives: 1691.0000 - val_true_positives: 1691.0000 Epoch 00004: val_accuracy improved from 0.51500 to 0.52844, saving model to mymodel.h5 Epoch 5/100 100/100 [==============================] - 34s 342ms/step - loss: 0.6932 - precision: 0.5105 - auc: 0.4990 - accuracy: 0.5109 - false_negatives: 1553.0000 - false_positives: 1579.0000 - true_negatives: 1621.0000 - true_positives: 1647.0000 - val_loss: 0.6929 - val_precision: 0.5113 - val_auc: 0.5113 - val_accuracy: 0.5113 - val_false_negatives: 1564.0000 - val_false_positives: 1564.0000 - val_true_negatives: 1636.0000 - val_true_positives: 1636.0000 Epoch 00005: val_accuracy did not improve from 0.52844 Epoch 6/100 100/100 [==============================] - 35s 351ms/step - loss: 0.6927 - precision: 0.5169 - auc: 0.5115 - accuracy: 0.5169 - false_negatives: 1546.0000 - false_positives: 1546.0000 - true_negatives: 1654.0000 - true_positives: 1654.0000 - val_loss: 0.6921 - val_precision: 0.5256 - val_auc: 0.5256 - val_accuracy: 0.5256 - val_false_negatives: 1518.0000 - val_false_positives: 1518.0000 - val_true_negatives: 1682.0000 - val_true_positives: 1682.0000 Epoch 00006: val_accuracy did not improve from 0.52844 Epoch 7/100 100/100 [==============================] - 34s 343ms/step - loss: 0.6927 - precision: 0.5203 - auc: 0.5140 - accuracy: 0.5203 - false_negatives: 1535.0000 - false_positives: 1535.0000 - true_negatives: 1665.0000 - true_positives: 1665.0000 - val_loss: 0.6929 - val_precision: 0.5131 - val_auc: 0.5131 - val_accuracy: 0.5131 - val_false_negatives: 1558.0000 - val_false_positives: 1558.0000 - val_true_negatives: 1642.0000 - val_true_positives: 1642.0000 Epoch 00007: val_accuracy did not improve from 0.52844 Epoch 8/100 100/100 [==============================] - 34s 338ms/step - loss: 0.6932 - precision: 0.5116 - auc: 0.5083 - accuracy: 0.5116 - false_negatives: 1563.0000 - false_positives: 1563.0000 - true_negatives: 1637.0000 - true_positives: 1637.0000 - val_loss: 0.6926 - val_precision: 0.5169 - val_auc: 0.5169 - val_accuracy: 0.5169 - val_false_negatives: 1546.0000 - val_false_positives: 1546.0000 - val_true_negatives: 1654.0000 - val_true_positives: 1654.0000 Epoch 00008: val_accuracy did not improve from 0.52844 Epoch 9/100 100/100 [==============================] - 34s 338ms/step - loss: 0.6925 - precision: 0.5197 - auc: 0.5165 - accuracy: 0.5197 - false_negatives: 1537.0000 - false_positives: 1537.0000 - true_negatives: 1663.0000 - true_positives: 1663.0000 - val_loss: 0.6924 - val_precision: 0.5200 - val_auc: 0.5200 - val_accuracy: 0.5200 - val_false_negatives: 1536.0000 - val_false_positives: 1536.0000 - val_true_negatives: 1664.0000 - val_true_positives: 1664.0000 Epoch 00009: val_accuracy did not improve from 0.52844 Epoch 10/100 100/100 [==============================] - 34s 337ms/step - loss: 0.6921 - precision: 0.5242 - auc: 0.5159 - accuracy: 0.5238 - false_negatives: 1521.0000 - false_positives: 1524.0000 - true_negatives: 1676.0000 - true_positives: 1679.0000 - val_loss: 0.6916 - val_precision: 0.5275 - val_auc: 0.5370 - val_accuracy: 0.5275 - val_false_negatives: 1512.0000 - val_false_positives: 1512.0000 - val_true_negatives: 1688.0000 - val_true_positives: 1688.0000 Epoch 00010: val_accuracy did not improve from 0.52844 Epoch 11/100 100/100 [==============================] - 34s 337ms/step - loss: 0.6931 - precision: 0.5102 - auc: 0.5135 - accuracy: 0.5097 - false_negatives: 1555.0000 - false_positives: 1579.0000 - true_negatives: 1621.0000 - true_positives: 1645.0000 - val_loss: 0.6933 - val_precision: 0.5116 - val_auc: 0.5116 - val_accuracy: 0.5116 - val_false_negatives: 1563.0000 - val_false_positives: 1563.0000 - val_true_negatives: 1637.0000 - val_true_positives: 1637.0000 Epoch 00011: val_accuracy did not improve from 0.52844 Epoch 12/100 100/100 [==============================] - 34s 337ms/step - loss: 0.6928 - precision: 0.5166 - auc: 0.5175 - accuracy: 0.5166 - false_negatives: 1547.0000 - false_positives: 1547.0000 - true_negatives: 1653.0000 - true_positives: 1653.0000 - val_loss: 0.6919 - val_precision: 0.5275 - val_auc: 0.5275 - val_accuracy: 0.5275 - val_false_negatives: 1512.0000 - val_false_positives: 1512.0000 - val_true_negatives: 1688.0000 - val_true_positives: 1688.0000 Epoch 00012: val_accuracy did not improve from 0.52844 Epoch 13/100 100/100 [==============================] - 34s 337ms/step - loss: 0.6925 - precision: 0.5203 - auc: 0.5132 - accuracy: 0.5203 - false_negatives: 1535.0000 - false_positives: 1535.0000 - true_negatives: 1665.0000 - true_positives: 1665.0000 - val_loss: 0.6927 - val_precision: 0.5153 - val_auc: 0.5153 - val_accuracy: 0.5153 - val_false_negatives: 1551.0000 - val_false_positives: 1551.0000 - val_true_negatives: 1649.0000 - val_true_positives: 1649.0000 Epoch 00013: val_accuracy did not improve from 0.52844 Epoch 14/100 100/100 [==============================] - 33s 334ms/step - loss: 0.6923 - precision: 0.5216 - auc: 0.5163 - accuracy: 0.5216 - false_negatives: 1531.0000 - false_positives: 1531.0000 - true_negatives: 1669.0000 - true_positives: 1669.0000 - val_loss: 0.6916 - val_precision: 0.5291 - val_auc: 0.5291 - val_accuracy: 0.5291 - val_false_negatives: 1507.0000 - val_false_positives: 1507.0000 - val_true_negatives: 1693.0000 - val_true_positives: 1693.0000 Epoch 00014: val_accuracy improved from 0.52844 to 0.52906, saving model to mymodel.h5 Epoch 15/100 100/100 [==============================] - 34s 337ms/step - loss: 0.6923 - precision: 0.5222 - auc: 0.5186 - accuracy: 0.5222 - false_negatives: 1529.0000 - false_positives: 1529.0000 - true_negatives: 1671.0000 - true_positives: 1671.0000 - val_loss: 0.6918 - val_precision: 0.5256 - val_auc: 0.5256 - val_accuracy: 0.5256 - val_false_negatives: 1518.0000 - val_false_positives: 1518.0000 - val_true_negatives: 1682.0000 - val_true_positives: 1682.0000 Epoch 00015: val_accuracy did not improve from 0.52906 Epoch 16/100 100/100 [==============================] - 34s 337ms/step - loss: 0.6926 - precision: 0.5175 - auc: 0.5173 - accuracy: 0.5175 - false_negatives: 1544.0000 - false_positives: 1544.0000 - true_negatives: 1656.0000 - true_positives: 1656.0000 - val_loss: 0.6917 - val_precision: 0.5263 - val_auc: 0.5271 - val_accuracy: 0.5263 - val_false_negatives: 1516.0000 - val_false_positives: 1516.0000 - val_true_negatives: 1684.0000 - val_true_positives: 1684.0000 Epoch 00016: val_accuracy did not improve from 0.52906 Epoch 17/100 100/100 [==============================] - 33s 335ms/step - loss: 0.6931 - precision: 0.5085 - auc: 0.5089 - accuracy: 0.5081 - false_negatives: 1562.0000 - false_positives: 1583.0000 - true_negatives: 1617.0000 - true_positives: 1638.0000 - val_loss: 0.6920 - val_precision: 0.5334 - val_auc: 0.5339 - val_accuracy: 0.5334 - val_false_negatives: 1493.0000 - val_false_positives: 1493.0000 - val_true_negatives: 1707.0000 - val_true_positives: 1707.0000 Epoch 00017: val_accuracy improved from 0.52906 to 0.53344, saving model to mymodel.h5 Epoch 18/100 100/100 [==============================] - 34s 336ms/step - loss: 0.6934 - precision: 0.5087 - auc: 0.5065 - accuracy: 0.5094 - false_negatives: 1569.0000 - false_positives: 1575.0000 - true_negatives: 1625.0000 - true_positives: 1631.0000 - val_loss: 0.6930 - val_precision: 0.5119 - val_auc: 0.5119 - val_accuracy: 0.5119 - val_false_negatives: 1562.0000 - val_false_positives: 1562.0000 - val_true_negatives: 1638.0000 - val_true_positives: 1638.0000 Epoch 00018: val_accuracy did not improve from 0.53344 Epoch 19/100 100/100 [==============================] - 33s 334ms/step - loss: 0.6929 - precision: 0.5156 - auc: 0.5094 - accuracy: 0.5156 - false_negatives: 1550.0000 - false_positives: 1550.0000 - true_negatives: 1650.0000 - true_positives: 1650.0000 - val_loss: 0.6921 - val_precision: 0.5250 - val_auc: 0.5250 - val_accuracy: 0.5250 - val_false_negatives: 1520.0000 - val_false_positives: 1520.0000 - val_true_negatives: 1680.0000 - val_true_positives: 1680.0000 Epoch 00019: val_accuracy did not improve from 0.53344 Epoch 20/100 100/100 [==============================] - 33s 333ms/step - loss: 0.6930 - precision: 0.5106 - auc: 0.5088 - accuracy: 0.5106 - false_negatives: 1566.0000 - false_positives: 1566.0000 - true_negatives: 1634.0000 - true_positives: 1634.0000 - val_loss: 0.6928 - val_precision: 0.5147 - val_auc: 0.5147 - val_accuracy: 0.5147 - val_false_negatives: 1553.0000 - val_false_positives: 1553.0000 - val_true_negatives: 1647.0000 - val_true_positives: 1647.0000 Epoch 00020: val_accuracy did not improve from 0.53344 Epoch 21/100 100/100 [==============================] - 33s 333ms/step - loss: 0.6907 - precision: 0.5450 - auc: 0.5407 - accuracy: 0.5450 - false_negatives: 1456.0000 - false_positives: 1456.0000 - true_negatives: 1744.0000 - true_positives: 1744.0000 - val_loss: 0.6941 - val_precision: 0.5100 - val_auc: 0.5100 - val_accuracy: 0.5100 - val_false_negatives: 1568.0000 - val_false_positives: 1568.0000 - val_true_negatives: 1632.0000 - val_true_positives: 1632.0000 Epoch 00021: val_accuracy did not improve from 0.53344 Epoch 22/100 100/100 [==============================] - 33s 334ms/step - loss: 0.6931 - precision: 0.5141 - auc: 0.5134 - accuracy: 0.5141 - false_negatives: 1555.0000 - false_positives: 1555.0000 - true_negatives: 1645.0000 - true_positives: 1645.0000 - val_loss: 0.6923 - val_precision: 0.5203 - val_auc: 0.5201 - val_accuracy: 0.5203 - val_false_negatives: 1535.0000 - val_false_positives: 1535.0000 - val_true_negatives: 1665.0000 - val_true_positives: 1665.0000 Epoch 00022: val_accuracy did not improve from 0.53344 Epoch 23/100 100/100 [==============================] - 34s 336ms/step - loss: 0.6927 - precision: 0.5178 - auc: 0.5068 - accuracy: 0.5178 - false_negatives: 1543.0000 - false_positives: 1543.0000 - true_negatives: 1657.0000 - true_positives: 1657.0000 - val_loss: 0.6923 - val_precision: 0.5203 - val_auc: 0.5203 - val_accuracy: 0.5203 - val_false_negatives: 1535.0000 - val_false_positives: 1535.0000 - val_true_negatives: 1665.0000 - val_true_positives: 1665.0000 Epoch 00023: val_accuracy did not improve from 0.53344 Epoch 24/100 100/100 [==============================] - 34s 335ms/step - loss: 0.6930 - precision: 0.5073 - auc: 0.5086 - accuracy: 0.5069 - false_negatives: 1575.0000 - false_positives: 1578.0000 - true_negatives: 1622.0000 - true_positives: 1625.0000 - val_loss: 0.6926 - val_precision: 0.5164 - val_auc: 0.5271 - val_accuracy: 0.5153 - val_false_negatives: 1536.0000 - val_false_positives: 1558.0000 - val_true_negatives: 1642.0000 - val_true_positives: 1664.0000 Epoch 00024: val_accuracy did not improve from 0.53344 Epoch 25/100 100/100 [==============================] - 35s 351ms/step - loss: 0.6931 - precision: 0.5093 - auc: 0.5081 - accuracy: 0.5094 - false_negatives: 1560.0000 - false_positives: 1563.0000 - true_negatives: 1619.0000 - true_positives: 1622.0000 - val_loss: 0.6930 - val_precision: 0.5056 - val_auc: 0.5108 - val_accuracy: 0.5056 - val_false_negatives: 1582.0000 - val_false_positives: 1582.0000 - val_true_negatives: 1618.0000 - val_true_positives: 1618.0000 Epoch 00025: val_accuracy did not improve from 0.53344 . . . . . . . Epoch 80/100 100/100 [==============================] - 34s 337ms/step - loss: 0.6927 - precision: 0.5163 - auc: 0.5134 - accuracy: 0.5163 - false_negatives: 1548.0000 - false_positives: 1548.0000 - true_negatives: 1652.0000 - true_positives: 1652.0000 - val_loss: 0.6918 - val_precision: 0.5284 - val_auc: 0.5284 - val_accuracy: 0.5284 - val_false_negatives: 1509.0000 - val_false_positives: 1509.0000 - val_true_negatives: 1691.0000 - val_true_positives: 1691.0000 Epoch 00080: val_accuracy did not improve from 0.54094 Epoch 81/100 100/100 [==============================] - 34s 335ms/step - loss: 0.6930 - precision: 0.5119 - auc: 0.5081 - accuracy: 0.5119 - false_negatives: 1562.0000 - false_positives: 1562.0000 - true_negatives: 1638.0000 - true_positives: 1638.0000 - val_loss: 0.6906 - val_precision: 0.5475 - val_auc: 0.5475 - val_accuracy: 0.5475 - val_false_negatives: 1448.0000 - val_false_positives: 1448.0000 - val_true_negatives: 1752.0000 - val_true_positives: 1752.0000 Epoch 00081: val_accuracy improved from 0.54094 to 0.54750, saving model to mymodel.h5 Epoch 82/100 100/100 [==============================] - 34s 337ms/step - loss: 0.6918 - precision: 0.5281 - auc: 0.5276 - accuracy: 0.5281 - false_negatives: 1510.0000 - false_positives: 1510.0000 - true_negatives: 1690.0000 - true_positives: 1690.0000 - val_loss: 0.6928 - val_precision: 0.5144 - val_auc: 0.5144 - val_accuracy: 0.5144 - val_false_negatives: 1554.0000 - val_false_positives: 1554.0000 - val_true_negatives: 1646.0000 - val_true_positives: 1646.0000 Epoch 00082: val_accuracy did not improve from 0.54750 Epoch 83/100 100/100 [==============================] - 34s 337ms/step - loss: 0.6927 - precision: 0.5163 - auc: 0.5163 - accuracy: 0.5163 - false_negatives: 1548.0000 - false_positives: 1548.0000 - true_negatives: 1652.0000 - true_positives: 1652.0000 - val_loss: 0.6926 - val_precision: 0.5169 - val_auc: 0.5169 - val_accuracy: 0.5169 - val_false_negatives: 1546.0000 - val_false_positives: 1546.0000 - val_true_negatives: 1654.0000 - val_true_positives: 1654.0000 Epoch 00083: val_accuracy did not improve from 0.54750 Epoch 84/100 100/100 [==============================] - 34s 337ms/step - loss: 0.6921 - precision: 0.5238 - auc: 0.5238 - accuracy: 0.5238 - false_negatives: 1524.0000 - false_positives: 1524.0000 - true_negatives: 1676.0000 - true_positives: 1676.0000 - val_loss: 0.6938 - val_precision: 0.5009 - val_auc: 0.5009 - val_accuracy: 0.5009 - val_false_negatives: 1597.0000 - val_false_positives: 1597.0000 - val_true_negatives: 1603.0000 - val_true_positives: 1603.0000 Epoch 00084: val_accuracy did not improve from 0.54750 Epoch 85/100 100/100 [==============================] - 34s 337ms/step - loss: 0.6923 - precision: 0.5203 - auc: 0.5203 - accuracy: 0.5203 - false_negatives: 1535.0000 - false_positives: 1535.0000 - true_negatives: 1665.0000 - true_positives: 1665.0000 - val_loss: 0.6935 - val_precision: 0.5056 - val_auc: 0.5056 - val_accuracy: 0.5056 - val_false_negatives: 1582.0000 - val_false_positives: 1582.0000 - val_true_negatives: 1618.0000 - val_true_positives: 1618.0000 Epoch 00085: val_accuracy did not improve from 0.54750 Epoch 86/100 100/100 [==============================] - 34s 336ms/step - loss: 0.6923 - precision: 0.5203 - auc: 0.5203 - accuracy: 0.5203 - false_negatives: 1535.0000 - false_positives: 1535.0000 - true_negatives: 1665.0000 - true_positives: 1665.0000 - val_loss: 0.6922 - val_precision: 0.5219 - val_auc: 0.5219 - val_accuracy: 0.5219 - val_false_negatives: 1530.0000 - val_false_positives: 1530.0000 - val_true_negatives: 1670.0000 - val_true_positives: 1670.0000 Epoch 00086: val_accuracy did not improve from 0.54750 Epoch 87/100 100/100 [==============================] - 34s 336ms/step - loss: 0.6916 - precision: 0.5306 - auc: 0.5192 - accuracy: 0.5306 - false_negatives: 1502.0000 - false_positives: 1502.0000 - true_negatives: 1698.0000 - true_positives: 1698.0000 - val_loss: 0.6906 - val_precision: 0.5394 - val_auc: 0.5394 - val_accuracy: 0.5394 - val_false_negatives: 1474.0000 - val_false_positives: 1474.0000 - val_true_negatives: 1726.0000 - val_true_positives: 1726.0000 Epoch 00087: val_accuracy did not improve from 0.54750 Epoch 88/100 100/100 [==============================] - 34s 336ms/step - loss: 0.6940 - precision: 0.4988 - auc: 0.4975 - accuracy: 0.4988 - false_negatives: 1604.0000 - false_positives: 1604.0000 - true_negatives: 1596.0000 - true_positives: 1596.0000 - val_loss: 0.6938 - val_precision: 0.4991 - val_auc: 0.4991 - val_accuracy: 0.4991 - val_false_negatives: 1603.0000 - val_false_positives: 1603.0000 - val_true_negatives: 1597.0000 - val_true_positives: 1597.0000 Epoch 00088: val_accuracy did not improve from 0.54750 Epoch 89/100 100/100 [==============================] - 34s 337ms/step - loss: 0.6916 - precision: 0.5319 - auc: 0.5257 - accuracy: 0.5319 - false_negatives: 1498.0000 - false_positives: 1498.0000 - true_negatives: 1702.0000 - true_positives: 1702.0000 - val_loss: 0.6928 - val_precision: 0.5147 - val_auc: 0.5147 - val_accuracy: 0.5147 - val_false_negatives: 1553.0000 - val_false_positives: 1553.0000 - val_true_negatives: 1647.0000 - val_true_positives: 1647.0000 Epoch 00089: val_accuracy did not improve from 0.54750 Epoch 90/100 100/100 [==============================] - 34s 338ms/step - loss: 0.6926 - precision: 0.5175 - auc: 0.5175 - accuracy: 0.5175 - false_negatives: 1544.0000 - false_positives: 1544.0000 - true_negatives: 1656.0000 - true_positives: 1656.0000 - val_loss: 0.6931 - val_precision: 0.5100 - val_auc: 0.5100 - val_accuracy: 0.5100 - val_false_negatives: 1568.0000 - val_false_positives: 1568.0000 - val_true_negatives: 1632.0000 - val_true_positives: 1632.0000 Epoch 00090: val_accuracy did not improve from 0.54750 Epoch 91/100 100/100 [==============================] - 34s 337ms/step - loss: 0.6907 - precision: 0.5397 - auc: 0.5373 - accuracy: 0.5397 - false_negatives: 1473.0000 - false_positives: 1473.0000 - true_negatives: 1727.0000 - true_positives: 1727.0000 - val_loss: 0.6934 - val_precision: 0.5100 - val_auc: 0.5100 - val_accuracy: 0.5100 - val_false_negatives: 1568.0000 - val_false_positives: 1568.0000 - val_true_negatives: 1632.0000 - val_true_positives: 1632.0000 Epoch 00091: val_accuracy did not improve from 0.54750 Epoch 92/100 100/100 [==============================] - 34s 336ms/step - loss: 0.6929 - precision: 0.5147 - auc: 0.5147 - accuracy: 0.5147 - false_negatives: 1553.0000 - false_positives: 1553.0000 - true_negatives: 1647.0000 - true_positives: 1647.0000 - val_loss: 0.6943 - val_precision: 0.4988 - val_auc: 0.4987 - val_accuracy: 0.4988 - val_false_negatives: 1604.0000 - val_false_positives: 1604.0000 - val_true_negatives: 1596.0000 - val_true_positives: 1596.0000 Epoch 00092: val_accuracy did not improve from 0.54750 Epoch 93/100 100/100 [==============================] - 34s 335ms/step - loss: 0.6921 - precision: 0.5229 - auc: 0.5191 - accuracy: 0.5229 - false_negatives: 1518.0000 - false_positives: 1518.0000 - true_negatives: 1664.0000 - true_positives: 1664.0000 - val_loss: 0.6924 - val_precision: 0.5200 - val_auc: 0.5200 - val_accuracy: 0.5200 - val_false_negatives: 1536.0000 - val_false_positives: 1536.0000 - val_true_negatives: 1664.0000 - val_true_positives: 1664.0000 Epoch 00093: val_accuracy did not improve from 0.54750 Epoch 94/100 100/100 [==============================] - 34s 338ms/step - loss: 0.6946 - precision: 0.4871 - auc: 0.4849 - accuracy: 0.4856 - false_negatives: 1610.0000 - false_positives: 1674.0000 - true_negatives: 1526.0000 - true_positives: 1590.0000 - val_loss: 0.6916 - val_precision: 0.5441 - val_auc: 0.5441 - val_accuracy: 0.5441 - val_false_negatives: 1459.0000 - val_false_positives: 1459.0000 - val_true_negatives: 1741.0000 - val_true_positives: 1741.0000 Epoch 00094: val_accuracy did not improve from 0.54750 Epoch 95/100 100/100 [==============================] - 34s 336ms/step - loss: 0.6914 - precision: 0.5328 - auc: 0.5401 - accuracy: 0.5328 - false_negatives: 1495.0000 - false_positives: 1495.0000 - true_negatives: 1705.0000 - true_positives: 1705.0000 - val_loss: 0.6925 - val_precision: 0.5219 - val_auc: 0.5219 - val_accuracy: 0.5219 - val_false_negatives: 1530.0000 - val_false_positives: 1530.0000 - val_true_negatives: 1670.0000 - val_true_positives: 1670.0000 Epoch 00095: val_accuracy did not improve from 0.54750 Epoch 96/100 100/100 [==============================] - 34s 341ms/step - loss: 0.6940 - precision: 0.5028 - auc: 0.5009 - accuracy: 0.5028 - false_negatives: 1591.0000 - false_positives: 1591.0000 - true_negatives: 1609.0000 - true_positives: 1609.0000 - val_loss: 0.6927 - val_precision: 0.5197 - val_auc: 0.5197 - val_accuracy: 0.5197 - val_false_negatives: 1537.0000 - val_false_positives: 1537.0000 - val_true_negatives: 1663.0000 - val_true_positives: 1663.0000 Epoch 00096: val_accuracy did not improve from 0.54750 Epoch 97/100 100/100 [==============================] - 34s 337ms/step - loss: 0.6929 - precision: 0.5060 - auc: 0.5129 - accuracy: 0.5059 - false_negatives: 1597.0000 - false_positives: 1565.0000 - true_negatives: 1635.0000 - true_positives: 1603.0000 - val_loss: 0.6922 - val_precision: 0.5222 - val_auc: 0.5222 - val_accuracy: 0.5222 - val_false_negatives: 1529.0000 - val_false_positives: 1529.0000 - val_true_negatives: 1671.0000 - val_true_positives: 1671.0000 Epoch 00097: val_accuracy did not improve from 0.54750 Epoch 98/100 100/100 [==============================] - 34s 336ms/step - loss: 0.6927 - precision: 0.5184 - auc: 0.5114 - accuracy: 0.5184 - false_negatives: 1541.0000 - false_positives: 1541.0000 - true_negatives: 1659.0000 - true_positives: 1659.0000 - val_loss: 0.6920 - val_precision: 0.5241 - val_auc: 0.5241 - val_accuracy: 0.5241 - val_false_negatives: 1523.0000 - val_false_positives: 1523.0000 - val_true_negatives: 1677.0000 - val_true_positives: 1677.0000 Epoch 00098: val_accuracy did not improve from 0.54750 Epoch 99/100 100/100 [==============================] - 33s 335ms/step - loss: 0.6923 - precision: 0.5222 - auc: 0.5166 - accuracy: 0.5222 - false_negatives: 1529.0000 - false_positives: 1529.0000 - true_negatives: 1671.0000 - true_positives: 1671.0000 - val_loss: 0.6933 - val_precision: 0.5088 - val_auc: 0.5087 - val_accuracy: 0.5088 - val_false_negatives: 1572.0000 - val_false_positives: 1572.0000 - val_true_negatives: 1628.0000 - val_true_positives: 1628.0000 Epoch 00099: val_accuracy did not improve from 0.54750 Epoch 100/100 100/100 [==============================] - 34s 336ms/step - loss: 0.6928 - precision: 0.5166 - auc: 0.5120 - accuracy: 0.5166 - false_negatives: 1547.0000 - false_positives: 1547.0000 - true_negatives: 1653.0000 - true_positives: 1653.0000 - val_loss: 0.6923 - val_precision: 0.5213 - val_auc: 0.5213 - val_accuracy: 0.5213 - val_false_negatives: 1532.0000 - val_false_positives: 1532.0000 - val_true_negatives: 1668.0000 - val_true_positives: 1668.0000 Epoch 00100: val_accuracy did not improve from 0.54750 code : trdata = ImageDataGenerator() traindata = trdata.flow_from_directory(directory=""/content/out/train"",target_size=(224,224)) tsdata = ImageDataGenerator() testdata = tsdata.flow_from_directory(directory=""/content/out/val"", target_size=(224,224)) model = Sequential() model.add(Conv2D(input_shape=(224,224,3),filters=32,kernel_size=(3,3),padding=""same"", activation=""relu"")) model.add(Conv2D(filters=32, kernel_size=(3,3), padding=""same"", activation=""relu"")) model.add(Conv2D(filters=32, kernel_size=(3,3), padding=""same"", activation=""relu"")) model.add(MaxPool2D(pool_size=(2,2),strides=(2,2))) model.add(Dropout(0.25)) model.add(Conv2D(filters=64, kernel_size=(3,3), padding=""same"", activation=""relu"")) model.add(Conv2D(filters=64, kernel_size=(3,3), padding=""same"", activation=""relu"")) model.add(MaxPool2D(pool_size=(2,2),strides=(2,2))) model.add(Dropout(0.25)) model.add(Conv2D(filters=64, kernel_size=(3,3), padding=""same"", activation=""relu"")) model.add(Conv2D(filters=64, kernel_size=(3,3), padding=""same"", activation=""relu"")) model.add(MaxPool2D(pool_size=(2,2),strides=(2,2))) model.add(Dropout(0.25)) model.add(Conv2D(filters=128, kernel_size=(3,3), padding=""same"", activation=""relu"")) model.add(Conv2D(filters=128, kernel_size=(3,3), padding=""same"", activation=""relu"")) model.add(MaxPool2D(pool_size=(2,2),strides=(2,2))) model.add(Dropout(0.25)) model.add(Flatten()) model.add(Dense(units=1024,activation=""relu"")) model.add(Dense(units=2048,activation=""relu"")) model.add(Dense(units=4096,activation=""relu"")) model.add(Dense(units=2, activation=""sigmoid"")) // for the last activation i tried relu and its the same from keras.optimizers import Adam from keras.optimizers import SGD opt = Adam(lr=1e-3) #opt = SGD(lr=0.01) model.compile(optimizer=opt, loss=keras.losses.binary_crossentropy, metrics=['Precision','AUC', 'accuracy','FalseNegatives','FalsePositives','TrueNegatives','TruePositives']) from keras.callbacks import ModelCheckpoint, EarlyStopping checkpoint = ModelCheckpoint(""mymodel.h5"", monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1) early = EarlyStopping(monitor='accuracy', min_delta=0, patience=100, verbose=1, mode='auto') hist = model.fit_generator(steps_per_epoch=100,generator=traindata, validation_data= testdata, validation_steps=100,epochs=100,shuffle= True,callbacks=[checkpoint,early])",|tensorflow|keras|conv-neural-network|,Model,0
67730059,"Very different results from same Keras model, built with Sequential or functional style. I am trying to implement a Keras regression model that learns to set some parameters, e.g there are some parameters in input and a set of unrelated outputs, coherent with the inputs (e.g. similar inputs give similar outputs in the training set, and there is partial linearity between some inputs and some outputs). Inputs and outputs are normalized, since the parameters have different units. The training phase results in a mse of ~ 0.48 and the predictions are rather sensible. This is the model: model = Sequential() model.add(Dense(78, activation='relu', input_shape = 3)) model.add(Dense(54, activation='relu')) model.add(Dense(54, activation='relu')) model.add(Dense(5)) summary: X: (2011, 3) y: (2011, 5) Model: ""sequential"" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense (Dense) (None, 78) 312 _________________________________________________________________ dense_1 (Dense) (None, 54) 4266 _________________________________________________________________ dense_2 (Dense) (None, 54) 2970 _________________________________________________________________ dense_3 (Dense) (None, 5) 275 ================================================================= Total params: 7,823 Trainable params: 7,823 Non-trainable params: 0 Then I make exactly the same model functional-style inputs = keras.layers.Input(shape=3) #(X.shape[1],) out = keras.layers.Dense(78, activation='relu')(inputs) out = keras.layers.Dense(54, activation='relu')(out) out = keras.layers.Dense(54, activation='relu')(out) out = keras.layers.Dense(5, activation='relu')(out) X: (2011, 3) y: (2011, 5) Model: ""func_model"" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) [(None, 3)] 0 _________________________________________________________________ dense (Dense) (None, 78) 312 _________________________________________________________________ dense_1 (Dense) (None, 54) 4266 _________________________________________________________________ dense_2 (Dense) (None, 54) 2970 _________________________________________________________________ dense_3 (Dense) (None, 5) 275 ================================================================= Total params: 7,823 Trainable params: 7,823 Non-trainable params: 0 The summary is exactly the same, except that the functional has the input layer added.. but the docs say: When a popular kwarg input_shape is passed, then keras will create an input layer to insert before the current layer. This can be treated equivalent to explicitly defining an InputLayer. https://keras.io/api/layers/core_layers/dense/ which is what I do in the first model. So the two models should be the same. But they are not: mse during training is noticeably higher, ~ 0.7, and contrary to the other model the predictions are ""flattened"": the output set is minimally responsive to the input parameters. Any consideration?",|python|keras|model|,Model,0
68011125,"TypeError: forward() takes 2 positional arguments but 4 were given, Pytorch. I am trying to write a GAN generator based on Densenet and Deconv method. I am new to PyTorch and unable to figure out TypeError: forward() takes 2 positional arguments but 4 were given. I tried the approach as suggested in Pytorch TypeError: forward() takes 2 positional arguments but 4 were given but I cannot figure out the solution. My code: class DenseLayer(nn.Module): def __init__(self, in_size, out_size, drop_rate=0.0): super(DenseLayer, self).__init__() self.bottleneck = nn.Sequential() # define bottleneck layers self.bottleneck.add_module('btch1', nn.BatchNorm2d(in_size)) self.bottleneck.add_module('relu1', nn.ReLU(inplace=True)) self.bottleneck.add_module('conv1', nn.ConvTranspose2d(in_size, int(out_size/4), kernel_size=1, stride=1, padding=0, bias=False)) self.basic = nn.Sequential() # define basic block self.basic.add_module('btch2', nn.BatchNorm2d(int(out_size/4))) self.basic.add_module('relu2', nn.ReLU(inplace=True)) self.basic.add_module('conv2', nn.ConvTranspose2d(int(out_size/4), out_size, kernel_size=3, stride=1, padding=1, bias=False)) self.droprate = drop_rate def forward(self, input): out = self.bottleneck(input) if self.droprate > 0: out = F.dropout(out, p=self.droprate, inplace=False, training=self.training) out = self.basic(out) if self.droprate > 0: out = F.dropout(out, p=self.droprate, inplace=False, training=self.training) return torch.cat((x,out), 1) class DenseBlock(nn.Module): def __init__(self, num_layers, in_size, growth_rate, block, droprate=0.0): super(DenseBlock, self).__init__() self.layer = self._make_layer(block, in_size, growth_rate, num_layers, droprate) def _make_layer(self, block, in_size, growth_rate, num_layers, droprate): layers = [] for i in range(num_layers): layers.append(block(in_size, in_size-i*growth_rate, droprate)) return nn.Sequential(*layers) def forward(self, input): return self.layer(input) class MGenDenseNet(nn.Module): def __init__(self, ngpu, growth_rate=32, block_config=(16,24,12,6), in_size=1024, drop_rate=0.0): super(MGenDenseNet, self).__init__() self.ngpu = ngpu self.features = nn.Sequential() self.features.add_module('btch0', nn.BatchNorm2d(in_size)) block = DenseLayer num_features = in_size for i, num_layers in enumerate(block_config): block = DenseBlock(num_layers=num_layers, in_size=num_features, growth_rate=growth_rate, block=block, droprate=drop_rate) ### Error thrown on this line self.features.add_module('denseblock{}'.format(i+1), block) num_features -= num_layers*growth_rate if i!=len(block_config)-1: trans = TransitionLayer(in_size=num_features, out_size=num_features*2, drop_rate=drop_rate) self.features.add_module('transitionblock{}'.format(i+1), trans) num_features *= 2 self.features.add_module('convfinal', nn.ConvTranspose2d(num_features, 3, kernel_size=7, stride=2, padding=3, bias=False)) self.features.add_module('Tanh', nn.Tanh()) def forward(self, input): return self.features(input) mGen = MGenDenseNet(ngpu).to(device) mGen.apply(weights_init) print(mGen)",|python|pytorch|typeerror|generative-adversarial-network|densenet|,API,4
68038676,"TypeError: 'retval_' has dtype int32 in the main branch, but dtype float32 in the else branch. I am training my model to address image classification problem, i have 1000 images classified into 4 classes. When training the model i am getting ""Type Error"", I have reviewed my code several times and don't know where i have committed an error in the code, if possible could some one please suggest me reason for the error, i am posting the code and error generated below for your reference """""" from tensorflow.keras.layers import Input, Concatenate, Dropout, Flatten, Dense, GlobalAveragePooling2D, Conv2D from tensorflow.keras import backend as K #from tensorflow.keras.utils import np_utils from tensorflow.keras.utils import to_categorical from tensorflow.keras.preprocessing.image import ImageDataGenerator from tensorflow.keras import optimizers from tensorflow.keras.metrics import top_k_categorical_accuracy from tensorflow.keras.models import Sequential, Model, load_model import tensorflow as tf from tensorflow.keras.initializers import he_uniform from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping, CSVLogger, ReduceLROnPlateau #from tensorflow.compat.keras.backend import KTF #import keras.backend.tensorflow_backend as KTF from tensorflow.keras.applications.resnet50 import ResNet50 from tensorflow.keras.applications.inception_v3 import InceptionV3 import os import matplotlib.pylab as plt import numpy as np import pandas as pd #import numpy as np, Pillow, skimage, imageio, matplotlib #from scipy.misc import imresize from skimage.transform import resize from tqdm import tqdm # Path to class files classes_file = ""/home/DEV/Downloads/IMAGE_1000_CLASSES"" data_classes = pd.read_csv(classes_file, header=None) # Instances with targets targets = data_classes[1].tolist() # Split data according to their classes class_0 = data_classes[data_classes[1] == 0] class_1 = data_classes[data_classes[1] == 1] class_2 = data_classes[data_classes[1] == 2] class_3 = data_classes[data_classes[1] == 3] # Holdout split train/test set (Other options are k-folds or leave-one-out) split_proportion = 0.9 split_size_0 = int(len(class_0)*split_proportion) split_size_1 = int(len(class_1)*split_proportion) split_size_2 = int(len(class_2)*split_proportion) split_size_3 = int(len(class_3)*split_proportion) new_class_0_train = np.random.choice(len(class_0), split_size_0, replace=False) new_class_0_train = class_0.iloc[new_class_0_train] new_class_0_test = ~class_0.iloc[:][0].isin(new_class_0_train.iloc[:][0]) new_class_0_test = class_0[new_class_0_test] new_class_1_train = np.random.choice(len(class_1), split_size_1, replace=False) new_class_1_train = class_1.iloc[new_class_1_train] new_class_1_test = ~class_1.iloc[:][0].isin(new_class_1_train.iloc[:][0]) new_class_1_test = class_1[new_class_1_test] new_class_2_train = np.random.choice(len(class_2), split_size_2, replace=False) new_class_2_train = class_2.iloc[new_class_2_train] new_class_2_test = ~class_2.iloc[:][0].isin(new_class_2_train.iloc[:][0]) new_class_2_test = class_2[new_class_2_test] new_class_3_train = np.random.choice(len(class_3), split_size_3, replace=False) new_class_3_train = class_3.iloc[new_class_3_train] new_class_3_test = ~class_3.iloc[:][0].isin(new_class_3_train.iloc[:][0]) new_class_3_test = class_3[new_class_3_test] x_train_list = pd.concat( [new_class_0_train, new_class_1_train, new_class_2_train, new_class_3_train]) x_test_list = pd.concat( [new_class_0_test, new_class_1_test, new_class_2_test, new_class_3_test]) # Load files imagePath = ""/home/DEV/Downloads/IMAGE_SET_1000/"" x_train = [] y_train = [] for index, row in tqdm(x_train_list.iterrows(), total=x_train_list.shape[0]): try: loadedImage = plt.imread(imagePath + str(row[0]) + "".jpg"") x_train.append(loadedImage) y_train.append(row[1]) except: # Try with .png file format if images are not properly loaded try: loadedImage = plt.imread(imagePath + str(row[0]) + "".png"") x_train.append(loadedImage) y_train.append(row[1]) except: # Print file names whenever it is impossible to load image files print(imagePath + str(row[0])) x_test = [] y_test = [] for index, row in tqdm(x_test_list.iterrows(), total=x_test_list.shape[0]): try: loadedImage = plt.imread(imagePath + str(row[0]) + "".jpg"") x_test.append(loadedImage) y_test.append(row[1]) except: # Try with .png file format if images are not properly loaded try: loadedImage = plt.imread(imagePath + str(row[0]) + "".png"") x_test.append(loadedImage) y_test.append(row[1]) except: # Print file names whenever it is impossible to load image files print(imagePath + str(row[0])) img_width, img_height = 139, 139 index = 0 for image in tqdm(x_train): #aux = imresize(image, (img_width, img_height, 3), ""bilinear"") aux = resize(image, (img_width, img_height)) x_train[index] = aux / 255.0 # Normalization index += 1 index = 0 for image in tqdm(x_test): #aux = imresize(image, (img_width, img_height, 3), ""bilinear"") aux = resize(image, (img_width, img_height)) x_test[index] = aux / 255.0 # Normalization index += 1 os.environ[""KERAS_BACKEND""] = ""tensorflow"" RANDOM_STATE = 42 def get_session(gpu_fraction=0.8): num_threads = os.environ.get('OMP_NUM_THREADS') gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_fraction) if num_threads: return tf.Session(config=tf.ConfigProto( gpu_options=gpu_options, intra_op_parallelism_threads=num_threads)) else: return tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) #KTF.set_session(get_session()) #k.tensorflow.set_session(get_session()) def precision(y_true, y_pred): true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1))) predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1))) precision = true_positives / (predicted_positives + K.epsilon()) return precision def recall(y_true, y_pred): true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1))) possible_positives = K.sum(K.round(K.clip(y_true, 0, 1))) recall = true_positives / (possible_positives + K.epsilon()) return recall def fbeta_score(y_true, y_pred, beta=1): if beta < 0: raise ValueError('The lowest choosable beta is zero (only precision).') # Set F-score as 0 if there are no true positives (sklearn-like). if K.sum(K.round(K.clip(y_true, 0, 1))) == 0: return 0 p = precision(y_true, y_pred) r = recall(y_true, y_pred) bb = beta ** 2 fbeta_score = (1 + bb) * (p * r) / (bb * p + r + K.epsilon()) return fbeta_score nb_classes = 4 final_model = [] # Option = InceptionV3 model = InceptionV3(weights=""imagenet"", include_top=False, input_shape=(img_width, img_height, 3), classifier_activation=""softmax"") model.summary() # Creating new outputs for the model x = model.output x = Flatten()(x) #x = GlobalAveragePooling2D()(x) x = Dense(512, activation=""relu"")(x) x = Dropout(0.5)(x) x = Dense(512, activation=""relu"")(x) x = Dropout(0.5)(x) predictions = Dense(nb_classes, activation='softmax')(x) final_model = Model(inputs=model.input, outputs=predictions) # Metrics learningRate = 0.001 #optimizer = model.compile(optimizer= 'adam' , loss= tensorflow.keras.losses.binary_crossentropy, metrics=['accuracy']) optimizer = tf.keras.optimizers.SGD(learning_rate=learningRate, momentum=0.88, nesterov=True) #optimizer = tf.keras.Adam(learning_rate=learningRate, momentum=0.88, nesterov=True) # Compiling the model... final_model.compile(loss=""categorical_crossentropy"", optimizer=optimizer, metrics=[""accuracy"", fbeta_score]) #x_train = np.array(x_train) x_train = np.asarray(x_train).astype(np.float32) #x_test = np.array(x_test) x_test = np.asarray(x_test).astype(np.float32) # Defining targets... y_train = np.concatenate([np.full((new_class_0_train.shape[0]), 0), np.full((new_class_1_train.shape[0]), 1), np.full((new_class_2_train.shape[0]), 2), np.full((new_class_3_train.shape[0]), 3)]) y_test = np.concatenate([np.full((new_class_0_test.shape[0]), 0), np.full((new_class_1_test.shape[0]), 1), np.full((new_class_2_test.shape[0]), 2), np.full((new_class_3_test.shape[0]), 3)]) #y_train = np_utils.to_categorical(y_train) y_train = tf.keras.utils.to_categorical(y_train) #y_test = np_utils.to_categorical(y_test) y_test = tf.keras.utils.to_categorical(y_test) modelFilename = ""./model_inception.h5"" trainingFilename = ""./training.csv"" nb_train_samples = y_train.shape[0] nb_test_samples = y_test.shape[0] #epochs = 10000 epochs = 1000 batch_size = 24 trainingPatience = 200 decayPatience = trainingPatience / 4 # Setting the data generator... train_datagen = ImageDataGenerator( horizontal_flip=True, fill_mode=""reflect"", zoom_range=0.2 ) train_generator = train_datagen.flow(x_train, y_train, batch_size=batch_size) # Saving the model checkpoint = ModelCheckpoint(modelFilename, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', save_freq=1) adaptativeLearningRate = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=decayPatience, verbose=1, mode='auto', min_delta=0.0001, cooldown=0, min_lr=1e-8) early = EarlyStopping(monitor='val_acc', min_delta=0, patience=trainingPatience, verbose=1, mode='auto') csv_logger = CSVLogger(trainingFilename, separator="","", append=False) # Callbacks callbacks = [checkpoint, early, csv_logger, adaptativeLearningRate] # Training of the model final_model.fit(train_generator, steps_per_epoch=nb_train_samples / batch_size, epochs=epochs, shuffle=True, validation_data=(x_test, y_test), validation_steps=nb_test_samples / batch_size, callbacks=callbacks) Error : File ""/home/DEV/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py"", line 986, in wrapper raise e.ag_error_metadata.to_exception(e) TypeError: in user code: /home/DEV/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:855 train_function * return step_function(self, iterator) /home/DEV/Downloads/codes/Classification_model.py:212 fbeta_score * if K.sum(K.round(K.clip(y_true, 0, 1))) == 0: /home/DEV/anaconda3/lib/python3.8/site-packages/tensorflow/python/autograph/operators/control_flow.py:1170 if_stmt _tf_if_stmt(cond, body, orelse, get_state, set_state, symbol_names, nouts) /home/DEV/anaconda3/lib/python3.8/site-packages/tensorflow/python/autograph/operators/control_flow.py:1216 _tf_if_stmt final_cond_vars = control_flow_ops.cond( /home/DEV/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:206 wrapper return target(*args, **kwargs) /home/DEV/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py:535 new_func return func(*args, **kwargs) /home/DEV/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/control_flow_ops.py:1254 cond return cond_v2.cond_v2(pred, true_fn, false_fn, name) /home/DEV/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/cond_v2.py:90 cond_v2 false_graph = func_graph_module.func_graph_from_py_func( /home/DEV/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py:999 func_graph_from_py_func func_outputs = python_func(*func_args, **func_kwargs) /home/DEV/anaconda3/lib/python3.8/site-packages/tensorflow/python/autograph/operators/control_flow.py:1213 aug_orelse _verify_tf_cond_vars(new_body_vars_[0], new_orelse_vars, symbol_names) /home/DEV/anaconda3/lib/python3.8/site-packages/tensorflow/python/autograph/operators/control_flow.py:365 _verify_tf_cond_vars nest.map_structure( /home/DEV/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/nest.py:867 map_structure structure[0], [func(*x) for x in entries], /home/DEV/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/nest.py:867 <listcomp> structure[0], [func(*x) for x in entries], /home/DEV/anaconda3/lib/python3.8/site-packages/tensorflow/python/autograph/operators/control_flow.py:335 verify_single_cond_var raise TypeError( TypeError: 'retval_' has dtype int32 in the main branch, but dtype float32 in the else branch",|python|tensorflow|deep-learning|typeerror|multilabel-classification|,API,4
68062571,"RuntimeError: The size of tensor a (4144) must match the size of tensor b (256) at non-singleton dimension 3 site:stackoverflow.com. I am training a generator network with an image size (3, 256, 256). The network is as shown in the below # Number of channels in the training images. For color images this is 3 nc = 3 # Size of z latent vector (i.e. size of generator input) nz = 3 # Size of feature maps in generator ngf = 64 class Generator(nn.Module): def __init__(self): super(Generator, self).__init__() self.main = nn.Sequential( # input is Z, going into a convolution nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False), nn.BatchNorm2d(ngf * 8), nn.ReLU(True), # state size. (ngf*8) x 4 x 4 nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False), nn.BatchNorm2d(ngf * 4), nn.ReLU(True), # state size. (ngf*4) x 8 x 8 nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False), nn.BatchNorm2d(ngf * 2), nn.ReLU(True), # state size. (ngf*2) x 16 x 16 nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False), nn.BatchNorm2d(ngf), nn.ReLU(True), # state size. (ngf) x 32 x 32 nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False), nn.Tanh() # state size. (nc) x 64 x 64 ) def forward(self, input): return self.main(input) net_input_saved = net_input.detach().clone() noise = net_input.detach().clone() out_avg = None last_net = None psrn_noisy_last = 0 loss = [] psnr_noise = [] psnr_ground = [] i = 0 def closure(): global i, out_avg, psrn_noisy_last, last_net, net_input, loss if reg_noise_std > 0: net_input = net_input_saved + (noise.normal_() * reg_noise_std) #changing the input to the netwok out = net(net_input) # Smoothing if out_avg is None: out_avg = out.detach() else: out_avg = out_avg * exp_weight + out.detach() * (1 - exp_weight) # calculating average network output total_loss = mse(out, img_noisy_torch) total_loss.backward() loss.append(total_loss.item()) # caculating psrn psrn_noisy = compare_psnr(img_noisy_np, out.detach().cpu().numpy()[0]) # comparing psnr for the output image and the actual noisy image psrn_gt = compare_psnr(img_noisy_np, out.detach().cpu().numpy()[0]) # comparing psnr for the output image and the original image psrn_gt_sm = compare_psnr(img_np, out_avg.detach().cpu().numpy()[0]) # comparing psnr for the output average and the original image psnr_noise.append(psrn_noisy) psnr_ground.append(psrn_gt) if PLOT and i % show_every == 0: out_np = torch_to_np(out) # plotting the output image along the average image calculated print(f'\n\nAfter {i} iterations: ') print ('Iteration %05d Loss %f PSNR_noisy: %f PSRN_gt: %f PSNR_gt_sm: %f' % (i, total_loss.item(), psrn_noisy, psrn_gt, psrn_gt_sm), '\r', end='\n') plot_image_grid([np.clip(out_np, 0, 1), np.clip(torch_to_np(out_avg), 0, 1)], factor=figsize, nrow=1) # Backtracking if i % show_every: if psrn_noisy - psrn_noisy_last < -5: print('Falling back to previous checkpoint.') for new_param, net_param in zip(last_net, net.parameters()): net_param.data.copy_(new_param.cuda()) return total_loss*0 else: last_net = [x.detach().cpu() for x in net.parameters()] psrn_noisy_last = psrn_noisy i += 1 return total_loss p = get_params(OPT_OVER, net, net_input) optimize(OPTIMIZER, p, closure, LR, num_iter) when i try to train i am getting error /usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([1, 3, 256, 256])) that is different to the input size (torch.Size([1, 3, 4144, 4144])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size. return F.mse_loss(input, target, reduction=self.reduction) --------------------------------------------------------------------------- RuntimeError Traceback (most recent call last) <ipython-input-83-ed04cb48a6ab> in <module>() 67 68 p = get_params(OPT_OVER, net, net_input) ---> 69 optimize(OPTIMIZER, p, closure, LR, num_iter) 5 frames /content/utils/common_utils.py in optimize(optimizer_type, parameters, closure, LR, num_iter) 227 for j in range(num_iter): 228 optimizer.zero_grad() --> 229 closure() 230 optimizer.step() 231 else: <ipython-input-83-ed04cb48a6ab> in closure() 25 out_avg = out_avg * exp_weight + out.detach() * (1 - exp_weight) # calculating average network output 26 ---> 27 total_loss = mse(out, img_noisy_torch) 28 total_loss.backward() 29 /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs) 1049 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks 1050 or _global_forward_hooks or _global_forward_pre_hooks): -> 1051 return forward_call(*input, **kwargs) 1052 # Do not call functions when jit is used 1053 full_backward_hooks, non_full_backward_hooks = [], [] /usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py in forward(self, input, target) 526 527 def forward(self, input: Tensor, target: Tensor) -> Tensor: --> 528 return F.mse_loss(input, target, reduction=self.reduction) 529 530 /usr/local/lib/python3.7/dist-packages/torch/nn/functional.py in mse_loss(input, target, size_average, reduce, reduction) 3087 reduction = _Reduction.legacy_get_string(size_average, reduce) 3088 -> 3089 expanded_input, expanded_target = torch.broadcast_tensors(input, target) 3090 return torch._C._nn.mse_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction)) 3091 /usr/local/lib/python3.7/dist-packages/torch/functional.py in broadcast_tensors(*tensors) 71 if has_torch_function(tensors): 72 return handle_torch_function(broadcast_tensors, tensors, *tensors) ---> 73 return _VF.broadcast_tensors(tensors) # type: ignore[attr-defined] 74 75 RuntimeError: The size of tensor a (4144) must match the size of tensor b (256) at non-singleton dimension 3 I understood this error is coming from mismatch of sizes in tensors, but i was unable to rectify the error. I am providing noise z of size torch.Size([1, 3, 256, 256]) same as the dimensions of the input image to the network but i am getting error.",|python|deep-learning|pytorch|tensor|,Tensors&Inputs,1
68101077,"Accuracy falling down tensorflow v1.5. I'm using tensorflow 1.5 and keras 2.1.6. The code is taken from this tutorial and reworked to work on version 1.5. There is it: import tensorflow as tf import keras import numpy as np import matplotlib.pyplot as plt fashion_mnist = keras.datasets.fashion_mnist (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data() class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'] train_images = train_images / 255.0 test_images = test_images / 255.0 model = tf.keras.Sequential([ tf.keras.layers.Flatten(input_shape=(28, 28)), tf.keras.layers.Dense(128, activation='relu'), tf.keras.layers.Dense(10) ]) model.compile(optimizer='adam', loss=""sparse_categorical_crossentropy"", metrics=['accuracy']); model.fit(train_images, train_labels, epochs=10); But accuracy drops with every epoch and does not rise above 0.3. What i'm doing wrong?",|python|tensorflow|keras|python-3.6|,Model,0
68152276,"Using roberta model cannot define the model .compile or summary. Using roberta model for sentiment analysis cannot define the model .compile or summary from transformers import RobertaTokenizer, RobertaForSequenceClassification from transformers import BertConfig tokenizer = RobertaTokenizer.from_pretrained('roberta-base') robertamodel = RobertaForSequenceClassification.from_pretrained('roberta-base',num_labels=7) print('\nBert Model',robertamodel.summary()) loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) metric = tf.keras.metrics.SparseCategoricalAccuracy('0accuracy') optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5,epsilon=1e-08) robertamodel.compile(loss=loss,optimizer=optimizer,metrics=[metric]) print(robertamodel.summary()) i got these errors 'RobertaForSequenceClassification' object has no attribute 'summary' 'RobertaForSequenceClassification' object has no attribute 'compile'",|python|keras|deep-learning|bert-language-model|roberta-language-model|,API,4
68283519,"Tensorflow - Multi-GPU doesnt work for model(inputs) nor when computing the gradients. When using multiple GPUs to perform inference on a model (e.g. the call method: model(inputs)) and calculate its gradients, the machine only uses one GPU, leaving the rest idle. For example in this code snippet below: import tensorflow as tf import numpy as np import os os.environ[""CUDA_VISIBLE_DEVICES""] = ""0,1"" # Make the tf-data path_filename_records = 'your_path_to_records' bs = 128 dataset = tf.data.TFRecordDataset(path_filename_records) dataset = (dataset .map(parse_record, num_parallel_calls=tf.data.experimental.AUTOTUNE) .batch(bs) .prefetch(tf.data.experimental.AUTOTUNE) ) # Load model trained using MirroredStrategy path_to_resnet = 'your_path_to_resnet' mirrored_strategy = tf.distribute.MirroredStrategy() with mirrored_strategy.scope(): resnet50 = tf.keras.models.load_model(path_to_resnet) for pre_images, true_label in dataset: with tf.GradientTape() as tape: tape.watch(pre_images) outputs = resnet50(pre_images) grads = tape.gradient(outputs, pre_images) Only one GPU is used. You can profile the behavior of the GPUs with nvidia-smi. I don't know if it is supposed to be like this, both the model(inputs) and tape.gradient to not have multi-GPU support. But if it is, then it's a big problem because if you have a large dataset and need to calculate the gradients with respect to the inputs (e.g. interpretability porpuses) it might take days with one GPU. Another thing I tried was using model.predict() but this isn't possible with tf.GradientTape. What I've tried so far and didn't work Put all the code inside mirrored strategy scope. Used different GPUs: I've tried A100, A6000 and RTX5000. Also changed the number of graphic cards and varied the batch size. Specified a list of GPUs, for instance, strategy = tf.distribute.MirroredStrategy(['/gpu:0', '/gpu:1']). Added this strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce()) as @Kaveh suggested. How do I know that only one GPU is working? I used the command watch -n 1 nvidia-smi in the terminal and observed that only one GPU is at 100%, the rest are at 0%. Working Example You can find a working example with a CNN trained on the dogs_vs_cats datasets below. You won't need to manually download the dataset as I used the tfds version, nor train a model. Notebook: Working Example.ipynb Saved Model: HDF5 Saved Format",|tensorflow|keras|multi-gpu|,GPU Usage,3
68321264,"Keras Sequential prediction always returning the same result. This is an algorithm that I used to classify the class of a picture - running shoes, pencil and book. However, after running the algorithm on 3000 shuffled images (that's all I have), I notice: val_accuracy for every one of the epochs is the same, equaling 0.3400 When I print the result from the prediction of 6 images that I took myself, the result array is the following when it's supposed to return a numerical value: [[1.][1.][1.][1.][1.][1.]] Since it is always 1, it will always predict the same class for every one of my image, in my case, book. I've conducted a test like another post suggested, to train with 1000 samples each of running shoes and pencils and 1 sample of book. The result is still book, always. Algorithm: model = Sequential() model.add(Conv2D(64,(3,3), input_shape = X_train.shape[1:])) model.add(Activation(""relu"")) model.add(MaxPooling2D(pool_size=(2,2))) model.add(Conv2D(64,3,3)) model.add(Activation(""relu"")) model.add(MaxPooling2D(pool_size=(2,2))) model.add(Flatten()) model.add(Dense(64)) model.add(Dense(1)) model.add(Activation('softmax')) model.compile(loss=""binary_crossentropy"", optimizer=""adam"", metrics=['accuracy']) model.fit(X_train, y_train, batch_size=24, epochs=3, validation_split=0.1) predictions = model.predict(X_test) Output (2692 samples because some are errors) Train on 2692 samples, validate on 300 samples Epoch 1/3 2692/2692 [==============================] - 17s 6ms/sample - loss: -0.0171 - accuracy: 0.3354 - val_loss: -0.5111 - val_accuracy: 0.3400 Epoch 2/3 2692/2692 [==============================] - 20s 8ms/sample - loss: -0.0171 - accuracy: 0.3354 - val_loss: -0.5111 - val_accuracy: 0.3400 Epoch 3/3 2692/2692 [==============================] - 21s 8ms/sample - loss: -0.0171 - accuracy: 0.3354 - val_loss: -0.5111 - val_accuracy: 0.3400 When I run print(predictions), the result is: [[1.][1.][1.][1.][1.][1.]] Thank you!",|python|tensorflow|machine-learning|keras|image-classification|,Model,0
68458462,"Why is keras accuracy and loss not changing between epochs and how to fix. I'm trying to train a model like the following: input1 = np.array([[2], [1], [4], [3], [5]]) input2 = np.array([[2, 1, 8, 4], [2, 6, 1, 9], [7, 3, 1, 4], [3, 1, 6, 10], [3, 2, 7, 5]]) outputs = np.array([[3,3,1,0], [3,3,3,0], [3,3,4,0], [3,3,1,0], [3,3,4,0]]) merged = np.column_stack([input1, input2]) model = keras.Sequential([ keras.layers.Dense(2, input_dim=5, activation='relu'), keras.layers.Dense(2, activation='relu'), keras.layers.Dense(4, activation='sigmoid'), ]) model.compile( loss=""mean_squared_error"", optimizer=""adam"", metrics=[""accuracy""] ) model.fit(merged, outputs, batch_size=16, epochs = 100) This results in an accuracy of 0.6000 and a loss of about 4.6 and these don't change between epochs. Why is this, and how can I get it to work? I've tried changing the optimizer and loss functions to a few various.",|python|tensorflow|keras|,Model,0
68546140,"Cant train with GPU in TensorFlow. I'm working on a CNN, and I noticed that during the training phase it uses CPU 100% instead of GPU (I have a GTX 1660Ti). Tensorflow doesn't recognize my 1660Ti I tried to follow this guide from TensorFlow website. import tensorflow as tf print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU'))) outputs Num GPUs Available: 0 I tried to read all devices recognized by TensorFlow tf.config.list_physical_devices() outputs [ PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU') ] What i read on the topic Searching in the internet I found that maybe I had to install NVidia CUDA toolkit. I did it from here, but it didn't solve it. I found that NVidia CUDA is not always enabled on all GPUs: source. I found that a little strange, why should NVidia cut off a part of their customers from using CUDA? Additional informations My requirements.txt (if software version can help to solve my problems): matplotlib==3.4.2 keras==2.4.3 tensorflow-gpu==2.5.0 seaborn==0.11.1 I'm running the python code in a Jupyter Notebook (installed via pip) My question There's a way to use my GPU for CUDA (or at least use TensorFlow, like in this case)?",|python|tensorflow|gpu|artificial-intelligence|,GPU Usage,3
68670866,"CUDA out of memory error when reloading Pytorch model. Common pytorch error here, but I'm seeing it under a unique circumstance: when reloading a model, I get a CUDA: Out of Memory error, even though I haven't yet placed the model on the GPU. model = model.load_state_dict(torch.load(model_file_path)) optimizer = optimizer.load_state_dict(torch.load(optimizer_file_path)) # Error happens here ^, before I send the model to the device. model = model.to(device_id)",|memory|pytorch|gpu|out-of-memory|,GPU Usage,3
68691256,"AttributeError: module 'tensorflow_privacy' has no attribute 'DPQuery'. I am new to machine learning and was trying out the ""federated learning for image classification"" code by Tensorflow (https://www.tensorflow.org/federated/tutorials/federated_learning_for_image_classification). I ran the code on Google Colab and did not modify anything. #@test {""skip"": true} # tensorflow_federated_nightly also bring in tf_nightly, which # can causes a duplicate tensorboard install, leading to errors. !pip uninstall --yes tensorboard tb-nightly !pip install --quiet --upgrade tensorflow-federated-nightly !pip install --quiet --upgrade nest-asyncio !pip install --quiet --upgrade tb-nightly # or tensorboard, but not both import nest_asyncio nest_asyncio.apply() %load_ext tensorboard The above works well. (no errors) But when it comes to the below: import collections import numpy as np import tensorflow as tf import tensorflow_federated as tff np.random.seed(0) tff.federated_computation(lambda: 'Hello, World!')() I got an error on the import tensorflow_federated as tff line: AttributeError: module 'tensorflow_privacy' has no attribute 'DPQuery' I've searched for solutions like pip install -U TensorFlow-privacy but none works. Please help. Thanks in advance!",|tensorflow|google-colaboratory|attributeerror|tensorflow-federated|federated-learning|,API,4
68751439,"How to solve constant model accuracy after each epoch. I am studying deep learning and as an assignment, I am doing a classification project, which has 17k records with 14 features and a target variable that have 11 classes. I tried to train a simple neural network # define the keras model model1 = keras.Sequential() model1.add(keras.layers.Dense(64, input_dim=14, activation='relu')) model1.add(keras.layers.Dense(128, activation='relu')) model1.add(keras.layers.Dense(64, activation='relu')) model1.add(keras.layers.Dense(1, activation='softmax')) # compile the keras model model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # fit the keras model on the dataset performance1 = model1.fit(x_train, y_train, epochs=100, validation_split=0.2) But the problem here is I am getting the same accuracy for each epoch, it doesn't seem that model is even learning. I tried to research this problem and found some similar problems on StackOverflow like this question and tried following things Applied StandardScaler Increased/Decreased the hidden layer and neurons Added dropout layer Changed the optimizers, loss, and activation function I also tried to batch_size But none of them worked, of course, the accuracy was different in the different trials (but has the same problem). Few of trials are as follows: # define the keras model model1 = keras.Sequential() model1.add(keras.layers.Dense(64, input_dim=14, activation='sigmoid')) model1.add(keras.layers.Dense(128, activation='sigmoid')) model1.add(keras.layers.Dense(64, activation='sigmoid')) model1.add(keras.layers.Dense(1, activation='softmax')) sgd = keras.optimizers.SGD(lr=0.01) # compile the keras model model1.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy']) # define the keras model model1 = keras.Sequential() model1.add(keras.layers.Dense(64, input_dim=14, activation='relu')) model1.add(keras.layers.Dense(128, activation='relu')) model1.add(keras.layers.Dropout(0.2)) model1.add(keras.layers.Dense(64, activation='relu')) model1.add(keras.layers.Dropout(0.2)) model1.add(keras.layers.Dense(1, activation='softmax')) # compile the keras model model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) I don't know what's the problem here. Please let me know if you require more details to process this. And please don't close this question I know this question stands a chance to marked as a duplicate question but trust me I tried many things which I can understand as a beginner.",|python|tensorflow|keras|deep-learning|,Model,0
68902851,"failed to alloc X bytes unified memory; result: CUDA_ERROR_OUT_OF_MEMORY: out of memory. I am trying to run a tensorflow project and I am encountering memory problems on the university HPC cluster. I have to run a prediction job for hundreds of inputs, with differing lengths. We have GPU nodes with different amounts of vmem, so I am trying to set up the scripts in a way that will not crash in any combination of GPU node - input length. After searching the net for solutions, I played around with TF_FORCE_UNIFIED_MEMORY, XLA_PYTHON_CLIENT_MEM_FRACTION, XLA_PYTHON_CLIENT_PREALLOCATE, and TF_FORCE_GPU_ALLOW_GROWTH, and also with tensorflow's set_memory_growth. As I understood, with unified memory, I should be able to use more memory than a GPU has in itself. This was my final solution (only relevant parts) os.environ['TF_FORCE_UNIFIED_MEMORY']='1' os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION']='2.0' #os.environ['XLA_PYTHON_CLIENT_PREALLOCATE']='false' os.environ['TF_FORCE_GPU_ALLOW_GROWTH ']='true' # as I understood, this is redundant with the set_memory_growth part :) import tensorflow as tf gpus = tf.config.list_physical_devices('GPU') if gpus: try: # Currently, memory growth needs to be the same across GPUs for gpu in gpus: print(gpu) tf.config.experimental.set_memory_growth(gpu, True) logical_gpus = tf.config.list_logical_devices('GPU') print(len(gpus), ""Physical GPUs,"", len(logical_gpus), ""Logical GPUs"") except RuntimeError as e: # Memory growth must be set before GPUs have been initialized print(e) and I submit it on the cluster with --mem=30G (slurm job scheduler) and --gres=gpu:1. And this is the error my code crashes with. As I understand, it does try to use the unified memory but is failing for some reason. Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5582 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN Black, pci bus id: 0000:02:00.0, compute capability: 3.5) 2021-08-24 09:22:02.053935: E external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to alloc 12758286336 bytes unified memory; result: CUDA_ERROR_OUT_OF_MEMORY: out of memory 2021-08-24 09:22:03.738635: E external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to alloc 11482457088 bytes unified memory; result: CUDA_ERROR_OUT_OF_MEMORY: out of memory 2021-08-24 09:22:05.418059: E external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to alloc 10334211072 bytes unified memory; result: CUDA_ERROR_OUT_OF_MEMORY: out of memory 2021-08-24 09:22:07.102411: E external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to alloc 9300789248 bytes unified memory; result: CUDA_ERROR_OUT_OF_MEMORY: out of memory 2021-08-24 09:22:08.784349: E external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to alloc 8370710016 bytes unified memory; result: CUDA_ERROR_OUT_OF_MEMORY: out of memory 2021-08-24 09:22:10.468644: E external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to alloc 7533638656 bytes unified memory; result: CUDA_ERROR_OUT_OF_MEMORY: out of memory 2021-08-24 09:22:12.150588: E external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to alloc 6780274688 bytes unified memory; result: CUDA_ERROR_OUT_OF_MEMORY: out of memory 2021-08-24 09:23:10.326528: W external/org_tensorflow/tensorflow/core/common_runtime/bfc_allocator.cc:272] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.33GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available. Traceback (most recent call last): File ""scripts/script.py"", line 654, in <module> prediction_result, (r, t) = cf.to(model_runner.predict(processed_feature_dict, random_seed=seed), ""cpu"") File ""env/lib/python3.7/site-packages/alphafold/model/model.py"", line 134, in predict result, recycles = self.apply(self.params, jax.random.PRNGKey(random_seed), feat) File ""env/lib/python3.7/site-packages/jax/_src/traceback_util.py"", line 183, in reraise_with_filtered_traceback return fun(*args, **kwargs) File ""env/lib/python3.7/site-packages/jax/_src/api.py"", line 402, in cache_miss donated_invars=donated_invars, inline=inline) File ""env/lib/python3.7/site-packages/jax/core.py"", line 1561, in bind return call_bind(self, fun, *args, **params) File ""env/lib/python3.7/site-packages/jax/core.py"", line 1552, in call_bind outs = primitive.process(top_trace, fun, tracers, params) File ""env/lib/python3.7/site-packages/jax/core.py"", line 1564, in process return trace.process_call(self, fun, tracers, params) File ""env/lib/python3.7/site-packages/jax/core.py"", line 607, in process_call return primitive.impl(f, *tracers, **params) File ""env/lib/python3.7/site-packages/jax/interpreters/xla.py"", line 608, in _xla_call_impl *unsafe_map(arg_spec, args)) File ""env/lib/python3.7/site-packages/jax/linear_util.py"", line 262, in memoized_fun ans = call(fun, *args) File ""env/lib/python3.7/site-packages/jax/interpreters/xla.py"", line 758, in _xla_callable compiled = compile_or_get_cached(backend, built, options) File ""env/lib/python3.7/site-packages/jax/interpreters/xla.py"", line 76, in compile_or_get_cached return backend_compile(backend, computation, compile_options) File ""env/lib/python3.7/site-packages/jax/interpreters/xla.py"", line 373, in backend_compile return backend.compile(built_c, compile_options=options) jax._src.traceback_util.UnfilteredStackTrace: RuntimeError: Resource exhausted: Out of memory while trying to allocate 4649385984 bytes. The stack trace below excludes JAX-internal frames. The preceding is the original exception that occurred, unmodified. -------------------- The above exception was the direct cause of the following exception: Traceback (most recent call last): File ""scripts/script.py"", line 654, in <module> prediction_result, (r, t) = cf.to(model_runner.predict(processed_feature_dict, random_seed=seed), ""cpu"") File ""env/lib/python3.7/site-packages/alphafold/model/model.py"", line 134, in predict result, recycles = self.apply(self.params, jax.random.PRNGKey(random_seed), feat) File ""env/lib/python3.7/site-packages/jax/interpreters/xla.py"", line 373, in backend_compile return backend.compile(built_c, compile_options=options) RuntimeError: Resource exhausted: Out of memory while trying to allocate 4649385984 bytes. I would be glad for any ideas on how to get it to work and use all the available memory. Thank you!",|tensorflow|out-of-memory|gpu|slurm|,GPU Usage,3
68905844,"Adding a number to the last dimension of a tensor. I am trying to add a number to a tensor, in the way that this integer will be added as a new dimension. The tensor is 2 rows and 7 columns: x = [1,2,3,4,5,6,7,8,9,10,11,12,13,14] x = torch.tensor(x) x = x.reshape(-1,7) print(x.shape) print(x) It results in: torch.Size([2, 7]) tensor([[ 1, 2, 3, 4, 5, 6, 7], [ 8, 9, 10, 11, 12, 13, 14]]) The number is a float: a= 0.19 b= torch.tensor([a]) b.reshape(-1,1) b= b.unsqueeze(dim=1) print(b.shape) b Which is: torch.Size([1, 1]) tensor([[0.1900]]) What I want to generate is a [2,8] tensor: tensor([[1, 2, 3, 4, 5, 6, 7,0.1900], [8, 9, 10, 11, 12, 13, 14,0.1900]]) So,I thought I can torch.stack to have a new dimension: c= torch.stack((x, b), dim=-1) Gives an error of : RuntimeError: stack expects each tensor to be equal size, but got [2, 7] at entry 0 and [1, 1] at entry 1 PS: I tried to reshape x into a shape of [14,1] and added [1,1] float tensor to make [15,1], but it added only once so I cannot make a new [2,8] anymore. x = [1,2,3,4,5,6,7,8,9,10,11,12,13,14] x = torch.tensor(x) x = x.reshape(-1,1) print(x.shape) print(x) torch.Size([14, 1]) tensor([[ 1], [ 2], [ 3], [ 4], [ 5], [ 6], [ 7], [ 8], [ 9], [10], [11], [12], [13], [14]]) print('b',b) c= torch.cat((x, b), dim=-2) print(c.shape) b tensor([[0.1900]]) torch.Size([15, 1]) I would be happy to have some help!",|python|pytorch|tensor|,Tensors&Inputs,1
68912706,"Tensorflow-gpu 2.5.0 from conda doesn't recognise GPU. I made a new conda environment and installed tensorflow-gpu from conda (the latest version is 2.5.0). Then, I tested to see if the environment recognizes my GPU, and it does not. It returns [name: ""/device:CPU:0"" device_type: ""CPU"" memory_limit: 268435456 locality { } incarnation: 1364016363571256103 ] when running the list locality function on tensorflow. What am I missing? I installed cuDNN and cudatoolkit as dependencies from the conda installation when installing tensorflow-gpu. cudnn==8.2.1.32 cudatoolkit==11.3.1 The list of commands I ran were: conda create --name ML4 conda activate ML4 conda install tensorflow-gpu=2.5 and then in python from tensorflow.python.client import device_lib print(device_lib.list_local_devices()) which yielded the above statement which only recognizes my CPU and not my GPU.",|tensorflow|gpu|conda|tensorflow2.0|,GPU Usage,3
68938619,"Why Epoch showing the same accuracy?. I am trying to build IDS intrusion detection system and trying to predict the label if it is benign or DDos. But I get the same accuracy along epochs. Code: from tensorflow import keras import numpy as np import datetime import time from tensorflow.keras.optimizers import Adam from keras.models import Sequential from keras.layers import Dense, Dropout from keras import callbacks x=pd.DataFrame(X) x = x.values sample = x.shape[0] features = x.shape[1] #Train: convert 2D to 3D for input RNN x_train = np.reshape(x,(sample,features,1)) #shape = (125973, 18, 1) #Test: convert 2D to 3D for input RNN x_test=pd.DataFrame(X_test) x_test = x_test.values x_test = np.reshape(x_test,(x_test.shape[0],x_test.shape[1],1)) Model = keras.Sequential([ keras.layers.LSTM(80,input_shape=(features,x_train.shape[2]), activation='sigmoid',recurrent_activation='hard_sigmoid'), keras.layers.Dense(1,activation=""softmax"") ]) Model.compile(optimizer='rmsprop',loss='mse', metrics=['accuracy']) #Training the model Model.fit(x_train, y, epochs=10, batch_size= 32) Model.summary() # Final evaluation of the model scores = Model.evaluate(x_test, y_test, verbose=0) print('/n') print(""Accuracy: %.2f%%"" % (scores[1]*100)) Epoch 1/10 1074/1074 [==============================] - 92s 83ms/step - loss: 0.4180 - accuracy: 0.5820 Epoch 2/10 1074/1074 [==============================] - 79s 74ms/step - loss: 0.4180 - accuracy: 0.5820 Epoch 3/10 1074/1074 [==============================] - 81s 76ms/step - loss: 0.4180 - accuracy: 0.5820 What is the solution?",|machine-learning|keras|deep-learning|lstm|recurrent-neural-network|,Model,0
68962721,"Tensorflow version 2.6.0, cuda version=10.2 not utilizing GPU. The tensorflow which I was using , is not utilizing gpu now, after it has been changed accidentally by other person. I remember that i installed my tensorflow using wheel. Currently, the version of Cuda is 10.2, python version is 3.7 and tensorflow version is 2.6.0. The output of the following code gives me: import tensorflow as tf print(""Num GPUs Available: "", len(tf.config.experimental.list_physical_devices('GPU'))) tf.debugging.set_log_device_placement(True) # Create some tensors a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]) b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]) c = tf.matmul(a, b) print(c) **Num GPUs Available: 0 Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:CPU:0 Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:CPU:0 Executing op MatMul in device /job:localhost/replica:0/task:0/device:CPU:0 tf.Tensor( [[22. 28.] [49. 64.]], shape=(2, 2), dtype=float32)** I have attached images of my nvidia-smi command output $ nvidia-smi Sat Aug 28 14:05:15 2021 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 440.33.01 Driver Version: 440.33.01 CUDA Version: 10.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce RTX 2060 Off | 00000000:21:00.0 Off | N/A | | 32% 43C P8 9W / 160W | 92MiB / 5934MiB | 0% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 13060 C /home/hemapriya/anaconda3/bin/python 81MiB | +-----------------------------------------------------------------------------+ !conda list cudatoolkit !conda list cudnn Output: # packages in environment at /home/hemapriya/anaconda3: # # Name Version Build Channel cudatoolkit 10.0.130 0 anaconda # packages in environment at /home/hemapriya/anaconda3: # # Name Version Build Channel cudnn 7.6.5 cuda10.0_0 anaconda from tensorflow.python.client import device_lib print(device_lib.list_local_devices()) output: [name: ""/device:CPU:0"" device_type: ""CPU"" memory_limit: 268435456 locality { } incarnation: 15409823727920383739 ] Also, now after that incident, my tf.keras.optimizers.Adam is not working now. Could you please let me know, how I should rectify, so that my tensorflow should include GPU.",|python-3.x|tensorflow|installation|gpu|,GPU Usage,3
69125887,"Understanding why memory allocation occurs during inference, backpropagation, and model update. In the process of tracking down a GPU OOM error, I made the following checkpoints in my Pytorch code (running on Google Colab P100): learning_rate = 0.001 num_epochs = 50 device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"") print('check 1') !nvidia-smi | grep MiB | awk '{print $9 $10 $11}' model = MyModel() print('check 2') !nvidia-smi | grep MiB | awk '{print $9 $10 $11}' model = model.to(device) print('check 3') !nvidia-smi | grep MiB | awk '{print $9 $10 $11}' optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) print('check 4') !nvidia-smi | grep MiB | awk '{print $9 $10 $11}' for epoch in range(num_epochs): train_running_loss = 0.0 train_accuracy = 0.0 model = model.train() print('check 5') !nvidia-smi | grep MiB | awk '{print $9 $10 $11}' ## training step for i, (name, output_array, input) in enumerate(trainloader): output_array = output_array.to(device) input = input.to(device) comb = torch.zeros(1,1,100,1632).to(device) print('check 6') !nvidia-smi | grep MiB | awk '{print $9 $10 $11}' ## forward + backprop + loss output = model(input, comb) print('check 7') !nvidia-smi | grep MiB | awk '{print $9 $10 $11}' loss = my_loss(output, output_array) print('check 8') !nvidia-smi | grep MiB | awk '{print $9 $10 $11}' optimizer.zero_grad() print('check 9') !nvidia-smi | grep MiB | awk '{print $9 $10 $11}' loss.backward() print('check 10') !nvidia-smi | grep MiB | awk '{print $9 $10 $11}' ## update model params optimizer.step() print('check 11') !nvidia-smi | grep MiB | awk '{print $9 $10 $11}' train_running_loss += loss.detach().item() print('check 12') !nvidia-smi | grep MiB | awk '{print $9 $10 $11}' temp = get_accuracy(output, output_array) print('check 13') !nvidia-smi | grep MiB | awk '{print $9 $10 $11}' train_accuracy += temp with the following output: check 1 2MiB/16160MiB check 2 2MiB/16160MiB check 3 3769MiB/16160MiB check 4 3769MiB/16160MiB check 5 3769MiB/16160MiB check 6 3847MiB/16160MiB check 7 6725MiB/16160MiB check 8 6725MiB/16160MiB check 9 6725MiB/16160MiB check 10 9761MiB/16160MiB check 11 16053MiB/16160MiB check 12 16053MiB/16160MiB check 13 16053MiB/16160MiB check 6 16053MiB/16160MiB check 7 16071MiB/16160MiB check 8 16071MiB/16160MiB check 9 16071MiB/16160MiB check 10 16071MiB/16160MiB --------------------------------------------------------------------------- RuntimeError Traceback (most recent call last) <ipython-input-11-f566d09448f9> in <module>() 65 66 ## update model params ---> 67 optimizer.step() 68 69 print('check 11') 3 frames /usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py in wrapper(*args, **kwargs) 86 profile_name = ""Optimizer.step#{}.step"".format(obj.__class__.__name__) 87 with torch.autograd.profiler.record_function(profile_name): ---> 88 return func(*args, **kwargs) 89 return wrapper 90 /usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py in decorate_context(*args, **kwargs) 26 def decorate_context(*args, **kwargs): 27 with self.__class__(): ---> 28 return func(*args, **kwargs) 29 return cast(F, decorate_context) 30 /usr/local/lib/python3.7/dist-packages/torch/optim/adam.py in step(self, closure) 116 lr=group['lr'], 117 weight_decay=group['weight_decay'], --> 118 eps=group['eps']) 119 return loss /usr/local/lib/python3.7/dist-packages/torch/optim/_functional.py in adam(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps) 92 denom = (max_exp_avg_sqs[i].sqrt() / math.sqrt(bias_correction2)).add_(eps) 93 else: ---> 94 denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(eps) 95 96 step_size = lr / bias_correction1 RuntimeError: CUDA out of memory. Tried to allocate 2.32 GiB (GPU 0; 15.78 GiB total capacity; 11.91 GiB already allocated; 182.75 MiB free; 14.26 GiB reserved in total by PyTorch) It makes sense to me that model = model.to(device) creates 3.7G of memory. But why does running the model output = model(input, comb) create another 3G of memory? And then loss.backward() creates another 3G of memory? And then optimizer.step() creates another 6.3G of memory? I would appreciate it if someone could explain how the PyTorch GPU memory allocation model is working in this example.",|pytorch|gpu|,GPU Usage,3
69137834,"Keras ValueError: Dimensions must be equal, but are 2 and 32 for '{{node Equal}} with input shapes: [?,2], [?,32,32]. I was trying to train a simple Keras network for classification when I faced the following error. I know there is something wrong with my inputs but I couldn't figure out how to fix it. Here is my code my data set shape : x_train : float32 0.0 1.0 (2444, 64, 64, 1) y_train : float32 0.0 1.0 (2444, 2) x_test : float32 0.0 1.0 (9123, 64, 64, 1) y_test : float32 0.0 1.0 (9123, 2) the model : inputs = keras.Input(shape=(64,64,1), dtype='float32') x = keras.layers.Conv2D(12,(9,9), padding=""same"",input_shape=(64,64,1), dtype='float32',activation='relu')(inputs) x = keras.layers.Conv2D(18,(7,7), padding=""same"", activation='relu')(x) x = keras.layers.MaxPool2D(pool_size=(2,2))(x) x = keras.layers.Dropout(0.25)(x) x = keras.layers.Dense(50, activation='relu')(x) x = keras.layers.Dropout(0.4)(x) outputs = keras.layers.Dense(2, activation='softmax')(x) model = keras.Model(inputs, outputs) model summary : Model: ""model_1"" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_2 (InputLayer) [(None, 64, 64, 1)] 0 _________________________________________________________________ conv2d_2 (Conv2D) (None, 64, 64, 12) 984 _________________________________________________________________ conv2d_3 (Conv2D) (None, 64, 64, 18) 10602 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 32, 32, 18) 0 _________________________________________________________________ dropout_2 (Dropout) (None, 32, 32, 18) 0 _________________________________________________________________ dense_2 (Dense) (None, 32, 32, 50) 950 _________________________________________________________________ dropout_3 (Dropout) (None, 32, 32, 50) 0 _________________________________________________________________ dense_3 (Dense) (None, 32, 32, 2) 102 ================================================================= Total params: 12,638 Trainable params: 12,638 Non-trainable params: 0 ________________________ compiler and fitter which error occurs when I wanna fit the model model.compile( loss=keras.losses.SparseCategoricalCrossentropy(), optimizer=keras.optimizers.Adam(0.01), metrics=[""acc""], ) model.fit(x_train, y_train, batch_size=32, epochs = 20, validation_split= 0.3, callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)]) and finally the error: ValueError Traceback (most recent call last) <ipython-input-31-e4cade46a08c> in <module>() 1 model.fit(x_train, y_train, batch_size=32, epochs = 20, validation_split= 0.3, ----> 2 callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)]) 9 frames /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs) 992 except Exception as e: # pylint:disable=broad-except 993 if hasattr(e, ""ag_error_metadata""): --> 994 raise e.ag_error_metadata.to_exception(e) 995 else: 996 raise ValueError: in user code: /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:853 train_function * return step_function(self, iterator) /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:842 step_function ** outputs = model.distribute_strategy.run(run_step, args=(data,)) /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:1286 run return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs) /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2849 call_for_each_replica return self._call_for_each_replica(fn, args, kwargs) /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3632 _call_for_each_replica return fn(*args, **kwargs) /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:835 run_step ** outputs = model.train_step(data) /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:792 train_step self.compiled_metrics.update_state(y, y_pred, sample_weight) /usr/local/lib/python3.7/dist-packages/keras/engine/compile_utils.py:457 update_state metric_obj.update_state(y_t, y_p, sample_weight=mask) /usr/local/lib/python3.7/dist-packages/keras/utils/metrics_utils.py:73 decorated update_op = update_state_fn(*args, **kwargs) /usr/local/lib/python3.7/dist-packages/keras/metrics.py:177 update_state_fn return ag_update_state(*args, **kwargs) /usr/local/lib/python3.7/dist-packages/keras/metrics.py:681 update_state ** matches = ag_fn(y_true, y_pred, **self._fn_kwargs) /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206 wrapper return target(*args, **kwargs) /usr/local/lib/python3.7/dist-packages/keras/metrics.py:3537 sparse_categorical_accuracy return tf.cast(tf.equal(y_true, y_pred), backend.floatx()) /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206 wrapper return target(*args, **kwargs) /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py:1864 equal return gen_math_ops.equal(x, y, name=name) /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_math_ops.py:3219 equal name=name) /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py:750 _apply_op_helper attrs=attr_protos, op_def=op_def) /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py:601 _create_op_internal compute_device) /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py:3569 _create_op_internal op_def=op_def) /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py:2042 __init__ control_input_ops, op_def) /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py:1883 _create_c_op raise ValueError(str(e)) ValueError: Dimensions must be equal, but are 2 and 32 for '{{node Equal}} = Equal[T=DT_FLOAT, incompatible_shape_error=true](IteratorGetNext:1, Cast_1)' with input shapes: [?,2], [?,32,32].",|python|keras|deep-learning|valueerror|dimensions|,Tensors&Inputs,1
69203342,"Paramiko SSH connection does not have access to GPU with tensorflow. Using Python's paramiko I try to access a server I own remotely via an SSH connection. This server has a python script that I execute via my SSH connection (I actually execute a .sh file that calls this python script). The first line of this python script is: print(""Devices:"", tf.config.experimental.list_physical_devices()) Here I only see: Devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')] If I execute the same .sh file after opening the SSH connection with PuTTY that same line of code generates the following result: Devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')] Here it does show the GPU as available. I suppose this is related to the way I open the paramiko SSH connection. This is the code: ssh = paramiko.SSHClient() ssh.load_system_host_keys() ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy()) ssh.connect(ip, port=port, username=u, password=pass) cmd_to_execute = ""./"" + shFileName + "".sh"" ssh_stdin, ssh_stdout, ssh_stderr = ssh.exec_command(cmd_to_execute) for line in ssh_stdout: print(line.strip('\n')) for line in ssh_stderr: print('ERR: ' + line.strip('\n')) ssh.close() Why do I not have access to the GPU when the SSH connection is established using paramiko? EDIT 1 If I allow tensorflow logs to be displayed, I get this error: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory And others that look more or less like this one but changing the .11.0 On the other hand, if I use PuTTY it loads the libraries correctly: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0",|python|tensorflow|ssh|gpu|paramiko|,GPU Usage,3
69549126,"Tensorflow TypeError: Cannot convert 1e-12 to EagerTensor of dtype int32. I have a multiclass classification machine learning application for which I want to calculate the f1 score using tensorflow. The predicted and actual values are stored in pandas dataframes y_pred and y_act respectively. Both are populated with 1's and 0's. So I do something like this: # convert dataframes to numpy pred_numpy = numpy.asarray([y_pred], numpy.int32) act_numpy = numpy.asarray([y_act], numpy.int32) # compute multiclass f1 metric = tfa.metrics.F1Score(num_classes=num_classes, average=""macro"") metric.update_state(act_numpy, pred_numpy) print(metric.result().numpy()) However I get the following error TypeError: Cannot convert 1e-12 to EagerTensor of dtype int32 There must be something with the type casting from pandas to tensorflow which is throwing the error. I have tried a series of mitigations to no avail. I tried converting the numpy arrays to tensors like so: pred_tf = tf.convert_to_tensor(pred_numpy, numpy.int32) I tried ensuring the pandas dataframe has no 1e-12 instances with: y_pred = y_pred.replace(1e-12, 0) I tried converting to numpy without the numpy.int32 option. However I still get the same error. Any tips for converting from pandas to tensors successfully without getting this error?",|python|pandas|numpy|tensorflow|tensor|,API,4
69582900,"Using Pytorch model trained on RTX2080 on RTX3060. I try to run my PyTorch model (trained on a Nvidia RTX2080) on the newer Nvidia RTX3060 with CUDA support. It is possible to load the model and to execute it. If I run it on the CPU with the --no_cuda flag it runs smootly and gives back the correct predictions, but if I want to run it with CUDA, it only returns wrong predictions which make no sense. Does the different GPU-architecture of the cards affect the prediction?",|neural-network|pytorch|gpu|nvidia|,GPU Usage,3
69660201,"Inputting some data for BERT model, using tf.data.Dataset.from_tensor_slices. Here's my model: def build_classifier_model(): text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='features') preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing') encoder_inputs = preprocessing_layer(text_input) encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder') outputs = encoder(encoder_inputs) net = outputs['pooled_output'] net = tf.keras.layers.Dropout(0.1)(net) net = tf.keras.layers.Dense(3, activation=""softmax"", name='classifier')(net) return tf.keras.Model(text_input, net) In the preprocessing layer, I'm using a BERT preprocessor from TF-Hub. I've already divided the data into corpus_train, corpus_test, labels_train, labels_test. The corpuses are panda dataframes with the texts that will be used as features, and the labels are NumPy arrays. corpus=df_speech_EN_merged[""contents""] corpus.shape (1768,) labels=np.asarray(df_speech_EN_merged[""Classes""].astype(""int"")) labels.shape (1768,) To create the train and test data set, I've used the following: train_dataset = ( tf.data.Dataset.from_tensor_slices( { ""features"":tf.cast(corpus_train.values, tf.string), ""labels"":tf.cast(labels_train, tf.int32) #labels is already an array, no need for .values } ) test_dataset = tf.data.Dataset.from_tensor_slices( {""features"":tf.cast(corpus_test.values, tf.string), ""labels"":tf.cast(labels_test, tf.int32) } #labels is already an array, no need for .values ) ) After building and compiling the model without any error message, when I fit the model with: classifier_model.fit(x=train_dataset, validation_data=test_dataset, epochs=2) I get the following error: ValueError: Could not find matching function to call loaded from the SavedModel. Got: Positional arguments (3 total): * Tensor(""inputs:0"", shape=(), dtype=string) * False * None Keyword arguments: {} Expected these arguments to match one of the following 4 option(s): Option 1: Positional arguments (3 total): * TensorSpec(shape=(None,), dtype=tf.string, name='sentences') * False * None Keyword arguments: {} Option 2: Positional arguments (3 total): * TensorSpec(shape=(None,), dtype=tf.string, name='sentences') * True * None Keyword arguments: {} Option 3: Positional arguments (3 total): * TensorSpec(shape=(None,), dtype=tf.string, name='inputs') * False * None Keyword arguments: {} Option 4: Positional arguments (3 total): * TensorSpec(shape=(None,), dtype=tf.string, name='inputs') * True * None Keyword arguments: {} I think this error occurs because I'm either building train_dataset/test_dataset wrong or because the text_input layer is expecting the wrong type of data. Any help would be appreciated.",|tensorflow|nlp|bert-language-model|,Training,2
69704467,"Concatenate layer shape error in sequence2sequence model with Keras attention. I'm trying to implement a simple word-level sequence-to-sequence model with Keras in Colab. I'm using the Keras Attention layer. Here is the definition of the model: embedding_size=200 UNITS=128 encoder_inputs = Input(shape=(None,), name=""encoder_inputs"") encoder_embs=Embedding(num_encoder_tokens, embedding_size, name=""encoder_embs"")(encoder_inputs) #encoder lstm encoder = LSTM(UNITS, return_state=True, name=""encoder_LSTM"") #(encoder_embs) encoder_outputs, state_h, state_c = encoder(encoder_embs) encoder_states = [state_h, state_c] decoder_inputs = Input(shape=(None,), name=""decoder_inputs"") decoder_embs = Embedding(num_decoder_tokens, embedding_size, name=""decoder_embs"")(decoder_inputs) #decoder lstm decoder_lstm = LSTM(UNITS, return_sequences=True, return_state=True, name=""decoder_LSTM"") decoder_outputs, _, _ = decoder_lstm(decoder_embs, initial_state=encoder_states) attention=Attention(name=""attention_layer"") attention_out=attention([encoder_outputs, decoder_outputs]) decoder_concatenate=Concatenate(axis=-1, name=""concat_layer"")([decoder_outputs, attention_out]) decoder_outputs = TimeDistributed(Dense(units=num_decoder_tokens, activation='softmax', name=""decoder_denseoutput""))(decoder_concatenate) model=Model([encoder_inputs, decoder_inputs], decoder_outputs, name=""s2s_model"") model.compile(optimizer='RMSprop', loss='categorical_crossentropy', metrics=['accuracy']) model.summary() Model compiling is fine, no problems whatsoever. The encoder and decoder input and output shapes are: Encoder training input shape: (4000, 21) Decoder training input shape: (4000, 12) Decoder training target shape: (4000, 12, 3106) -- Encoder test input shape: (385, 21) This is the model.fit code: model.fit([encoder_training_input, decoder_training_input], decoder_training_target, epochs=100, batch_size=32, validation_split=0.2,) When I run the fit phase, I get this error from the Concatenate layer: ValueError: Dimension 1 in both shapes must be equal, but are 12 and 32. Shapes are [32,12] and [32,32]. for '{{node s2s_model/concat_layer/concat}} = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32](s2s_model/decoder_LSTM/PartitionedCall:1, s2s_model/attention_layer/MatMul_1, s2s_model/concat_layer/concat/axis)' with input shapes: [32,12,128], [32,32,128], [] and with computed input tensors: input[2] = <2>. So, the first 32 are batch_size, 128 are output shape from decoder_outputs and attention_out, 12 is the number of tokens of decoder inputs. I can't understand how to solve this error, I can't change the number of input tokens I think, any suggestions for me?",|python|keras|nlp|attention-model|sequence-to-sequence|,Tensors&Inputs,1
69787272,"Keras predicting floating point output for a binary problem. I have a model in Keras: import tensorflow as tf import numpy as np import pandas as pd from tensorflow import keras from tensorflow.keras import layers from sklearn.model_selection import train_test_split import random df = pd.read_csv('/home/Datasets/creditcard.csv') output = df['Class'] features = df.drop('Class', 1) train_features, test_features, train_labels, test_labels = train_test_split(df, output, test_size = 0.2, random_state = 42) train_features = train_features.to_numpy() test_features = test_features.to_numpy() train_labels = train_labels.to_numpy() test_labels = test_labels.to_numpy() model = tf.keras.Sequential() num_nodes = [1] act_functions = [tf.nn.relu] optimizers = ['SGD'] loss_functions = ['categorical_crossentropy'] epochs_count = ['10'] batch_sizes = ['500'] act = random.choice(act_functions) opt = random.choice(optimizers) ep = random.choice(epochs_count) batch = random.choice(batch_sizes) loss = random.choice(loss_functions) count = random.choice(num_nodes) model.add(tf.keras.layers.Dense(31, activation = act, input_shape=(31,))) model.add(tf.keras.layers.Dense(count, activation = act)) model.add(tf.keras.layers.Dense(1, activation = act)) model.compile(loss = loss, optimizer = opt, metrics = ['accuracy']) epochs = int(ep) batch_size = int(batch) model.fit(train_features, train_labels, epochs=epochs, batch_size=batch_size) The train labels are binary: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 But, the output of: z = model.predict(test_features) is: array([[ 4574.6 ], [ 4896.158 ], [ 3867.8225], ..., [15516.117 ], [ 6441.43 ], [ 5453.437 ]], dtype=float32) Why would it be predicting these values? Thanks",|python|python-3.x|keras|deep-learning|,Model,0
69901379,"Overfitting a small data set through the ReLU activation. I would like to train a network by only using the ReLU activation and completely overfit the data. However, no matter how many different network structures I utilize (e.g., increasing the number of neurons and layers), I'm not able to reach a loss value close to zero. It is important to emphasize that i) I don't want to use another activation function, and ii) for now, I won't be normalizing the data points. n = 50 x = np.random.randint(50, 2000, (n, 10)) y = np.random.randint(600, 4000, (n, 1)) k = 16 model = tf.keras.Sequential([ tf.keras.layers.Flatten(input_shape=(10,)), tf.keras.layers.Dense(k, activation='relu'), tf.keras.layers.Dense(k, activation='relu'), tf.keras.layers.Dense(k, activation='relu'), tf.keras.layers.Dense(k, activation='relu'), tf.keras.layers.Dense(1) ]) model.compile(loss='mse',optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)) model.fit(x, y, epochs=1000, batch_size=1, verbose=2) My current network is quite vanilla, and in my opinion, the ReLU activation should ""easily"" memorize/overfit the data especially when considering the size and dimension of the data set. Is there a chance that I might be doing something wrong in my code? or What could be the reason that my network does not work?",|python|tensorflow|machine-learning|keras|deep-learning|,Model,0
69919854,"PyTorch with CUDA and Nvidia card: RuntimeError: CUDA error: all CUDA-capable devices are busy or unavailable, but torch.cuda.is_available() is True. Problem: I occasionally get the following CUDA error when running PyTorch scripts with CUDA on an Nvidia GPU, running on CentOS 7. If I run: python3 -c 'import torch; print(torch.cuda.is_available()); torch.randn(1).to(""cuda"")'?I get the following output: True?Traceback (most recent call last):?File ""<string>"", line 1, in <module>?RuntimeError: CUDA error: all CUDA-capable devices are busy or unavailable?CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.?For debugging consider passing CUDA_LAUNCH_BLOCKING=1. PyTorch seems to think the GPU is available, but I can't put anything onto it's memory. When I restart the computer, the error goes away. I can't seem to get the error to come back consistently.",|pytorch|cuda|nvidia|,GPU Usage,3
69999033,"Tensorflow custom loss function NaNs during training. I am trying to write a custom loss function for a CNN that detects an object and draws a non axis aligned bounding box on it. My inputs are 200x200 images and the outputs / labels are 6d vectors of the form [object_present, x,y, angle, width, height] Where object_present is a binary feature representing whether the object is present or not, (x,y) is the centre of the bounding box, angle is the angle of rotation of the bbox from being axis aligned, and width and height are the dimensions of the bbox. When object_present = 0, all the other features are set to NaN. Consequently, my custom loss function needs to ignore the NaNs for a negative sample and apply Binary Cross entropy loss to object_present feature. For positive samples, I also have to include MSE loss for (x,y) and width, height, and an angular regression loss that I have defined as arctan(sin(angle1 - angle2), cos(angle1-angle2)). My implementation is as follows: binary_loss_func = tf.keras.losses.BinaryCrossentropy() def loss_func(true_labels, pred_labels): binary_loss = binary_loss_func(true_labels[:,0], pred_labels[:, 0]) mse_loss1 = tf.reduce_mean(tf.where(tf.math.is_nan(true_labels[:,1:3]), tf.zeros_like(true_labels[:, 1:3]), tf.square(tf.subtract(true_labels[:, 1:3], pred_labels[:, 1:3])))) mse_loss2 = tf.reduce_mean(tf.where(tf.math.is_nan(true_labels[:,4:]), tf.zeros_like(true_labels[:, 4:]), tf.square(tf.subtract(true_labels[:, 4:], pred_labels[:, 4:])))) angular_loss = tf.reduce_mean(tf.where(is_nan(true_labels[:,3]), tf.zeros_like(true_labels[:, 3]), tf.abs(tf.atan2(tf.sin(true_labels[:, 3] - pred_labels[:, 3]), tf.cos(true_labels[:, 3] - pred_labels[:, 3]))))) return mse_loss1 + mse_loss2 + binary_loss + angular_loss My issue is that this returns NaN loss values after the first batch of training (only the first batch does not give NaN loss), even though I think the above code should return 0 loss for negative samples. I have confirmed that the Binary Loss function is returning real numbers as it should, so the issue is with the other components of the loss. After some debugging with tf.print statements, I found that pred_labels becomes NaN after the first batch of training. I am not sure why this is happening, and if it is an issue with how my custom loss function is defined or if it is an issue with my model. The model I am using is: IMAGE_SIZE = 200 CONV_PARAMS = {""kernel_size"": 3, ""use_bias"": False, ""padding"": ""same""} CONV_PARAMS2 = {""kernel_size"": 5, ""use_bias"": False, ""padding"": ""same""} model = Sequential() model.add( Reshape((IMAGE_SIZE, IMAGE_SIZE, 1), input_shape=(IMAGE_SIZE, IMAGE_SIZE)) ) model.add(Conv2D(16, **CONV_PARAMS)) model.add(BatchNormalization()) model.add(Activation('relu')) model.add(MaxPool2D()) model.add(Conv2D(32, **CONV_PARAMS)) model.add(BatchNormalization()) model.add(Activation('relu')) model.add(MaxPool2D()) model.add(Conv2D(64, **CONV_PARAMS)) model.add(BatchNormalization()) model.add(Activation('relu')) model.add(MaxPool2D()) model.add(Flatten()) model.add(Dense(6))",|python|tensorflow|keras|nan|,Training,2
70115610,"Weird behaviour of loss function in pytorch. I'm computing a custom cost function that is simply taking the exponential of cross-entropy divided by a parameter \eta. During the first iterations (around 20), the training loss is decreasing, but after that, I get suddenly a nan, which I don't understand why is happening. The code I'm using is the following: e_loss = [] eta = 2 #just an example of value of eta I'm using criterion = nn.CrossEntropyLoss() for e in range(epoch): train_loss = 0 for batch_idx, (data, target) in enumerate(train_loader): client_model.train() optimizer.zero_grad() output = client_model(data) loss = torch.exp(criterion(output, target)/eta) # this is the line where I input my custom loss function loss.backward() optimizer.step() train_loss += loss.item()*data.size(0) train_loss = train_loss/len(train_loader) # average losses e_loss.append(train_loss)",|python|deep-learning|pytorch|nan|loss-function|,Training,2
70148149,"How to fix no validation accuracy?. I'm working on a Neural Network and I've been training it recently, and it has approximately 93% accuracy on the training data and 0% accuracy on the validation data. My first thought was overfitting, but the model doesn't save in between training and I get these results in the first Epoch. I'm using keras in python with the following model code: model = Sequential( [ Conv1D(320, 8, input_shape=(560, 560), activation=""relu""), # Conv1D(320, 8, activation=""relu""), # Conv1D(320, 8, activation=""relu""), # Dense(750, activation=""relu""), # Dropout(0.6), Dense(1500, activation=""relu""), Dropout(0.6), Dense(750, activation=""relu""), Dropout(0.6), GlobalMaxPooling1D(keepdims=True), Dense(1, activation='softmax') ] ) model.compile(optimizer=Adam(learning_rate=0.00001), loss=""binary_crossentropy"", metrics=['accuracy']) earlystopping = callbacks.EarlyStopping(monitor=""val_accuracy"", mode=""max"", patience=2, restore_best_weights=True) model1 = model.fit(x=training_x, y=training_y, batch_size=150, epochs=5, shuffle=True, verbose=1, callbacks=[earlystopping], validation_data=(val_x, val_y)) The results I'm getting look like this: Epoch 1/5 167/167 [==============================] - 1266s 8s/step - loss: 6.4154 - accuracy: 0.9262 - val_loss: 0.0054 - val_accuracy: 0.0000e+00 I've tried changing almost all of the hyperparameters and changing the model's architecture but I keep getting similar results. Does this have anything to do with the data? The data I'm using is a 3d NumPy array containing pixel data from a bunch of images. Any help here would be greatly appreciated.",|python|numpy|tensorflow|keras|neural-network|,Model,0
70159221,"RuntimeError: mean(): input dtype should be either floating point or complex dtypes. Got Long instead. I wrote below code using pytorch and ran into a runtime error: tns = torch.tensor([1,0,1]) tns.mean() --------------------------------------------------------------------------- RuntimeError Traceback (most recent call last) <ipython-input-666-194e5ab56931> in <module> ----> 1 tns.mean() RuntimeError: mean(): input dtype should be either floating point or complex dtypes. Got Long instead. However, if I change the tensor to float, the error goes away: tns = torch.tensor([1.,0,1]) tns.mean() --------------------------------------------------------------------------- tensor(0.6667) My question is why the error happens. The data type of the first tenor is int64 instead of Long, why does PyTorch take it as Long?",|pytorch|tensor|,API,4
70205917,"Pytorch Geometric Expected all tensors to be on the same device [demo code not working]. I wanted to try out the link prediction functionality demonstrated here. Here are my versions: PyTorch Geometric v2.0.2 PyTorch v1.9.0+cu111 I'm very baffled why cuda:0 is printed for every tensor yet I see the error when I pass the data through RandomLinkSplit. import torch import torch_geometric.transforms as T from torch_geometric.nn import GCNConv from torch_geometric.datasets import Planetoid from torch_geometric.utils import negative_sampling device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') transform = T.Compose([ T.NormalizeFeatures(), T.ToDevice(device), ]) dataset = Planetoid(root='/tmp/Planetoid', name='Cora', transform=transform) data = dataset[0] print(data.to_dict()) print(data.keys) transform = T.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True,) train_data, val_data, test_data = transform(data) output: {'x': tensor([[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]], device='cuda:0'), 'edge_index': tensor([[ 0, 0, 0, ..., 2707, 2707, 2707], [ 633, 1862, 2582, ..., 598, 1473, 2706]], device='cuda:0'), 'y': tensor([3, 4, 4, ..., 3, 3, 3], device='cuda:0'), 'train_mask': tensor([ True, True, True, ..., False, False, False], device='cuda:0'), 'val_mask': tensor([False, False, False, ..., False, False, False], device='cuda:0'), 'test_mask': tensor([False, False, False, ..., True, True, True], device='cuda:0')} ['val_mask', 'test_mask', 'edge_index', 'train_mask', 'x', 'y'] --------------------------------------------------------------------------- RuntimeError Traceback (most recent call last) /tmp/ipykernel_72/414574324.py in <module> 20 21 transform = T.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True,) ---> 22 train_data, val_data, test_data = transform(data) /usr/local/lib/python3.7/dist-packages/torch_geometric/transforms/random_link_split.py in __call__(self, data) 204 train_edges, 205 neg_edge_index[:, num_neg_val + num_neg_test:], --> 206 out=train_store, 207 ) 208 self._create_label( /usr/local/lib/python3.7/dist-packages/torch_geometric/transforms/random_link_split.py in _create_label(self, store, index, neg_edge_index, out) 284 if neg_edge_index.numel() > 0: 285 edge_label = torch.cat([edge_label, neg_edge_label], dim=0) --> 286 edge_index = torch.cat([edge_index, neg_edge_index], dim=-1) 287 out[self.key] = edge_label 288 out[f'{self.key}_index'] = edge_index RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking arugment for argument tensors in method wrapper__cat)",|python|pytorch|gpu|pytorch-geometric|,GPU Usage,3
70233512,"tf.concat tensors with different length. I have 2 tensors like: a = tf.constant([[1, 2, 3], [1, 2, 3]]) b = tf.constant([1, 2, 3, 4, 5]) My desired output would be: <tf.Tensor: shape=(4, 2), dtype=int64, numpy= array([[1, 2, 3, 0, 0], [1, 2, 3, 0, 0], [1, 2, 3, 4, 5]], dtype=int64)> But when I try tf.concat([a, b], axis=0) I get this error: InvalidArgumentError: ConcatOp : Dimensions of inputs should match: shape[0] = [2,3] vs. shape[1] = [1,5] [Op:ConcatV2] name: concat",|python|tensorflow|tensorflow2.0|tensor|,Tensors&Inputs,1
70316929,"ToPILImage : TypeError: Input type int64 is not supported. I'm trying to develop a GAN using FastAi. When converting the Tensor to an Image I get this error. Traceback (most recent call last): File ""/Users/DevDog/Documents/vsc/pokemon/dementad.py"", line 44, in <module> im =transforms.ToPILImage()(img[0]).convert('RGBA') File ""/Users/DevDog/miniforge3/envs/python386/lib/python3.8/site-packages/torchvision/transforms/transforms.py"", line 179, in __call__ return F.to_pil_image(pic, self.mode) File ""/Users/DevDog/miniforge3/envs/python386/lib/python3.8/site-packages/torchvision/transforms/functional.py"", line 290, in to_pil_image raise TypeError('Input type {} is not supported'.format(npimg.dtype)) TypeError: Input type int64 is not supported Here's the full code import fastai from fastai.data import transforms from fastai.data.block import DataBlock, TransformBlock from fastai.data.transforms import get_image_files from fastai.optimizer import RMSProp from fastai.vision.data import ImageBlock, ImageDataLoaders from fastcore.imports import noop from numpy import negative import torch import cv2 import PIL from torchvision import transforms from PIL import Image from torch import nn from fastai.vision import * from fastai.vision.augment import * from fastai.imports import * from fastai.vision.gan import * from fastai.data.block import * from fastai.data.transforms import * from fastai.callback.all import * path = Path('pokeman') bs=100 size=64 dblock = DataBlock(blocks = (TransformBlock, ImageBlock), get_x = generate_noise, get_items = get_image_files, splitter = IndexSplitter([]), item_tfms=Resize(size, method=ResizeMethod.Crop), batch_tfms = Normalize.from_stats(torch.tensor([0.5,0.5,0.5]), torch.tensor([0.5,0.5,0.5]))) dls = dblock.dataloaders(path,path=path,bs=bs) generator = basic_generator(64,3,n_extra_layers=1) critic = basic_critic(64, 3, n_extra_layers=1,act_cls=partial(nn.LeakyReLU)) student = GANLearner.wgan(dls,generator,critic,opt_func = RMSProp) student.recorder.train_metrics=True student.recorder.valid_metrics=False student.fit(1,2e-4,wd=0.) #cv2.waitKey(0) student.show_results(max_n=9,ds_idx=0) student.gan_trainer.switch(gen_mode=True) img = student.predict(generate_noise('pocheman',size=100)) print(img[0].size()) im =transforms.ToPILImage()(img[0]).convert('RGB') The point of the Code is to generate pokemon images. But whenever I predict and convert it to a PIL Image the code fails with the aforementioned error.",|python|pytorch|tensor|torchvision|fast-ai|,API,4
70373342,"Error with tensorflow map_fn. Unable to specify output signature. I am trying to use tensorflow's tf.map_fn to map a ragged tensor but I am getting an error that I can't fix. Here is some minimal code that demonstrates the error: import tensorflow as tf X = tf.ragged.constant([[0,1,2], [0,1]]) def outer_product(x): return x[...,None]*x[None,...] tf.map_fn(outer_product, X) My desired output is: tf.ragged.constant([ [[0, 0, 0], [0, 1, 2], [0, 2, 4]], [[0, 0], [0, 1]] ]) The error I am getting is: ""InvalidArgumentError: All flat_values must have compatible shapes. Shape at index 0: [3]. Shape at index 1: [2]. If you are using tf.map_fn, then you may need to specify an explicit fn_output_signature with appropriate ragged_rank, and/or convert output tensors to RaggedTensors. [Op:RaggedTensorFromVariant]"" I realize I need to specify fn_output_signature but despite experimentation, I cannot figure out what it should be. EDIT: I cleaned up AloneTogether's excellent answer a little bit and created a function that maps ragged tensors. His answer uses the tf.ragged.stack function to convert the tensors to ragged tensors which tf.map_fn needs for some reason def ragged_map_fn(func, t): def new_func(t): return tf.ragged.stack(func(t),0) signature = tf.type_spec_from_value(new_func(t[0])) ans = tf.map_fn(new_func, t, fn_output_signature=signature) ans = tf.squeeze(ans, 1) return ans",|python|tensorflow|tensorflow2.0|tensor|ragged-tensors|,API,4
70542788,"Tensorflow Image classification get train_images/train_X and train_labels/train_y. I'm working on a tensorflow model for identifying different butterfies. I'm using Neural networks for this and I'm reading images from folders and all the data gets split in a train dataset and validation dataset, but I want split these like this: (train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data() instead of: train_ds = utils.image_dataset_from_directory(data_dir, validation_split=0.2, subset=""training"", seed=123, image_size=(img_height, img_width), batch_size=BATCH_SIZE) val_ds = utils.image_dataset_from_directory(data_dir, validation_split=0.2, subset=""validation"", seed=123, image_size=(img_height, img_width), batch_size=BATCH_SIZE) I have tried doing this, but it makes the accuracy of my model real crap so I don't think it's correct: train_images = np.concatenate([x for x, y in train_ds], axis=0) train_labels = np.concatenate([y for x, y in train_ds], axis=0) test_images = np.concatenate([x for x, y in val_ds], axis=0) test_labels = np.concatenate([y for x, y in val_ds], axis=0) I have tried a lot of methods from stackoverflow, but they also don't work. My model: model = tf.keras.Sequential([ # Please reread this link for a better understanding of the data being entered: #https://www.codespeedy.com/determine-input-shape-in-keras-tensorflow/ layers.Conv2D(32, (3, 3), activation='relu', input_shape=(180, 180, 3)), layers.MaxPooling2D((2, 2)), layers.Conv2D(64, (3, 3), activation='relu'), layers.MaxPooling2D((2, 2), strides=2), layers.Flatten(), layers.Dropout(0.2, input_shape=(180, 180, 3)), layers.Dense(64, activation='relu'), layers.Dense(5, activation='softmax') # there are 5 classes_names/folders or 5 kinds of butterflies ])",|python|tensorflow|validation|keras|training-data|,Training,2
70554441,"Multi Label Imbalanced dataset classification. I am currently working on an multi label fashion item dataset which is highly imbalanced I tried using class_weights to tackle it, but still the accuracy is stuck at 0.7556 every epoch. Is there any way, I can avoid this problem. Did I implement the class weights in a wrong way? I tried using data augmentation too. I have like 224 unique classes in train set. And some of them have only one example which is very frustrating Tried to solve the problem with the help of this notebook as well, but I am unable to get the same accuracy score. Looks like, in this notebook the possibility of imbalance in the dataset is not considered. def calculating_class_weights(classes,df): number_dim = np.shape(classes)[0] weights = np.empty([number_dim, 2]) for i in range(len(classes)): weights[i] = compute_class_weight(class_weight='balanced', classes=[0.,1.], y=df[classes[i]]) return weights def get_weighted_loss(weights): def weighted_loss(y_true, y_pred): y_true = tf.cast(y_true, tf.float32) return K.mean((weights[:,0]**(1-y_true))*(weights[:,1]**(y_true))* K.binary_crossentropy(y_true, y_pred), axis=-1) return weighted_loss weights=calculating_class_weights(train_labels,train_df) train_dataGen = ImageDataGenerator( rescale=1./255, rotation_range=40, width_shift_range=0.2, height_shift_range=0.2, shear_range = 0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest', ) valid_dataGen = ImageDataGenerator(rescale=1./255) model = keras.models.Sequential([ keras.layers.Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(256,256,3)), keras.layers.BatchNormalization(), keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)), keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=""same""), keras.layers.BatchNormalization(), keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)), keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=""same""), keras.layers.BatchNormalization(), keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=""same""), keras.layers.BatchNormalization(), keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=""same""), keras.layers.BatchNormalization(), keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)), keras.layers.Flatten(), keras.layers.Dense(4096, activation='relu'), keras.layers.Dropout(0.5), keras.layers.Dense(4096, activation='relu'), keras.layers.Dropout(0.5), keras.layers.Dense(224, activation='sigmoid') ]) model.compile(loss=get_weighted_loss(weights), optimizer='adam', metrics=['accuracy']) model.fit(train_generator, epochs=10, validation_data=valid_generator, callbacks=[tensorboard_cb,lrr])",|tensorflow|keras|deep-learning|multilabel-classification|imbalanced-data|,Training,2
70577039,"Lists of PyTorch Lightning sub-models don't get transferred to GPU. When using PyTorch Lightning on CPU, everything works fine. However when using GPUs, I get a RuntimeError: Expected all tensors to be on the same device. It seems that the trouble comes from the model using a list of sub-models which don't get passed to the GPU: class LambdaLayer(LightningModule): def __init__(self, fun): super(LambdaLayer, self).__init__() self.fun = fun def forward(self, x): return self.fun(x) class TorchModel(LightningModule): def __init__(self): super(TorchModel, self).__init__() self.cat_layers = [TorchCatEmbedding(cat) for cat in columns_to_embed] self.num_layers = [LambdaLayer(lambda x: x[:, idx:idx+1]) for _, idx in numeric_columns] self.ffo = TorchFFO(len(self.num_layers) + sum([embed_dim(l) for l in self.cat_layers]), y.shape[1]) self.softmax = torch.nn.Softmax(dim=1) model = TorchModel() trainer = Trainer(gpus=-1) Before running trainer(model): >>> model.device device(type='cpu') >>> model.ffo.device device(type='cpu') >>> model.cat_layers[0].device device(type='cpu') After running trainer(model): >>> model.device device(type='cuda', index=0) # <---- correct >>> model.ffo.device device(type='cuda', index=0) # <---- correct >>> model.cat_layers[0].device device(type='cpu') # <---- still showing 'cpu' Apparently, PyTorch Lightning is not able to transfer the lists of sub-models to the GPU. How to proceed so that the entire model, including list of sub-models (cat_layers and num_layers) is transferred to the GPU?",|python|pytorch|gpu|pytorch-lightning|,GPU Usage,3
70589997,"Keras loss: 0.0000e+00 and accuracy stays constant. I have 101 folders from 0-100 containing synthetic training images. This is my code: dataset = tf.keras.utils.image_dataset_from_directory( 'Pictures/synthdataset5', labels='inferred', label_mode='int', class_names=None, color_mode='rgb', batch_size=32, image_size=(128,128), shuffle=True, seed=None, validation_split=None, subset=None,interpolation='bilinear', follow_links=False,crop_to_aspect_ratio=False ) from keras.models import Sequential from keras.layers import Dense, Conv2D, Flatten model = Sequential() model.add(Conv2D(32, kernel_size=5, activation='relu', input_shape=(128,128,3))) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Conv2D(64, kernel_size=5, activation='relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Conv2D(128, kernel_size=3, activation='relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Conv2D(256, kernel_size=3, activation='relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Flatten()) model.add(Dense(1, activation='sigmoid')) model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) model.fit(dataset,epochs=75) And I always get the same result for every epoch: Epoch 1/75 469/469 [==============================] - 632s 1s/step - loss: 0.0000e+00 - accuracy: 0.0098 What's wrong???",|python|tensorflow|machine-learning|keras|deep-learning|,Model,0
70607355,"Sparse Categorical CrossEntropy causing NAN loss. So, I've been trying to implement a few custom losses, and so thought I'd start off with implementing SCE loss, without using the built in TF object. Here's the function I wrote for it. def custom_loss(y_true, y_pred): print(y_true, y_pred) return tf.cast(tf.math.multiply(tf.experimental.numpy.log2(y_pred[y_true[0]]), -1), dtype=tf.float32) y_pred is the set of probabilties, and y_true is the index of the correct one. This setup should work according to all that I've read, but it returns NAN loss. I checked if there's a problem with the training loop, but it works prefectly with the builtin losses. Could someone tell me what the problem is with this code?",|python|tensorflow|keras|loss-function|,Training,2
70817268,"Predictions become irrational after adding weights to the fit. I have a model with several dense layers that behaves normally in all aspects. Then, I add weights to the training events (their values are between 0 and 1): w = mydata.Weight #... kfold = GroupKFold(n_splits=num_folds) for train, test in kfold.split(X, y, groups=groups): X_train, X_test = X.iloc[train], X.iloc[test] y_train, y_test = y.iloc[train], y.iloc[test] w_train = w.iloc[train] #... le_fit = model.fit(X_train, y_train, batch_size=200, epochs=10, sample_weight=w_train, verbose=0) #... predictions = np.rint(model.predict(X_test)) and the prediction becomes useless: InvalidArgumentError: `predictions` contains negative values Condition x >= 0 did not hold element-wise: x (confusion_matrix_1/Cast:0) = [-9223372036854775808 ....... Just to be safe, I added constraints in the layers, eg: layers.Dense(units=800, activation='relu', kernel_constraint=constraints.MinMaxNorm(min_value=0.0, max_value=1.0)) but nothing changed. Can you suggest what is going wrong? Edit: I now realized that the training loss is also a nan. Edit: I made all weights equal to one. The results don't change. Edit: I don't know why this question was closed as asking for debugging. The answer makes it obvious that it wasn't about debugging. It is about the correct usage of two very commonly used items (Keras with GroupKFold), which turns out to include a counter-intuitive element, and it is not problem-specific.",|python|keras|neural-network|training-data|weighted|,Training,2
70890894,"ValueError: Input 0 of layer sequential is incompatible with the layer: expected ndim=4, found ndim=3. Full shape received: (None, 28, 28). I am doing an AI with an interface, using Tensorflow and Flask. I have the following code hosted on PythonAnywhere: @app.route('/output', methods=[""GET"", ""POST""]) def processing(): if request.args.get('sample'): image = Image.open(f'/home/ainum/web/static/samples/{request.args.get(""sample"")}.jpg') filename = f'/home/ainum/web/static/samples/{request.args.get(""sample"")}.jpg' else: img = request.files['img'] name, extension = img.filename.split('.') filename = name + datetime.now().strftime(""%Y-%m-%d-%H-%M-%S"") + '.' + extension img.save('/home/ainum/web/inputs/' + filename) image = Image.open('/home/ainum/web/inputs/' + filename) model = load_model('/home/ainum/web/model_fmr_all.h5') img_width, img_height = Image.HAMMING, Image.HAMMING img = image.resize((28, 28), 1) img = img.convert('L') image = np.array(img, dtype='float64') / 255. image = np.expand_dims(image, axis=0) pred = model.predict(image) return render_template('res.html', res=str(pred)) The error I get when I upload an image is: **NO MATCH** 2022-01-28 08:36:09,018: Exception on /output [POST] Traceback (most recent call last): File ""/usr/local/lib/python3.7/site-packages/flask/app.py"", line 2051, in wsgi_app response = self.full_dispatch_request() File ""/usr/local/lib/python3.7/site-packages/flask/app.py"", line 1501, in full_dispatch_request rv = self.handle_user_exception(e) File ""/usr/local/lib/python3.7/site-packages/flask/app.py"", line 1499, in full_dispatch_request rv = self.dispatch_request() File ""/usr/local/lib/python3.7/site-packages/flask/app.py"", line 1485, in dispatch_request return self.ensure_sync(self.view_functions[rule.endpoint])(**req.view_args) File ""/home/ainum/web/app.py"", line 38, in processing pred = model.predict(image) File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py"", line 1629, in predict tmp_batch_outputs = self.predict_function(iterator) File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 828, in __call__ result = self._call(*args, **kwds) File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 871, in _call self._initialize(args, kwds, add_initializers_to=initializers) File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 726, in _initialize *args, **kwds)) File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 2969, in _get_concrete_function_internal_garbage_collected graph_function, _ = self._maybe_define_function(args, kwargs) File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 3361, in _maybe_define_function graph_function = self._create_graph_function(args, kwargs) File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 3206, in _create_graph_function capture_by_value=self._capture_by_value), File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py"", line 990, in func_graph_from_py_func func_outputs = python_func(*func_args, **func_kwargs) File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 634, in wrapped_fn out = weak_wrapped_fn().__wrapped__(*args, **kwds) File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py"", line 977, in wrapper raise e.ag_error_metadata.to_exception(e) ValueError: in user code: **NO MATCH** /usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1478 predict_function * return step_function(self, iterator) /usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1468 step_function ** outputs = model.distribute_strategy.run(run_step, args=(data,)) /usr/local/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:1259 run return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs) /usr/local/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica return self._call_for_each_replica(fn, args, kwargs) /usr/local/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica return fn(*args, **kwargs) /usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1461 run_step ** outputs = model.predict_step(data) /usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1434 predict_step return self(x, training=False) /usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:998 __call__ input_spec.assert_input_compatibility(self.input_spec, inputs, self.name) /usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/input_spec.py:223 assert_input_compatibility str(tuple(shape))) **NO MATCH** ValueError: Input 0 of layer sequential is incompatible with the layer: expected ndim=4, found ndim=3. Full shape received: (None, 28, 28) **NO MATCH** On the other hand, on localhost everything is correct. Can you explain me this, please? I absolutely don't know AI; the code is not mine; I'm just creating the interface. Thank you!",|python|tensorflow|keras|tensor|,Tensors&Inputs,1
71014038,"TensorFlow BinaryCrossentropy loss quickly reaches NaN. TL;DR - ML model loss, when retrained with new data, reaches NaN quickly. All of the ""standard"" solutions don't work. Hello, Recently, I (successfully) trained a CNN/dense-layered model to be able to classify spectrograms (image representations of audio.) I wanted to try training this model again with new data and made sure that it was the correct dimensions, etc. However, for some reason, the BinaryCrossentropy loss function steadily declines until around 1.000 and suddenly becomes ""NaN"" within the first epoch. I have tried lowering the learning rate to 1e-8, am using ReLu throughout and sigmoid for the last layer, but nothing seems to be working. Even simplifying the network to only dense layers, this problem still happens. While I have manually normalized my data, I am pretty confident I did it right so that all of my data falls between [0, 1]. There might be a hole here, but I think that is unlikely. I attached my code for the model architecture here: input_shape = (125, 128, 1) model = models.Sequential([ layers.Conv2D(16, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.001), input_shape=input_shape), layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same'), layers.BatchNormalization(), layers.Conv2D(16, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.001)), layers.MaxPooling2D((2, 2), strides=(1, 1), padding='same'), layers.BatchNormalization(), layers.Conv2D(16, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.001)), layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same'), layers.BatchNormalization(), layers.Conv2D(16, (2, 2), activation='relu', kernel_regularizer=regularizers.l2(0.001)), layers.MaxPooling2D((2, 2), strides=(1, 1), padding='same'), layers.BatchNormalization(), layers.Conv2D(16, (2, 2), activation='relu', kernel_regularizer=regularizers.l2(0.001)), layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same'), layers.BatchNormalization(), layers.Conv2D(16, (2, 2), activation='relu', kernel_regularizer=regularizers.l2(0.001)), layers.MaxPooling2D((2, 2), strides=(1, 1), padding='same'), layers.BatchNormalization(), layers.Dropout(0.3), layers.Flatten(), layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001)), layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001)), layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)), layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)), layers.Dropout(0.5), layers.Dense(1, activation='sigmoid') ]) Interestingly though, I tried using this new data to fine-tune a VGG16 model, and it worked! (there is no loss NaN problem.) I've attached that code here, but I genuinely have no idea where/if there is any difference causing the problem: base_model = keras.applications.VGG16( weights=""imagenet"", input_shape=(125, 128, 3), include_top=False, ) # Freeze the base_model base_model.trainable = False # Create new model on top inputs = keras.Input(shape=(125, 128, 3)) x = inputs x = base_model(x, training=False) x = keras.layers.GlobalAveragePooling2D()(x) x = keras.layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x) x = keras.layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x) x = keras.layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x) x = keras.layers.Dropout(0.5)(x) # Regularize with dropout outputs = keras.layers.Dense(1, activation='sigmoid')(x) model = keras.Model(inputs, outputs) model.summary() I think I've been through all of the ""book"" solutions, and still can't seem to find the source of the problem. Any help would be much appreciated.",|tensorflow|keras|deep-learning|nan|loss-function|,Training,2
71159722,"AttributeError: 'Tensor' object has no attribute 'is_initialized'. I got this error when I try to fit the model. I tried to use a single GPU version but it remains. If I upgrade to TensorFlow 2 it will be solved but I need to keep it that in this version of TensorFlow. This is the code for the model that I have used. This model consists of different layers. def hybrid_LSTM(depth=2,conv_size=16,dense_size=512,input_dim=(100,5,9,),dropoutRate=0.2): """""" Autoencoder model builder composes of CNNs and a LSTM Args: depth (int): number of CNN blocks, each has 3 CNN layers with BN and a dropout conv_size (int): initial CNN filter size, doubled in each depth level dense_size (int): size of latent vector and a number of filters of ConvLSTM2D input_dim (tuple): input dimention, should be in (y_spatial,x_spatial,temporal) dropoutRate (float): dropout rate used in all nodes Return: keras model """""" """"""Setup"""""" temp_filter = conv_size X = Input(shape=input_dim, name = 'input') model_input = X # X = Permute((3,1,2))(X) #move temporal axes to be first dim X = Reshape((100,5,9,1))(X) #reshape (,1) to be feature of each spatial """"""Encoder"""""" for i in range(depth): for j in range(3): if j == 0: #j==0 is first layer(j) of the CNN block(i); apply stride with double filter size X = TimeDistributed(Conv2D(2*temp_filter,(3,3),padding='same' ,strides=(2,2),data_format=""channels_last""),name = 'encoder_'+str(i)+str(j)+'_timeConv2D')(X) else: X = TimeDistributed(Conv2D(temp_filter,(3,3), padding='same', data_format=""channels_last""),name = 'encoder_'+str(i)+str(j)+'_timeConv2D')(X) X = BatchNormalization(name = 'encoder_'+str(i)+str(j)+'_BN')(X) X = LeakyReLU(alpha=0.1,name = 'encoder_'+str(i)+str(j)+'_relu')(X) X = Dropout(dropoutRate,name = 'encoder_'+str(i)+str(j)+'_drop')(X) temp_filter = int(temp_filter * 2) X = TimeDistributed(Flatten())(X) X = LSTM(dense_size, recurrent_dropout=dropoutRate ,return_sequences=False, implementation=2)(X) """"""Latent"""""" latent = X """"""Setup for decoder"""""" X = RepeatVector(100)(X) temp_filter = int(temp_filter/2) """"""Decoder"""""" X = LSTM(temp_filter*2*3, recurrent_dropout=dropoutRate ,return_sequences=True, implementation=2)(X) X = Reshape((100,2,3,temp_filter))(X) for i in range(depth): for j in range(3): if j == 0: X = TimeDistributed(UpSampling2D((2,2)),name = 'decoder_'+str(i)+str(j)+'_upsampling')(X) X = TimeDistributed(ZeroPadding2D(((1,0),(1,0))),name = 'decoder_'+str(i)+str(j)+'_padding')(X) X = TimeDistributed(Conv2D(temp_filter,(3,3),data_format=""channels_last""),name = 'decoder_'+str(i)+str(j)+'_timeConv2D')(X) else: X = TimeDistributed(Conv2D(temp_filter,(3,3), padding='same', data_format=""channels_last""),name = 'decoder_'+str(i)+str(j)+'_timeConv2D')(X) X = BatchNormalization(name = 'decoder_'+str(i)+str(j)+'_BN')(X) X = LeakyReLU(alpha=0.1,name = 'decoder_'+str(i)+str(j)+'_relu')(X) X = Dropout(dropoutRate,name = 'decoder_'+str(i)+str(j)+'_drop')(X) temp_filter = int(temp_filter / 2) X = TimeDistributed(Conv2D(1,(1,1), padding='same', data_format=""channels_last""),name = 'decoder__timeConv2D')(X) X = Reshape((100,5,9))(X) # X = Permute((2,3,1))(X) decoded = X X = latent X = Dense(1,name = 'Dense10',activation='sigmoid')(X) return Model(inputs = model_input, outputs = [decoded,X]) File ""/Midgard/home/projects/Pre-trained-EEG-for-Deep-Learning-master/trainSafe_version1.py"", line 167, in train_subtasks_all_tasks_keras parallel_model = multi_gpu_model(model, gpus=2) File ""/Midgard/home/miniconda3/envs/erpenet5/lib/python3.7/site-packages/tensorflow/python/keras/utils/multi_gpu_utils.py"", line 172, in multi_gpu_model available_devices = _get_available_devices() File ""/Midgard/home/miniconda3/envs/erpenet5/lib/python3.7/site-packages/tensorflow/python/keras/utils/multi_gpu_utils.py"", line 28, in _get_available_devices return [x.name for x in K.get_session().list_devices()] File ""/Midgard/home/miniconda3/envs/erpenet5/lib/python3.7/site-packages/tensorflow/python/keras/backend.py"", line 462, in get_session _initialize_variables(session) File ""/Midgard/home/miniconda3/envs/erpenet5/lib/python3.7/site-packages/tensorflow/python/keras/backend.py"", line 879, in _initialize_variables [variables_module.is_variable_initialized(v) for v in candidate_vars]) File ""/Midgard/home/miniconda3/envs/erpenet5/lib/python3.7/site-packages/tensorflow/python/keras/backend.py"", line 879, in <listcomp> [variables_module.is_variable_initialized(v) for v in candidate_vars]) File ""/Midgard/home/miniconda3/envs/erpenet5/lib/python3.7/site-packages/tensorflow/python/util/tf_should_use.py"", line 193, in wrapped return _add_should_use_warning(fn(*args, **kwargs)) File ""/Midgard/home/miniconda3/envs/erpenet5/lib/python3.7/site-packages/tensorflow/python/ops/variables.py"", line 3083, in is_variable_initialized return state_ops.is_variable_initialized(variable) File ""/Midgard/home/miniconda3/envs/erpenet5/lib/python3.7/site-packages/tensorflow/python/ops/state_ops.py"", line 133, in is_variable_initialized return ref.is_initialized(name=name) AttributeError: 'Tensor' object has no attribute 'is_initialized' model = hybrid_LSTM(depth=2, conv_size=8, dense_size=512, input_dim=(100, 5, 9), dropoutRate=0.2) model.compile(optimizer=SGD(learning_rate=lr, decay=1E-5), loss=[mean_squared_error_ignore_0, 'binary_crossentropy'], # metrics=['AUC','Recall', 'Precision','binary_accuracy','accuracy'], metrics={'Dense10': ['AUC', 'Recall', tf.keras.metrics.SensitivityAtSpecificity(specificity=0.02), tf.keras.metrics.SpecificityAtSensitivity(sensitivity=0.02), 'accuracy']}, loss_weights=[0.4, 0.6]) parallel_model = multi_gpu_model(model, gpus=2) parallel_model.__setattr__('callback_model', model) parallel_model.compile(optimizer=SGD(learning_rate=lr, decay=1E-5), loss=[mean_squared_error_ignore_0, 'binary_crossentropy'], # metrics=['AUC','Recall', 'Precision','binary_accuracy','accuracy'], metrics={'Dense10': ['AUC', 'Recall', tf.keras.metrics.SensitivityAtSpecificity(specificity=0.02), tf.keras.metrics.SpecificityAtSensitivity(sensitivity=0.02), 'accuracy']}, loss_weights=[0.4, 0.6]) tensorflow-gpu 1.14.0, cudatoolkit 10.1.243, cudnn 7.6.5,",|tensorflow|keras|deep-learning|gpu|tf.keras|,API,4
71223747,"Pytorch error when modifying unpacked tensor. I have an error when modifying an unpacked tensor. import torch a1, a2 = torch.tensor([1,2], dtype = torch.float64) b = torch.rand(2, requires_grad = True) a1 += b.sum() This code produces the following error: RuntimeError: A view was created in no_grad mode and is being modified inplace with grad mode enabled. This view is the output of a function that returns multiple views. Such functions do not allow the output views to be modified inplace. You should replace the inplace operation by an out-of-place one. However, I didn't receive that error when I created a1 and a2 separately as follows: a1 = torch.tensor([1], dtype = torch.float64) a1 += b.sum() I have no idea why unpacking a tensor would lead to that error. Any help is greatly appreciated",|python|pytorch|tensor|,API,4
71457035,"PyTorch NN not training. I have a bespoke NN model which works and wanted to move it to the PyTorch framework. However, the network is not training likely due to some misconfiguration. Please advise if you see something that is odd/wrong or could be a contributing reason. import torch from torch import nn, optim import torch.nn.functional as F X_train_t = torch.tensor(X_train).float() X_test_t = torch.tensor(X_test).float() y_train_t = torch.tensor(y_train).long().reshape(y_train_t.shape[0], 1) y_test_t = torch.tensor(y_test).long().reshape(y_test_t.shape[0], 1) class Classifier(nn.Module): def __init__(self): super().__init__() self.fc1 = nn.Linear(22, 10) self.fc2 = nn.Linear(10, 1) def forward(self, x): # make sure input tensor is flattened x = x.view(x.shape[0], -1) x = F.relu(self.fc1(x)) x = F.log_softmax(self.fc2(x), dim=1) return x model = Classifier() criterion = nn.BCELoss() optimizer = optim.SGD(model.parameters(), lr=0.003) epochs = 2000 steps = 0 train_losses, test_losses = [], [] for e in range(epochs): # training loss optimizer.zero_grad() log_ps = model(X_train_t) loss = criterion(log_ps, y_train_t.type(torch.float32)) loss.backward() optimizer.step() train_loss = loss.item() # test loss # Turn off gradients for validation, saves memory and computations with torch.no_grad(): log_ps = model(X_test_t) test_loss = criterion(log_ps, y_test_t.to(torch.float32)) ps = torch.exp(log_ps) train_losses.append(train_loss/len(X_train_t)) test_losses.append(test_loss/len(X_test_t)) if (e % 100 == 0): print(""Epoch: {}/{}.. "".format(e, epochs), ""Training Loss: {:.3f}.. "".format(train_loss/len(X_train_t)), ""Test Loss: {:.3f}.. "".format(test_loss/len(X_test_t))) Training is not happening: Epoch: 0/2000.. Training Loss: 0.014.. Test Loss: 0.082.. Epoch: 100/2000.. Training Loss: 0.014.. Test Loss: 0.082.. ...",|python|neural-network|pytorch|training-data|,Model,0
71514447,"loss increasing significantly in training loop. My training loss is increasing quite dramatically. Not sure why this is. I was thinking it could be something to do with how I am calculating the loss, but not sure. I think it might be because i am printing running loss instead of loss per batch. Below is my training loop: def train_model(model, optimizer, train_loader, num_epochs, criterion=criterion): total_epochs = notebook.tqdm(range(num_epochs)) model.train() running_loss=0 correct=0 total=0 for epoch in total_epochs: for i, (x_train, y_train) in enumerate(train_loader): x_train = x_train.to(device) y_train = y_train.to(device) y_pred = model(x_train) loss = criterion(y_pred, y_train) optimizer.zero_grad() loss.backward() optimizer.step() running_loss += loss.item() _, predicted = y_pred.max(1) train_loss=running_loss/len(train_loader) total += y_train.size(0) correct += predicted.eq(y_train).sum().item() train_loss=running_loss/len(train_loader) train_accu=100.*correct/total print('Train Loss: %.3f | Train Accuracy: %.3f'%(train_loss,train_accu)) However when i call train_model(): train_md = train_model(cnn_net, optimizer, data_loaders['train'], 10) It returns this: Train Loss: 1.472 | Train Accuracy: 47.949 Train Loss: 2.655 | Train Accuracy: 53.324 Train Loss: 3.732 | Train Accuracy: 56.521 Train Loss: 4.750 | Train Accuracy: 58.565 Train Loss: 5.728 | Train Accuracy: 60.130 Train Loss: 6.673 | Train Accuracy: 61.364 Train Loss: 7.590 | Train Accuracy: 62.335 Train Loss: 8.484 | Train Accuracy: 63.190 Train Loss: 9.365 | Train Accuracy: 63.934 Train Loss: 10.225 | Train Accuracy: 64.571",|python|pytorch|training-data|loss|,Training,2
71652014,"matmul to every row in pytorch tensor. I currently have a tensor that looks like this (numbers randomly selected), which I will call x: tensor([[ 1., -5.], [ 2., -4.], [ 3., 2.], [ 4., 1.], [ 5., 2.]]) I also have another 2D tensor (call it i) tensor([[-1., 1.], [ 1., -1.]], requires_grad=True) I hope to pytorch.matmul i to each row in x. Is there a way for me to achieve this? Below is my attempt: apply_i = lambda x: torch.matmul(x, i) final = pytorch.tensor([apply_i(a) for a in x]) It throws an error saying ""only one element tensors can be converted to Python scalars"". Does not work even when I remove the square bracket. Any help would be appreciated!",|python|pytorch|tensor|,API,4
71715697,"Test accuracy not increasing more than 45%. Im trying to build a CONV2D network for image classification. I have generated my own dataset by scraping images for 9 classes airplanes, animals, birds, cars, flowers, people, ships, traffic_signs, trains. The images are of resolution 612 x 400 plus (400 varies between 430 and 480). I have resized the images to 100x100 for convenience of system resources. I have designed the network and training accuracy is more than 90% but the test accuracy isn't increasing more than 45%. Any idea what im doing wrong? Total images in each class - 500 Total images - 4479 resolution - 612 x 400plus (varies between 400 & 440). Below is the code import matplotlib.pyplot as plt import numpy as np import cv2 import os import PIL import tensorflow as tf import pathlib import requests import urllib import time from bs4 import BeautifulSoup from tensorflow import keras from tensorflow.keras.models import * from tensorflow.keras.layers import * from keras.optimizers import * from keras.losses import sparse_categorical_crossentropy data_dir = pathlib.Path('D:/mixed images') data_dir len(list(data_dir.glob('*/*.jpg'))) # planes = list(data_dir.glob('airplanes/*.jpg')) # PIL.Image.open(planes[10]) img_list = [list(data_dir.glob('airplanes/*')), list(data_dir.glob('animals/*')), list(data_dir.glob('birds/*')), list(data_dir.glob('cars/*')), list(data_dir.glob('flowers/*')), list(data_dir.glob('people/*')), list(data_dir.glob('ships/*')), list(data_dir.glob('traffic_signs/*')), list(data_dir.glob('trains/*'))] obj_list = os.listdir(data_dir) obj_img_dict = dict(zip(obj_list,img_list)) obj_label_dict = dict(zip(obj_list,[0,1,2,3,4,5,6,7,8])) obj_label_dict X = [] y = [] for image_name,images in obj_img_dict.items(): for image in images: img = cv2.imread(str(image)) resized_img = cv2.resize(img,(100,100)) X.append(resized_img) y.append(obj_label_dict[image_name]) X = np.array(X) y = np.array(y) from sklearn.model_selection import train_test_split X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=42) X_train_scaled = X_train/255 X_test_scaled = X_test/255 obj_classes = 9 model = keras.Sequential() # building the convolution layers model.add(keras.layers.Conv2D(32,(3,3),input_shape= (100,100,3),padding='same',activation='relu')) model.add(keras.layers.MaxPooling2D(pool_size=(2,2))) model.add(keras.layers.Conv2D(64,(3,3), padding='same',activation='relu')) model.add(keras.layers.MaxPooling2D(pool_size=(2,2))) model.add(keras.layers.Conv2D(128,(3,3), padding='same',activation='relu')) model.add(keras.layers.MaxPooling2D(pool_size=(2,2))) model.add(keras.layers.Conv2D(256,(3,3), padding='same',activation='relu')) model.add(keras.layers.MaxPooling2D(pool_size=(2,2))) model.add(keras.layers.Conv2D(512,(3,3), padding='same',activation='relu')) model.add(keras.layers.MaxPooling2D(pool_size=(2,2))) model.add(keras.layers.Flatten()) # building the dense layers model.add(keras.layers.Dense(1024, activation='relu')) model.add(Dropout(0.5)) model.add(keras.layers.Dense(512, activation='relu')) model.add(Dropout(0.5)) model.add(keras.layers.Dense(256, activation='relu')) # model.add(Dropout(0.6)) model.add(keras.layers.Dense(128, activation='relu')) # model.add(Dropout(0.6)) model.add(keras.layers.Dense(64, activation='relu')) model.add(keras.layers.Dense(obj_classes,activation='softmax')) model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy']) model.fit(X_train_scaled, y_train, batch_size=64,epochs=50, verbose=2) model.evaluate(X_test_scaled,y_test)",|tensorflow|keras|,Model,0
71709778,"Tensorflow Addons R2 ValueError: Dimension 0 in both shapes must be equal, but are 1 and 5. I have been trying to add a tfa metric to my model compile to be tracked throughout the training. However, when I add the R2 metric, I get the following error. I thought y_shape=(1,) would fix this, however it did not. ValueError: Dimension 0 in both shapes must be equal, but are 1 and 5. Shapes are [1] and [5]. for '{{node AssignAddVariableOp_8}} = AssignAddVariableOp[dtype=DT_FLOAT](AssignAddVariableOp_8/resource, Sum_6)' with input shapes: [], [5]. My code is shown below: model = Sequential() model.add(Input(shape=(4,))) model.add(Normalization()) model.add(Dense(5, activation=""relu"", kernel_regularizer=l2(l2=1e-2))) print(model.summary()) opt = Adam(learning_rate = 1e-2) model.compile(loss=""mean_squared_error"", optimizer=tf.keras.optimizers.Adam(learning_rate=1e-2), metrics=[MeanSquaredError(name=""mse""), MeanAbsoluteError(name=""mae""), tfa.metrics.RSquare(name=""R2"", y_shape=(1,))]) history = model.fit(x = training_x, y = training_y, epochs = 10, batch_size = 64, validation_data = (validation_x, validation_y) ) Any help is greatly appreciated! Note, I also tried changing the y_shape to (5,), but then I get the error that the dimensions are not equal, but are 5 and 1...",|python|tensorflow|keras|valueerror|tensorflow-addons|,Tensors&Inputs,1
71766129,"Dataset gets re-copied to GPU (causing out of memory) when calling to eval twice. This is my bunch of code: # I train a model, save it and then clear all with del model tf.keras.backend.clear_session() gc.collect() print(f""memory usage {tf.config.experimental.get_memory_info('GPU:0')['current'] / 10 ** 9} GB"") checkpoint_model = open_saved_model() # returns a tf.keras.Model() print(f""memory usage {tf.config.experimental.get_memory_info('GPU:0')['current'] / 10 ** 9} GB"") eval_result = checkpoint_model.evaluate(train_ds[0], train_ds[1], batch_size=30) print(f""memory usage {tf.config.experimental.get_memory_info('GPU:0')['current'] / 10 ** 9} GB"") eval_result = checkpoint_model.evaluate(train_ds[0], train_ds[1], batch_size=30) The memory outputs are: memory usage 0.0 GB memory usage 0.013005312 GB memory usage 5.893292544 GB And on the last line I get tensorflow.python.framework.errors_impl.InternalError (full message at the end) My train dataset is supposed to be train_ds[0].size * train_ds[0].itemsize / 10**9 = 4.395368448 GB. My GPU available size (using nvidia-smi command) is 10481MiB / 11016MiB. If I do the used memory plus the numpy array I get 10.27146624 which is borderline to the 10.48 that tensorflow decided to allocate. Even more, although it reserved 10GB, there is a message (see full error message at the end) that it has 8GB memory (weird but it explains why I'm out of memory). Regardless of this borderline result, it seems super wrong that the dataset is allocated AGAIN. I should either re-use the dataset used in evaluate or just replace it with the new one. I tried using train_dataset = tf.data.Dataset.from_tensor_slices((train_ds[0], train_ds[1])).batch(32) and the MWE worked (with an increase of memory usage to 7.35GB) but if I change the second evaluate with a predict (which is actually my real goal) I then get the same error. I read about using os.environ[""TF_GPU_ALLOCATOR""] = ""cuda_malloc_async"" and then I just get a Process finished with exit code 139 (interrupted by signal 11: SIGSEGV) without any error message. But comparing it to the other messages, it stops before the message Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8965 MB memory: -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5 meaning that I think it fails to ""create device"". MOTIVATION This was the MWE I managed to re-create but the truth is I want to evaluate and predict on MANY datasets that should be each 5GB size. The current solution for me should be: Clear all GPU Load model Evaluate Clear all GPU Load model again Predict And then repeat steps 1 to 6 for my several datasets (Highly inefficient right?). Full error message 2022-04-06 13:24:49.708029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-04-06 13:24:49.713198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-04-06 13:24:49.713526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-04-06 13:24:49.713988: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-04-06 13:24:49.714414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-04-06 13:24:49.714715: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-04-06 13:24:49.715002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-04-06 13:24:50.044152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-04-06 13:24:50.044479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-04-06 13:24:50.044766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-04-06 13:24:50.045036: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8965 MB memory: -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5 memory usage 0.0 GB 2022-04-06 13:25:00.250155: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing. 2022-04-06 13:25:00.250170: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started. 2022-04-06 13:25:00.250192: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs 2022-04-06 13:25:00.250349: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcupti.so.11.2'; dlerror: libcupti.so.11.2: cannot open shared object file: No such file or directory 2022-04-06 13:25:00.356485: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down. 2022-04-06 13:25:00.356639: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed 2022-04-06 13:25:00.372969: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 4396941312 exceeds 10% of free system memory. 2022-04-06 13:25:03.200488: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 4396941312 exceeds 10% of free system memory. /home/barrachina/anaconda3/envs/tf-pip/lib/python3.9/site-packages/keras/utils/generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument. warnings.warn('Custom mask layers require a config and must override ' 2022-04-06 13:25:05.075473: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2) 2022-04-06 13:25:07.796065: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8303 2022-04-06 13:25:08.177722: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory 2022-04-06 13:25:08.177947: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory 2022-04-06 13:25:08.177972: W tensorflow/stream_executor/gpu/asm_compiler.cc:77] Couldn't get ptxas version string: Internal: Couldn't invoke ptxas --version 2022-04-06 13:25:08.178231: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory 2022-04-06 13:25:08.178262: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: Failed to launch ptxas Relying on driver to perform ptx compilation. Modify $PATH to customize ptxas location. This message will be only logged once. 1/187 [..............................] - ETA: 11:00 - loss: 0.8855 - accuracy: 0.3187 - average_accuracy: 0.2666 - precision: 0.3264 - recall: 0.00222022-04-06 13:25:08.716360: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing. 2022-04-06 13:25:08.716379: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started. 2/187 [..............................] - ETA: 1:05 - loss: 0.8388 - accuracy: 0.3063 - average_accuracy: 0.2759 - precision: 0.3169 - recall: 0.0049 2022-04-06 13:25:09.011233: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data. 2022-04-06 13:25:09.011432: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed 2022-04-06 13:25:09.040157: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673] GpuTracer has collected 705 callback api events and 707 activity events. 2022-04-06 13:25:09.049283: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down. 2022-04-06 13:25:09.061327: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: log/2022/04April/06Wednesday/run-13h24m42/tensorboard/train/plugins/profile/2022_04_06_13_25_09 2022-04-06 13:25:09.071522: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to log/2022/04April/06Wednesday/run-13h24m42/tensorboard/train/plugins/profile/2022_04_06_13_25_09/barrachina-SONDRA.trace.json.gz 2022-04-06 13:25:09.096291: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: log/2022/04April/06Wednesday/run-13h24m42/tensorboard/train/plugins/profile/2022_04_06_13_25_09 2022-04-06 13:25:09.101018: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to log/2022/04April/06Wednesday/run-13h24m42/tensorboard/train/plugins/profile/2022_04_06_13_25_09/barrachina-SONDRA.memory_profile.json.gz 2022-04-06 13:25:09.101899: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: log/2022/04April/06Wednesday/run-13h24m42/tensorboard/train/plugins/profile/2022_04_06_13_25_09 Dumped tool data for xplane.pb to log/2022/04April/06Wednesday/run-13h24m42/tensorboard/train/plugins/profile/2022_04_06_13_25_09/barrachina-SONDRA.xplane.pb Dumped tool data for overview_page.pb to log/2022/04April/06Wednesday/run-13h24m42/tensorboard/train/plugins/profile/2022_04_06_13_25_09/barrachina-SONDRA.overview_page.pb Dumped tool data for input_pipeline.pb to log/2022/04April/06Wednesday/run-13h24m42/tensorboard/train/plugins/profile/2022_04_06_13_25_09/barrachina-SONDRA.input_pipeline.pb Dumped tool data for tensorflow_stats.pb to log/2022/04April/06Wednesday/run-13h24m42/tensorboard/train/plugins/profile/2022_04_06_13_25_09/barrachina-SONDRA.tensorflow_stats.pb Dumped tool data for kernel_stats.pb to log/2022/04April/06Wednesday/run-13h24m42/tensorboard/train/plugins/profile/2022_04_06_13_25_09/barrachina-SONDRA.kernel_stats.pb 187/187 [==============================] - 10s 37ms/step - loss: 0.8277 - accuracy: 0.5412 - average_accuracy: 0.3043 - precision: 0.5026 - recall: 0.0087 - val_loss: 0.8309 - val_accuracy: 0.6880 - val_average_accuracy: 0.2931 - val_precision: 0.6810 - val_recall: 0.0047 memory usage 6.042584576 GB memory usage 0.006478336 GB 2022-04-06 13:25:16.022531: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 4396941312 exceeds 10% of free system memory. memory usage 0.012938752 GB 2022-04-06 13:25:18.885690: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 4396941312 exceeds 10% of free system memory. 187/187 [==============================] - 4s 16ms/step - loss: 0.8138 - accuracy: 0.6999 - average_accuracy: 0.2968 - precision: 0.6710 - recall: 0.0058 memory usage 5.90003712 GB 2022-04-06 13:25:24.458057: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 4396941312 exceeds 10% of free system memory. 2022-04-06 13:25:35.851249: W tensorflow/core/common_runtime/bfc_allocator.cc:457] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.09GiB (rounded to 4396941312)requested by op _EagerConst If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. Current allocation summary follows. Current allocation summary follows. 2022-04-06 13:25:35.851336: I tensorflow/core/common_runtime/bfc_allocator.cc:1004] BFCAllocator dump for GPU_0_bfc 2022-04-06 13:25:35.851375: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (256): Total Chunks: 263, Chunks in use: 263. 65.8KiB allocated for chunks. 65.8KiB in use in bin. 15.2KiB client-requested in use in bin. 2022-04-06 13:25:35.851405: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (512): Total Chunks: 71, Chunks in use: 70. 42.2KiB allocated for chunks. 41.8KiB in use in bin. 36.0KiB client-requested in use in bin. 2022-04-06 13:25:35.851432: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (1024): Total Chunks: 10, Chunks in use: 9. 15.0KiB allocated for chunks. 14.0KiB in use in bin. 12.6KiB client-requested in use in bin. 2022-04-06 13:25:35.851456: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (2048): Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin. 2022-04-06 13:25:35.851483: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (4096): Total Chunks: 6, Chunks in use: 6. 31.5KiB allocated for chunks. 31.5KiB in use in bin. 30.4KiB client-requested in use in bin. 2022-04-06 13:25:35.851511: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (8192): Total Chunks: 12, Chunks in use: 12. 123.0KiB allocated for chunks. 123.0KiB in use in bin. 121.5KiB client-requested in use in bin. 2022-04-06 13:25:35.851534: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (16384): Total Chunks: 1, Chunks in use: 0. 30.2KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin. 2022-04-06 13:25:35.851560: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (32768): Total Chunks: 13, Chunks in use: 11. 579.0KiB allocated for chunks. 475.5KiB in use in bin. 445.5KiB client-requested in use in bin. 2022-04-06 13:25:35.851586: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (65536): Total Chunks: 1, Chunks in use: 1. 73.8KiB allocated for chunks. 73.8KiB in use in bin. 40.5KiB client-requested in use in bin. 2022-04-06 13:25:35.851610: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (131072): Total Chunks: 18, Chunks in use: 18. 2.85MiB allocated for chunks. 2.85MiB in use in bin. 2.72MiB client-requested in use in bin. 2022-04-06 13:25:35.851634: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (262144): Total Chunks: 2, Chunks in use: 1. 769.5KiB allocated for chunks. 283.5KiB in use in bin. 162.0KiB client-requested in use in bin. 2022-04-06 13:25:35.851658: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (524288): Total Chunks: 11, Chunks in use: 10. 6.96MiB allocated for chunks. 6.33MiB in use in bin. 6.33MiB client-requested in use in bin. 2022-04-06 13:25:35.851682: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (1048576): Total Chunks: 2, Chunks in use: 2. 2.25MiB allocated for chunks. 2.25MiB in use in bin. 1.27MiB client-requested in use in bin. 2022-04-06 13:25:35.851704: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (2097152): Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin. 2022-04-06 13:25:35.851725: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (4194304): Total Chunks: 1, Chunks in use: 0. 4.43MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin. 2022-04-06 13:25:35.851769: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (8388608): Total Chunks: 1, Chunks in use: 0. 12.44MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin. 2022-04-06 13:25:35.851799: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (16777216): Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin. 2022-04-06 13:25:35.851821: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (33554432): Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin. 2022-04-06 13:25:35.851841: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (67108864): Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin. 2022-04-06 13:25:35.851865: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (134217728): Total Chunks: 1, Chunks in use: 0. 128.08MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin. 2022-04-06 13:25:35.851889: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (268435456): Total Chunks: 3, Chunks in use: 2. 8.60GiB allocated for chunks. 5.48GiB in use in bin. 5.46GiB client-requested in use in bin. 2022-04-06 13:25:35.851911: I tensorflow/core/common_runtime/bfc_allocator.cc:1027] Bin for 4.09GiB was 256.00MiB, Chunk State: 2022-04-06 13:25:35.851941: I tensorflow/core/common_runtime/bfc_allocator.cc:1033] Size: 3.12GiB | Requested Size: 1.97MiB | in_use: 0 | bin_num: 20, prev: Size: 512B | Requested Size: 384B | in_use: 1 | bin_num: -1 2022-04-06 13:25:35.851960: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Next region of size 9401270272 2022-04-06 13:25:35.851981: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7fc942000000 of size 256 next 4 2022-04-06 13:25:35.851999: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7fc942000100 of size 256 next 6 2022-04-06 13:25:35.852016: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7fc942000200 of size 256 next 3 2022-04-06 13:25:35.852032: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7fc942000300 of size 256 next 5 2022-04-06 13:25:35.852048: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7fc942000400 of size 256 next 9 2022-04-06 13:25:35.852064: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7fc942000500 of size 256 next 7 2022-04-06 13:25:35.852080: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7fc942000600 of size 256 next 8 2022-04-06 13:25:35.852097: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7fc942000700 of size 256 next 10 2022-04-06 13:25:35.852113: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7fc942000800 of size 256 next 13 2022-04-06 13:25:35.852128: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7fc942000900 of size 256 next 14 2022-04-06 13:25:35.852144: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7fc942000a00 of size 256 next 15 2022-04-06 13:25:35.852159: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7fc942000b00 of size 256 next 83 2022-04-06 13:25:35.852174: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7fc942000c00 of size 256 next 17 2022-04-06 13:25:35.852189: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7fc942000d00 of size 256 next 18 2022-04-06 13:25:35.852204: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7fc942000e00 of size 256 next 21 .... Many messages like this, StackOverflow limits my max characters so I cropped it. 2022-04-06 13:25:35.858545: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7fcaaace5700 of size 512 next 320 2022-04-06 13:25:35.858561: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] Free at 7fcaaace5900 of size 3347949312 next 18446744073709551615 2022-04-06 13:25:35.858576: I tensorflow/core/common_runtime/bfc_allocator.cc:1065] Summary of in-use Chunks by size: 2022-04-06 13:25:35.858600: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 263 Chunks of size 256 totalling 65.8KiB 2022-04-06 13:25:35.858620: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 43 Chunks of size 512 totalling 21.5KiB 2022-04-06 13:25:35.858639: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 27 Chunks of size 768 totalling 20.2KiB 2022-04-06 13:25:35.858658: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 1024 totalling 1.0KiB 2022-04-06 13:25:35.858675: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 2 Chunks of size 1280 totalling 2.5KiB 2022-04-06 13:25:35.858694: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 6 Chunks of size 1792 totalling 10.5KiB 2022-04-06 13:25:35.858712: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 6 Chunks of size 5376 totalling 31.5KiB 2022-04-06 13:25:35.858732: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 12 Chunks of size 10496 totalling 123.0KiB 2022-04-06 13:25:35.858751: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 9 Chunks of size 41472 totalling 364.5KiB 2022-04-06 13:25:35.858770: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 51456 totalling 50.2KiB 2022-04-06 13:25:35.858789: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 62208 totalling 60.8KiB 2022-04-06 13:25:35.858807: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 75520 totalling 73.8KiB 2022-04-06 13:25:35.858826: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 5 Chunks of size 147456 totalling 720.0KiB 2022-04-06 13:25:35.858845: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 11 Chunks of size 165888 totalling 1.74MiB 2022-04-06 13:25:35.858863: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 176640 totalling 172.5KiB 2022-04-06 13:25:35.858882: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 248832 totalling 243.0KiB 2022-04-06 13:25:35.858901: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 290304 totalling 283.5KiB 2022-04-06 13:25:35.858919: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 10 Chunks of size 663552 totalling 6.33MiB 2022-04-06 13:25:35.858937: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 2 Chunks of size 1179648 totalling 2.25MiB 2022-04-06 13:25:35.858955: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 1489978112 totalling 1.39GiB 2022-04-06 13:25:35.858973: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 4396941312 totalling 4.09GiB 2022-04-06 13:25:35.858991: I tensorflow/core/common_runtime/bfc_allocator.cc:1072] Sum Total of in-use chunks: 5.49GiB 2022-04-06 13:25:35.859009: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] total_region_allocated_bytes_: 9401270272 memory_limit_: 9401270272 available bytes: 0 curr_region_allocation_bytes_: 18802540544 2022-04-06 13:25:35.859036: I tensorflow/core/common_runtime/bfc_allocator.cc:1080] Stats: Limit: 9401270272 InUse: 5900037120 MaxInUse: 6431716864 NumAllocs: 165083 MaxAllocSize: 4396941312 Reserved: 0 PeakReserved: 0 LargestFreeBlock: 0 2022-04-06 13:25:35.859127: W tensorflow/core/common_runtime/bfc_allocator.cc:468] *****************************************************************___________________________________ Traceback (most recent call last): File ""/home/barrachina/Documents/onera/PolSar/principal_simulation.py"", line 524, in <module> run_wrapper(model_name=args.model[0], balance=args.balance[0], tensorflow=args.tensorflow, File ""/home/barrachina/Documents/onera/PolSar/principal_simulation.py"", line 504, in run_wrapper df, dataset_handler, eval_df = run_model(model_name=model_name, balance=balance, tensorflow=tensorflow, File ""/home/barrachina/Documents/onera/PolSar/principal_simulation.py"", line 440, in run_model prediction_result = checkpoint_model.predict(train_ds[0]) File ""/home/barrachina/anaconda3/envs/tf-pip/lib/python3.9/site-packages/keras/engine/training.py"", line 1720, in predict data_handler = data_adapter.get_data_handler( File ""/home/barrachina/anaconda3/envs/tf-pip/lib/python3.9/site-packages/keras/engine/data_adapter.py"", line 1383, in get_data_handler return DataHandler(*args, **kwargs) File ""/home/barrachina/anaconda3/envs/tf-pip/lib/python3.9/site-packages/keras/engine/data_adapter.py"", line 1138, in __init__ self._adapter = adapter_cls( File ""/home/barrachina/anaconda3/envs/tf-pip/lib/python3.9/site-packages/keras/engine/data_adapter.py"", line 230, in __init__ x, y, sample_weights = _process_tensorlike((x, y, sample_weights)) File ""/home/barrachina/anaconda3/envs/tf-pip/lib/python3.9/site-packages/keras/engine/data_adapter.py"", line 1031, in _process_tensorlike inputs = tf.nest.map_structure(_convert_numpy_and_scipy, inputs) File ""/home/barrachina/anaconda3/envs/tf-pip/lib/python3.9/site-packages/tensorflow/python/util/nest.py"", line 869, in map_structure structure[0], [func(*x) for x in entries], File ""/home/barrachina/anaconda3/envs/tf-pip/lib/python3.9/site-packages/tensorflow/python/util/nest.py"", line 869, in <listcomp> structure[0], [func(*x) for x in entries], File ""/home/barrachina/anaconda3/envs/tf-pip/lib/python3.9/site-packages/keras/engine/data_adapter.py"", line 1026, in _convert_numpy_and_scipy return tf.convert_to_tensor(x, dtype=dtype) File ""/home/barrachina/anaconda3/envs/tf-pip/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py"", line 206, in wrapper return target(*args, **kwargs) File ""/home/barrachina/anaconda3/envs/tf-pip/lib/python3.9/site-packages/tensorflow/python/framework/ops.py"", line 1430, in convert_to_tensor_v2_with_dispatch return convert_to_tensor_v2( File ""/home/barrachina/anaconda3/envs/tf-pip/lib/python3.9/site-packages/tensorflow/python/framework/ops.py"", line 1436, in convert_to_tensor_v2 return convert_to_tensor( File ""/home/barrachina/anaconda3/envs/tf-pip/lib/python3.9/site-packages/tensorflow/python/profiler/trace.py"", line 163, in wrapped return func(*args, **kwargs) File ""/home/barrachina/anaconda3/envs/tf-pip/lib/python3.9/site-packages/tensorflow/python/framework/ops.py"", line 1566, in convert_to_tensor ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref) File ""/home/barrachina/anaconda3/envs/tf-pip/lib/python3.9/site-packages/tensorflow/python/framework/tensor_conversion_registry.py"", line 52, in _default_conversion_function return constant_op.constant(value, dtype, name=name) File ""/home/barrachina/anaconda3/envs/tf-pip/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py"", line 271, in constant return _constant_impl(value, dtype, shape, name, verify_shape=False, File ""/home/barrachina/anaconda3/envs/tf-pip/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py"", line 283, in _constant_impl return _constant_eager_impl(ctx, value, dtype, shape, verify_shape) File ""/home/barrachina/anaconda3/envs/tf-pip/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py"", line 308, in _constant_eager_impl t = convert_to_eager_tensor(value, ctx, dtype) File ""/home/barrachina/anaconda3/envs/tf-pip/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py"", line 106, in convert_to_eager_tensor return ops.EagerTensor(value, ctx.device_name, dtype) tensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",|tensorflow|gpu|out-of-memory|,GPU Usage,3
72099844,"mat1 and mat2 shapes cannot be multiplied (19x1 and 19x1). I have a handmade dataset and all want to do is set a linear regression model with Pytorch. These are the codes I wrote: from torch.autograd import Variable train_x = np.asarray([1,2,3,4,5,6,7,8,9,10,5,4,6,8,5,2,1,1,6]) train_y = train_x * 2 X = Variable(torch.from_numpy(train_x).type(torch.FloatTensor), requires_grad = False).view(19, 1) y = Variable(torch.from_numpy(train_y).type(torch.FloatTensor), requires_grad = False) from torch import nn lr = nn.Linear(19, 1) loss = nn.MSELoss() optimizer = torch.optim.SGD(lr.parameters(), lr = 0.01) output = lr(X) #error occurs here I guess this is the simplest Pytorch neural network code in the world but it's still giving this error message: mat1 and mat2 shapes cannot be multiplied (19x1 and 19x1) I just did all the things on the book but it's still giving this error. Can you help me?",|python|deep-learning|pytorch|tensor|,Tensors&Inputs,1
72198387,"Why is the actual runtime still cpu after I set cuda for tensor. I want to gpu-accelerate a custom function, and below is the original function: import numpy as np def py_cpu_nms(dets, thresh): """"""Pure Python NMS baseline."""""" x1 = dets[:, 0] y1 = dets[:, 1] x2 = dets[:, 2] y2 = dets[:, 3] scores = dets[:, 4] areas = (x2 - x1 + 1) * (y2 - y1 + 1) order = scores.argsort()[::-1] keep = [] while order.size > 0: i = order[0] keep.append(i) xx1 = np.maximum(x1[i], x1[order[1:]]) yy1 = np.maximum(y1[i], y1[order[1:]]) xx2 = np.minimum(x2[i], x2[order[1:]]) yy2 = np.minimum(y2[i], y2[order[1:]]) w = np.maximum(0.0, xx2 - xx1 + 1) h = np.maximum(0.0, yy2 - yy1 + 1) inter = w * h ovr = inter / (areas[i] + areas[order[1:]] - inter) inds = np.where(ovr <= thresh)[0] order = order[inds + 1] return keep It would use the CPU to compute, but that wasn't fast enough so I wanted to speed it up directly with PyTorch, which I converted into a Torch implementation: import numpy as np import torch def py_cpu_nms(dets, thresh): """"""Pure Python NMS baseline."""""" dets = torch.from_numpy(dets) device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"") dets.cuda() x1 = dets[:, 0] y1 = dets[:, 1] x2 = dets[:, 2] y2 = dets[:, 3] scores = dets[:, 4] areas = ((x2 - x1 + 1) * (y2 - y1 + 1)).cuda() order = torch.argsort(scores) order = torch.flip(order, dims=[0]) keep = [] while order.size()[0] > 0: i = order[0] keep.append(i) xx1 = torch.maximum(x1[i], x1[order[1:]]).cuda() yy1 = torch.maximum(y1[i], y1[order[1:]]).cuda() xx2 = torch.minimum(x2[i], x2[order[1:]]).cuda() yy2 = torch.minimum(y2[i], y2[order[1:]]).cuda() w = torch.maximum(torch.tensor(0.0), xx2 - xx1 + 1).cuda() h = torch.maximum(torch.tensor(0.0), yy2 - yy1 + 1).cuda() inter = (w * h).cuda() ovr = inter / (areas[i] + areas[order[1:]] - inter) inds = torch.where(ovr <= thresh)[0].cuda() order = order[inds + 1].cuda() return keep But in fact, all the calculations still use the CPU, does anyone know why?",|python|pytorch|gpu|,GPU Usage,3
72262957,"Selecting entries of a pytorch tensor with another tensor. I have a tensor a with with float entries and torch.Size([64,2]) and I also have a tensor b with torch.Size([64]). The entries of b are only 0 or 1. I would like to get a new tensor c with torch.Size([64]) such that c[i] == a[i,b[i]] for every index i. How can I do that? My attempt I tried with torch.gather but without success. The following code gives me RuntimeError: Index tensor must have the same number of dimensions as input tensor import torch a = torch.zeros([64,2]) b = torch.ones(64).long() torch.gather(input=a, dim=1,index=b) Any help will be highly appreciated!",|python|indexing|pytorch|tensor|,Tensors&Inputs,1
72242669,"ValueError: Shapes (None, 1) and (None, 5) are incompatible in keras. model = Sequential() model.add(Conv2D(128, (3, 3), activation='relu', input_shape=(64, 64, 3), padding='same')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Conv2D(64, (3, 3), activation='relu', padding='same')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Conv2D(32, (3, 3), activation='relu', padding='same')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Flatten()) model.add(Dense(128, activation = 'relu')) model.add(Dropout(0.5)) model.add(Dense(5, activation = 'softmax')) model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=[tf.keras.metrics.Recall()]) This code works fine for metrics=['accuracy']), but it shows ValueError: Shapes (None, 1) and (None, 5) are incompatible for metrics=[tf.keras.metrics.Recall()]) Please help me. Thanks in advance.",|google-colaboratory|tf.keras|valueerror|precision-recall|,Tensors&Inputs,1
72344302,"Trying to create optimizer slot variable under the scope for tf.distribute.Strategy, which is different from the scope used for the original variable. I want to develop a DCGAN with a resolution of 1024x1024. For this, I need to use multiple GPUs, otherwise it might take too much time. I refer to the introduction in https://www.tensorflow.org/guide/distributed_training documentation At the top of the script I used strategy = tf.distribute.MirroredStrategy() Then inside the DCGAN I used with strategy.scope(): The error I get is: ValueError:Trying to create optimizer slot variable under the scope for tf.distribute.Strategy, which is different from the scope used for the original variable. Make sure the slot variables are created under the same strategy scope. This may happen if you're restoring from a checkpoint outside the scope. The following is my code: strategy = tf.distribute.MirroredStrategy() dataset = keras.preprocessing.image_dataset_from_directory( ""test2"", label_mode=None, image_size=(1024, 1024), batch_size=4) dataset = dataset.map(lambda x: x / 255.0) discriminator = keras.Sequential( [ keras.Input(shape=(1024, 1024, 3)), layers.Conv2D(8, kernel_size=4, strides=2, padding=""same""), layers.LeakyReLU(alpha=0.2), layers.Conv2D(8, kernel_size=4, strides=2, padding=""same""), layers.LeakyReLU(alpha=0.2), layers.Conv2D(16, kernel_size=4, strides=2, padding=""same""), layers.LeakyReLU(alpha=0.2), layers.Conv2D(16, kernel_size=4, strides=2, padding=""same""), layers.LeakyReLU(alpha=0.2), layers.Conv2D(32, kernel_size=4, strides=2, padding=""same""), layers.LeakyReLU(alpha=0.2), layers.Conv2D(32, kernel_size=4, strides=2, padding=""same""), layers.LeakyReLU(alpha=0.2), layers.Flatten(), layers.Dropout(0.2), layers.Dense(1, activation=""sigmoid""), ], name=""discriminator"", ) discriminator.summary() latent_dim = 1024 generator = keras.Sequential( [ keras.Input(shape=(latent_dim,)), layers.Dense(16 * 16 * 32), layers.Reshape((16, 16, 32)), layers.Conv2DTranspose(32, kernel_size=4, strides=2, padding=""same""), layers.LeakyReLU(alpha=0.2), layers.Conv2DTranspose(32, kernel_size=4, strides=2, padding=""same""), layers.LeakyReLU(alpha=0.2), layers.Conv2DTranspose(32, kernel_size=4, strides=2, padding=""same""), layers.LeakyReLU(alpha=0.2), layers.Conv2DTranspose(32, kernel_size=4, strides=2, padding=""same""), layers.LeakyReLU(alpha=0.2), layers.Conv2DTranspose(32, kernel_size=4, strides=2, padding=""same""), layers.LeakyReLU(alpha=0.2), layers.Conv2DTranspose(32, kernel_size=4, strides=2, padding=""same""), layers.LeakyReLU(alpha=0.2), layers.Conv2D(3, kernel_size=5, padding=""same"", activation=""sigmoid""), ], name=""generator"", ) generator.summary() class GAN(keras.Model): def __init__(self, strategy, discriminator, generator, latent_dim): super(GAN, self).__init__() self.discriminator = discriminator self.generator = generator self.latent_dim = latent_dim self.global_batchsize = 32 self.strategy = strategy self.batchsize_per_replica = int(self.global_batchsize/self.strategy.num_replicas_in_sync) def loss_fn(self, labels, predictions): loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True,\ reduction=tf.keras.losses.Reduction.NONE) return loss_fn(labels, predictions) def compile(self, d_optimizer, g_optimizer): super(GAN, self).compile() self.d_optimizer = d_optimizer self.g_optimizer = g_optimizer self.d_loss_metric = keras.metrics.Mean(name=""d_loss"") self.g_loss_metric = keras.metrics.Mean(name=""g_loss"") def metrics(self): return [self.d_loss_metric, self.g_loss_metric] def disc_loss(self, real_output, fake_output): real_loss = self.loss_fn(tf.ones_like(real_output), real_output) fake_loss = self.loss_fn(tf.zeros_like(fake_output), fake_output) total_loss = real_loss + fake_loss total_loss = total_loss/self.global_batchsize return total_loss def gen_loss(self, fake_output): gen_loss = self.loss_fn(tf.ones_like(fake_output), fake_output) gen_loss = gen_loss / self.global_batchsize return gen_loss def distribute_trainstep(self, dist_dataset): per_replica_g_losses, per_replica_d_losses = self.strategy.experimental_run_v2(self.train_step,dist_dataset) total_g_loss = self.strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_g_losses,axis=0) total_d_loss = self.strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_d_losses, axis=0) return total_g_loss, total_d_loss def train_step(self, real_images): batch_size = tf.shape(real_images)[0] random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim)) generated_images = self.generator(random_latent_vectors) combined_images = tf.concat([generated_images, real_images], axis=0) labels = tf.concat( [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0 ) labels += 0.05 * tf.random.uniform(tf.shape(labels)) noise = tf.random.normal(shape=[tf.shape(real_images)[0], self.latent_dim]) with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape: generated_imgs = self.generator(noise, training=True) real_output = self.discriminator(real_images, training=True) fake_output = self.discriminator(generated_imgs, training=True) d_loss = self.disc_loss(real_output, fake_output) g_loss = self.gen_loss(fake_output) G_grads = gen_tape.gradient(g_loss, self.generator.trainable_variables) D_grads = disc_tape.gradient(d_loss, self.discriminator.trainable_variables) self.g_optimizer.apply_gradients(zip(G_grads, self.generator.trainable_variables)) self.d_optimizer.apply_gradients(zip(D_grads, self.discriminator.trainable_variables)) with tf.GradientTape() as gen_tape: generated_imgs = self.generator_model(noise, training=True) fake_output = self.discriminator(generated_imgs, training=True) g_loss = self.gen_loss(fake_output) G_grads = gen_tape.gradient(g_loss, self.generator_model.trainable_variables) self.g_optimizer.apply_gradients(zip(G_grads, self.generator.trainable_variables)) return g_loss, d_loss class GANMonitor(keras.callbacks.Callback): def __init__(self, num_img=6, latent_dim=32): self.num_img = num_img self.latent_dim = latent_dim def on_epoch_end(self, epoch, logs=None): random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim)) generated_images = self.model.generator(random_latent_vectors) generated_images *= 255 generated_images.numpy() for i in range(self.num_img): img = keras.preprocessing.image.array_to_img(generated_images[i]) if epoch %50 ==0: img.save(""./1024/generated_img_%03d_%d.png"" % (epoch, i)) epochs = 5000 with strategy.scope(): gan = GAN(strategy, discriminator=discriminator, generator=generator, latent_dim=latent_dim) gan.compile( d_optimizer=keras.optimizers.Adam(learning_rate=0.0001), g_optimizer=keras.optimizers.Adam(learning_rate=0.0001), ) gan.fit( dataset, epochs=epochs, callbacks=[GANMonitor(num_img=60, latent_dim=latent_dim)] ) The error is the following Epoch 1/5000 /home/kuo/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: ""`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?"" return dispatch_target(*args, **kwargs) Traceback (most recent call last): File ""1024.py"", line 253, in <module> gan.fit( File ""/home/kuo/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py"", line 67, in error_handler raise e.with_traceback(filtered_tb) from None File ""/home/kuo/.local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py"", line 1147, in autograph_handler raise e.ag_error_metadata.to_exception(e) ValueError: in user code: File ""/home/kuo/.local/lib/python3.8/site-packages/keras/engine/training.py"", line 1021, in train_function * return step_function(self, iterator) File ""/home/kuo/.local/lib/python3.8/site-packages/keras/engine/training.py"", line 1010, in step_function ** outputs = model.distribute_strategy.run(run_step, args=(data,)) File ""/usr/local/lib/python3.8/dist-packages/six.py"", line 703, in reraise raise value File ""/home/kuo/.local/lib/python3.8/site-packages/keras/engine/training.py"", line 1000, in run_step ** outputs = model.train_step(data) File ""1024.py"", line 179, in train_step self.g_optimizer.apply_gradients(zip(G_grads, self.generator.trainable_variables)) File ""/home/kuo/.local/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py"", line 639, in apply_gradients self._create_all_weights(var_list) File ""/home/kuo/.local/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py"", line 825, in _create_all_weights self._create_slots(var_list) File ""/home/kuo/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py"", line 117, in _create_slots self.add_slot(var, 'm') File ""/home/kuo/.local/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py"", line 902, in add_slot raise ValueError( ValueError: Trying to create optimizer slot variable under the scope for tf.distribute.Strategy (<tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x7f72f39c0430>), which is different from the scope used for the original variable (<tf.Variable 'dense_1/kernel:0' shape=(1024, 8192) dtype=float32, numpy= array([[-0.00106893, 0.01506512, -0.01771315, ..., -0.01528796, -0.02354955, -0.0135217 ], [-0.01760183, -0.02044552, 0.00945723, ..., -0.02140231, 0.01164402, 0.01851213], [ 0.00233763, -0.0196434 , 0.01152603, ..., -0.02139488, 0.0125667 , 0.0251492 ], ..., [ 0.00782686, 0.00941393, 0.00423452, ..., -0.0052203 , -0.02194414, -0.0167138 ], [ 0.02420759, -0.02258933, 0.01125678, ..., -0.00626962, 0.00758442, 0.0015665 ], [-0.00925244, -0.02154037, -0.0209455 , ..., -0.01146874, 0.00285936, 0.01914702]], dtype=float32)>). Make sure the slot variables are created under the same strategy scope. This may happen if you're restoring from a checkpoint outside the scope.",|python|tensorflow|dcgan|multiple-gpu|,API,4
72408888,"PyTorch: why does running output = model(images) use so much GPU memory?. In trying to understand why my maximum batch size is limited for my PyTorch model, I noticed that it's not the model itself nor loading the tensors onto the GPU that uses the most memory. Most memory is used up when generating a prediction for the first time, e.g. with the following line in the training loop: output = model(images) where images is some input tensor, and model is my PyTorch model. Before running the line, I have something like 9GB of GPU memory available, and afterwards I'm down to 2.5GB (it then further drops to 1GB available after running loss = criterion(outputs, labels). Two questions: Is this normal? Why is it happening? What is all that memory being used for? From what I understand the model is already loaded in, and the actual input tensors are already on the GPU before making that call. The output tensors themselves can't be that big. Does it have something to do with storing the computational graph?",|python|machine-learning|pytorch|gpu|,GPU Usage,3
72497840,"Mask RCNN model doesn't save weights after epoch 2. I have used this implementation of mrcnn: https://github.com/matterport/Mask_RCNN. I have later changed to using this version: https://github.com/sabderra/Mask_RCNN to support TensorFlow 2. I'm running the training code on my university Linux VM using GPU, and it doesn't save all the weights after each iteration. when running the code first with a small training set size (4 images) for 5 epochs, all the weights get saved except for epoch 4 for some reason. when running it for 10 epochs using a larger training set size of 700 images, it only saves the weights for epochs 1 and 2 while still running until finishing the last epoch (sometimes saving the weights only for epochs 1 and 3). did anybody experience this or know how to fix it? thanks! Edit: it uses the Keras ModelCheckpoint callback function to save the model weights using the path defined here: # Path to save after each epoch. Include placeholders that get filled by Keras. self.checkpoint_path = os.path.join(self.log_dir, ""mask_rcnn_{}_*epoch*.h5"".format( self.config.NAME.lower())) self.checkpoint_path = self.checkpoint_path.replace( ""*epoch*"", ""{epoch:04d}"") this is the entire train function which calls the keras fit function: def train(self, train_dataset, val_dataset, learning_rate, epochs, layers, augmentation=None, custom_callbacks=None, no_augmentation_sources=None, patience=10): """"""Train the model. train_dataset, val_dataset: Training and validation Dataset objects. learning_rate: The learning rate to train with epochs: Number of training epochs. Note that previous training epochs are considered to be done already, so this actually determines the epochs to train in total rather than in this particular call. layers: Allows selecting which layers to train. It can be: - A regular expression to match layer names to train - One of these predefined values: heads: The RPN, classifier and mask heads of the network all: All the layers 3+: Train Resnet stage 3 and up 4+: Train Resnet stage 4 and up 5+: Train Resnet stage 5 and up augmentation: Optional. An imgaug (https://github.com/aleju/imgaug) augmentation. For example, passing imgaug.augmenters.Fliplr(0.5) flips images right/left 50% of the time. You can pass complex augmentations as well. This augmentation applies 50% of the time, and when it does it flips images right/left half the time and adds a Gaussian blur with a random sigma in range 0 to 5. augmentation = imgaug.augmenters.Sometimes(0.5, [ imgaug.augmenters.Fliplr(0.5), imgaug.augmenters.GaussianBlur(sigma=(0.0, 5.0)) ]) custom_callbacks: Optional. Add custom callbacks to be called with the keras fit_generator method. Must be list of type keras.callbacks. no_augmentation_sources: Optional. List of sources to exclude for augmentation. A source is string that identifies a dataset and is defined in the Dataset class. """""" assert self.mode == ""training"", ""Create model in training mode."" # Pre-defined layer regular expressions layer_regex = { # all layers but the backbone ""heads"": r""(mrcnn\_.*)|(rpn\_.*)|(fpn\_.*)"", # From a specific Resnet stage and up ""3+"": r""(res3.*)|(bn3.*)|(res4.*)|(bn4.*)|(res5.*)|(bn5.*)|(mrcnn\_.*)|(rpn\_.*)|(fpn\_.*)"", ""4+"": r""(res4.*)|(bn4.*)|(res5.*)|(bn5.*)|(mrcnn\_.*)|(rpn\_.*)|(fpn\_.*)"", ""5+"": r""(res5.*)|(bn5.*)|(mrcnn\_.*)|(rpn\_.*)|(fpn\_.*)"", # All layers ""all"": "".*"", } if layers in layer_regex.keys(): layers = layer_regex[layers] # Data generators train_generator = DataGenerator(train_dataset, self.config, shuffle=True, augmentation=augmentation) val_generator = DataGenerator(val_dataset, self.config, shuffle=True) # Create log_dir if it does not exist if not os.path.exists(self.log_dir): os.makedirs(self.log_dir) # Callbacks callbacks = [ keras.callbacks.TensorBoard(log_dir=self.log_dir, histogram_freq=0, write_graph=True, write_images=False), keras.callbacks.ModelCheckpoint(self.checkpoint_path, verbose=1, save_best_only=True, save_weights_only=True, period=1), ] # Add custom callbacks to the list if custom_callbacks: callbacks += custom_callbacks # Train log(f""\nStarting at epoch {self.epoch}. LR={learning_rate}\n"") log(f""Checkpoint Path: {self.checkpoint_path}"") self.set_trainable(layers) self.compile(learning_rate, self.config.LEARNING_MOMENTUM) # Work-around for Windows: Keras fails on Windows when using # multiprocessing workers. See discussion here: # https://github.com/matterport/Mask_RCNN/issues/13#issuecomment-353124009 if os.name == 'nt': workers = 0 else: workers = multiprocessing.cpu_count() history = self.keras_model.fit( train_generator, initial_epoch=self.epoch, epochs=epochs, verbose=1, steps_per_epoch=self.config.STEPS_PER_EPOCH, callbacks=callbacks, validation_data=val_generator, validation_steps=self.config.VALIDATION_STEPS, max_queue_size=100, workers=workers, use_multiprocessing=self.config.USE_MULTIPROCESSING, ) self.epoch = max(self.epoch, epochs) return history",|python|tensorflow|keras|neural-network|gpu|,Training,2
72445866,"GPU available in Tensorflow but not in Torch. I'm currently working on a server and I would like to be able the GPUs for PyTorch network training. I am not able to detect GPU by using torch but, if I use TensorFlow, I can detect both of the GPUs I am supposed to have. I suppose it's a problem with versions within PyTorch/TensorFlow and the CUDA versions on it. However, after trying different versions of Pytorch, I am not still able to use them... I am attaching the specificities of the GPUs and the current version of Tensorflow and Pytorch I am using. Does anyone have any hint on it? Would be very helpful. | NVIDIA-SMI 4--.--.-- Driver Version: 465.19.01 CUDA Version: 11.3 | |-------------------------------+----------------------+----------------------| | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 NVIDIA GeForce ... On | 00000000:02:00.0 Off | N/A | | 27% 39C P8 17W / 250W | 1MiB / 11176MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 1 NVIDIA GeForce ... On | 00000000:81:00.0 Off | N/A | | 28% 45C P8 11W / 250W | 1MiB / 11178MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ $ nvcc --version nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2020 NVIDIA Corporation Built on Wed_Jul_22_19:09:09_PDT_2020 Cuda compilation tools, release 11.0, V11.0.221 Build cuda_11.0_bu.TC445_37.28845127_0 Torch version: 1.10.2 Tensorflow Version: 2.6.2 Cuda toolkit: 11.3.1 >>> print('Number of GPUs: %d' % len(tf.config.list_physical_devices('GPU'))) Number of GPUs: 2 >>> torch.cuda.is_available() False I am so lost... Thank you in advance!",|tensorflow|pytorch|gpu|,GPU Usage,3
72571288,"Tensorflow-keras 2.X multi gpu prediction. I have 4GPU(rtx 3090) in one pc. I used only 1GPU for training and prediction, but now I'm going to use 4GPU. During training, 4gpu activation was successful, but only 1GPU is active for prediction. mirrored_strategy = tf.distribute.MirroredStrategy() with mirrored_strategy.scope(): model = MyModel() model_checkpoint = ModelCheckpoint(model_path, monitor='loss', verbose=1, save_best_only=True) history = model.fit(trainData, steps_per_epoch=num_images//batch_size, verbose=1) It Use 4 GPU model.predict(testData, verbose=1) #for x in testData: # model.predict_on_batch(x) It use only 1 GPU How can I use my all GPU?",|python|tensorflow|keras|multi-gpu|,GPU Usage,3
72676542,"Sigmoid activation output layer produce Many near-1 value. :) I have a Datset of ~16,000 .wav recording from 70 bird species. I'm training a model using tensorflow to classify the mel-spectrogram of these recordings using Convolution based architectures. One of the architectures used is simple multi-layer convolutional described below. The pre-processing phase include: extract mel-spectrograms and convert to dB Scale segment audio to 1-second segment (pad with zero Or gaussian noise if residual is longer than 250ms, discard otherwise) z-score normalization of training data - reduce mean and divide result by std pre-processing while inference: same as described above z-score normalization BY training data - reduce mean (of training) and divide result by std (of training data) I understand that the output layer's probabilities with sigmoid activation is not suppose to accumulate to 1, But I get many (8-10) very high prediction (~0.999) probabilities. and some is exactly 0.5. The current test set correct classification rate is ~84%, tested with 10-fold cross validation, So it seems that the the network mostly operates well. notes: 1.I understand there are similar features in the vocalization of different birds species, but the recieved probabilities doesn't seem to reflect them correctly 2. probabilities for example - a recording of natural noise: Natural noise: 0.999 Mallard - 0.981 I'm trying to understand the reason for these results, if it's related the the data etc extensive mislabeling (probably not) or from another source. Any help will be much appreciated! :) EDIT: I use sigmoid because the probabilities of all classes are necessary, and I don't need them to accumulate to 1. def convnet1(input_shape, numClasses, activation='softmax'): # Define the network model = tf.keras.Sequential() model.add(InputLayer(input_shape=input_shape)) # model.add(Augmentations1(p=0.5, freq_type='mel', max_aug=2)) model.add(Conv2D(64, (3, 3), activation='relu', padding='same')) model.add(BatchNormalization()) model.add(MaxPooling2D(pool_size=(2, 1))) model.add(Conv2D(128, (3, 3), activation='relu', padding='same')) model.add(BatchNormalization()) model.add(MaxPooling2D(pool_size=(2, 1))) model.add(Conv2D(128, (5, 5), activation='relu', padding='same')) model.add(BatchNormalization()) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Conv2D(256, (5, 5), activation='relu', padding='same')) model.add(BatchNormalization()) model.add(Flatten()) # model.add(Dense(numClasses, activation='relu')) model.add(Dropout(0.2)) model.add(Dense(numClasses, activation='sigmoid')) model.compile( loss='categorical_crossentropy', metrics=['accuracy'], optimizer=optimizers.Adam(learning_rate=0.001), run_eagerly=False) # this parameter allows to debug and use regular functions inside layers: print(), save() etc.. return model",|python|tensorflow|keras|deep-learning|sigmoid|,Model,0
72661641,"Adding my custom loss messes up autograd in Pytorch?. I'm trying to use two different losses, MSELoss for some of my labels and a custom loss for the other labels. I'm then trying to sum these losses together before backprop. My model prints out the same loss after every epoch so I must be doing something wrong. Any help is appreciated! I suspect my implementation is messing up Pytorch's autograd. See code below: mse_loss = torch.nn.MSELoss() ... loss1 = mse_loss(preds[:,(0,1,3)], label[:,(0,1,3)]) print(""loss1"", loss1) loss2 = my_custom_loss(preds[:,2], label[:,2]) print(""loss2"", loss2) print(""summing losses"") loss = sum([loss1, loss2]) # tensor + float = tensor print(""loss sum"", loss) loss = torch.autograd.Variable(loss, requires_grad=True) print(""loss after Variable(loss, requires_grad=True)"", loss) These print statements yield: loss1 tensor(4946.1221, device='cuda:0', grad_fn=<MseLossBackward0>) loss2 34.6672 summing losses loss sum tensor(4980.7891, device='cuda:0', grad_fn=<AddBackward0>) loss after Variable() tensor(4980.7891, device='cuda:0', requires_grad=True) My custom loss function is below: def my_custom_loss(preds, label): angle_diff = preds - label # /2 to bring angle diff between -180<theta<180 half_angle_diff = angle_diff.detach().cpu().numpy()/2 sine_diff = np.sin(half_angle_diff) square_sum = np.nansum(sine_diff**2) return square_sum",|pytorch|loss-function|autograd|,Training,2
72744278,"Keras tuner Bayesian Optmization graph error. I am trying to optimize a convolutional neural network with Bayesian Optimization algorithm provided in keras tuner library. When I perform the line: tuner_cnn.search(datagen.flow(X_trainRusReshaped,Y_trainRusHot), epochs=50, batch_size=256) I encounter this error: InvalidArgumentError: Graph execution error One-Hot-Encode y_train and y_test as the following: y_train = to_categorical(y_train) y_test = to_categorical(y_test) X_trainShape = X_train.shape[1]*X_train.shape[2]*X_train.shape[3] X_testShape = X_test.shape[1]*X_test.shape[2]*X_test.shape[3] X_trainFlat = X_train.reshape(X_train.shape[0], X_trainShape) X_testFlat = X_test.reshape(X_test.shape[0], X_testShape) # One-hot-encoding Y_trainRusHot = to_categorical(Y_trainRus, num_classes = 2) Y_testRusHot = to_categorical(Y_testRus, num_classes = 2) I defined my model builder like that: datagen = ImageDataGenerator( featurewise_center=True, featurewise_std_normalization=True, rotation_range=180, horizontal_flip=True,vertical_flip = True) def model_builder(hp): model = Sequential() #model.add(Input(shape=(50,50,3))) for i in range(hp.Int('num_blocks', 1, 2)): hp_padding=hp.Choice('padding_'+ str(i), values=['valid', 'same']) hp_filters=hp.Choice('filters_'+ str(i), values=[32, 64]) model.add(Conv2D(hp_filters, (3, 3), padding=hp_padding, activation='relu', kernel_initializer='he_uniform', input_shape=(50, 50, 3))) model.add(MaxPooling2D((2, 2))) model.add(Dropout(hp.Choice('dropout_'+ str(i), values=[0.0, 0.1, 0.2]))) model.add(Flatten()) hp_units = hp.Int('units', min_value=25, max_value=150, step=25) model.add(Dense(hp_units, activation='relu', kernel_initializer='he_uniform')) model.add(Dense(10,activation=""softmax"")) hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3]) hp_optimizer=hp.Choice('Optimizer', values=['Adam', 'SGD']) if hp_optimizer == 'Adam': hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3]) elif hp_optimizer == 'SGD': hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3]) nesterov=True momentum=0.9 model.compile(loss=keras.losses.binary_crossentropy, optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate), metrics=['accuracy']) return model perform the tuner search: tuner_cnn = kt.tuners.BayesianOptimization( model_builder, objective='val_loss', max_trials=100, directory='.', project_name='tuning-cnn') tuner_cnn.search(datagen.flow(X_trainRusReshaped,Y_trainRusHot), epochs=50, batch_size=256) I also tried to do: tuner_cnn.search(X_trainRusReshaped, Y_trainRusHot, epochs=80, validation_data=(X_testRusReshaped, Y_testRusHot), callbacks=[stop_early]) But it does not work neither. Any idea?",|python|keras|conv-neural-network|bayesian|keras-tuner|,Model,0
72791888,"Value Error: Torch target size and torch input size in GAN do not match. Hi I am working on a GAN with custom images. I got the following error, which doesn't add up for me: ValueError: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([47, 1])) is deprecated. Please ensure they have the same size. I do not see where either of these sizes come from. Could someone please help me out? The error is to be found at the loss_disrimenator in the course of the training (marked with an arrow) after epoch 0. Below you find the related code. I am using vs code windows. Also is it normal that epoch 0 works and then the problem appears? [Sceenshot of Terminal- Epoch 0 Loss Discriminatorand Generator][1] import torch from glob import glob from torch.utils.data import Dataset, DataLoader from torchvision import transforms from skimage import io import matplotlib.pyplot as plt path = 'Punks' image_paths = glob(path + '/*.png') img_size = 28 batch_size = 32 transform = transforms.Compose( [ transforms.ToPILImage(), transforms.Resize(img_size), transforms.CenterCrop(img_size), transforms.ToTensor(), transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]), ] ) class ImageDataset(Dataset): def __init__(self, paths, transform): self.paths = paths self.transform = transform def __len__(self): return len(self.paths) def __getitem__(self, index): image_path = self.paths[index] image = io.imread(image_path) if self.transform: image_tensor = self.transform(image) return image_tensor if __name__ == '__main__': dataset = ImageDataset(image_paths, transform) train_loader = DataLoader( dataset, batch_size=batch_size, num_workers=1, shuffle=True) # PLOTTING SAMPLES real_samples = next(iter(train_loader)) for i in range(9): ax = plt.subplot(3, 3, 3 + 1) plt.imshow(real_samples[i].reshape(28, 28, 3)) plt.xticks([]) plt.yticks([]) plt.show() device = 'cuda' if torch.cuda.is_available() else 'cpu' class Discriminator(nn.Module): def __init__(self): super().__init__() self.model = nn.Sequential( nn.Linear(784*3, 2048), nn.ReLU(), nn.Dropout(0.3), nn.Linear(2048, 1024), nn.ReLU(), nn.Dropout(0.3), nn.Linear(1024, 512), nn.ReLU(), nn.Dropout(0.3), nn.Linear(512, 256), nn.ReLU(), nn.Dropout(0.3), nn.Linear(256, 1), nn.Sigmoid(), ) def forward(self, x): x = x.view(x.size(0), 784*3) # change required for 3 channel image output = self.model(x) return output discriminator = Discriminator().to(device=device) class Generator(nn.Module): def __init__(self): super().__init__() self.model = nn.Sequential( nn.Linear(100, 256), nn.ReLU(), nn.Linear(256, 512), nn.ReLU(), nn.Linear(512, 1024), nn.ReLU(), nn.Linear(1024, 2048), nn.ReLU(), nn.Linear(2048, 784*3), nn.Tanh(), ) def forward(self, x): output = self.model(x) output = output.view(x.size(0), 3, 28, 28) return output generator = Generator().to(device=device) # TRAINING PARAMS lr = 0.0001 num_epochs = 10 loss_function = nn.BCELoss() optimizer_discriminator = torch.optim.Adam(discriminator.parameters(), lr=lr) optimizer_generator = torch.optim.Adam(generator.parameters(), lr=lr) for epoch in range(num_epochs): for n, real_samples in enumerate(train_loader): # Data for training the discriminator real_samples = real_samples.to(device=device) real_samples_labels = torch.ones((batch_size, 1)).to( device=device ) latent_space_samples = torch.randn((batch_size, 100)).to( device=device ) print(f'Latent space samples : {latent_space_samples.shape}') generated_samples = generator(latent_space_samples) generated_samples_labels = torch.zeros((batch_size, 1)).to( device=device ) all_samples = torch.cat((real_samples, generated_samples)) print(f'Real samples : {real_samples.shape}, generated samples : {generated_samples.shape}') all_samples_labels = torch.cat( (real_samples_labels, generated_samples_labels) ) # Training the discriminator discriminator.zero_grad() output_discriminator = discriminator(all_samples) loss_discriminator = loss_function( output_discriminator, all_samples_labels ) -------> loss_discriminator.backward() optimizer_discriminator.step() # Data for training the generator latent_space_samples = torch.randn((batch_size, 100)).to( device=device ) # Training the generator generator.zero_grad() generated_samples = generator(latent_space_samples) output_discriminator_generated = discriminator(generated_samples) loss_generator = loss_function( output_discriminator_generated, real_samples_labels ) loss_generator.backward() optimizer_generator.step() # Show loss if n == batch_size - 1: print(f""Epoch: {epoch} Loss D.: {loss_discriminator}"") print(f""Epoch: {epoch} Loss G.: {loss_generator}"") latent_space_samples = torch.randn(batch_size, 100).to(device=device) generated_samples = generator(latent_space_samples) generated_samples = generated_samples.cpu().detach() for i in range(9): ax = plt.subplot(3, 3, i + 1) plt.imshow(generated_samples[i].reshape(28, 28, 3)) plt.xticks([]) plt.yticks([]) plt.show()",|python|pytorch|valueerror|generative-adversarial-network|,Tensors&Inputs,1
72845812,"bert-base-uncased: TypeError: tuple indices must be integers or slices, not tuple. I want to see embeddings for the input text I give to the model, and then feed it to the rest of the BERT. To do so, I partitioned the model into two sequential models, but I must have done it wrong because rest_of_bert model raises TypeError. Original model does not raise any error with the input_ids as input processed with text_to_input function. Input[0]: import torch from transformers import BertTokenizer, BertModel tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') cls_token_id = tokenizer.cls_token_id sep_token_id = tokenizer.sep_token_id pad_token_id = tokenizer.pad_token_id model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True) model.eval() Output[0]: Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias'] - This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). BertModel( (embeddings): BertEmbeddings( (word_embeddings): Embedding(30522, 768, padding_idx=0) (position_embeddings): Embedding(512, 768) (token_type_embeddings): Embedding(2, 768) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) (encoder): BertEncoder( (layer): ModuleList( (0): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) (intermediate_act_fn): GELUActivation() ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (1): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) (intermediate_act_fn): GELUActivation() ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (2): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) (intermediate_act_fn): GELUActivation() ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (3): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) (intermediate_act_fn): GELUActivation() ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (4): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) (intermediate_act_fn): GELUActivation() ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (5): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) (intermediate_act_fn): GELUActivation() ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (6): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) (intermediate_act_fn): GELUActivation() ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (7): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) (intermediate_act_fn): GELUActivation() ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (8): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) (intermediate_act_fn): GELUActivation() ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (9): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) (intermediate_act_fn): GELUActivation() ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (10): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) (intermediate_act_fn): GELUActivation() ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (11): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) (intermediate_act_fn): GELUActivation() ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) ) ) (pooler): BertPooler( (dense): Linear(in_features=768, out_features=768, bias=True) (activation): Tanh() ) ) Input[1]: def text_to_input(text): x = tokenizer.encode(text, add_special_tokens=False) # returns python list x = [cls_token_id] + x + [sep_token_id] token_count = len(x) pad_count = 512 - token_count x = x + [pad_token_id for i in range(pad_count)] return torch.tensor([x]) extract_embeddings = torch.nn.Sequential(list(model.children())[0]) rest_of_bert = torch.nn.Sequential(*list(model.children())[1:]) input_ids = text_to_input('A sentence.') x_embedding = extract_embeddings(input_ids) output = rest_of_bert(x_embedding) Output[1]: --------------------------------------------------------------------------- TypeError Traceback (most recent call last) <ipython-input-5-d371d8a2fb3c> in <module>() 12 input_ids = text_to_input('A sentence.') 13 x_embedding = extract_embeddings(input_ids) ---> 14 output = rest_of_bert(x_embedding) 4 frames /usr/local/lib/python3.7/dist-packages/transformers/utils/generic.py in __getitem__(self, k) 220 return inner_dict[k] 221 else: --> 222 return self.to_tuple()[k] 223 224 def __setattr__(self, name, value): TypeError: tuple indices must be integers or slices, not tuple",|python|machine-learning|pytorch|huggingface-transformers|bert-language-model|,API,4
72872021,"ValueError: Exception encountered when calling layer ""conv2d_1"" (type Conv2D). I aim to train my convolutional neural network to identify png images. I first convert images into tensor. file_path = f""./STFT_spectra/STFT_spectra0.png"" image = io.read_file(file_path) image = io.decode_png(image) image = tf.image.convert_image_dtype(image, tf.float32) image = tf.image.resize(image, [128,128]) print(""----Here-----"") print(type(image)) print(image.shape) Output: ----Here----- <class 'tensorflow.python.framework.ops.EagerTensor'> (128, 128, 4) Then, I convert all my generated images, and save the numpy array as ""image_list.p"" file on the hard disk. # total = 100 # image_list = np.empty(shape=(total, 128, 128, 4)) # for i in tqdm(range(total)): # file_path = f""./STFT_spectra/STFT_spectra{i}.png"" # image = io.read_file(file_path) # image = io.decode_png(image) # image = tf.image.convert_image_dtype(image, tf.float32) # image = tf.image.resize(image, [128, 128]) # image_list[i] = image # pickle.dump(image_list, open(""image_list.p"", ""wb"")) As for ground truth, each label is a 10 float combination, like [0.2, 0.3, 0.5, 0.6, 0.9, 0.5, 0.4, 0.6, 0.7, 0.1]. Then, I assemble the dataset: labels = pickle.load(open("".././labels.p"", ""rb"")) fetched_image_list = pickle.load(open(""../image_list.p"", ""rb"")) fetched_image_list = fetched_image_list.reshape(fetched_image_list.shape[0], fetched_image_list.shape[1], fetched_image_list.shape[2], fetched_image_list.shape[3], 1) dataset = tf.data.Dataset.from_tensor_slices((fetched_image_list, labels)) The CNN model goes like this: model = tf.keras.Sequential([ tf.keras.layers.Conv2D(32, (3, 3), strides=(2,2), dilation_rate=(1,1), input_shape=(128,128,4,1), activation='relu'), tf.keras.layers.Conv2D(71, (3, 3), strides=(2,2), dilation_rate=(1,1), activation='relu'), tf.keras.layers.Conv2D(128, (3, 4), strides=(2,3), dilation_rate=(1,1),activation='relu'), tf.keras.layers.Conv2D(128, (3, 3), strides=(2,2), dilation_rate=(1,1),activation='relu'), tf.keras.layers.Conv2D(128, (3, 4), strides=(2, 3), dilation_rate=(1, 1), activation='relu'), tf.keras.layers.Conv2D(128, (3, 3), strides=(2, 2), dilation_rate=(1, 1), activation='relu'), tf.keras.layers.Dropout(0.20), tf.keras.layers.Flatten(), tf.keras.layers.Dense(10) ]) Everything looks fine, but problem comes, ValueError: Exception encountered when calling layer ""conv2d_1"" (type Conv2D). Negative dimension size caused by subtracting 3 from 1 for '{{node conv2d_1/Conv2D/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=""NHWC"", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=""VALID"", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true](conv2d_1/Conv2D/Reshape, conv2d_1/Conv2D/Conv2D/ReadVariableOp)' with input shapes: [?,63,1,32], [3,3,32,71]. Call arguments received by layer ""conv2d_1"" (type Conv2D): ?inputs=tf.Tensor(shape=(None, 128, 63, 1, 32), dtype=float32) How can I fix this issue? Is there any problem with the definition of CNN?",|python|image|tensorflow|conv-neural-network|layer|,Model,0
72965428,"Why is a CNN model struggling to classify a colored MNIST?. I'm trying to classify colored MNIST digits with a basic CNN architecture on Keras. Here is the piece of code that colors the original dataset into purely either red, green or blue. def load_norm_data(): ## load basic mnist (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data() train_images = np.zeros((*x_train.shape, 3)) # orig shape: (60 000, 28, 28, 1) -> rgb shape: (60 000, 28, 28, 3) for num in range(x_train.shape[0]): rgb = np.random.randint(3) train_images[num, ..., rgb] = x_train[num]/255 return train_images, y_train if __name__ == '__main__': ims, labels = load_norm_data() for num in range(10): plt.subplot(2, 5, num+1) plt.imshow(ims[num]) plt.axis('off') which gives for the first couple of digits: Then, I attempt to classify this colored dataset into the same 10 digit classes of MNIST, so the labels aren't changing --and yet the models accuracy drops from 95% for non-colored MNIST, to wildly variable 30-70% on colored MNIST, vastly depending on weight initialization... Please find below the architecture of said model: model = keras.Sequential() model.add(keras.layers.Conv2D(64, kernel_size=(3,3), padding='same')) model.add(keras.layers.MaxPool2D(pool_size=(2,2))) model.add(keras.layers.Conv2D(64, kernel_size=(3,3), padding='same')) model.add(keras.layers.MaxPool2D(pool_size=(2,2), padding='same')) model.add(keras.layers.Flatten()) model.add(keras.layers.Dense(10, activation='relu')) model.add(keras.layers.Softmax()) input_shape = train_images.shape model.build(input_shape) model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy']) model.summary() model.fit(train_images, train_numbers, batch_size=12, epochs=25) Initially, I thought that this drop in performance might be linked to data irregularity (e.g. imagine a lot of 3s in the data ended up being green, thus the model learns green = 3). So I checked the data, the counts are good and the rgb distribution for each class is near 33% for each color too. I also checked the misclassified images to see if there were many representatives of a certain color or digit, but it doesn't seem to be the case either. In any case, after reading Keras' documentation and because of the fact that Conv2D forces you to pass it a 2-dimensional kernel_size that I imagine thus operates on all channels of the input image, the model shouldn't be taking color into account for classification here. Am I missing something here? Please let me know if you need any further information. Thank you in advance.",|python|keras|conv-neural-network|rgb|mnist|,Model,0
73019486,"torch.optim.LBFGS() does not change parameters. I'm trying to optimize the coordinates of the corners of an image. A similar technique works fine in Ceres Solver. But in torch.optim I'm having some issues. In particular, the optimizer for some reason does not change the parameters being optimized. I don't have much experience with pytorch, so I'm pretty sure the error is trivial. Unfortunately, reading the documentation did not help me much. Optimization model class: class OptimizeCorners(torch.nn.Module): def __init__(self, real_corners): super().__init__() self._real_corners = torch.nn.Parameter(real_corners) def forward(self, real_image, synt_image, synt_corners, _threshold): # Find homography if visualize_warp_interpolate: real_image_before_processing = real_image synt_image_before_processing = synt_image homography_matrix = kornia.geometry.homography.find_homography_dlt(synt_corners, self._real_corners, weights=None) # Warp and resize synt image synt_image = kornia.geometry.transform.warp_perspective(synt_image.float(), homography_matrix, dsize=(int(real_image.shape[2]), int(real_image.shape[3])), mode='bilinear', padding_mode='zeros', align_corners=True, fill_value=torch.zeros(3)) # Interpolate images real_image = torch.nn.functional.interpolate(real_image.float(), scale_factor=5, mode='bicubic', align_corners=None, recompute_scale_factor=None, antialias=False) synt_image = torch.nn.functional.interpolate(synt_image.float(), scale_factor=5, mode='bicubic', align_corners=None, recompute_scale_factor=None, antialias=False) # Calculate loss loss_map = torch.sub(real_image, synt_image, alpha=1) # if element > _threshold: element = 0 loss_map = torch.nn.Threshold(_threshold, 0)(loss_map) cumulative_loss = torch.sqrt(torch.sum(torch.pow(loss_map, 2)) / (loss_map.size(dim=2) * loss_map.size(dim=3))) return torch.autograd.Variable(cumulative_loss.data, requires_grad=True) The way, how I am trying to execute optimization: # Convert corresponding images to PyTorch tensors _image = kornia.utils.image_to_tensor(_image, keepdim=False) _synt_image = kornia.utils.image_to_tensor(_synt_image, keepdim=False) _corners = torch.from_numpy(_corners) _synt_corners = torch.from_numpy(_synt_corners) # Optimizer L-BFGS n_iters = 100 h_lbfgs = [] lr = 1 optimize_corners = OptimizeCorners(_corners) optimizer = torch.optim.LBFGS(optimize_corners.parameters(), lr=lr) for it in tqdm(range(n_iters), desc='Fitting corners', leave=False, position=1): loss = optimize_corners(_image, _synt_image, _synt_corners, _threshold) optimizer.zero_grad() loss.backward() optimizer.step(lambda: optimize_corners(_image, _synt_image, _synt_corners, _threshold)) h_lbfgs.append(loss.item()) print(h_lbfgs) Output from console: pic So, as you can see, parameters to be optimized do not change. UPD: I changed return torch.autograd.Variable(cumulative_loss.data, requires_grad=True) to return cumulative_loss.requires_grad_(), and it actually works, but now I get this error after few iterations: console output UPD: this happens because the parameters being optimized turn into NaN after a few iterations.",|python|pytorch|mathematical-optimization|torch|,Training,2
73144065,"Tensorflow does not apply data augmentation properly. I'm trying to apply the process of data augmentation to a database. I use the following code: train_generator = keras.utils.image_dataset_from_directory( directory= train_dir, subset = ""training"", image_size = (50,50), batch_size = 32, validation_split = 0.3, seed = 1337, labels = ""inferred"", label_mode = 'binary' ) validation_generator = keras.utils.image_dataset_from_directory( subset=""validation"", directory=validation_dir, image_size=(50,50), batch_size =40, seed=1337, validation_split = 0.3, labels = ""inferred"", label_mode ='binary' ) data_augmentation = keras.Sequential([ keras.layers.RandomFlip(""horizontal""), keras.layers.RandomRotation(0.1), keras.layers.RandomZoom(0.1), ]) train_dataset = train_generator.map(lambda x, y: (data_augmentation(x, training=True), y)) But when I try to run the training processe using this method, I get a ""insuficient data"" warning: 6/100 [>.............................] - ETA: 21s - loss: 0.7602 - accuracy: 0.5200WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 2000 batches). You may need to use the repeat() function when building your dataset. WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10 batches). You may need to use the repeat() function when building your dataset. Yes, the original dataset is insuficient, but the data augmentation should provide more than enough data for the training. Does anyone know what's going on ? EDIT: fit call: history = model.fit( train_dataset, epochs = 20, steps_per_epoch = 100, validation_data = validation_generator, validation_steps = 10, callbacks=callbacks_list) This is the version I have using DataImageGenerator: train_datagen = keras.preprocessing.image.ImageDataGenerator(rescale =1/255,rotation_range = 40,width_shift_range = 0.2,height_shift_range = 0.2,shear_range = 0.2,zoom_range = 0.2,horizontal_flip = True) train_generator = train_datagen.flow_from_directory(directory= train_dir,target_size = (50,50),batch_size = 32,class_mode = 'binary') val_datagen = keras.preprocessing.image.ImageDataGenerator(rescale=1/255) validation_generator = val_datagen.flow_from_directory(directory=validation_dir,target_size=(50,50),batch_size =40,class_mode ='binary') This specific code (with this same number of epochs, steps_per_epoch and batchsize) was taken from the book deeplearning with python, by Franois Chollet, it's an example on page 141 of a data augmentation system. As you may have guessed, this produces the same results as the other method displayed.",|python|database|tensorflow|keras|data-augmentation|,Training,2
73197501,"raise ValueError No gradients provided for any variables -Custom loss function. I'm writing a custom loss function for a neural network with keras back-end, to reduce attitude error. def LossQuat(y_true, y_pred): a, b = y_true.get_shape() error = np.zeros([a,1]) for i in range(a): w0,x0,y0,z0 = y_true[i,:] w1,x1,y1,z1 = y_pred[i,:]/tf.norm(y_pred, ord='euclidean', axis=None, keepdims=None, name=None) w = w0 * w1 - x0 * x1 - y0 * y1 - z0 * z1 error[i,] = tf.square(2*tf.math.acos(w)) err = tf.reduce_sum(error) return tf.reduce_mean(err) The model is: model = keras.models.Sequential() model.add(keras.layers.Dense(100, input_dim=1, activation='sigmoid')) model.add(keras.layers.Dense((4), activation='linear')) model.compile(loss=LossQuat, optimizer = tf.keras.optimizers.Adam(lr=1e-5),run_eagerly=True) # training batch_size = 32 epochs = 1000 model.fit(x_train, quat_pitch, batch_size=batch_size, epochs=epochs, verbose=1) But I got this error: raise ValueError(f""No gradients provided for any variable: {variable}. "" ValueError: No gradients provided for any variable: (['dense/kernel:0', 'dense/bias:0', 'dense_1/kernel:0', 'dense_1/bias:0'],). Provided grads_and_vars is ((None, <tf.Variable 'dense/kernel:0' shape=(1, 10) dtype=float32, numpy= array([[ 0.027, 0.718, 0.436, 0.588, 0.597, -0.712, 0.038, 0.629, 0.305, 0.463]], dtype=float32)>), (None, <tf.Variable 'dense/bias:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>), (None, <tf.Variable 'dense_1/kernel:0' shape=(10, 4) dtype=float32, numpy= array([[ 0.449, 0.503, -0.456, 0.521], [-0.365, 0.423, 0.55 , 0.032], [-0.311, 0.348, -0.056, 0.174], [ 0.521, 0.498, -0.131, -0.507], [-0.107, 0.321, 0.638, 0.117], [ 0.248, 0.416, -0.259, -0.273], [ 0.121, 0.137, -0.575, 0.094], [ 0.41 , -0.565, -0.394, -0.239], [-0.531, -0.056, 0.13 , 0.201], [ 0.225, -0.122, 0.556, -0.266]], dtype=float32)>), (None, <tf.Variable 'dense_1/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>)). What is my mistake? I tried to use Keras Loss Functions such as MSE. When I used them the model has been trained without any problem. Also, for the custom loss, I tried different batch size, for all of them the problem is exist. DATA SHAPE x_train.shape = (304414,) y_train.shape = (304414,4) MATH: The main formula is as follows: quat_predict * inverse(quat_ref) = [w ,x, y, z] The error: error = 2*arccos(w)",|keras|deep-learning|neural-network|loss-function|,Training,2
73229163,"AMD ROCm with Pytorch on Navi10 (RX 5700 / RX 5700 XT). I am one of those miserable creatures who own a AMD GPU (RX 5700, Navi10). I want to use up-to-date PyTorch libraries to do some Deep Learning on my local machine and stop using cloud instances. I saw all over the internet that AMD is promising Navi10 support in the next 2-4 months(posts that were written 1-2 years back) however, I do not think they released an ""official"" support. I installed ROCm on local machine and it actually detects my GPU and everything seems nice, here is rocminfo output. I installed the necessary PyTorch ROCm version but when I try to run a code, I get the following error. hipErrorNoBinaryForGpu: Unable to find code object for all current devices! I suppose this is because ROCm still does not have a support for gfx1010 or I am lost at this point. I would be happy if someone can provide a way to make ROCm work (preferable without compiling whole package for gfx1010 again) or provide way to use an AMD GPU just like a CUDA user.",|deep-learning|pytorch|gpu|amd-gpu|amd-rocm|,GPU Usage,3
73266661,"Multiple input model with PyTorch - input and output features. Good afternoon! Im building a multiple-input model with 2 types of inputs: Images (torch.Size([1, 3, 224, 224])) and landmark features (torch.Size([1, 96])). Heres the model itself: class MixedNetwork(nn.Module): def __init__(self): super(MixedNetwork, self).__init__() image_modules = list(models.resnet50().children())[:-1] self.image_features = nn.Sequential(*image_modules) self.landmark_features = nn.Sequential( nn.Linear(in_features=96, out_features=192,bias=False), nn.ReLU(inplace=True), nn.Dropout(p=0.25), nn.Linear(in_features=192,out_features=1000,bias=False), nn.ReLU(inplace=True), nn.Dropout(p=0.25)) self.combined_features = nn.Sequential( nn.Linear(1000, 512), nn.ReLU(), nn.Linear(512, 32), nn.ReLU(), nn.Linear(32,1)) def forward(self, image, landmarks): a = self.image_features(image) print(a.shape) b = self.landmark_features(landmarks) x = torch.cat((a.view(a.size(0), -1), b.view(b.size(0), -1)), dim=1) x = self.combined_features(x) x = F.sigmoid(x) return x Im getting confused when it comes to defining input-output features for Linear layers and combined layers. The last FC layer of resnet50 Linear(in_features=2048, out_features=1000). Does it mean that the last output of self.landmark_features layers also has to be 1000 and the first linear layer of self.combined_features should also be 1000? Is it correct to assume that if the landmark input size is [1, 96] then the in_features for the first layer of self.landmark_features has to be 96? With the current dimensions Im getting the error message: RuntimeError: mat1 and mat2 shapes cannot be multiplied (1x3048 and 1000x512) (why 3048 and not 2048?)",|python|machine-learning|pytorch|tensor|resnet|,Tensors&Inputs,1
73308371,"model.predict() returning random float instead of 3 probabilities. My model takes as input like 'USD' and predicts the category of it, in this case 1 since I defined currency to be 1. In total there's three categories to predict: 0=book name, 1=currency, or 2=security number. My training, validation, and test sets look like this, where the value is mapped to it's label. Value Label 0 CAT_USD_CORP 0 1 USD 1 2 US348595EV89 2 3 ATTR_IRT_LDN_ISD_ISD 0 4 CAD 1 ... and I trained and tested the model with this code which works perfectly fine and gives me good accuracy: model = tf.keras.Sequential([ feature_layer, layers.Dense(128, activation='relu'), layers.Dense(128, activation='relu'), layers.Dropout(.1), layers.Dense(1) ]) model.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=['accuracy']) model.fit(train_ds, validation_data=val_ds, epochs=12) loss, accuracy = model.evaluate(test_ds) print(""Accuracy"", accuracy) but when I try to predict a value I input it always gives me an array of a random float instead of the probability of the 3 categories it falls under for example: model.predict(np.array(['USD'])) returns array([[1195770.]], dtype=float32) and model.predict(np.array(['US568A45EV89'])) returns array([[1938599.4]], dtype=float32), when they're supposed to return an array of 3 probabilities.",|python|tensorflow|machine-learning|keras|,Model,0
73361613,"Tensorflow define lossfunction. The following code works, converges and the neural net approximates the exponential on the interval from 0 to 1: # code works import tensorflow as tf import numpy as np import matplotlib.pyplot as plt # fit an exponential n = 101 x = np.linspace(start=0, stop=1, num=n) y_e = np.exp(x) # any odd neural net with sufficient degrees of freedom model = tf.keras.models.Sequential([ tf.keras.layers.Dense(units=1, input_shape=[1]), tf.keras.layers.Dense(units=50, activation=""softmax""), tf.keras.layers.Dense(units=1) ]) # loss function def loss(y_true, y_pred): L_ode = tf.reduce_mean(tf.square(y_pred - y_true), axis=-1) return L_ode model.compile('adam', loss) model.fit(x, y_e, epochs=100, batch_size=1) y_NN = model.predict(x).flatten() plt.plot(x, y_NN, color='blue') plt.plot(x, y_e, color='red') plt.title('NN (blue) and exp (red)') Now I redefine the loss function and replace y_true by tf.zeros(n) in the model.fit. But this code - which should do the same thing - runs nicely but converges to a constant: import tensorflow as tf import numpy as np import matplotlib.pyplot as plt # fit an exponential n = 101 x = np.linspace(start=0, stop=1, num=n) y_e = np.exp(x) # any odd neural net with sufficient degrees of freedom model = tf.keras.models.Sequential([ tf.keras.layers.Dense(units=1, input_shape=[1]), tf.keras.layers.Dense(units=50, activation=""softmax""), tf.keras.layers.Dense(units=1) ]) # loss function def loss(y_true, y_pred): L_ode = tf.reduce_mean(tf.square((y_pred - y_e) - y_true), axis=-1) return L_ode model.compile('adam', loss) model.fit(x, tf.zeros(n), epochs=100, batch_size=1) y_NN = model.predict(x).flatten() plt.plot(x, y_NN, color='blue') plt.plot(x, y_e, color='red') plt.title('NN (blue) and exp (red)') Where is the mistake? Background: I would like to approximate solutions of odes with neural nets and therefore define my own loss-function. The above is a very reduced example of the ""zero degree ode"" y(x) = exp(x).",|python|tensorflow|loss-function|,Training,2
73380197,"Custom Loss Function Leads to High MSE and an offset in the output Keras. I am training a neural network for time series regression. The model is #################################################################################################################### # Define ANN Model # define two sets of inputs acc = layers.Input(shape=(3,1,)) gyro = layers.Input(shape=(3,1,)) # the first branch operates on the first input x = Conv1D(256, 1, activation='relu')(acc) x = Conv1D(128, 1, activation='relu')(x) x = Conv1D(128, 1, activation='relu')(x) x = MaxPooling1D(pool_size=3)(x) x = Model(inputs=acc, outputs=x) # the second branch opreates on the second input y = Conv1D(256, 1, activation='relu')(gyro) y = Conv1D(128, 1, activation='relu')(y) y = Conv1D(128, 1, activation='relu')(y) y = MaxPooling1D(pool_size=3)(y) y = Model(inputs=gyro, outputs=y) # combine the output of the three branches combined = layers.concatenate([x.output, y.output]) # combined outputs z = Bidirectional(LSTM(128, dropout=0.25, return_sequences=False,activation='tanh'))(combined) z = Reshape((256,1),input_shape=(128,)) z = Bidirectional(LSTM(128, dropout=0.25, return_sequences=False,activation='tanh'))(combined) #z = Dense(10, activation=""relu"")(z) z = Flatten()(z) z = Dense(4, activation=""linear"")(z) model = Model(inputs=[x.input, y.input], outputs=z) model.compile(loss=loss, optimizer = tf.keras.optimizers.Adam(),metrics=['mse'],run_eagerly=True) I have tried to implement a custom loss function (based on different papers). Math The error will calculated as follows: y_pred = [w x y z] y_true = [w1 x1 y1 z1] error = 2 * acos(w*w1 + x*x1 + y*y1 + z*z1) Based on this formula I wrote the custom loss function: def loss(y_true, y_pred): z = y_true * (y_pred ) wtot = tf.reduce_sum(z,axis=1) error = 2*tf.math.acos(K.clip(tf.math.sqrt(wtot*wtot), -1.,1.)) return error But while the loss value is decreasing the MSE increased and I can see an offset in the output which will grow by the number of epochs. I understand that we do not optimize this Network for MSE but based on mathematics the MSE must be reduced or converge to some value near 1. Orange is the Target/Reference Blue is the Network ouptut for 1 epoch for 10 epochs for 50 epochs",|tensorflow|keras|loss-function|,Training,2
73445876,"ValueError: base_distribution needs to have shape with size at least 6, but got torch.Size([6]). I have the following architecture for my neural network import torch import torch.distributions as pyd import toch.nn as nn from torch.distributions import transforms as tT from torch.distributions.transformed_distribution import TransformedDistribution LOG_STD_MIN = -5 LOG_STD_MAX = 0 class TanhTransform(pyd.transforms.Transform): domain = pyd.constraints.real codomain = pyd.constraints.interval(-1.0, 1.0) bijective = True sign = +1 def __init__(self, cache_size=1): self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') super().__init__(cache_size=cache_size) @staticmethod def atanh(x): return 0.5 * (x.log1p() - (-x).log1p()) def __eq__(self, other): return isinstance(other, TanhTransform) def _call(self, x): return x.tanh() def _inverse(self, y): return self.atanh(y.clamp(-0.99, 0.99)) def log_abs_det_jacobian(self, x, y): return 2.0 * (math.log(2.0) - x - F.softplus(-2.0 * x)) def get_spec_means_mags(spec): means = (spec.maximum + spec.minimum) / 2.0 mags = (spec.maximum - spec.minimum) / 2.0 means = Variable(torch.tensor(means).type(torch.FloatTensor), requires_grad=False) mags = Variable(torch.tensor(mags).type(torch.FloatTensor), requires_grad=False) return means, mags class Split(torch.nn.Module): def __init__(self, module, n_parts: int, dim=1): super().__init__() self._n_parts = n_parts self._dim = dim self._module = module def forward(self, inputs): output = self._module(inputs) if output.ndim==1: result=torch.hsplit(output, self._n_parts ) else: chunk_size = output.shape[self._dim] // self._n_parts result =torch.split(output, chunk_size, dim=self._dim) return result class Network(nn.Module): def __init__( self, state, act, fc_layer_params=(), ): super(Network, self).__init__() self._act = act self._layers = nn.ModuleList() for hidden_size in fc_layer_params: if len(self._layers)==0: self._layers.append(nn.Linear(state.shape[0], hidden_size)) else: self._layers.append(nn.Linear(hidden_size, hidden_size)) self._layers.append(nn.ReLU()) output_layer = nn.Linear(hidden_size,self._act.shape[0] * 2) self._layers.append(output_layer) self._act_means, self._act_mags = get_spec_means_mags( self._act) def _get_outputs(self, state): h = state for l in nn.Sequential(*(list(self._layers.children())[:-1])): h = l(h) self._mean_logvar_layers = Split( self._layers[-1], n_parts=2, ) mean, log_std = self._mean_logvar_layers(h) a_tanh_mode = torch.tanh(mean) * self._action_mags + self._action_means log_std = torch.tanh(log_std).to(device=self.device) log_std = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (log_std + 1) std = torch.exp(log_std) a_distribution = TransformedDistribution( base_distribution=Normal(loc=torch.full_like(mean, 0).to(device=self.device), scale=torch.full_like(mean, 1).to(device=self.device)), transforms=tT.ComposeTransform([ tT.AffineTransform(loc=self._action_means, scale=self._action_mags, event_dim=mean.shape[-1]), TanhTransform(), tT.AffineTransform(loc=mean, scale=std, event_dim=mean.shape[-1])])) return a_distribution, a_tanh_mode def get_log_density(self, state, action): a_dist, _ = self._get_outputs(state) log_density = a_dist.log_prob(action) return log_density def __call__(self, state): a_dist, a_tanh_mode = self._get_outputs(state) a_sample = a_dist.sample() log_pi_a = a_dist.log_prob(a_sample) return a_tanh_mode, a_sample, log_pi_a When I run my code I get this error message: action = self._a_network(latent_states)[1] File ""/home/planner_regularizer.py"", line 182, in __call__ a_dist, a_tanh_mode = self._get_outputs(state.to(device=self.device)) File ""/home/planner_regularizer.py"", line 159, in _get_outputs a_distribution = TransformedDistribution( File ""/home/dm_control/lib/python3.8/site-packages/torch/distributions/transformed_distribution.py"", line 61, in __init__ raise ValueError(""base_distribution needs to have shape with size at least {}, but got {}."" ValueError: base_distribution needs to have shape with size at least 6, but got torch.Size([6]). How can I fix this error message? Update: if I remove event_dim from AffineTransform, I wouldn't get above error but the output of log_prob would be size 1 which is not correct. Any suggestion?",|python|deep-learning|pytorch|tensor|,Tensors&Inputs,1
73543195,"torch.cuda.device_count() returns 2, but torch.load(model_path, map_location='cuda:1') throws an error. I have two GPUs and when I run import torch print('count: ', torch.cuda.device_count()) # prints count: 2 However, my model throws an error RuntimeError: Attempting to deserialize object on CUDA device 2 but torch.cuda.device_count() is 1 on the line torch.load(model_path, map_location='cuda:1') What could cause it and how to fix it? This issue is somehow linked to my Flask, because the training itself works with torch.load(model_path, map_location='cuda:1')",|pytorch|gpu|,GPU Usage,3
73567385,"Neural networks - why is loss always 0.0 and accuracy 1.0. I'm trying to train my neural network with 10 epochs. But my attempts are unsuccessful. I don't get it why am I always getting something like this: 35/300 [==>...........................] - ETA: 1:09 - loss: 0.0000e+00 - accuracy: 1.0000 36/300 [==>...........................] - ETA: 1:09 - loss: 0.0000e+00 - accuracy: 1.0000 37/300 [==>...........................] - ETA: 1:08 - loss: 0.0000e+00 - accuracy: 1.0000 Here are my batch size and image width/height and whole feeding proccess: batch_size = 32 img_height = 150 img_width = 150 dataset_url = ""http://cnrpark.it/dataset/CNR-EXT-Patches-150x150.zip"" print(dataset_url) data_dir = tf.keras.utils.get_file(origin=dataset_url, fname='CNR-EXT-Patches-150x150', untar=True) train_ds = tf.keras.utils.image_dataset_from_directory( data_dir, validation_split=0.2, subset=""training"", seed=123, image_size=(img_height, img_width), batch_size=batch_size) num_classes = 1 val_ds = tf.keras.utils.image_dataset_from_directory( data_dir, validation_split=0.2, subset=""validation"", seed=123, image_size=(img_height, img_width), batch_size=batch_size) class_names = train_ds.class_names print(class_names) for image_batch, labels_batch in train_ds: print(image_batch.shape) print(labels_batch.shape) break normalization_layer = tf.keras.layers.Rescaling(1./255) normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y)) image_batch, labels_batch = next(iter(normalized_ds)) first_image = image_batch[0] print(np.min(first_image), np.max(first_image)) model = tf.keras.Sequential([ tf.keras.layers.Rescaling(1./255), tf.keras.layers.Conv2D(32, 3, activation='relu'), tf.keras.layers.MaxPooling2D(), tf.keras.layers.Conv2D(32, 3, activation='relu'), tf.keras.layers.MaxPooling2D(), tf.keras.layers.Conv2D(32, 3, activation='relu'), tf.keras.layers.MaxPooling2D(), tf.keras.layers.Flatten(), tf.keras.layers.Dense(1, activation='sigmoid'), tf.keras.layers.Dense(num_classes) ]) AUTOTUNE = tf.data.AUTOTUNE train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE) val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE) model.compile( optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(), metrics=['accuracy']) model.fit( train_ds, validation_data=val_ds, epochs=10 ) From np.min and np.max I'm getting these values: 0.08627451 0.5568628 so this obviously wouldn't be the case. What should be wrong in my attempt? EDIT, my epoch looks like this now: 5/300 [..............................] - ETA: 1:12 - loss: 0.1564 - accuracy: 0.9750 6/300 [..............................] - ETA: 1:15 - loss: 0.1311 - accuracy: 0.8333 7/300 [..............................] - ETA: 1:13 - loss: 0.1124 - accuracy: 0.7143 8/300 [..............................] - ETA: 1:13 - loss: 0.0984 - accuracy: 0.6250 9/300 [..............................] - ETA: 1:12 - loss: 0.0874 - accuracy: 0.5556 And a little later: 51/300 [====>.........................] - ETA: 1:04 - loss: 0.0154 - accuracy: 0.0980",|python|tensorflow|machine-learning|keras|neural-network|,Model,0
73643887,"gpu memory is still occupied after validation phase is finished, pytorch. As far as I know, when training and validating a model with GPU, GPU memory is mainly used for loading data, forward & backward. and to what I know, I think GPU memory usage should be same 1)before training, 2)after training, 3)before validation, 4)after validation. But in my case, GPU memory used in the validation phase is still occupied in the training phase and vice versa. It is not increasing per epoch so I'm sure it is not a common mistake like loss.item(). Here is the summary of my question Shouldn't the GPU memory used in one phase be cleaned up before another(except for model weights)? If it should, are there any noob mistakes I'm making here..? Thank you for your help. Here is the code for training loop eval_result = evaluate(model,val_loader,True,True) print(eval_result) print('start training') for epoch in range(num_epoch): model.train() time_ = datetime.datetime.now() for iter_, data in enumerate(tr_loader): x, y = data x = x.to(device).view(x.shape[0],1,*(x.shape[1:])) y = y.to(device).long() pred = model.forward(x) loss = loss_fn(pred,y) optimizer.zero_grad() loss.backward() optimizer.step() # print print_iter = 16 if (iter_+1) % print_iter == 0: elapsed = datetime.datetime.now() - time_ expected = elapsed * (num_batches / print_iter) _epoch = epoch + ((iter_ + 1) / num_batches) print('\rTRAIN [{:.3f}/{}] loss({}) ' 'elapsed {} expected per epoch {}'.format( _epoch,num_epoch, loss.item(), elapsed, expected) ,end=""\t\t\t"") time_ = datetime.datetime.now() print() eval_result = evaluate(model,val_loader,True,True) print(eval_result) scheduler.step(eval_result[0]) if (epoch+1) %1 == 0: save_model(model, optimizer, scheduler) I've read about how making a validation phase a function helps since python is function scoping language. so the evaluate() is def evaluate(model, val_loader, get_acc = True, get_IOU = True): """""" pred: Tensor of shape B C D H W label Tensor of shape B D H W """""" val_loss = 0 val_acc = 0 val_IOU = 0 with torch.no_grad(): model.eval() for data in tqdm(val_loader): x, y = data x = x.to(device).view(x.shape[0],1,*(x.shape[1:])) y = y.to(device).long() pred = model.forward(x) loss = loss_fn(pred,y) val_loss += loss.item() pred = torch.argmax(pred, dim=1) if get_acc: total = np.prod(y.shape) total = total if total != 0 else 1 val_acc += torch.sum((pred == y)).cpu().item()/total if get_IOU: iou = 0 for class_num in range(1,8): iou += torch.sum((pred==class_num)&(y==class_num)).cpu().item()\ / torch.sum((pred==class_num)|(y==class_num)).cpu().item() val_IOU += iou/7 val_loss /= len(val_loader) val_acc /= len(val_loader) val_IOU /= len(val_loader) return (val_loss, val_acc, val_IOU) and here is GPU usage in colab. 1 is the point where the evaluate() is first called, and 2 is when the train started.",|pytorch|gpu|,GPU Usage,3
73656975,"Pytorch silent data corruption. I am on a workstation with 4 A6000 GPUs. Moving a Torch tensor from one GPU to another GPU corrupts the data, silently!!! See the simple example below. x >tensor([1], device='cuda:0') x.to(1) >tensor([1], device='cuda:1') x.to(2) >tensor([0], device='cuda:2') x.to(3) >tensor([0], device='cuda:3') Any ideas what is the cause of this issue? Other info that might be handy: (there was two nvlinks which I manually removed trying to solve the problem) GPU0 GPU1 GPU2 GPU3 CPU Affinity NUMA Affinity GPU0 X SYS SYS SYS 0-63 N/A GPU1 SYS X SYS SYS 0-63 N/A GPU2 SYS SYS X SYS 0-63 N/A GPU3 SYS SYS SYS X 0-63 N/A nvcc --version nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2022 NVIDIA Corporation Built on Tue_Mar__8_18:18:20_PST_2022 Cuda compilation tools, release 11.6, V11.6.124 Build cuda_11.6.r11.6/compiler.31057947_0 NVIDIA-SMI 510.47.03 Driver Version: 510.47.03 CUDA Version: 11.6 Edit: adding some screenshots It seems to be stateful. Changes which GPUs work fine together after starting a new python runtime.",|python|pytorch|gpu|,GPU Usage,3
73675303,"Tensorflow recognizes my GPU, but when I use it to train a model RAM usage explodes and crashes. The Code I'm working with is as follows, this first block runs perfectly without problems: import tensorflow as tf data = tf.keras.datasets.fashion_mnist class myCallback(tf.keras.callbacks.Callback): def on_epoch_end(self, epoch, logs={}): if(logs.get('accuracy')>0.99): print(""\nReached 99% accuracy so cancelling training!"") self.model.stop_training = True callbacks = myCallback() (training_images, training_labels), (test_images, test_labels) = data.load_data() print(type(training_images)) training_images=training_images.reshape(60000, 28, 28, 1) training_images = training_images / 255.0 test_images = test_images.reshape(10000, 28, 28, 1) test_images = test_images / 255.0 model = tf.keras.models.Sequential([ tf.keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(28, 28, 1)), tf.keras.layers.MaxPooling2D(2, 2), tf.keras.layers.Conv2D(64, (3, 3), activation='relu'), tf.keras.layers.MaxPooling2D(2,2), tf.keras.layers.Flatten(), tf.keras.layers.Dense(128, activation=tf.nn.relu), tf.keras.layers.Dense(10, activation=tf.nn.softmax)]) model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']) model.summary() model.fit(training_images, training_labels, epochs=50, callbacks=[callbacks], verbose=1) This is actually an example code from the ""AI and Machine Learning for Coders"" book. I can create the model and all, but when I call the fit method: model.fit(training_images, training_labels, epochs=50, callbacks=[callbacks], verbose=1) It prints ""epoch 1/50"", it freezes there, makes no progress and shows the following warning: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8401 RAM usage explodes and after a while the environment crashes and it shows this error: Process finished with exit code -1073740791 (0xC0000409) I'm using: tensorflow==2.9.0 tensorflow-gpu==2.1.0 CUDA --> v11.7 CUDNN --> v8.4.1.50 GPU --> NVidia GeForce GTX 960 4GB from tensorflow I can see my GPU with the command: tf.config.list_logical_devices('GPU') Which gives the following results: 2022-09-10 23:30:04.052968: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX AVX2 To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-09-10 23:30:04.496787: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2810 MB memory: -> device: 0, name: NVIDIA GeForce GTX 960, pci bus id: 0000:06:00.0, compute capability: 5.2 [LogicalDevice(name='/device:GPU:0', device_type='GPU')] Please I need help!",|python|tensorflow|gpu|cudnn|,GPU Usage,3
73849758,"ValueError: Dimensions must be equal, but are 68 and 10 for '{{node mean_squared_error/SquaredDifference}} = SquaredDifference[T=DT_FLOAT]. I am using a LSTM and a VNN to convert active text to passive text. I am feeding in tokenized data to the lstm and getting a feature vector of shape (68,1) and I am then using that as input data for a vanilla neural net along with an output probability matrix of shape (68,10,10). However, I am getting the following error when I try model.fit(): Epoch 1/100 Traceback (most recent call last): File""/Users/pranavpallavalli/PycharmProjects/pythonProject3/LstmSeq2Seq/ActivePassiveLSTM.py"", line 77, in <module> vnn.fit(feature_vec,output_prob_matrix,32,100) File ""/Users/pranavpallavalli/miniforge3/envs/env_tensorflow/lib/python3.9/site-packages/keras/utils/traceback_utils.py"", line 67, in error_handler raise e.with_traceback(filtered_tb) from None File ""/var/folders/6q/v9z_sbmd25q2ntjw2pn6hfk80000gn/T/__autograph_generated_fileg6buiumj.py"", line 15, in tf__train_function retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope) ValueError: in user code: File ""/Users/pranavpallavalli/miniforge3/envs/env_tensorflow/lib/python3.9/site-packages/keras/engine/training.py"", line 1051, in train_function * return step_function(self, iterator) File ""/Users/pranavpallavalli/miniforge3/envs/env_tensorflow/lib/python3.9/site-packages/keras/engine/training.py"", line 1040, in step_function ** outputs = model.distribute_strategy.run(run_step, args=(data,)) File ""/Users/pranavpallavalli/miniforge3/envs/env_tensorflow/lib/python3.9/site-packages/keras/engine/training.py"", line 1030, in run_step ** outputs = model.train_step(data) File ""/Users/pranavpallavalli/miniforge3/envs/env_tensorflow/lib/python3.9/site-packages/keras/engine/training.py"", line 890, in train_step loss = self.compute_loss(x, y, y_pred, sample_weight) File ""/Users/pranavpallavalli/miniforge3/envs/env_tensorflow/lib/python3.9/site-packages/keras/engine/training.py"", line 948, in compute_loss return self.compiled_loss( File ""/Users/pranavpallavalli/miniforge3/envs/env_tensorflow/lib/python3.9/site-packages/keras/engine/compile_utils.py"", line 201, in __call__ loss_value = loss_obj(y_t, y_p, sample_weight=sw) File ""/Users/pranavpallavalli/miniforge3/envs/env_tensorflow/lib/python3.9/site-packages/keras/losses.py"", line 139, in __call__ losses = call_fn(y_true, y_pred) File ""/Users/pranavpallavalli/miniforge3/envs/env_tensorflow/lib/python3.9/site-packages/keras/losses.py"", line 243, in call ** return ag_fn(y_true, y_pred, **self._fn_kwargs) File ""/Users/pranavpallavalli/miniforge3/envs/env_tensorflow/lib/python3.9/site-packages/keras/losses.py"", line 1327, in mean_squared_error return backend.mean(tf.math.squared_difference(y_pred, y_true), axis=-1) ValueError: Dimensions must be equal, but are 68 and 10 for '{{node mean_squared_error/SquaredDifference}} = SquaredDifference[T=DT_FLOAT](sequential_1/dense_1/Sigmoid, IteratorGetNext:1)' with input shapes: [68,10], [?,10,10]. My code for the model set up is: lstm = Sequential() lstm.add(LSTM(10,input_shape=(1,10))) lstm.add(Dense(1)) feature_vec = lstm(lstm_input_sequences) feature_vec = np.array(feature_vec) # np.reshape(feature_vec,(68,10,10)) vnn = Sequential() vnn.add(Input(1,68)) vnn.add(Dense(units=10,activation='sigmoid')) loss_fn = keras.losses.MeanSquaredError() vnn.compile(loss='mse',optimizer='adam',metrics=['accuracy']) print(vnn.summary()) vnn.fit(feature_vec,output_prob_matrix,32,100) Once again, the shape of feature_vec is (68,1) and the shape of output_prob_matrix is (68,10,10) I am a beginner and I am not too sure about why Im getting this. Please do help me out!! Appreciate it!!",|python|machine-learning|deep-learning|tf.keras|keras-layer|,Tensors&Inputs,1
74065619,"NLP BERT in R with tensorflow/Keras setup. I am trying to get BERT to run in R. I got other NLP tasks (e.g. word2vec) done with Keras, so the general setup should be ok. I adapted the model code from here: https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379 The problem is how to insert the inputs (tokens) correctly. I have tried a lot of different ways to transform them (as tensors, various forms of arrays etc), but can't seem to figure out what kind of data structure/type/shape is expected as input. Here is a simplified, replicable example: #rm(list=ls()) packages <- c(""reticulate"", ""keras"", ""tensorflow"", ""tfdatasets"", ""tidyverse"", ""data.table"") for (p in packages) if (!(p %in% installed.packages()[,1])) install.packages(p, character.only = TRUE) else require(p, character.only = TRUE) rm(packages, p) #reticulate::install_miniconda(force = TRUE) # 1time reticulate::use_condaenv(""~/.local/share/r-miniconda"") # win? reticulate::use_condaenv(""r-miniconda"") Sys.setenv(TF_KERAS=1) tensorflow::tf_version() # install_tensorflow() if NULL reticulate::py_config() #reticulate::py_install('transformers', pip = TRUE) #reticulate::py_install('torch', pip = TRUE) transformer = reticulate::import('transformers') tf = reticulate::import('tensorflow') builtins <- import_builtins() #built in python methods set.tf.repos <- ""distilbert-base-german-cased"" tokenizer <- transformer$AutoTokenizer$from_pretrained(set.tf.repos) # tokenizer_vocab_size <- length(tokenizer$vocab) ###### load model model_tf = transformer$TFDistilBertModel$from_pretrained(set.tf.repos, from_pt = T, trainable = FALSE) model_tf$config # set configs model_tf$config$output_hidden_states = TRUE summary(model_tf) ###### data & tokens ##### data <- data.table::fread(""https://raw.githubusercontent.com/michael-eble/nlp-dataset-health-german-language/master/nlp-health-data-set-german-language.txt"", encoding = ""Latin-1"") txt <- data$V1 y <- data$V2 table(y, exclude = NULL) set.max_length = 100 tokens <- tokenizer( txt, max_length = set.max_length %>% as.integer(), padding = 'max_length', #'longest' #implements dynamic padding truncation = TRUE, return_attention_mask = TRUE, return_token_type_ids = FALSE ) #tokens[[""input_ids""]] %>% str() #tokens[[""attention_mask""]] %>% str() tokens <- list(tokens[[""input_ids""]], tokens[[""attention_mask""]]) str(tokens) ####### model ######## input_word_ids <- layer_input(shape = c(set.max_length), dtype = 'int32', name = ""input_word_ids"") input_mask <- layer_input(shape = c(set.max_length), dtype = 'int32', name = ""input_attention_mask"") #input_segment_ids <- layer_input(shape = c(max_len), dtype = 'int32', name=""input_segment_ids"") last_hidden_state <- model_tf(input_word_ids, attention_mask = input_mask)[[1]] cls_token <- last_hidden_state[, 1,] output <- cls_token %>% layer_dense(units = 32, input_shape = c(set.max_length, 768), activation = 'relu') %>% layer_dense(units = 1, activation = 'sigmoid') model <- keras_model(inputs = list(input_word_ids, input_mask), outputs = output) model %>% compile(optimizer = ""adam"", loss = ""binary_crossentropy"" ) history = model %>% keras::fit( x = list(input_word_ids = tokens$input_ids, input_mask = tokens$attention_mask), y = y, epochs = 2, batch_size = 256, #metrics = ""accuracy"", validation_split = .2 ) Error message: Error in py_call_impl(callable, dots$args, dots$keywords) : ValueError: Failed to find data adapter that can handle input: (<class 'dict'> containing {""<class 'str'>""} keys and {""<class 'NoneType'>""} values), <class 'numpy.ndarray'> Detailed traceback: File ""/home/sz/.local/share/r-miniconda/lib/python3.9/site-packages/keras/utils/traceback_utils.py"", line 67, in error_handler raise e.with_traceback(filtered_tb) from None File ""/home/sz/.local/share/r-miniconda/lib/python3.9/site-packages/keras/engine/data_adapter.py"", line 984, in select_data_adapter raise ValueError( Many thanks in advance!",|r|keras|bert-language-model|reticulate|,Tensors&Inputs,1
74200607,"All predicted values of LSTM model is almost same. I have trained a LSTM model to predict multiple output value. Predicted values are almost same even though the loss is less. Why is it so? How can I improve it? `from keras import backend as K import math from sklearn.metrics import mean_squared_error, mean_absolute_error from keras.layers.core import Dense, Dropout, Activation def create_model(): model = Sequential() model.add(LSTM(50, return_sequences=True, input_shape=(40000, 7))) model.add(LSTM(50, return_sequences= True)) model.add(LSTM(50, return_sequences= False)) model.add(Dense(25)) model.add(Dense(2, activation='linear')) model.compile(optimizer='adam', loss='mean_squared_error') model.summary() return model model = create_model() model.fit(X_train, Y_train, shuffle=False, verbose=1, epochs=10) prediction = model.predict(X_test, verbose=0) print(prediction) prediction = [[0.26766795 0.00193274] [0.2676593 0.00192017] [0.2676627 0.00193239] [0.2676644 0.00192784] [0.26766634 0.00193461] [0.2676624 0.00192487] [0.26766685 0.00193129] [0.26766685 0.00193165] [0.2676621 0.00193216] [0.26766127 0.00192624]] ` calculate mean_relative error `mean_relative_error = tf.reduce_mean(tf.abs((Y_test-prediction)/Y_test)) print(mean_relative_error)` `mean_relative_error= 1.9220362`",|python|tensorflow|keras|lstm|loss-function|,Model,0
74376277,"Keras Input 0 of layer ""conv2d_1"" is incompatible with the layer: expected min_ndim=4, found ndim=2. Full shape received: (None, 1). I am trying to create a branched Keras model to output multiple classes (age and gender) My input X_train and X_test have the shape: (4000,128,128,3) and (1000,128,128,3) this is my code to create the layers: from keras.models import Sequential,load_model,Model from keras.layers import Conv2D,MaxPool2D,MaxPooling2D,Dense,Dropout,BatchNormalization,Flatten,Input from keras.layers import * #model creation # input_shape = (128, 128, 3) # inputs = Input((input_shape)) input = Input(shape=(128,128,3)) conv1 = Conv2D(32,(3,3),activation=""relu"")(input) pool1 = MaxPool2D((2,2))(conv1) conv2 = Conv2D(64,(3,3),activation=""relu"")(pool1) pool2 = MaxPool2D((2,2))(conv2) conv3 = Conv2D(128,(3,3),activation=""relu"")(pool2) pool3 = MaxPool2D((2,2))(conv3) flt = Flatten()(pool3) #age age_l = Dense(128,activation=""relu"")(flt) age_l = Dense(64,activation=""relu"")(age_l) age_l = Dense(32,activation=""relu"")(age_l) age_l = Dense(1,activation=""relu"")(age_l) #gender gender_l = Dense(128,activation=""relu"")(flt) gender_l = Dense(80,activation=""relu"")(gender_l) gender_l = Dense(64,activation=""relu"")(gender_l) gender_l = Dense(32,activation=""relu"")(gender_l) gender_l = Dropout(0.5)(gender_l) gender_l = Dense(2,activation=""softmax"")(gender_l) modelA = Model(inputs=input,outputs=[age_l,gender_l]) modelA.compile(loss=['mse', 'sparse_categorical_crossentropy'], optimizer='adam', metrics=['accuracy', 'mae']) modelA.summary() however, i keep getting this error: ValueError Traceback (most recent call last) Cell In [27], line 1 ----> 1 save = modelA.fit(X_train_arr, [y_train, y_train2], 2 validation_data = (X_test, [y_test, y_test2]), 3 epochs = 30) ValueError: Exception encountered when calling layer ""model"" "" f""(type Functional). Input 0 of layer ""conv2d_1"" is incompatible with the layer: expected min_ndim=4, found ndim=2. Full shape received: (None, 1) Call arguments received by layer ""model"" "" f""(type Functional): ?inputs=tf.Tensor(shape=(None, 1), dtype=string) ?training=False ?mask=None I cannot see what the issue is as the input dimensions seem to be correct? Apologies I have tried studying similar posts and the required text but still do not understand what the issue is!",|keras|valueerror|,Tensors&Inputs,1
74610068,"Tensorflow Multi Head Attention on Inputs: 4 x 5 x 20 x 64 with attention_axes=2 throwing mask dimension error (tf 2.11.0). The expectation here is that the attention is applied on the 2nd dimension (4, 5, 20, 64). I am trying to apply self attention using the following code (issue reproducible with this code): import numpy as np import tensorflow as tf from keras import layers as tfl class Encoder(tfl.Layer): def __init__(self,): super().__init__() self.embed_layer = tfl.Embedding(4500, 64, mask_zero=True) self.attn_layer = tfl.MultiHeadAttention(num_heads=2, attention_axes=2, key_dim=16) return def call(self, x): # Input shape: (4, 5, 20) (Batch size: 4) x = self.embed_layer(x) # Output: (4, 5, 20, 64) x = self.attn_layer(query=x, key=x, value=x) # Output: (4, 5, 20, 64) return x eg_input = tf.constant(np.random.randint(0, 150, (4, 5, 20))) enc = Encoder() enc(eg_input) However, the above layer defined throws the following error. Could someone please explain why is this happening & how to fix this? {{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [4,5,2,20,20] vs. [4,5,1,5,20] [Op:AddV2] Call arguments received by layer 'softmax_2' (type Softmax): ?inputs=tf.Tensor(shape=(4, 5, 2, 20, 20), dtype=float32) ?mask=tf.Tensor(shape=(4, 5, 1, 5, 20), dtype=bool) PS: If I set mask_zero = False in defining the embedding layer, the code runs fine as expected without any issues.",|python|python-3.x|tensorflow|attention-model|self-attention|,Tensors&Inputs,1
74743921,"Error optimizer parameter not legal in Keras function. I am using the following code to calculate the probability labels for the goodness of fit study of a data generation quality metric. from sklearn.model_selection import StratifiedKFold from sklearn.model_selection import KFold from sklearn.model_selection import GridSearchCV, cross_val_score from keras.models import Sequential from keras.layers import Dense from keras.wrappers.scikit_learn import KerasClassifier from keras.optimizers import Adam from sklearn.metrics import accuracy_score def baseline_model(X, optimizer='adam', learn_rate=0.1): model = Sequential() model.add(Dense(100, input_dim=X.shape[1], activation='relu')) model.add(Dense(50, activation='relu')) # 8 is the dim/ the number of hidden units (units are the kernel) model.add(Dense(2, activation='softmax')) model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy']) return model def get_probability_labels(x, y, optimizer='adam'): all_predictions = [] estimator = KerasClassifier(optimizer=optimizer, batch_size=32, epochs=100, build_fn=baseline_model(x), verbose=0) for train_index, test_index in cv_5.split(x, y): X_train, X_test = x.iloc[train_index], x.iloc[test_index] y_train, y_test = y.iloc[train_index], y.iloc[test_index] estimator.fit(X_train, y_train) predictions = estimator.predict_proba(X_test) predictions = list(predictions[:, 1]) all_predictions.append(predictions) a = [j for i in all_predictions for j in i] #remove nested list return a def add_labels(real_data, synthetic_data): # add labels 0 for real and 1 for synthetic data = pd.concat([real_data, synthetic_data], ignore_index=True) o_labels = np.zeros((len(real_data)), dtype=int) s_labels = np.ones((len(synthetic_data)), dtype=int) labels = np.concatenate([o_labels, s_labels], axis=0) data['class'] = labels x = data.drop('class', axis=1) y = data['class'] return x, y def main(): X, Y = add_labels(df, df_synth) probability_labels = get_probability_labels(X, Y) print(probability_labels) When I run the code, I get an error associated with the optimizer parameter that I don't know how to solve to make it work. File ""/home/*"", line 42, in get_probability_labels estimator = KerasClassifier(optimizer=optimizer, batch_size=32, epochs=100, build_fn=baseline_model(x), verbose=0) File ""/home/*"", line 77, in __init__ self.check_params(sk_params) File ""/home/*"", line 106, in check_params raise ValueError('{} is not a legal parameter'.format(params_name)) ValueError: optimizer is not a legal parameter",|python|keras|optimization|,API,4
74741897,"Tensorflow: Data dependent loss function. I am trying to implement a loss function that computes a loss depending on the (unaugmented) data. So far I found an example detailing the process using the model.add_loss() method of a tf.keras.models.Model() here, but I struggle to implement it. I have a tf.Dataset object containing my data, labels, and the data dependent variable for every sample calculated before augmentation (let's call it z). The data dependent variable is what I want to pass to my custom loss function. I am dropping the ball in trying to pass the predictions, label and z to my loss function when calling it with model.add_loss. Given a simple model like: import tensorflow as tf from tensorflow.keras.layers import Dense, Input from tensorflow.keras.models import Model from tensorflow.keras.losses import Loss import numpy as np data = Input(shape=(1,), dtype=tf.float32) label = Input(shape=(3,), dtype=tf.float32) z = Input(shape=(1,), dtype=tf.float32) out = Dense(3)(data) m = Model(inputs=[data, label, z], outputs=out) def my_loss(y_true, y_pred, z): cce = tf.keras.losses.CategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE) cce_loss = cce(y_true, y_pred) return tf.reduce_mean(tf.multiply(cce_loss, z)) m.add_loss(my_loss(label, out, z)) m.compile(loss=None, optimizer='adam') dataset = tf.data.Dataset.from_tensor_slices(([1, 2, 3], [[1, 0, 0], [0, 1, 0], [0, 0, 1]], [0.1, 0.2, 0.3])) m.fit(dataset, epochs=10) Trying to run this, I get: ValueError: Layer ""model_17"" expects 3 input(s), but it received 1 input tensors. Is there a way to use an input array [data, label, z] with a tf.dataset object? Or how do I access the three different values inside the model, if I just pass the dataset object as one input value?",|python|tensorflow|keras|loss-function|,Tensors&Inputs,1
74977873,"the evaluation section leds to bug. there is a problem in running code hi, pop out an error and can not generate the result, Error: ValueError: Input 2 is incompatible with layer model_2: expected shape=(None, 50), found shape=(None, 51) so is there any solution for? Much obliged the full part that triggers the bug droped below full error: ValueError: in user code: /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1478 predict_function * return step_function(self, iterator) /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1468 step_function ** outputs = model.distribute_strategy.run(run_step, args=(data,)) /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/tpu_strategy.py:540 run return self.extended.tpu_run(fn, args, kwargs, options) /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/tpu_strategy.py:1296 tpu_run return func(args, kwargs) /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/tpu_strategy.py:1364 tpu_function xla_options=tpu.XLAOptions(use_spmd_for_xla_partitioning=False)) /opt/conda/lib/python3.7/site-packages/tensorflow/python/tpu/tpu.py:968 replicate xla_options=xla_options)[1] /opt/conda/lib/python3.7/site-packages/tensorflow/python/tpu/tpu.py:1439",|tensorflow|google-colaboratory|transformer-model|,Tensors&Inputs,1
75251168,"BCELoss between logits and labels not working. I am using a GPT2 model that outputs logits (before softmax) in the shape (batch_size, num_input_ids, vocab_size) and I need to compare it with the labels that are of shape (batch_size, num_input_ids) to calculate BCELoss. How do I calculate it? logits = output.logits #--of shape (32, 56, 592) logits = torch.nn.Softmax()(logits) labels = labels #---------of shape (32, 56) torch.nn.BCELoss()(logits, labels) but the dimensions do not match, so how do I contract logits to labels shape or expand labels to logits shape?",|pytorch|nlp|loss-function|text-classification|gpt-2|,Training,2
75426868,"TensorFlow GPU problem 'libnvinfer.so.7' and ' 'libnvinfer.so.7'' could not load. I installed TensorFlow under WSL 2, Ubuntu 22.04 (Jammy Jellyfish), I followed the instructions in Install TensorFlow with pip. *I also installed Nvidia drivers for Windows and in my other WSL 2, I use GPU-supported simulation program. Everything seemed OK. I didn't get any error message during installation, but when I imported TensorFlow in Python 3, I got this error: 2023-02-12 14:49:58.544771: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvrtc.so.11.0: cannot open shared object file: No such file or directory 2023-02-12 14:49:58.544845: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory 2023-02-12 14:49:58.544874: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. I searched my libnvinfer_plugin.so.7 files: sudo find / -name libnvinfer.so.7 2> /dev/null and I found them in this directory: cat /usr/lib/x86_64-linux-gnu/libnvinfer.so.7 and I added this directory to LD_LIBRARY_PATH like in Could not load dynamic library 'libnvinfer.so.7', but nothing changed. Still TensorFlow is working, but I can't use the GPU. nvidia-smi: +-----------------------------------------------------------------------------+ | NVIDIA-SMI 515.65.01 Driver Version: 516.94 CUDA Version: 11.7 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 NVIDIA GeForce ... On | 00000000:01:00.0 Off | N/A | | N/A 43C P0 22W / N/A | 0MiB / 6144MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ nvcc--version: nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2023 NVIDIA Corporation Built on Fri_Jan__6_16:45:21_PST_2023 Cuda compilation tools, release 12.0, V12.0.140 Build cuda_12.0.r12.0/compiler.32267302_0 *The TensorFlow version is: 2.11.0 So, how can I fix this problem?",|python|tensorflow|gpu|lib|,GPU Usage,3
75424120,Tensorflow Nvidia GPU not detected. Hi I'm struggling to get Tensorflow V2.11 to find my eGPU (RTX 3060 Ti) I am currently on Windows 11 CUDA version is 12 I am currently downloading CUDA 11 as well as CUDnn as I've heard it is recommended I have tried the following code: import tensorflow as tf tf.config.list_physical_devices('GPU') which outputs: [] any help would be great,|python|tensorflow|gpu|,GPU Usage,3
75614728,"Cuda 12 + tf-nightly 2.12: Could not find cuda drivers on your machine, GPU will not be used, while every checking is fine and in torch it works. tf-nightly version = 2.12.0-dev2023203 Python version = 3.10.6 CUDA drivers version = 525.85.12 CUDA version = 12.0 Cudnn version = 8.5.0 I am using Linux (x86_64, Ubuntu 22.04) I am coding in Visual Studio Code on a venv virtual environment I am trying to run some models on the GPU (NVIDIA GeForce RTX 3050) using tensorflow nightly 2.12 (to be able to use Cuda 12.0). The problem that I have is that apparently every checking that I am making seems to be correct, but in the end the script is not able to detect the GPU. I've dedicated a lot of time trying to see what is happening and nothing seems to work, so any advice or solution will be more than welcomed. The GPU seems to be working for torch as you can see at the very end of the question. I will show some of the most common checkings regarding CUDA that I did (Visual Studio Code terminal), I hope you find them useful: Check CUDA version: $ nvcc --version nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2023 NVIDIA Corporation Built on Fri_Jan__6_16:45:21_PST_2023 Cuda compilation tools, release 12.0, V12.0.140 Build cuda_12.0.r12.0/compiler.32267302_0 Check if the connection with the CUDA libraries is correct: $ echo $LD_LIBRARY_PATH /usr/cuda/lib Check nvidia drivers for the GPU and check if GPU is readable for the venv: $ nvidia-smi +-----------------------------------------------------------------------------+ | NVIDIA-SMI 525.85.12 Driver Version: 525.85.12 CUDA Version: 12.0 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 NVIDIA GeForce ... On | 00000000:01:00.0 On | N/A | | N/A 40C P5 6W / 20W | 46MiB / 4096MiB | 22% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | 0 N/A N/A 1356 G /usr/lib/xorg/Xorg 45MiB | +-----------------------------------------------------------------------------+ Add cuda/bin PATH and Check it: $ export PATH=""/usr/local/cuda/bin:$PATH"" $ echo $PATH /usr/local/cuda-12.0/bin:/home/victus-linux/Escritorio/MasterThesis_CODE/to_share/venv_master/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/snap/bin Custom function to check if CUDA is correctly installed: [function by Sherlock] function lib_installed() { /sbin/ldconfig -N -v $(sed 's/:/ /' <<< $LD_LIBRARY_PATH) 2>/dev/null | grep $1; } function check() { lib_installed $1 && echo ""$1 is installed"" || echo ""ERROR: $1 is NOT installed""; } check libcuda check libcudart libcudart.so.12 -> libcudart.so.12.0.146 libcuda.so.1 -> libcuda.so.525.85.12 libcuda.so.1 -> libcuda.so.525.85.12 libcudadebugger.so.1 -> libcudadebugger.so.525.85.12 libcuda is installed libcudart.so.12 -> libcudart.so.12.0.146 libcudart is installed Custom function to check if Cudnn is correctly installed: [function by Sherlock] function lib_installed() { /sbin/ldconfig -N -v $(sed 's/:/ /' <<< $LD_LIBRARY_PATH) 2>/dev/null | grep $1; } function check() { lib_installed $1 && echo ""$1 is installed"" || echo ""ERROR: $1 is NOT installed""; } check libcudnn libcudnn_cnn_train.so.8 -> libcudnn_cnn_train.so.8.8.0 libcudnn_cnn_infer.so.8 -> libcudnn_cnn_infer.so.8.8.0 libcudnn_adv_train.so.8 -> libcudnn_adv_train.so.8.8.0 libcudnn.so.8 -> libcudnn.so.8.8.0 libcudnn_ops_train.so.8 -> libcudnn_ops_train.so.8.8.0 libcudnn_adv_infer.so.8 -> libcudnn_adv_infer.so.8.8.0 libcudnn_ops_infer.so.8 -> libcudnn_ops_infer.so.8.8.0 libcudnn is installed So, once I did these previous checkings I used a script to evaluate if everything was finally ok and then the following error appeared: import tensorflow as tf print(f'\nTensorflow version = {tf.__version__}\n') print(f'\n{tf.config.list_physical_devices(""GPU"")}\n') 2023-03-02 12:05:09.463343: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used. 2023-03-02 12:05:09.489911: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used. 2023-03-02 12:05:09.490522: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations. To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-03-02 12:05:10.066759: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT Tensorflow version = 2.12.0-dev20230203 2023-03-02 12:05:10.748675: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355 2023-03-02 12:05:10.771263: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform. Skipping registering GPU devices... [] Extra check: I tried to run a checking script on torch and in here it worked so I guess the problem is related with tensorflow/tf-nightly import torch print(f'\nAvailable cuda = {torch.cuda.is_available()}') print(f'\nGPUs availables = {torch.cuda.device_count()}') print(f'\nCurrent device = {torch.cuda.current_device()}') print(f'\nCurrent Device location = {torch.cuda.device(0)}') print(f'\nName of the device = {torch.cuda.get_device_name(0)}') Available cuda = True GPUs availables = 1 Current device = 0 Current Device location = <torch.cuda.device object at 0x7fbe26fd2ec0> Name of the device = NVIDIA GeForce RTX 3050 Laptop GPU Please, if you know something that might help solve this issue, don't hesitate on telling me.",|python|tensorflow|gpu|,GPU Usage,3
75644077,"Pytorch: RuntimeError: zero-dimensional tensor (at position 0) cannot be concatenated. I have two tensors # losses_q tensor(0.0870, device='cuda:0', grad_fn=<SumBackward0>) # this_loss_q tensor([0.0874], device='cuda:0', grad_fn=<AddBackward0>) when I am trying to concat them, pytorch raises an error: losses_q = torch.cat((losses_q, this_loss_q), dim=0) RuntimeError: zero-dimensional tensor (at position 0) cannot be concatenated How to resolve this error?",|python-3.x|pytorch|tensor|pytorch-geometric|,Tensors&Inputs,1
75728844,"TypeError: VariableMetaclass._variable_v1_call() got an unexpected keyword argument 'experimental_enable_variable_lifting'. I am getting this TypeError while creating a keras model from TensorFlow inside the __init__ method of a custom class, any ideas? My code: self.model = keras.Sequential([ keras.layers.Dense(1, input_dim=self.degree), keras.layers.Dense(1) ]) self.model.compile(optimizer=optimizer, loss=loss) self.model.summary() Error: self.model = keras.Sequential([ ^^^^^^^^^^^^^^^^^^ File ""C:\Python311\Lib\site-packages\tensorflow\python\trackable\base.py"", line 205, in _method_wrapper result = method(self, *args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File ""C:\Python311\Lib\site-packages\keras\utils\traceback_utils.py"", line 70, in error_handler raise e.with_traceback(filtered_tb) from None File ""C:\Python311\Lib\site-packages\tensorflow\python\ops\variables.py"", line 285, in __call__ return cls._variable_v1_call(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ TypeError: VariableMetaclass._variable_v1_call() got an unexpected keyword argument 'experimental_enable_variable_lifting' Tensorflow version information: Name: tensorflow Version: 2.12.0rc1 Summary: TensorFlow is an open source machine learning framework for everyone. Home-page: https://www.tensorflow.org/ Author: Google Inc. Author-email: packages@tensorflow.org License: Apache 2.0 Location: C:\Python311\Lib\site-packages Requires: tensorflow-intel Required-by:",|tensorflow|keras|typeerror|tensorflow2.0|tf.keras|,API,4
75915809,"Accuracy value more than 1 with nn.BCEWithLogitsLoss() loss function pytorch in Binary Classifier. I am trying to use nn.BCEWithLogitsLoss() for model which initially used nn.CrossEntropyLoss(). However, after doing some changes to the training function to accommodate the nn.BCEWithLogitsLoss() loss function the model accuracy values are shown as more than 1. Please find the code below. # Data augmentation and normalization for training # Just normalization for validation data_transforms = { 'train': transforms.Compose([ transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]), 'val': transforms.Compose([ transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]), } data_dir = '/kaggle/input/catsndogsorg/hymenoptera_data' image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']} dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=4) for x in ['train', 'val']} dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']} class_names = image_datasets['train'].classes device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"") ############# def train_model(model, criterion, optimizer, scheduler, num_epochs=25): since = time.time() best_model_wts = copy.deepcopy(model.state_dict()) best_acc = 0.0 for epoch in range(num_epochs): print(f'Epoch {epoch}/{num_epochs - 1}') print('-' * 10) # Each epoch has a training and validation phase for phase in ['train', 'val']: if phase == 'train': model.train() # Set model to training mode else: model.eval() # Set model to evaluate mode running_loss = 0.0 running_corrects = 0 # Iterate over data. for inputs, labels in dataloaders[phase]: inputs = inputs.to(device) labels = labels.to(device).unsqueeze(1) # zero the parameter gradients optimizer.zero_grad() # forward # track history if only in train with torch.set_grad_enabled(phase == 'train'): outputs = model(inputs) _, preds = torch.max(outputs, 1) #print(outputs, labels) loss = criterion(outputs, labels.float()) print(loss) # backward + optimize only if in training phase if phase == 'train': loss.backward() optimizer.step() # statistics running_loss += loss.item() * inputs.size(0) running_corrects += torch.sum(preds == labels.data) if phase == 'train': scheduler.step() epoch_loss = running_loss / dataset_sizes[phase] epoch_acc = running_corrects.double() / dataset_sizes[phase] print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}') # deep copy the model if phase == 'val' and epoch_acc > best_acc: best_acc = epoch_acc best_model_wts = copy.deepcopy(model.state_dict()) print() time_elapsed = time.time() - since print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s') print(f'Best val Acc: {best_acc:4f}') # load best model weights model.load_state_dict(best_model_wts) return model EDIT:Model pipeline model_ft = models.resnet18(weights='ResNet18_Weights.DEFAULT') num_ftrs = model_ft.fc.in_features # Here the size of each output sample is set to 2. # Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)). model_ft.fc = nn.Linear(num_ftrs, 1) model_ft = model_ft.to(device) criterion = nn.BCEWithLogitsLoss() # Observe that all parameters are being optimized optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9) # Decay LR by a factor of 0.1 every 7 epochs exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1) model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,num_epochs=25) The outputs of training loop: outputs shape: torch.Size([4, 1]) labels shape: torch.Size([4, 1]) logits: tensor(0.3511,grad_fn<BinaryCrossEntropyWithLogitsBackward0>) train Loss: 1.0000 Acc: 2.0164 val Loss: 1.0000 Acc: 1.8105 Would anyone be able to help me in this matter please. Thanks & Best Regards AMJS",|python|machine-learning|pytorch|loss-function|transfer-learning|,Training,2
75929010,"ModuleNotFoundError: No module named 'pytorch_lightning.core.decorators' | Google Colab GPU session. In my Google Colab GPU runtime, I try to install pytorch_lightning. So I do the following in the order: !pip list | grep torch torch 2.0.0+cu118 torchaudio 2.0.1+cu118 torchdata 0.6.0 torchsummary 1.5.1 torchtext 0.15.1 torchvision 0.15.1+cu118 Then using !pip install pytorch_lightning, I manage to install 2.0.1 version: Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/ Collecting pytorch_lightning Downloading pytorch_lightning-2.0.1-py3-none-any.whl (716 kB)  716.4/716.4 KB 34.0 MB/s eta 0:00:00 Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.9/dist-packages (from pytorch_lightning) (2023.3.0) Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.9/dist-packages (from pytorch_lightning) (6.0) Collecting torchmetrics>=0.7.0 Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)  519.2/519.2 KB 48.1 MB/s eta 0:00:00 Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from pytorch_lightning) (4.5.0) Collecting lightning-utilities>=0.7.0 Downloading lightning_utilities-0.8.0-py3-none-any.whl (20 kB) Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.9/dist-packages (from pytorch_lightning) (1.22.4) Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.9/dist-packages (from pytorch_lightning) (4.65.0) Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from pytorch_lightning) (2.0.0+cu118) Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.9/dist-packages (from pytorch_lightning) (23.0) Collecting aiohttp!=4.0.0a0,!=4.0.0a1 Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)  1.0/1.0 MB 71.8 MB/s eta 0:00:00 Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (2.27.1) Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.11.0->pytorch_lightning) (2.0.0) Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.11.0->pytorch_lightning) (3.10.7) Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.11.0->pytorch_lightning) (3.1.2) Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.11.0->pytorch_lightning) (3.0) Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.11.0->pytorch_lightning) (1.11.1) Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.11.0->pytorch_lightning) (16.0.0) Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.11.0->pytorch_lightning) (3.25.2) Collecting multidict<7.0,>=4.5 Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)  114.2/114.2 KB 17.2 MB/s eta 0:00:00 Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (22.2.0) Collecting yarl<2.0,>=1.0 Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)  264.6/264.6 KB 32.6 MB/s eta 0:00:00 Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (2.0.12) Collecting async-timeout<5.0,>=4.0.0a3 Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB) Collecting frozenlist>=1.1.1 Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)  158.8/158.8 KB 23.9 MB/s eta 0:00:00 Collecting aiosignal>=1.1.2 Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB) Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.11.0->pytorch_lightning) (2.1.2) Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (1.26.15) Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (3.4) Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (2022.12.7) Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.11.0->pytorch_lightning) (1.3.0) Installing collected packages: multidict, lightning-utilities, frozenlist, async-timeout, yarl, aiosignal, aiohttp, torchmetrics, pytorch_lightning Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 lightning-utilities-0.8.0 multidict-6.0.4 pytorch_lightning-2.0.1 torchmetrics-0.11.4 yarl-1.8.2 Here's also to double check, if it's installed: !pip list | grep torch pytorch-lightning 2.0.1 torch 2.0.0+cu118 torchaudio 2.0.1+cu118 torchdata 0.6.0 torchmetrics 0.11.4 torchsummary 1.5.1 torchtext 0.15.1 torchvision 0.15.1+cu118 However, I have a compatibility issue of importing a module: import pytorch_lightning # works without error! from pytorch_lightning.core.decorators import auto_move_data # ERROR! which returns the following error: ModuleNotFoundError Traceback (most recent call last) <ipython-input-6-2a2afba7b5b2> in <cell line: 1>() ----> 1 from pytorch_lightning.core.decorators import auto_move_data ModuleNotFoundError: No module named 'pytorch_lightning.core.decorators' I believe this module has been deprecated in the current version, does anyone know how to fix this compatibility issue? Cheers,",|pytorch|importerror|pytorch-lightning|,API,4
75948480,"Tensorflow Keras RandomFlip is applying the same flip to all images. version: tensorflow-gpu 2.10.0 I have created a data augmentation layer that is applied during pre-processing. In addition to flips, it involves random changes in contrast, rotation and brightness. I use the following code to test the augmentation: #Defining data augmentation Keras layer data_augmentation = tf.keras.Sequential([ tf.keras.layers.RandomFlip(""horizontal_and_vertical""), tf.keras.layers.RandomBrightness(0.25, seed=10), tf.keras.layers.RandomContrast(0.5, seed=20), tf.keras.layers.RandomRotation(0.028, fill_mode=""constant"", seed=35), ]) #Augmenting 1 image 9 times for testing image = train_ds.take(1) images = image.repeat(9) images = images.map(lambda x,y: (data_augmentation(x, training=True),y),num_parallel_calls=AUTOTUNE) n=0 for i,l in images: i=tf.cast(i, tf.uint8) ax = plt.subplot(3, 3, n + 1) _ = plt.imshow(i, cmap=""gray"",vmin=0,vmax=255) plt.axis(""off"") n+=1 The random changes in contrast, brightness and rotation act how I want: each of the 9 images has a different contrast, brightness and rotation. The seed allows me to reproduce these results (although funnily enough, the subplots will be ordered differently). I have used seeds so that I can recreate the same dataset. However, RandomFlip applies the exact same flip to all images. What's more, even with a seed, a different flip is applied each time I re-run the code. It seems to me that RandomFlip is not behaving like it should because it isn't applying a random flip to each image? I have tried varying the seed and the position of RandomFlip in the data augmentation layer. I was hoping it was just the particular seed, but it appears to happen for other seeds. I have also tried with and without the following at the start of my code: tf.random.set_seed(12) np.random.seed(0) With no effect.",|tensorflow|keras|deep-learning|conv-neural-network|data-augmentation|,API,4
76101948,"TensorFlow GPU recognized in the terminal but not in the Jupyter notebook. I installed TensorFlow with the GPU support according to the official installation page and GPU is recognized from the terminal but not from the Jupyter notebook with the Jupyter kernel from the same Conda environment tensor_gpu (see the screenshot below). The Jupyter Lab 3.6.3 (Windows 10) is installed and run from the separate Conda environment jupyter. I also see the following warning in the Jupyter Lab console: 2023-04-25 16:34:44.493879: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found 2023-04-25 16:34:44.494185: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine. 2023-04-25 16:34:47.012660: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found 2023-04-25 16:34:47.014859: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cublas64_11.dll'; dlerror: cublas64_11.dll not found 2023-04-25 16:34:47.017536: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cublasLt64_11.dll'; dlerror: cublasLt64_11.dll not found 2023-04-25 16:34:47.019433: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cufft64_10.dll'; dlerror: cufft64_10.dll not found 2023-04-25 16:34:47.021223: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'curand64_10.dll'; dlerror: curand64_10.dll not found 2023-04-25 16:34:47.023012: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cusolver64_11.dll'; dlerror: cusolver64_11.dll not found 2023-04-25 16:34:47.024836: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cusparse64_11.dll'; dlerror: cusparse64_11.dll not found 2023-04-25 16:34:47.026672: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudnn64_8.dll'; dlerror: cudnn64_8.dll not found 2023-04-25 16:34:47.026872: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform. Skipping registering GPU devices... Could you please suggest what I am missing here, like environment variables, additional paths, etc.? The list of installed packages in the environment tensor_gpu: (tensor_gpu) C:\Users\Pavlo Fesenko\Desktop>mamba list # packages in environment at C:\Users\Pavlo Fesenko\.conda\envs\tensor_gpu: # # Name Version Build Channel absl-py 1.4.0 pypi_0 pypi asttokens 2.2.1 pyhd8ed1ab_0 conda-forge astunparse 1.6.3 pypi_0 pypi backcall 0.2.0 pyh9f0ad1d_0 conda-forge backports 1.0 pyhd8ed1ab_3 conda-forge backports.functools_lru_cache 1.6.4 pyhd8ed1ab_0 conda-forge brotli 1.0.9 hcfcfb64_8 conda-forge brotli-bin 1.0.9 hcfcfb64_8 conda-forge brotlipy 0.7.0 py39ha55989b_1005 conda-forge bzip2 1.0.8 h8ffe710_4 conda-forge ca-certificates 2022.12.7 h5b45459_0 conda-forge cachetools 5.3.0 pypi_0 pypi certifi 2022.12.7 pypi_0 pypi cffi 1.15.1 py39h68f70e3_3 conda-forge charset-normalizer 3.1.0 pyhd8ed1ab_0 conda-forge colorama 0.4.6 pyhd8ed1ab_0 conda-forge comm 0.1.3 pyhd8ed1ab_0 conda-forge contourpy 1.0.7 py39h1f6ef14_0 conda-forge cryptography 40.0.2 py39hb6bd5e6_0 conda-forge cudatoolkit 11.2.2 h7d7167e_11 conda-forge cudnn 8.1.0.77 h3e0f4f4_0 conda-forge cycler 0.11.0 pyhd8ed1ab_0 conda-forge debugpy 1.6.7 py39h99910a6_0 conda-forge decorator 5.1.1 pyhd8ed1ab_0 conda-forge executing 1.2.0 pyhd8ed1ab_0 conda-forge flatbuffers 23.3.3 pypi_0 pypi fonttools 4.39.3 py39ha55989b_0 conda-forge freetype 2.12.1 h546665d_1 conda-forge gast 0.4.0 pypi_0 pypi gettext 0.21.1 h5728263_0 conda-forge glib 2.76.2 h12be248_0 conda-forge glib-tools 2.76.2 h12be248_0 conda-forge google-auth 2.17.3 pypi_0 pypi google-auth-oauthlib 0.4.6 pypi_0 pypi google-pasta 0.2.0 pypi_0 pypi grpcio 1.54.0 pypi_0 pypi gst-plugins-base 1.22.0 h001b923_2 conda-forge gstreamer 1.22.0 h6b5321d_2 conda-forge h5py 3.8.0 pypi_0 pypi icu 72.1 h63175ca_0 conda-forge idna 3.4 pyhd8ed1ab_0 conda-forge importlib-metadata 6.6.0 pyha770c72_0 conda-forge importlib-resources 5.12.0 pyhd8ed1ab_0 conda-forge importlib_metadata 6.6.0 hd8ed1ab_0 conda-forge importlib_resources 5.12.0 pyhd8ed1ab_0 conda-forge intel-openmp 2023.1.0 h57928b3_46319 conda-forge ipykernel 6.22.0 pyh025b116_0 conda-forge ipython 8.12.0 pyh08f2357_0 conda-forge jedi 0.18.2 pyhd8ed1ab_0 conda-forge jupyter_client 8.2.0 pyhd8ed1ab_0 conda-forge jupyter_core 5.3.0 py39hcbf5309_0 conda-forge keras 2.10.0 pypi_0 pypi keras-preprocessing 1.1.2 pypi_0 pypi kiwisolver 1.4.4 py39h1f6ef14_1 conda-forge krb5 1.20.1 heb0366b_0 conda-forge lcms2 2.15 h3e3b177_1 conda-forge lerc 4.0.0 h63175ca_0 conda-forge libblas 3.9.0 16_win64_mkl conda-forge libbrotlicommon 1.0.9 hcfcfb64_8 conda-forge libbrotlidec 1.0.9 hcfcfb64_8 conda-forge libbrotlienc 1.0.9 hcfcfb64_8 conda-forge libcblas 3.9.0 16_win64_mkl conda-forge libclang 16.0.0 pypi_0 pypi libclang13 16.0.2 default_h45d3cf4_0 conda-forge libdeflate 1.18 hcfcfb64_0 conda-forge libffi 3.4.2 h8ffe710_5 conda-forge libglib 2.76.2 he8f3873_0 conda-forge libhwloc 2.9.1 h51c2c0f_0 conda-forge libiconv 1.17 h8ffe710_0 conda-forge libjpeg-turbo 2.1.5.1 hcfcfb64_0 conda-forge liblapack 3.9.0 16_win64_mkl conda-forge libogg 1.3.4 h8ffe710_1 conda-forge libpng 1.6.39 h19919ed_0 conda-forge libsodium 1.0.18 h8d14728_1 conda-forge libsqlite 3.40.0 hcfcfb64_1 conda-forge libtiff 4.5.0 h6c8260b_6 conda-forge libvorbis 1.3.7 h0e60522_0 conda-forge libwebp-base 1.3.0 hcfcfb64_0 conda-forge libxcb 1.13 hcd874cb_1004 conda-forge libxml2 2.10.4 hc3477c8_0 conda-forge libzlib 1.2.13 hcfcfb64_4 conda-forge m2w64-gcc-libgfortran 5.3.0 6 conda-forge m2w64-gcc-libs 5.3.0 7 conda-forge m2w64-gcc-libs-core 5.3.0 7 conda-forge m2w64-gmp 6.1.0 2 conda-forge m2w64-libwinpthread-git 5.0.0.4634.697f757 2 conda-forge markdown 3.4.3 pypi_0 pypi markupsafe 2.1.2 pypi_0 pypi matplotlib 3.7.1 py39hcbf5309_0 conda-forge matplotlib-base 3.7.1 py39haf65ace_0 conda-forge matplotlib-inline 0.1.6 pyhd8ed1ab_0 conda-forge mkl 2022.1.0 h6a75c08_874 conda-forge msys2-conda-epoch 20160418 1 conda-forge munkres 1.1.4 pyh9f0ad1d_0 conda-forge nest-asyncio 1.5.6 pyhd8ed1ab_0 conda-forge numpy 1.24.3 py39h816b6a6_0 conda-forge oauthlib 3.2.2 pypi_0 pypi openjpeg 2.5.0 ha2aaf27_2 conda-forge openssl 3.1.0 hcfcfb64_2 conda-forge opt-einsum 3.3.0 pypi_0 pypi packaging 23.1 pyhd8ed1ab_0 conda-forge pandas 2.0.1 py39h1679cfb_0 conda-forge parso 0.8.3 pyhd8ed1ab_0 conda-forge pcre2 10.40 h17e33f8_0 conda-forge pickleshare 0.7.5 py_1003 conda-forge pillow 9.5.0 py39haa1d754_0 conda-forge pip 23.1.1 pyhd8ed1ab_0 conda-forge platformdirs 3.2.0 pyhd8ed1ab_0 conda-forge ply 3.11 py_1 conda-forge pooch 1.7.0 pyha770c72_3 conda-forge prompt-toolkit 3.0.38 pyha770c72_0 conda-forge prompt_toolkit 3.0.38 hd8ed1ab_0 conda-forge protobuf 3.19.6 pypi_0 pypi psutil 5.9.5 py39ha55989b_0 conda-forge pthread-stubs 0.4 hcd874cb_1001 conda-forge pthreads-win32 2.9.1 hfa6e2cd_3 conda-forge pure_eval 0.2.2 pyhd8ed1ab_0 conda-forge pyasn1 0.5.0 pypi_0 pypi pyasn1-modules 0.3.0 pypi_0 pypi pycparser 2.21 pyhd8ed1ab_0 conda-forge pygments 2.15.1 pyhd8ed1ab_0 conda-forge pyopenssl 23.1.1 pyhd8ed1ab_0 conda-forge pyparsing 3.0.9 pyhd8ed1ab_0 conda-forge pyqt 5.15.7 py39hb77abff_3 conda-forge pyqt5-sip 12.11.0 py39h99910a6_3 conda-forge pysocks 1.7.1 pyh0701188_6 conda-forge python 3.9.16 h4de0772_0_cpython conda-forge python-dateutil 2.8.2 pyhd8ed1ab_0 conda-forge python-tzdata 2023.3 pyhd8ed1ab_0 conda-forge python_abi 3.9 3_cp39 conda-forge pytz 2023.3 pyhd8ed1ab_0 conda-forge pywin32 304 py39h99910a6_2 conda-forge pyzmq 25.0.2 py39hea35a22_0 conda-forge qt-main 5.15.8 h7f2b912_9 conda-forge requests 2.28.2 pyhd8ed1ab_1 conda-forge requests-oauthlib 1.3.1 pypi_0 pypi rsa 4.9 pypi_0 pypi scipy 1.10.1 py39hde5eda1_0 conda-forge setuptools 67.7.2 pyhd8ed1ab_0 conda-forge sip 6.7.9 py39h99910a6_0 conda-forge six 1.16.0 pyh6c4a22f_0 conda-forge stack_data 0.6.2 pyhd8ed1ab_0 conda-forge tbb 2021.9.0 h91493d7_0 conda-forge tensorboard 2.10.1 pypi_0 pypi tensorboard-data-server 0.6.1 pypi_0 pypi tensorboard-plugin-wit 1.8.1 pypi_0 pypi tensorflow 2.10.1 pypi_0 pypi tensorflow-estimator 2.10.0 pypi_0 pypi tensorflow-io-gcs-filesystem 0.31.0 pypi_0 pypi termcolor 2.3.0 pypi_0 pypi tk 8.6.12 h8ffe710_0 conda-forge toml 0.10.2 pyhd8ed1ab_0 conda-forge tomli 2.0.1 pyhd8ed1ab_0 conda-forge tornado 6.3 py39ha55989b_0 conda-forge traitlets 5.9.0 pyhd8ed1ab_0 conda-forge typing-extensions 4.5.0 hd8ed1ab_0 conda-forge typing_extensions 4.5.0 pyha770c72_0 conda-forge tzdata 2023c h71feb2d_0 conda-forge ucrt 10.0.22621.0 h57928b3_0 conda-forge unicodedata2 15.0.0 py39ha55989b_0 conda-forge urllib3 1.26.15 pyhd8ed1ab_0 conda-forge vc 14.3 hb6edc58_10 conda-forge vs2015_runtime 14.34.31931 h4c5c07a_10 conda-forge wcwidth 0.2.6 pyhd8ed1ab_0 conda-forge werkzeug 2.2.3 pypi_0 pypi wheel 0.40.0 pyhd8ed1ab_0 conda-forge win_inet_pton 1.1.0 pyhd8ed1ab_6 conda-forge wrapt 1.15.0 pypi_0 pypi xorg-libxau 1.0.9 hcd874cb_0 conda-forge xorg-libxdmcp 1.1.3 hcd874cb_0 conda-forge xz 5.2.6 h8d14728_0 conda-forge zeromq 4.3.4 h0e60522_1 conda-forge zipp 3.15.0 pyhd8ed1ab_0 conda-forge zstd 1.5.2 h12be248_6 conda-forge",|python|tensorflow|gpu|,GPU Usage,3
76134365,"kernel error in CUDA when moving Tensors to GPU. I'm trying to replicate this code but I moved my tensors to the GPU, when I run my code on the CPU it runs pretty well, but when I switch to GPU I get this error: RuntimeError: CUDA error: initialization error CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect. For debugging consider passing CUDA_LAUNCH_BLOCKING=1. Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions. My Code is: #!/usr/bin/python3 import torch from torch.utils.data import Dataset, DataLoader import numpy as np if torch.cuda.is_available(): device_ = torch.device(""cuda"") print(""========================\nYou are running on GPU!\n========================"") else: device_ = torch.device(""cpu"") print(""------------------------\nYou are running on CPU!\n------------------------"") class WineDataset(Dataset): def __init__(self): # data loading xy = np.loadtxt(""./dataset/wine.csv"", dtype=np.float32, delimiter="","", skiprows=1) self.n_samples = xy.shape[0] self.x = torch.from_numpy(xy[:, 1:]) self.x = self.x.to(device_) self.y = torch.from_numpy(xy[:, [0]]) # n_samples, 1 self.y = self.y.to(device_) print(self.x.shape, self.y.shape) def __getitem__(self, index): # dataset[0] return self.x[index], self.y[index] def __len__(self): # len(dataset) return self.n_samples dataset = WineDataset() train_loader = DataLoader(dataset=dataset, batch_size=4, shuffle=True, num_workers=2) dataiter = iter(train_loader) data = next(dataiter) features, labels = data I have created this Colab Notebook to make my problem reproducible, can you please tell me how can I run the code with my tensors on the GPU? thanks.",|pytorch|gpu|,GPU Usage,3
76204465,"RuntimeError: Cannot set version_counter for inference - Trying DirectML in AI Project for AMD. Actually is converting a PyTorch CUDA project (https://github.com/suno-ai/bark) with DirectML for use my AMD GPU RX6700xt, I am having the problem RuntimeError: Cannot set version_counter for inference tensor. I've Tried write to the developer but he says that he doesn't have experience with AMD. I've changed all the .to(device) to .to(dml) of generation.py according to gpu-pytorch-windows Docs the files modifies are, generation.py in bark folder and build\lib\bark\ respectively. When I try run the project I see that the GPU started correctly but then I get the next error. I really appreciate all the help you've given me so far. I was hoping you could help me out again. I've been reading a lot and trying different things, but I can't find much information on this error I'm getting. I don't know what to do or if I'm doing something wrong. From here: https://github.com/suno-ai/bark/issues/271 python .\run.py No GPU being used. Careful, inference might be very slow! 0%| | 0/100 [00:00<?, ?it/s]Traceback (most recent call last): File ""C:\Users\NoeXVanitasXJunk\bark\run.py"", line 13, in <module> audio_array = generate_audio(text_prompt) File ""C:\Users\NoeXVanitasXJunk\bark\bark\api.py"", line 107, in generate_audio semantic_tokens = text_to_semantic( File ""C:\Users\NoeXVanitasXJunk\bark\bark\api.py"", line 25, in text_to_semantic x_semantic = generate_text_semantic( File ""C:\Users\NoeXVanitasXJunk\bark\bark\generation.py"", line 460, in generate_text_semantic logits, kv_cache = model( File ""C:\Users\NoeXVanitasXJunk\miniconda3\envs\tfdml_plugin\lib\site-packages\torch\nn\modules\module.py"", line 1501, in _call_impl return forward_call(*args, **kwargs) File ""C:\Users\NoeXVanitasXJunk\bark\bark\model.py"", line 208, in forward x, kv = block(x, past_kv=past_layer_kv, use_cache=use_cache) File ""C:\Users\NoeXVanitasXJunk\miniconda3\envs\tfdml_plugin\lib\site-packages\torch\nn\modules\module.py"", line 1501, in _call_impl return forward_call(*args, **kwargs) File ""C:\Users\NoeXVanitasXJunk\bark\bark\model.py"", line 121, in forward attn_output, prev_kvs = self.attn(self.ln_1(x), past_kv=past_kv, use_cache=use_cache) File ""C:\Users\NoeXVanitasXJunk\miniconda3\envs\tfdml_plugin\lib\site-packages\torch\nn\modules\module.py"", line 1501, in _call_impl return forward_call(*args, **kwargs) File ""C:\Users\NoeXVanitasXJunk\bark\bark\model.py"", line 50, in forward q, k ,v = self.c_attn(x).split(self.n_embd, dim=2) File ""C:\Users\NoeXVanitasXJunk\miniconda3\envs\tfdml_plugin\lib\site-packages\torch\nn\modules\module.py"", line 1501, in _call_impl return forward_call(*args, **kwargs) File ""C:\Users\NoeXVanitasXJunk\miniconda3\envs\tfdml_plugin\lib\site-packages\torch\nn\modules\linear.py"", line 114, in forward return F.linear(input, self.weight, self.bias) RuntimeError: Cannot set version_counter for inference tensor 0%| | 0/100 [00:00<?, ?it/s] I am in Python 3.9.16 Could you try help me with this? Other thing: I've read that with torch-mlir it is possible to use an AMD card, but I'm not sure if it works on Windows. I tried it but I am not sure if I need something more or some DirectMl Special. I tried installing torch-mlir and it works in the project but it only uses the CPU not the GPU. I am not sure how to configure for using the GPU. Update 2: When I try to set mode=False in interference inference_mode() was replaced by inference_mode(mode=False) I get this error: UserWarning: The operator 'aten::tril.out' is not currently supported on the DML backend and will fall back to run on the CPU. This may have performance implications No GPU being used. Careful, inference might be very slow! 0%| | 0/100 [00:00<?, ?it/s]C:\Users\NoeXVanitasXJunk\bark\bark\model.py:80: UserWarning: The operator 'aten::tril.out' is not currently supported on the DML backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at D:\a\_work\1\s\pytorch-directml-plugin\torch_directml\csrc\dml\dml_cpu_fallback.cpp:17.) y = torch.nn.functional.scaled_dot_product_attention(q, k, v, dropout_p=self.dropout, is_causal=is_causal) 100%|| 100/100 [00:32<00:00, 3.04it/s] 0%| | 0/31 [00:00<?, ?it/s] Traceback (most recent call last): File ""C:\Users\NoeXVanitasXJunk\bark\run.py"", line 13, in <module> audio_array = generate_audio(text_prompt) File ""C:\Users\NoeXVanitasXJunk\bark\bark\api.py"", line 113, in generate_audio out = semantic_to_waveform( File ""C:\Users\NoeXVanitasXJunk\bark\bark\api.py"", line 54, in semantic_to_waveform coarse_tokens = generate_coarse( File ""C:\Users\NoeXVanitasXJunk\bark\bark\generation.py"", line 633, in generate_coarse x_in = torch.hstack( RuntimeError",|pytorch|gpu|amd-gpu|directml|,GPU Usage,3
76199563,"TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type uint8 of argument 'x'. I'm working on a GAN with generator and discriminator. @tf.function def train_step(input_image, target, step): with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape: gen_output = generator(input_image, training=True) disc_real_output = discriminator([input_image, target], training=True) disc_generated_output = discriminator([input_image, gen_output], training=True) gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target) disc_loss = discriminator_loss(disc_real_output, disc_generated_output) generator_gradients = gen_tape.gradient(gen_total_loss, generator.trainable_variables) discriminator_gradients = disc_tape.gradient(disc_loss, discriminator.trainable_variables) generator_optimizer.apply_gradients(zip(generator_gradients, generator.trainable_variables)) discriminator_optimizer.apply_gradients(zip(discriminator_gradients, discriminator.trainable_variables)) with summary_writer.as_default(): tf.summary.scalar('gen_total_loss', gen_total_loss, step=step//1000) tf.summary.scalar('gen_gan_loss', gen_gan_loss, step=step//1000) tf.summary.scalar('gen_l1_loss', gen_l1_loss, step=step//1000) tf.summary.scalar('disc_loss', disc_loss, step=step//1000) This function throws an error: TypeError: in user code: File ""/tmp/ipykernel_34/3224399777.py"", line 9, in train_step * gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target) File ""/tmp/ipykernel_34/3072633757.py"", line 5, in generator_loss * l1_loss = tf.reduce_mean(tf.abs(target - gen_output)) TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type uint8 of argument 'x'. But I try to subtract it manually, it works just fine, they are both float32 target - gen_output <tf.Tensor: shape=(1, 256, 256, 3), dtype=float32, numpy= array([[[[185.98402 , 151.92749 , 81.13361 ], [186.15788 , 151.78894 , 80.930176], [185.86765 , 151.81358 , 80.65687 ], ..., [183.64613 , 151.91382 , 87.36469 ], [183.17218 , 152.08833 , 86.43396 ], [183.51439 , 152.04149 , 87.40147 ]], ...",|python|tensorflow|typeerror|generative-adversarial-network|,API,4
76226936,"Keras custom Optimizer ValueError: Missing learning rate. I'm trying to create a custom optimizer using the Keras library in TensorFlow. I'm coding the optimizer from scratch. It's showing the following error: ValueError: Missing learning rate, please set self.learning_rate at optimizer creation time. In my optimizer's creation, I'm adding the self.learning_rate and still the problem persists. Here's my code for the init function of the optimizer: def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999, momentum=0.0, epsilon=None, decay=0.0, name='AdamSGD', **kwargs): super(AdamSGD, self).__init__(name=name, **kwargs) self.iterations = K.variable(0, dtype='int64', name='iterations') self.learning_rate = K.variable(lr, name='lr') self.beta_1 = K.variable(beta_1, name='beta_1') self.beta_2 = K.variable(beta_2, name='beta_2') self.momentum = K.variable(momentum, name='momentum') self.epsilon = epsilon or K.epsilon() self.decay = K.variable(decay, name='decay') This is how I'm compiling my model and where the error is being triggered: model.compile( optimizer=AdamSGD(lr=0.01), loss='categorical_crossentropy', metrics=['accuracy'] ) The entire error: 24 # Compile the model 25 model.compile( ---> 26 optimizer=AdamSGD(lr=0.01), 27 loss='categorical_crossentropy', 28 metrics=['accuracy'] 24 self.iterations = K.variable(0, dtype='int64', name='iterations') ---> 25 self.learning_rate = K.variable(lr, name='lr') 26 self.beta_1 = K.variable(beta_1, name='beta_1') 27 self.beta_2 = K.variable(beta_2, name='beta_2') 60 try: ---> 61 if getattr(self, name) is value: 62 # Short circuit for `self.$x = self.$x`. 63 return 330 def learning_rate(self): 331 if not hasattr(self, ""_learning_rate"") or self._learning_rate is None: --> 332 raise ValueError( 333 ""Missing learning rate, please set self.learning_rate at"" 334 "" optimizer creation time."" ValueError: Missing learning rate, please set self.learning_rate at optimizer creation time. If someone has any suggestions or knows where I'm going wrong, please let me know as I'm stuck on this error for quite some time now.",|python|tensorflow|keras|optimization|,Training,2
76338180,"RuntimeError: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions. So, I am using this clip model for some labelling task. But when I use the clip model's text encoder, it gives the following error: <ipython-input-117-4c513cc2d787> in forward(self, batch) 34 print(y.size()) 35 print(y.dim()) ---> 36 y = self.text_encoder(y) 37 y = self.classifier(y) 38 /usr/local/lib/python3.10/dist-packages/clip/model.py in encode_text(self, text) 345 x = x + self.positional_embedding.type(self.dtype) 346 x = x.permute(1, 0, 2) # NLD -> LND --> 347 x = self.transformer(x) 348 x = x.permute(1, 0, 2) # LND -> NLD 349 x = self.ln_final(x).type(self.dtype) RuntimeError: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 4 is not equal to len(dims) = 3 The thing is, the labels are multiple for one image, so I am using a collate_fn with pad_sequence in the dataloader before feeding into the model. def pad_sequence(batch): return torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=0) def my_collate_fn(batch): batch['i'] = torch.stack(batch['i'].float()) batch['l'] = pad_sequence(batch['l'].long()) return batch class CustomCLIP(torch.nn.Module): def __init__(self, num_classes: int = 10, bias=False): super().__init__() #model, _ = clip.load(""RN50"") def forward(self, batch): x = batch['i'] x = self.encoder(x) x = self.classifier(x) y = batch['l'] print(y) print(y.size()) print(y.dim()) y = self.text_encoder(y) #error on this line y = self.classifier(y) x_similarity = x @ x.T y_similarity = y @ y.T targets = F.softmax( (x_similarity + y_similarity) / 2 * temperature, dim=-1 ) outputs = (y @ x.T) / temperature return outputs, targets I have printed out the dimensions of y. its 3 which matches the length dimension. then why is it giving error that the input tensor dimension is 4? [[49406, 332, 49407, ..., 0, 0, 0], [49406, 320, 49407, ..., 0, 0, 0], [49406, 333, 49407, ..., 0, 0, 0], ..., [ 0, 0, 0, ..., 0, 0, 0], [ 0, 0, 0, ..., 0, 0, 0], [ 0, 0, 0, ..., 0, 0, 0]]], device='cuda:0') torch.Size([32, 392, 77]) 3 Someone please point out whats the issue and how to solve it. Thanks in advance.",|python|pytorch|tensor|clip|vision|,Tensors&Inputs,1
76369850,"Tensorflow could not detect my RTX 3050 notebook GPU. Tensorflow could not detect available GPU. I am working on image classification using below module and notebook (local) spec; Python 3.11.X Tensorflow 2.12.X CUDA 11.8 cuDNN 8.6 CPU: Ryzen 5 GPU: RTX 3050 notebook However Tensorflow could not see my available GPU, see below information; No GPU Available Screenshot print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU'))) # output Num GPUs Available: 0 I also already added environmental variables; Path Variable Screenshot Anyone have solution on this?",|python|tensorflow|gpu|,GPU Usage,3
76520092,"TF.MultiHeadAttention with 1D Data and Ghost Dimension. Background: My data has shape of (batch_size, data_length) and the dimensions seem to be incompatible with the inside MultiHeadAttention operations, especially softmax. Someone kindly suggested that I should use a size 1 ghost dimension as the last dimension. Error message I got: (32, 512, 1) Epoch 1/200 --------------------------------------------------------------------------- ValueError Traceback (most recent call last) <ipython-input-7-870abeaa4b93> in <cell line: 281>() 279 model.compile(loss=""mean_squared_error"", optimizer=""rmsprop"", metrics=[""accuracy""]) 280 --> 281 history = model.fit(dataset, epochs=200, validation_data=val_dataset) 1 frames /usr/local/lib/python3.10/dist-packages/keras/engine/training.py in tf__train_function(iterator) 13 try: 14 do_return = True ---> 15 retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope) 16 except: 17 do_return = False ValueError: in user code: File ""/usr/local/lib/python3.10/dist-packages/keras/engine/training.py"", line 1284, in train_function * return step_function(self, iterator) File ""/usr/local/lib/python3.10/dist-packages/keras/engine/training.py"", line 1268, in step_function ** outputs = model.distribute_strategy.run(run_step, args=(data,)) File ""/usr/local/lib/python3.10/dist-packages/keras/engine/training.py"", line 1249, in run_step ** outputs = model.train_step(data) File ""/usr/local/lib/python3.10/dist-packages/keras/engine/training.py"", line 1050, in train_step y_pred = self(x, training=True) File ""/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler raise e.with_traceback(filtered_tb) from None ValueError: Exception encountered when calling layer 'query' (type EinsumDense). Dimensions must be equal, but are 1 and 512 for '{{node model/multi_head_attention/query/einsum/Einsum}} = Einsum[N=2, T=DT_HALF, equation=""abc,cde->abde""](model/Cast, model/multi_head_attention/query/einsum/Einsum/Cast)' with input shapes: [32,512,1], [512,2,512]. Call arguments received by layer 'query' (type EinsumDense): ?inputs=tf.Tensor(shape=(32, 512, 1), dtype=float16)",|python|tensorflow|machine-learning|keras|transformer-model|,Tensors&Inputs,1
76511182,"Tensorflow custom learning rate scheduler gives unexpected EagerTensor type error. Below is my custom LR Scheduler that subclasses tensorflow.keras.optimizers.schedules.LearningRateSchedule, got error TypeError: Cannot convert -0.5 to EagerTensor of dtype int64. Really baffled as to why Eagertensor is relevant to a simple inverse square calculation for the return call of this custom class.. class lr_schedule(tensorflow.keras.optimizers.schedules.LearningRateSchedule): def __init__(self, dim_embed, warmup_steps): self.dim_embed = dim_embed self.warmup_steps = warmup_steps def __call__(self, step): return (self.dim_embed ** -0.5) * min((step ** -0.5), step * (self.warmup_steps ** -1.5)) Not specifically relevant to this error, but this is a custom LR Scheduler that replicates warmup scheduler that is used at 'Attention is All You Need' paper..",|python|tensorflow|machine-learning|deep-learning|transformer-model|,API,4
76604686,"PyTorch Distributed Run with SLURM results in ""Adress family not found"". When trying to run an example python file via torch.distributed.run on 2 Nodes with 2 GPUs each on a cluster by using a SLURM script I encounter the following error: [W socket.cpp:426] [c10d] The server socket cannot be initialized on [::]:16773 (errno: 97 - Address family not supported by protocol). [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [clara06.url.de]:16773 (errno: 97 - Address family not supported by protocol). This is the SLURM script: #!/bin/bash #SBATCH --job-name=distribution-test # name #SBATCH --nodes=2 # nodes #SBATCH --ntasks-per-node=1 # crucial - only 1 task per dist per node! #SBATCH --cpus-per-task=4 # number of cores per tasks #SBATCH --partition=clara #SBATCH --gres=gpu:v100:2 # number of gpus #SBATCH --time 0:15:00 # maximum execution time (HH:MM:SS) #SBATCH --output=%x-%j.out # output file name module load Python pip install --user -r requirements.txt MASTER_ADDR=$(scontrol show hostnames ""$SLURM_JOB_NODELIST"" | head -n 1) MASTER_PORT=$(expr 10000 + $(echo -n $SLURM_JOBID | tail -c 4)) GPUS_PER_NODE=2 LOGLEVEL=INFO python -m torch.distributed.run --rdzv_id=$SLURM_JOBID --rdzv_backend=c10d --rdzv_endpoint=$MASTER_ADDR\:$MASTER_PORT --nproc_per_node $GPUS_PER_NODE --nnodes $SLURM_NNODES torch-distributed-gpu-test.py and the python code that should be running: import fcntl import os import socket import torch import torch.distributed as dist def printflock(*msgs): """"""solves multi-process interleaved print problem"""""" with open(__file__, ""r"") as fh: fcntl.flock(fh, fcntl.LOCK_EX) try: print(*msgs) finally: fcntl.flock(fh, fcntl.LOCK_UN) local_rank = int(os.environ[""LOCAL_RANK""]) torch.cuda.set_device(local_rank) device = torch.device(""cuda"", local_rank) hostname = socket.gethostname() gpu = f""[{hostname}-{local_rank}]"" try: # test distributed dist.init_process_group(""nccl"") dist.all_reduce(torch.ones(1).to(device), op=dist.ReduceOp.SUM) dist.barrier() # test cuda is available and can allocate memory torch.cuda.is_available() torch.ones(1).cuda(local_rank) # global rank rank = dist.get_rank() world_size = dist.get_world_size() printflock(f""{gpu} is OK (global rank: {rank}/{world_size})"") dist.barrier() if rank == 0: printflock(f""pt={torch.__version__}, cuda={torch.version.cuda}, nccl={torch.cuda.nccl.version()}"") except Exception: printflock(f""{gpu} is broken"") raise I have tried different python runs like this: LOGLEVEL=INFO python -m torch.distributed.run --master_addr $MASTER_ADDR --master_port $MASTER_PORT --nproc_per_node $GPUS_PER_NODE --nnodes $SLURM_NNODES torch-distributed-gpu-test.py LOGLEVEL=INFO torchrun --rdzv_id=$SLURM_JOBID --rdzv_backend=c10d --rdzv_endpoint=$MASTER_ADDR\:$MASTER_PORT --nproc_per_node $GPUS_PER_NODE --nnodes $SLURM_NNODES torch-distributed-gpu-test.py LOGLEVEL=INFO python -m torch.distributed.launch --rdzv_id=$SLURM_JOBID --rdzv_backend=c10d --rdzv_endpoint=$MASTER_ADDR\:$MASTER_PORT --nproc_per_node $GPUS_PER_NODE --nnodes $SLURM_NNODES torch-distributed-gpu-test.py All resulting in the same error. I have tried specifing the IP Adress explicitly instead of the MASTER_ADDR IP_ADDRESS=$(srun hostname --ip-address | head -n 1) I have looked at ports that are open: everything above 1023 is open And inspected the /etc/resolv.conf: the hostnames are clearly mapped And pinged the nodes, which also succeeded. I have specified the ip version by appending .ipv4 to the MASTER_ADDR with no success.",|python|pytorch|artificial-intelligence|slurm|multi-gpu|,API,4
76807981,"Trained LSTM model receiving 0.00 on accuracy score. I wrote a model in order to classify text as ""hate speech"" or ""not hate speech"" as a fun side project, however, when my model was done training it scored 0.00 on accuracy which I can't seem to think of an explanation for. I had around 15,000 examples of non toxic language and 15,000 examples of toxic language. Not a massive dataset but I thought enough for a model that has a fair degree of effectiveness. Regardless here is my code that I used to train my model (for reference my labels NDArray has 30,000 entries with the first 15,000 being 1s and the last 15,000 being 0s as to line up with my array of samples.) import tensorflow as tf from clean import data from keras.models import Sequential from keras.layers import Embedding, LSTM, Dense from keras.preprocessing.text import Tokenizer from keras.preprocessing.sequence import pad_sequences from keras.utils import to_categorical texts, labels = data() tokenizer = Tokenizer() tokenizer.fit_on_texts(texts) sequences = tokenizer.texts_to_sequences(texts) vocab_size = len(tokenizer.word_index) + 1 max_seq_length = max(len(seq) for seq in sequences) print(max_seq_length) sequences = pad_sequences(sequences, maxlen=max_seq_length, padding='post') labels = to_categorical(labels) split_index = int(0.8 * len(sequences)) x_train, x_test = sequences[:split_index], sequences[split_index:] y_train, y_test = labels[:split_index], labels[split_index:] model = Sequential() model.add(Embedding(input_dim=vocab_size, output_dim=64, input_length=max_seq_length)) model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2)) model.add(Dense(2, activation='softmax')) model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.1) model.save(""model_save"") loss, accuracy = model.evaluate(x_test, y_test) print(""Test loss:"", loss) print(""Test accuracy:"", accuracy) Any advice or help would be appreciated thank you",|python|tensorflow|machine-learning|training-data|,Training,2
76886837,"with torch.no_grad() Changes Sequence Length During Evaluation Mode. I built a TransformerEncoder model, and it changes the output's sequence length if I use ""with torch.no_grad()"" during the evaluation mode. My model details: class TransEnc(nn.Module): def __init__(self,ntoken: int,encoder_embedding_dim: int,max_item_count: int,encoder_num_heads: int,encoder_hidden_dim: int,encoder_num_layers: int,padding_idx: int,dropout: float = 0.2): super().__init__() self.encoder_embedding = nn.Embedding(ntoken, encoder_embedding_dim, padding_idx=padding_idx) self.pos_encoder = PositionalEncoding(encoder_embedding_dim, max_item_count, dropout) encoder_layers = nn.TransformerEncoderLayer(encoder_embedding_dim, encoder_num_heads, encoder_hidden_dim, dropout, batch_first=True) self.transformer_encoder = nn.TransformerEncoder(encoder_layers, encoder_num_layers) self.encoder_embedding_dim = encoder_embedding_dim def forward(self,src: torch.Tensor,src_key_padding_mask: torch.Tensor = None) -> torch.Tensor: src = self.encoder_embedding(src.long()) * math.sqrt(self.encoder_embedding_dim) src = self.pos_encoder(src) src = self.transformer_encoder(src, src_key_padding_mask=src_key_padding_mask) with batch_size = 32 ntoken = 4096 encoder_embedding_dim = 256 max_item_count = 64 # max sequence length with padding encoder_num_heads = 8 encoder_hidden_dim = 256 encoder_num_layers = 4 padding_idx = 0 I have a tensor (src) containing 32 word-level tokenized sentences (with different paddings) with a shape of (32,64)(batch_size,max_item_count). When I activate training mode with ""model.train()"", set ""src_key_padding_mask = src == tokenizer.pad_token_id"" and run ""logits = model(src = src, src_key_padding_mask = src_key_padding_mask)"", I get logits with an expected shape of (32,64,256)(batch_size,max_item_count,encoder_embedding_dim). However, when I activate evaluation mode with ""model.eval()"", set ""src_key_padding_mask = src == tokenizer.pad_token_id"" and run with ""torch.no_grad(): logits = model(src = src, src_key_padding_mask = src_key_padding_mask)"", I get different logits' shapes every time like (32,31,256), (32,25,256), etc. I want to get logits with a shape of (32,64,256). How can I solve this problem? OS: Windows 10 x64 Python: 3.10.12 Torch: Tried on both 1.13.1+cu117 and 2.0.1+cu117, but the problem is still the same.",|python|deep-learning|pytorch|transformer-model|encoder-decoder|,API,4
77067838,"Keras Transformers - Dimensions must be equal. I wanted to do NER with keras model using transformers. The example was working correctly but I wanted to add some context to each words in order to help the model being more accurate. What I mean by context is ""coordinate X"", ""coordinate Y"", ""width of the word"", ""height of the word"", ""page index"", ... For example some informations are usually on the top right corner of a document so having the coordinate of the word might help (I'm new to ML so feel free to tell me I'm wrong if it's the case). In order to have this ""context"" I've transformed the x_train and x_val in this format: [ [ [pageIndex, wordVocabId, x, y, width, height, ocrScore], [pageIndex, wordVocabId, x, y, width, height, ocrScore], ... ], [ [pageIndex, wordVocabId, x, y, width, height, ocrScore], [pageIndex, wordVocabId, x, y, width, height, ocrScore], ... ], ... ] Where each array of 2nd level represent a document and each array of 3nd level represent a word with its context. The 3nd level array is a numpy array of numbers. Even if I tried to edit the model to make it working I don't think I went in the right direction so I'll post here the model from the example of keras that I try to use and that I would like to adapt to my usecase: class TransformerBlock(layers.Layer): def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1): super().__init__() self.att = keras.layers.MultiHeadAttention( num_heads=num_heads, key_dim=embed_dim ) self.ffn = keras.Sequential( [ keras.layers.Dense(ff_dim, activation=""relu""), keras.layers.Dense(embed_dim), ] ) self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6) self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6) self.dropout1 = keras.layers.Dropout(rate) self.dropout2 = keras.layers.Dropout(rate) def call(self, inputs, training=False): attn_output = self.att(inputs, inputs) attn_output = self.dropout1(attn_output, training=training) out1 = self.layernorm1(inputs + attn_output) ffn_output = self.ffn(out1) ffn_output = self.dropout2(ffn_output, training=training) return self.layernorm2(out1 + ffn_output) class TokenAndPositionEmbedding(layers.Layer): def __init__(self, maxlen, vocab_size, embed_dim): super().__init__() self.token_emb = keras.layers.Embedding( input_dim=vocab_size, output_dim=embed_dim ) self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim) def call(self, inputs): maxlen = tf.shape(inputs)[-1] positions = tf.range(start=0, limit=maxlen, delta=1) position_embeddings = self.pos_emb(positions) token_embeddings = self.token_emb(inputs) return token_embeddings + position_embeddings class NERModel(keras.Model): def __init__( self, num_tags, vocab_size, maxlen=128, embed_dim=32, num_heads=2, ff_dim=32 ): super().__init__() self.embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim) self.transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim) self.dropout1 = layers.Dropout(0.1) self.ff = layers.Dense(ff_dim, activation=""relu"") self.dropout2 = layers.Dropout(0.1) self.ff_final = layers.Dense(num_tags, activation=""softmax"") def call(self, inputs, training=False): x = self.embedding_layer(inputs) x = self.transformer_block(x) x = self.dropout1(x, training=training) x = self.ff(x) x = self.dropout2(x, training=training) x = self.ff_final(x) return x Source: https://keras.io/examples/nlp/ner_transformers/ I try to compile and fit this way: print(len(tag_mapping), vocab_size, len(x_train), len(y_train)) model = NERModel(len(tag_mapping), vocab_size, embed_dim=32, num_heads=4, ff_dim=64) model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy']) model.fit(tf.convert_to_tensor(x_train), tf.convert_to_tensor(y_train), validation_data=(x_val, y_val), epochs=10) model.save(""model.keras"") The result of the print is (I have only 3 tags for now because I first try to make the model working): 3 20000 1000 1000 The format of my y_train is the follow: [ [tagId_document1_word1, tagId_document1_word2, ...], [tagId_document2_Word1, tagId_document2_word1, ...] ] When I run model.fit I have this error: ValueError: Dimensions must be equal, but are 516 and 7 for '{{node Equal}} = Equal[T=DT_FLOAT, incompatible_shape_error=true](Cast_1, Cast_2)' with input shapes: [?,516], [?,516,7]. I hope with all these informations someone can pin me in the right direction because I'm a bit lost here. Thank you.",|python|keras|transformer-model|,Tensors&Inputs,1
77041713,"Parallelize DeepFace on multiple GPUs. I am trying to use the DeepFace python library to do face recognition and analysis on long videos: https://github.com/serengil/deepface. Using the library out of the box, I am able to get desired results by selecting frames from a video and then iterating through a for loop. Single GPU import decord import tensorflow as tf from deepface import DeepFace video_path = 'myvideopath' vr = decord.VideoReader(video_path) for i in range(0, 100, FRAME_STEP): image_bgr = vr[i].asnumpy()[:,:,::-1] results = DeepFace.find(img_path = image_bgr, **other_parameters) This works, but is too slow for the amount of video and frames that I need to go through. When running the model, I notice that it uses ~600 MB for prediction, so I should be able to run multiple instances on the same physical GPU. I am only using DeepFace for prediction and am not training or fine tuning any models. gpus = tf.config.list_physical_devices('GPU') for gpu in gpus: try: tf.config.set_logical_device_configuration(gpu, [tf.config.LogicalDeviceConfiguration(memory_limit=630)] * 12) except RuntimeError as e: # Virtual devices must be set before GPUs have been initialized print(e) logical_gpus = tf.config.list_logical_devices('GPU') print(len(gpus), ""Physical GPU,"", len(logical_gpus), ""Logical GPUs"") 2 Physical GPU, 24 Logical GPUs I would like to be able to parallelize the DeepFace.find and DeepFace.analyze functions. The first thing that I tried to do was to have a queue of free gpu devices and use concurrent.futures.ThreadPoolExecutor. def multigpu_helper(index, device_name, image_bgr, fn, fn_dict, q): print(f'{index:5} {device_name}') start_timer = timeit.default_timer() with tf.device(device_name): results = fn(img_path=image_bgr, **fn_dict) q.put(device_name) end_timer = timeit.default_timer() print(f'MultiGPU Time: {end_timer-start_timer} sec.') return results def multigpu_process(iterable, vr, fn, fn_dict): logical_devices = tf.config.list_logical_devices(device_type='GPU') print(logical_devices) q = queue.Queue() for logical_device in logical_devices: q.put(logical_device.name) results_dict = dict() item_list = list(iterable) with concurrent.futures.ThreadPoolExecutor(max_workers=len(logical_devices)) as pool: future_jobs = dict() while item_list: device_name = q.get() index = item_list.pop(0) image_bgr = vr[index].asnumpy()[:, :, ::-1] future_jobs[pool.submit(multigpu_helper, index, device_name, image_bgr, fn, fn_dict, q)] = index for future in concurrent.futures.as_completed(future_jobs): index = future_jobs.get(future) results = future.result() results_dict[index] = results return results_dict I am able to get the code to execute and output results, but it is no faster than doing it in a single for loop on a single GPU. [LogicalDevice(name='/device:GPU:0', device_type='GPU'), LogicalDevice(name='/device:GPU:1', device_type='GPU'), LogicalDevice(name='/device:GPU:2', device_type='GPU'), LogicalDevice(name='/device:GPU:3', device_type='GPU'), LogicalDevice(name='/device:GPU:4', device_type='GPU'), LogicalDevice(name='/device:GPU:5', device_type='GPU'), LogicalDevice(name='/device:GPU:6', device_type='GPU'), LogicalDevice(name='/device:GPU:7', device_type='GPU'), LogicalDevice(name='/device:GPU:8', device_type='GPU'), LogicalDevice(name='/device:GPU:9', device_type='GPU'), LogicalDevice(name='/device:GPU:10', device_type='GPU'), LogicalDevice(name='/device:GPU:11', device_type='GPU'), LogicalDevice(name='/device:GPU:12', device_type='GPU'), LogicalDevice(name='/device:GPU:13', device_type='GPU'), LogicalDevice(name='/device:GPU:14', device_type='GPU'), LogicalDevice(name='/device:GPU:15', device_type='GPU'), LogicalDevice(name='/device:GPU:16', device_type='GPU'), LogicalDevice(name='/device:GPU:17', device_type='GPU'), LogicalDevice(name='/device:GPU:18', device_type='GPU'), LogicalDevice(name='/device:GPU:19', device_type='GPU'), LogicalDevice(name='/device:GPU:20', device_type='GPU'), LogicalDevice(name='/device:GPU:21', device_type='GPU'), LogicalDevice(name='/device:GPU:22', device_type='GPU'), LogicalDevice(name='/device:GPU:23', device_type='GPU')] 0 /device:GPU:0 30 /device:GPU:1 60 /device:GPU:2 90 /device:GPU:3 120 /device:GPU:4 150 /device:GPU:5 180 /device:GPU:6 210 /device:GPU:7 240 /device:GPU:8 270 /device:GPU:9 300 /device:GPU:10 330 /device:GPU:11 360 /device:GPU:12 390 /device:GPU:13 420 /device:GPU:14 450 /device:GPU:15 480 /device:GPU:16 510 /device:GPU:17 540 /device:GPU:18 570 /device:GPU:19 600 /device:GPU:20 630 /device:GPU:21 660 /device:GPU:22 690 /device:GPU:23 MultiGPU Time: 16.968208671023604 sec. 720 /device:GPU:2 MultiGPU Time: 17.829027735977434 sec. 750 /device:GPU:1 MultiGPU Time: 17.852755011990666 sec. 780 /device:GPU:8 MultiGPU Time: 19.71368485200219 sec.MultiGPU Time: 19.543589979992248 sec. MultiGPU Time: 19.8676836140221 sec. 810 /device:GPU:4 MultiGPU Time: 19.85990399698494 sec. 840 /device:GPU:11 870 /device:GPU:0 MultiGPU Time: 20.076353634009138 sec. 900 /device:GPU:6 930 /device:GPU:3 MultiGPU Time: 20.145404886978213 sec. MultiGPU Time: 20.27192261395976 sec. 960 /device:GPU:9 990 /device:GPU:7 MultiGPU Time: 20.459441539016552 sec. MultiGPU Time: 20.418532160052564 sec. MultiGPU Time: 20.581610807043035 sec. MultiGPU Time: 20.545571406022646 sec. MultiGPU Time: 20.832303048984613 sec. MultiGPU Time: 20.97456920897821 sec. MultiGPU Time: 20.994418176996987 sec. MultiGPU Time: 21.35945221298607 sec. MultiGPU Time: 21.50979186099721 sec. MultiGPU Time: 21.405662977020256 sec. MultiGPU Time: 21.542257393943146 sec. MultiGPU Time: 22.063301149988547 sec. MultiGPU Time: 21.665760322008282 sec. MultiGPU Time: 22.105394209967926 sec. MultiGPU Time: 6.661869053030387 sec. MultiGPU Time: 9.814038792042993 sec. MultiGPU Time: 7.658941667003091 sec. MultiGPU Time: 8.546573753003031 sec. MultiGPU Time: 10.831304075953085 sec. MultiGPU Time: 9.250181486015208 sec. MultiGPU Time: 8.87483947101282 sec. MultiGPU Time: 12.432360459002666 sec. MultiGPU Time: 9.511910478991922 sec. MultiGPU Time: 9.66243519296404 sec. Face Recognition MultiGPU Total Time: 29.63435428502271 sec. In fact, a single GPU iteration of the DeepFace.find function in a for loop should take about 0.5 sec. It seems that the multithreading is causing all the threads to finish around their cumulative time which is slower and undesired. I tried a second time without using a queue, just splitting the input indices into separate lists and then processing separately. def cycle_baskets(items: List[Any], maxbaskets: int) -> List[List[Any]]: baskets = [[] for _ in range(min(maxbaskets, len(items)))] for item, basket in zip(items, cycle(baskets)): basket.append(item) return baskets def multigpu_helper_split(device_name, item_list, video_path, fn, fn_dict): print(device_name) start_timer = timeit.default_timer() results_dict = dict() vr = decord.VideoReader(str(video_path)) with tf.device(device_name): for index in item_list: start_index_timer = timeit.default_timer() image_bgr = vr[index].asnumpy()[:, :, ::-1] results_dict[index] = fn(img_path=image_bgr, **fn_dict) end_index_timer = timeit.default_timer() print(f'Device {device_name} Index {index:5} {end_index_timer - start_index_timer} sec.') end_timer = timeit.default_timer() print(f'MultiGPU Time: {end_timer - start_timer} sec.') return results_dict def multigpu_process_split(iterable, video_path, fn, fn_dict): logical_devices = [device.name for device in tf.config.list_logical_devices(device_type='GPU')] print(logical_devices) results_dict = dict() item_lists = cycle_baskets(list(iterable), len(logical_devices)) with concurrent.futures.ThreadPoolExecutor(max_workers=len(logical_devices)) as pool: future_jobs = {pool.submit(multigpu_helper_split, logical_devices[i], item_lists[i], video_path, fn, fn_dict) for i in range(len(logical_devices))} for future in concurrent.futures.as_completed(future_jobs): results_dict.update(future.result()) return results_dict This is also considerably slower and also caused the kernal to crash. Device /device:GPU:18 Index 540 305.03293917299015 sec. MultiGPU Time: 311.7356750360341 sec. Device /device:GPU:22 Index 660 305.6161605300149 sec. MultiGPU Time: 312.3281374910148 sec. Device /device:GPU:5 Index 150 309.5672924729879 sec. Device /device:GPU:13 Index 390 311.9252848789911 sec. MultiGPU Time: 318.34215058299014 sec. Device /device:GPU:0 Index 0 312.96517166896956 sec. Device /device:GPU:3 Index 90 312.41818467900157 sec. Device /device:GPU:4 Index 120 312.507540087041 sec. Device /device:GPU:10 Index 300 312.49839297297876 sec. MultiGPU Time: 319.4717267890228 sec. Device /device:GPU:23 Index 690 313.53694368101424 sec. MultiGPU Time: 320.6566755659878 sec. I realize that with tf.device(device_name): is over the entire DeepFace function. Looking at the DeepFace source code, it looks like there is quite a lot more than tensorflow and what I really would want to parallelize is model.predict(). DeepFace.py def represent(): ... # represent if ""keras"" in str(type(model)): # new tf versions show progress bar and it is annoying embedding = model.predict(img, verbose=0)[0].tolist() else: # SFace and Dlib are not keras models and no verbose arguments embedding = model.predict(img)[0].tolist() How would I be able to parallelize the DeepFace.find and DeepFace.analyze functions to run on 24 logical GPUs that I have? I would like to be able to get a x24 speedup for processing the selected frames. It would be much preferred if I could wrap something around the DeepFace functions themselves, but if that is not possible, then I could try to parallelize the source code of the DeepFace library.",|tensorflow|gpu|tensorflow2.0|ray|deepface|,GPU Usage,3
77209413,"TensorFlow Custom Loss Function Error: Node: 'gradient_tape/contrastive_loss/mul/BroadcastGradientArgs' Incompatible shapes [0,1] vs. [32,1]. I'm working on a Siamese network using TensorFlow and Keras. As the data is huge I am also trying to use the generator for batch loading. I have a custom contrastive loss function that I'm using for training, but I'm facing an error during the gradient calculation. Error: Epoch 1/10 y_true shape: (None, 1) y_pred shape: (None, 1) loss: Tensor(""contrastive_loss/Mean:0"", shape=(), dtype=float32) y_true shape: (None, 1) y_pred shape: (None, 1) loss: Tensor(""contrastive_loss/Mean:0"", shape=(), dtype=float32) 1/1125 [..............................] - ETA: 7:56:52 - loss: 0.1281 - accuracy: 0.0625 --------------------------------------------------------------------------- InvalidArgumentError Traceback (most recent call last) <ipython-input-17-d373b6e0f1af> in <cell line: 11>() 9 #val_gen = data_generator(val_pairs, val_labels, batch_size, os.path.join(extract_path, 'train')) 10 ---> 11 model.fit( 12 train_gen, 13 # validation_data=val_gen, 1 frames /usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name) 51 try: 52 ctx.ensure_initialized() ---> 53 tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name, 54 inputs, attrs, num_outputs) 55 except core._NotOkStatusException as e: InvalidArgumentError: Graph execution error: Detected at node 'gradient_tape/contrastive_loss/mul_1/BroadcastGradientArgs' defined at (most recent call last): File ""/usr/lib/python3.10/runpy.py"", line 196, in _run_module_as_main return _run_code(code, main_globals, None, ------ ------""Error messages condensed"" ------ File ""/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py"", line 276, in compute_gradients grads = tape.gradient(loss, var_list) Node: 'gradient_tape/contrastive_loss/mul/BroadcastGradientArgs' Incompatible shapes: [0,1] vs. [32,1] [[{{node gradient_tape/contrastive_loss/mul/BroadcastGradientArgs}}]] [Op:__inference_train_function_11308] Data Generator: def data_generator(pairs, labels, batch_size, img_dir): """""" Generate batches of images and labels for training/validation. Parameters: - pairs: List of tuples containing left image id, list of candidate right image ids, and index of ground truth right image. - batch_size: Number of pairs to load in each batch. - img_dir: Directory containing the images. Yields: Batch of images and labels. """""" num_samples = len(pairs) while True: # Shuffle pairs for randomness in each epoch # np.random.shuffle(pairs) # Create a list of sequence indices indices = np.arange(num_samples) # Shuffle the indices np.random.shuffle(indices) # Use the shuffled indices to shuffle the sequences pairs = np.array(pairs) pairs = pairs[indices] pairs = pairs.tolist() labels = np.array(labels) labels = labels[indices] labels = labels.tolist() for start_idx in range(0, num_samples, batch_size): end_idx = min(start_idx + batch_size, num_samples) batch_pairs = pairs[start_idx:end_idx] labels = labels[start_idx:end_idx] left_images = [] right_images = [] # labels = [] for pair in batch_pairs: left_img_id, right_img_id = pair # Load left image left_img = load_and_preprocess_image(left_img_id, img_dir, 'left') left_images.append(left_img) # Load right images right_img = load_and_preprocess_image(right_img_id, img_dir, 'right') right_images.append(right_img) # Convert lists to numpy arrays left_images = np.array(left_images) right_images = np.array(right_images) labels = np.array(labels) yield [left_images, right_images], labels Model Training Code: batch_size = 32 train_gen = data_generator(train_pairs, train_labels, batch_size, os.path.join(extract_path, 'train')) val_gen = data_generator(val_pairs, val_labels, batch_size, os.path.join(extract_path, 'train')) model.compile(optimizer='rmsprop', loss=contrastive_loss) model.fit( train_gen, validation_data=val_gen, steps_per_epoch=len(train_pairs) // batch_size, validation_steps=len(val_pairs) // batch_size, epochs=10 ) I've verified that both y_true and y_pred have the shape (None, 1) during the forward pass as it can be seen from my print messages output in the above error. I'm unsure why I'm seeing a shape of [32,1] in the error. Does anyone have any ideas on what might be causing this or how to resolve it? I have tried using print messages in the loss function to determine the shape, double checked the shapes generated by my generator function which seems right. I expected to find the shape mismatch for y_true and y_pred but it was same. This puzzled me. I looked for solutions throughout internet but couldn't resolve it. UPDATE: Issue fixed. The issue was with generator method where i was updating the labels internally because of which in the second iteration there were no labels returned and thus the shape mismatch error. The line labels = labels[start_idx:end_idx] should be changed to batch_labels = labels[start_idx:end_idx]",|python|tensorflow|keras|loss-function|siamese-network|,Tensors&Inputs,1
77248509,"RlLib PPO training doesn't use GPU. I use PPO algorithm in Rllib to train my deep reinforcement learning model. Training is on an AWS p2.xlarge instance which has 4 vCPU and 1 GPU(Tesla K80). And I found PPO doesn't use the GPU. The training log shows: Trial status: 1 RUNNING Current time: 2023-10-07 05:08:00. Total running time: 13s Logical resource usage: 3.0/4 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:K80) ??Trial name status ???PPO_CartPole-v1_74ca6_00000 RUNNING ??This is my code: from ray import tune from ray.rllib.algorithms.ppo import PPO from ray.rllib.algorithms.a2c import A2C from ray.rllib.algorithms.appo import APPO def train() -> None: # config training parameters train_config = { ""env"": ""CartPole-v1"", # MyCustomEnv_v0, ""framework"": ""torch"", ""num_workers"": 2, ""num_gpus"": 1, # Add this line to specify using one GPU ""num_envs_per_worker"": 1, ""model"": { ""fcnet_hiddens"": [512, 512, 256], ""fcnet_activation"": ""relu"", }, ""lr"": 3e-4, ""optimization"": { ""optimizer"": ""adam"", ""adam_epsilon"": 1e-8, ""adam_beta1"": 0.9, ""adam_beta2"": 0.999, }, ""gamma"": 0.99, ""num_sgd_iter"": 10, ""sgd_minibatch_size"": 500, ""rollout_fragment_length"": 500, ""train_batch_size"": 4000, ""prioritized_replay"": True, ""prioritized_replay_alpha"": 0.6, ""prioritized_replay_beta"": 0.4, ""buffer_size"": 500000, ""stop"": {""episodes_total"": 5000000}, ""exploration_config"": {}, } stop_criteria = {""training_iteration"": training_iteration} # start to train try: results = tune.run( PPO, # PPO, config=train_config, stop=stop_criteria, checkpoint_at_end=True, checkpoint_freq=50, restore=model_restore_dir, verbose=1, ) except BaseException as e: print(f""training error: {e}"") if __name__ == ""__main__"": train() Firstly, I trained for my custom environment ""MyCustomEnv_v0"", PPO doesn't use GPU. And I tried ""CartPole-v1"", it still doesn't use GPU. After I changed algorithm from PPO to APPO, it starts to use GPU, A2C is ok too(I changed nothing else), Like this: Current time: 2023-10-07 05:07:01. Total running time: 0s Logical resource usage: 3.0/4 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:K80) ??Trial name status ???APPO_CartPole-v1_59115_00000 PENDING ??I checked Rllib official document, and confirm PPO support GPU training. Why does this happen? How to use GPU in RlLib PPO training?",|deep-learning|pytorch|gpu|rllib|,GPU Usage,3
77400236,"Converting from list of very small floats to a PyTorch tensor gives a TypeError because PyTorch interprets scientific notation as str instead of float. I have data that I want to setup as a PyTorch tensor as my objective for a ML algorithm. If I inspect the list as is I get : ydata = [1.39e-11, 1.5e-12, 4.7e-16] I want to have this list in a tensor, so I tried multiple functions but everytime I get a TypeError for instance : torch.FloatTensor(ydata) -> TypeError('must be real number, not str') However if I inspect the type of each element of ydata it is a float. How can I go about getting the data I want into a tensor? Thank you very much.",|python|python-3.x|pytorch|tensor|,API,4
77656160,"Pytorch, random number generators and devices. I always put on top of my Pytorch's notebooks a cell like this: device = ( ""cuda"" if torch.cuda.is_available() else ""mps"" if torch.backends.mps.is_available() else ""cpu"" ) torch.set_default_device(device) In this convenient way, I can use the GPU, if the system has one, MPS on a Mac, or the cpu on a vanilla system. EDIT: Please note that, due to torch.set_default_device(device), any tensor is created, by default, on the device, e.g.: Now, I'm trying to use a Pytorch generator: g = torch.Generator(device=device).manual_seed(1) and then: A = torch.randn((3, 2), generator=g) No problem whatsoever on my Macbook (where the device is MPS) or on systems with cpu only. But on my Cuda-enabled desktop, I get: RuntimeError: Expected a 'cpu' device type for generator but found 'cuda' Any solution? If I just abstain from specifying the device for the generator, it will use the cpu, but then the tensor A will be created on the CPU too...",|python|pytorch|gpu|,GPU Usage,3
77658529,"Why does saving and reloading a 1D convolutional keras model cause it to fail to generalize to wider windows?. So I'm working with the example notebook that tensorflow provides to detail working with time-series formatted data. https://www.tensorflow.org/tutorials/structured_data/time_series Everything is going fine, just had a quick question on saving and loading models. For my current research, I need to be able to be able to train a model, save it, and then be able to reload it for testing at a later time. The entire code for the notebook can be found at the link above, but essentially the training and compiling process involves the following method, where a model and window shape are fed into MAX_EPOCHS = 20 def compile_and_fit(model, window, patience=2): early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, mode='min') model.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=tf.keras.optimizers.Adam(), metrics=[tf.keras.metrics.MeanAbsoluteError()]) history = model.fit(window.train, epochs=MAX_EPOCHS, validation_data=window.val, callbacks=[early_stopping]) return history The model in question looks like conv_model = tf.keras.Sequential([ tf.keras.layers.Conv1D(filters=32, kernel_size=(CONV_WIDTH,), activation='relu'), tf.keras.layers.Dense(units=32, activation='relu'), tf.keras.layers.Dense(units=1), ]) In the notebook, this is essentially the process that runs the training/compiling method and tests to see if it evaluates history = compile_and_fit(conv_model, conv_window) IPython.display.clear_output() val_performance['Conv'] = conv_model.evaluate(conv_window.val) performance['Conv'] = conv_model.evaluate(conv_window.test, verbose=0) After this it is tested on a wider window in the following procedure wide_window = WindowGenerator( input_width=24, label_width=24, shift=1, label_columns=['T (degC)']) print(""Wide window"") print('Input shape:', wide_window.example[0].shape) print('Labels shape:', wide_window.example[1].shape) print('Output shape:', conv_model(wide_window.example[0]).shape) This part works fine, but if I add in two lines to save and reload the model as shown history = compile_and_fit(conv_model, conv_window) conv_model.save('test.keras') conv_model = tf.keras.models.load_model('test.keras') IPython.display.clear_output() val_performance['Conv'] = conv_model.evaluate(conv_window.val) performance['Conv'] = conv_model.evaluate(conv_window.test, verbose=0) and then run print(""Wide window"") print('Input shape:', wide_window.example[0].shape) print('Labels shape:', wide_window.example[1].shape) print('Output shape:', conv_model(wide_window.example[0]).shape) I receive the following error. Wide window Input shape: (32, 24, 19) Labels shape: (32, 24, 1) --------------------------------------------------------------------------- ValueError Traceback (most recent call last) Cell In[58], line 4 2 print('Input shape:', wide_window.example[0].shape) 3 print('Labels shape:', wide_window.example[1].shape) ----> 4 print('Output shape:', conv_model(wide_window.example[0]).shape) File ~/py38-env/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs) 67 filtered_tb = _process_traceback_frames(e.__traceback__) 68 # To get the full stack trace, call: 69 # `tf.debugging.disable_traceback_filtering()` ---> 70 raise e.with_traceback(filtered_tb) from None 71 finally: 72 del filtered_tb File ~/py38-env/lib/python3.8/site-packages/keras/src/engine/input_spec.py:298, in assert_input_compatibility(input_spec, inputs, layer_name) 296 if spec_dim is not None and dim is not None: 297 if spec_dim != dim: --> 298 raise ValueError( 299 f'Input {input_index} of layer ""{layer_name}"" is ' 300 ""incompatible with the layer: "" 301 f""expected shape={spec.shape}, "" 302 f""found shape={display_shape(x.shape)}"" 303 ) ValueError: Input 0 of layer ""sequential_3"" is incompatible with the layer: expected shape=(None, 3, 19), found shape=(32, 24, 19) This occurs when I save and reload using the .h5 file format as well. Even if I change the name and try again it still throws an error. Note that the window size it is trained on is CONV_WIDTH = 3 conv_window = WindowGenerator( input_width=CONV_WIDTH, label_width=1, shift=1, label_columns=['T (degC)']) but it should generalize to wider windows, and indeed does when the model is not saved and reloaded. Any insight into why this is occurring would be greatly appreciated, thanks!",|tensorflow|keras|model|save|,Tensors&Inputs,1
77772327,"TypeError when trying to display transformed images PyTorch. I have some trouble defining the transforms for images using PyTorch. Here you are the transforms I need: mean = [0.485, 0.456, 0.406] std = [0.229, 0.224, 0.225] train_transform = transforms.Compose([ transforms.RandomHorizontalFlip(p=0.5), transforms.Resize((256, 256), interpolation=torchvision.transforms.InterpolationMode.BILINEAR), transforms.CenterCrop(size=[224,224]), transforms.Normalize(mean, std), transforms.PILToTensor() ]) test_transform = transforms.Compose([ transforms.Resize((256, 256), interpolation=torchvision.transforms.InterpolationMode.BILINEAR), transforms.CenterCrop(size=[224,224]), transforms.Normalize(mean, std), transforms.PILToTensor(), ]) Then, I create the loaders for the images of the dataset: BATCH_SIZE = 32 trainset = torchvision.datasets.ImageFolder(root='CVPR2023_project_2_and_3_data/train/', loader=open_image) trainset_classes = trainset.classes.copy() subset_size = int(0.15*len(trainset)) validset = torchvision.datasets.ImageFolder(root='CVPR2023_project_2_and_3_data/train/', loader=open_image) indices = torch.randperm(len(trainset)) valid_indices = indices[:subset_size] train_indices = indices[subset_size:] trainset = Subset(trainset, train_indices) validset = Subset(validset, valid_indices) # Apply transformations only to the training set trainset.dataset.transform = train_transform # Apply transformations to the validation set validset.dataset.transform = test_transform trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True) # batch size of 1 because we have to crop in order to get all images to same size (64x64), also see pin_memory optin validloader = torch.utils.data.DataLoader(validset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True) testset = torchvision.datasets.ImageFolder(root='CVPR2023_project_2_and_3_data/test/', transform=test_transform, loader=Image.open) testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True) print(f'entire train folder: {len(trainset)}, entire test folder: {len(testset)}, splitted trainset: {len(trainset)}, splitted validset: {len(validset)}') Then I load a pre-trained network and freeze all the layers but the last one: model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True) model.classifier[6] = torch.nn.Linear(in_features=4096, out_features=15, bias=True) #adapting to 15 classes for param in model.parameters(): param.requires_grad = False for param in model.classifier[6].parameters(): param.requires_grad = True Then, I define a function for showing an image and I try to print one: def imshow(img): npimg = img.numpy() plt.axis(""off"") plt.imshow(np.transpose(npimg, (1, 2, 0))) images, labels = next(iter(trainloader)) # <-- error print(images[0]) This last piece of code does not work and the program crashes with the following error: img should be Tensor Image. Got <class 'PIL.Image.Image'> I have already tried to change the transforms' order but I get the inverse error, i.e. img should be <class 'PIL.Image.Image'> Image. Got Tensor Can anyone explain how I should solve this error? Thank you in advance for your patience",|python|pytorch|pre-trained-model|,API,4
78008233,"Why KL divergence is negative in Pytorch?. I'm trying to get the KL divergence between 2 distributions using Pytorch, but the output is often negative which shouldn't be the case: import torch import torch.nn.functional as F x_axis_kl_div_values = [] for epoch in range(200): # each epoch generates 2 different distributions input_1 = torch.empty(10).normal_(mean=torch.randint(1,50,(1,)).item(),std=0.5).unsqueeze(0) input_2 = torch.empty(10).normal_(mean=torch.randint(1,50,(1,)).item(),std=0.5).unsqueeze(0) kl_divergence = F.kl_div(input_1.log(), input_2, reduction='batchmean') x_axis_kl_div_values.append(kl_divergence.item()) x_axis_kl_div_values >>> [324.4713134765625, -69.10758972167969, -92.42606353759766, From the Pytorch forum I found this that mentions that their issue was that the inputs were not proper distributions, which is not the case in my code as I'm creating a normal distribution. From this SO thread it seems like their issue was that the nn.KLDivLoss expects the input to be log-probabiltie, but again, I did that in my code. So I'm not sure what I'm missing",|machine-learning|pytorch|loss-function|,Training,2
78190915,"Error when using keras: module 'keras.layers' has no attribute 'TextVectorization'. import os os.environ[""KERAS_BACKEND""] = ""tensorflow"" import pandas as pd import pathlib import random import string import re import numpy as np import tensorflow as tf import keras from keras import layers MAX_SEQUENCE_LENGTH = 40 INP_VOCAB_SIZE = 15000 input_vectorization = keras.layers.TextVectorization( max_tokens=INP_VOCAB_SIZE, output_mode=""int"", output_sequence_length=MAX_SEQUENCE_LENGTH ) AttributeError: module 'keras.layers' has no attribute 'TextVectorization' I import the keras module and get layers from keras, but when I try to use keras.layers.TextVectorization, my code throws that error. I've tried writing the call differently but then I get a new AttributeError. input_vectorization = tf.keras.TextVectorization( #same inside as before ) AttributeError: module 'keras' has no attribute '__version__' I've also tried reinstalling both tensorflow and keras using pip, but that didn't change anything. What should I do? Keras version: 3.1.1 Tensorflow version: 2.16.1",|python|tensorflow|keras|keras-layer|,API,4
